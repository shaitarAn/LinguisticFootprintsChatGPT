center has to be cooled . And it has to be secure from intrusion , whether over the Internet or physically by someone driving through the front door . Well , with multi-core processors , it is n't a million pizza boxes that get you to a million servers . But it is still a huge number , easily in the range of a small city in terms of the power demand that they have . Such a facility is often deployed in a part of the country where electricity is relatively inexpensive , and there is a surplus . No , and you usually do n't . It is crucial to have some diversity in the connection that you have to the utility company . If someone digs up a power line , you do n't want an entire million-processor data center to go offline . Also , you have to be careful that it saves whatever it is doing at the time . So , built into the data center are backup power supplies , typically rooms full of batteries that decouple the operation from the utility company , giving them enough time to run for a few minutes before shutting down . Typically , there are diesel generators , but they are there for support for a few hours while , first of all , everything is rationally backed up , and second , the data center works with the utility company toward the restoration of power . Things will go down ; there 's no question about that . In an earthquake or other natural disaster , what are you going to do ? If you 're Google or Microsoft or Amazon , you do n't have one million-computer centers , you have many of them spread around the world . And you have the software architecture that gracefully redirects the work to the other data centers in the event that one of your data centers has such a problem . PUE is the ratio of total energy consumed by the data center - cooling , air-conditioning and electricity distribution - compared with the total energy consumed by the IT equipment alone . Typically , leading-edge centers are running at PUEs of 1.2 or 1.25 , which means the electrical overhead is only 25 percent on top of the IT equipment . Five years ago , the PUE was more like 4 or 5 , an overhead of 400 percent . We 've made tremendous advances in reducing the relative energy consumption of these support functions . The focus now is on reducing the footprint of the servers themselves - by far the most expensive component of running a data center . That 's one of the ideas from our ( LoCal energy ) project . Because of the size , scale and timeliness of information , it 's hard to drive your million computers to very high levels of use . They are sitting there , powered on , but only being used maybe 30 percent of the time . " Doing nothing well " means , if you have nothing to do , at least do it with very little power being consumed . Unfortunately , that 's not the way computer servers work today . They do n't really go to sleep . Our mobile phones and our laptop computers are much better at doing nothing well . 1.5 percent sounds like a big number , but I believe it is comparable to the world 's airline travel . And I challenge you , how many hours a month do you spend on an airplane , and how many hours do you spend online , with all that data center