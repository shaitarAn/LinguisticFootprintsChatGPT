sys_sd_paragraph^4 / (sample2_n_para^2 * (sample2_n_para - 1)))
# Calculate p-value using t-distribution
p_value_para <- 2 * pt(abs(t_statistic_para), df_para, lower.tail = FALSE)
# Add the results to the dataframe
t_test_results_df <<- rbind(t_test_results_df, data.frame(Language = lang_df$lang[1],
System = sys,
Level = "Paragraph",
t_statistic = t_statistic_para,
p_value = p_value_para,
stringsAsFactors = FALSE))
}
# Check if there is data available for the system at sentence level
if (any(sentence_data$system == sys)) {
# calculate the t-test at sentence level
sys_mean_sentence <- sentence_data[sentence_data$system == sys, ]$xwr_mean
sys_sd_sentence <- sentence_data[sentence_data$system == sys, ]$xwr_std
sample2_n_sent <- sentence_data[sentence_data$system == sys, ]$xwr_observation
# Calculate t-test at sentence level
se_diff <- sqrt(((human_sd_paragraph^2)/ sample1_n) + ((sys_sd_sentence^2)/ sample2_n_sent))
t_statistic_sent <- abs(human_mean_paragraph - sys_mean_sentence) / se_diff
# get degrees of freedom using Welch's correction
df_sent <- (human_sd_paragraph^2 / sample1_n + sys_sd_sentence^2 / sample2_n_sent)^2 /
(human_sd_paragraph^4 / (sample1_n^2 * (sample1_n - 1)) +
sys_sd_sentence^4 / (sample2_n_sent^2 * (sample2_n_sent - 1)))
# calculate p-value
p_value_sent <- 2 * pt(abs(t_statistic_sent), df_sent, lower.tail = FALSE)
# Add the results to the dataframe
t_test_results_df <<- rbind(t_test_results_df, data.frame(Language = lang_df$lang[1],
System = sys,
Level = "Sentence",
t_statistic = t_statistic_sent,
p_value = p_value_sent,
stringsAsFactors = FALSE))
}
}
}
# Iterate over each language dataframe in the list_df
for (lang_df in list_df) {
calculate_cohens_d(lang_df)
}
# Print the combined results dataframe
print(results_df)
# Save the results dataframe to a CSV file
write.csv(results_df, file = "../results/cohen_d_effect_size.csv", row.names = FALSE)
# Plot the effect size results
ggplot(results_df, aes(x = System, y = Cohen_d, fill = Level)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~Language, scales = "free") +
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0.5)) +
labs(title = "Cohen's d Effect Size with Hedge's g Correction for Systems' XWR as Compared to Human Values",
x = NULL,
y = NULL,
fill = "Level") +
scale_fill_manual(values = c("Paragraph" = "lightblue", "Sentence" = "gray")) +
theme(legend.position = "bottom") +
theme(legend.title = element_blank()) +
theme(legend.text = element_text(size = 12)) +
theme(axis.text.x = element_text(size = 8)) +
theme(axis.text.y = element_text(size = 8)) +
theme(plot.title = element_text(size = 14)) +
theme(strip.text = element_text(size = 10)) +  # Bold and size 12 subplot titles
theme(legend.key.size = unit(0.5, "cm")) +
theme(legend.key = element_rect(fill = "white", colour = "white")) +
theme(legend.background = element_rect(fill = "white", colour = "white"))
# Save the plot to a file
ggsave("../viz/cohen_d_effect_size.pdf", width = 12, height = 8, units = "in")
# Perform t-test for each language
for (lang_df in list_df) {
calculate_t_test(lang_df)
}
# Print the combined results dataframe
print(t_test_results_df)
# Save the results dataframe to a CSV file
write.csv(t_test_results_df, file = "../results/t_test_results.csv", row.names = FALSE)
# Plot the t-test results
ggplot(t_test_results_df, aes(x = System, y = t_statistic, fill = Level)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~Language, scales = "free") +
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0.5)) +
labs(title = "Welch t-test Results for Systems' XWR as Compared to Human XWR",
x = NULL,
y = NULL,
fill = "Level") +
scale_fill_manual(values = c("Paragraph" = "lightblue", "Sentence" = "gray")) +
theme(legend.position = "bottom") +
theme(legend.title = element_blank()) +
theme(legend.text = element_text(size = 12)) +
theme(axis.text.x = element_text(size = 8)) +
theme(axis.text.y = element_text(size = 8)) +
theme(plot.title = element_text(size = 14)) +
theme(strip.text = element_text(size = 10)) +  # Bold and size 12 subplot titles
theme(legend.key.size = unit(0.5, "cm")) +
theme(legend.key = element_rect(fill = "white", colour = "white")) +
theme(legend.background = element_rect(fill = "white", colour = "white"))
# Save the plot to a file
ggsave("../viz/t_test_results.pdf", width = 12, height = 8, units = "in")
# Plot the p-values
ggplot(t_test_results_df, aes(x = System, y = p_value, fill = Level)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~Language, scales = "free") +
geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +  # Add threshold line
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0.5)) +
labs(title = "Welch t-test p-values for systems' XWR as Compared to Human XWR",
x = NULL,
y = NULL,
fill = "Level") +
scale_fill_manual(values = c("Paragraph" = "lightblue", "Sentence" = "gray")) +
theme(legend.position = "bottom") +
theme(legend.title = element_blank()) +
theme(legend.text = element_text(size = 12)) +
theme(axis.text.x = element_text(size = 8)) +
theme(axis.text.y = element_text(size = 8)) +
theme(plot.title = element_text(size = 14)) +
theme(strip.text = element_text(size = 10)) +  # Bold and size 12 subplot titles
theme(legend.key.size = unit(0.5, "cm")) +
theme(legend.key = element_rect(fill = "white", colour = "white")) +
theme(legend.background = element_rect(fill = "white", colour = "white"))
# Save the plot to a file
ggsave("../viz/p_value_results.pdf", width = 12, height = 8, units = "in")
# close device
dev.off()
# empty environment
rm(list=ls())
dev.off()
# Load the necessary library
# install.packages("data.table")
# install.packages("ggplot2")
library(data.table)
library(ggplot2)
# Install and load the effsize package
install.packages("effsize")
install.packages("effsize")
library(effsize)
# Read the first CSV file into a data frame
paras_table <- fread("../results/para_syntax_scores.csv")
# Read the second CSV file into a data frame
sents_table <- fread("../results/sent_syntax_scores.csv")
# Add a column to each data frame to indicate the level
paras_table$Level <- "Paragraph"
sents_table$Level <- "Sentence"
combined_table <- rbind(paras_table, sents_table)
# Convert 'system' and 'Level' to factor for better plotting control
combined_table$system <- factor(
combined_table$system, levels = unique(combined_table$system)
)
combined_table$Level <- factor(
combined_table$Level, levels = c('Paragraph', 'Sentence')
) # nolint
# order the levels of 'system' with "human" first
combined_table$system <- factor(
combined_table$system, levels = c("human", "gpt3", "gpt4", "llama2", "nmt")
)
# select only the necessary columns in the combined table
combined_table <- combined_table[, .(lang, system, xwr_mean, xwr_std, xwr_observation, Level)]
list_df <- split(combined_table, combined_table$lang)
# Create an empty dataframe to store the results
results_df <- data.frame(Language = character(),
System = character(),
Level = character(),
Cohen_d = numeric(),
stringsAsFactors = FALSE)
t_test_results_df <- data.frame(Language = character(),
System = character(),
Level = character(),
t_statistic = numeric(),
p_value = numeric(),
stringsAsFactors = FALSE)
# Function to calculate Cohen's d effect size
calculate_cohens_d <- function(lang_df) {
# Filter data for Paragraph and Sentence levels separately
paragraph_data <- lang_df[lang_df$Level == "Paragraph", ]
sentence_data <- lang_df[lang_df$Level == "Sentence", ]
# Get the human mean and standard deviation at paragraph level
human_mean_paragraph <- paragraph_data[paragraph_data$system == "human", ]$xwr_mean
human_sd_paragraph <- paragraph_data[paragraph_data$system == "human", ]$xwr_std
human_observation <- paragraph_data[paragraph_data$system == "human", ]$xwr_observation
# Loop through each system at both paragraph and sentence levels
systems <- unique(c("gpt3", "gpt4", "llama2", "nmt"))
for (sys in systems) {
# Check if there is data available for the system at paragraph level
if (any(paragraph_data$system == sys)) {
# Get the mean and standard deviation for the system at paragraph level
sys_mean_paragraph <- paragraph_data[paragraph_data$system == sys, ]$xwr_mean
sys_sd_paragraph <- paragraph_data[paragraph_data$system == sys, ]$xwr_std
sys_observation <- paragraph_data[paragraph_data$system == sys, ]$xwr_observation
# total number of observations
total_observation <- human_observation + sys_observation
# calculate the pooled standard deviation
pooled_sd <- sqrt(((human_observation - 1) * human_sd_paragraph^2 + (sys_observation - 1) * sys_sd_paragraph^2) / (human_observation + sys_observation - 2))
# Calculate Cohen's d effect size at paragraph level with hedge's g correction
cohens_d_paragraph <- (sys_mean_paragraph - human_mean_paragraph) / pooled_sd * sqrt((total_observation - 3) / (total_observation - 2.25))
# cohens_d_paragraph <- (sys_mean_paragraph - human_mean_paragraph) / sqrt((human_sd_paragraph^2 + sys_sd_paragraph^2) / 2)
# Add the results to the dataframe
results_df <<- rbind(results_df, data.frame(Language = lang_df$lang[1],
System = sys,
Level = "Paragraph",
Cohen_d = cohens_d_paragraph,
stringsAsFactors = FALSE))
}
# Check if there is data available for the system at sentence level
if (any(sentence_data$system == sys)) {
# Get the mean and standard deviation for the system at sentence level
sys_mean_sentence <- sentence_data[sentence_data$system == sys, ]$xwr_mean
sys_sd_sentence <- sentence_data[sentence_data$system == sys, ]$xwr_std
sys_observation <- sentence_data[sentence_data$system == sys, ]$xwr_observation
# total number of observations
total_observation <- human_observation + sys_observation
# calculate the pooled standard deviation
pooled_sd_sentence <- sqrt(((human_observation - 1) * human_sd_paragraph^2 + (sys_observation - 1) * sys_sd_sentence^2) / (human_observation + sys_observation - 2))
# Calculate Cohen's d effect size at sentence level with hedge's g correction
cohens_d_sentence <- (sys_mean_sentence - human_mean_paragraph) / pooled_sd_sentence * sqrt((total_observation - 3) / (total_observation - 2.25))
# Add the results to the dataframe
results_df <<- rbind(results_df, data.frame(Language = lang_df$lang[1],
System = sys,
Level = "Sentence",
Cohen_d = cohens_d_sentence,
stringsAsFactors = FALSE))
}
}
}
# function to calculcate the Welch t-test (not assuming equal standard deviations)
calculate_t_test <- function(lang_df) {
# Filter data for Paragraph and Sentence levels separately
paragraph_data <- lang_df[lang_df$Level == "Paragraph", ]
sentence_data <- lang_df[lang_df$Level == "Sentence", ]
# Get the human mean and standard deviation at paragraph level
human_mean_paragraph <- paragraph_data[paragraph_data$system == "human", ]$xwr_mean
human_sd_paragraph <- paragraph_data[paragraph_data$system == "human", ]$xwr_std
sample1_n <- paragraph_data[paragraph_data$system == "human", ]$xwr_observation
# Loop through each system at both paragraph and sentence levels
systems <- unique(c("gpt3", "gpt4", "llama2", "nmt"))
for (sys in systems) {
# Check if there is data available for the system at paragraph level
if (any(paragraph_data$system == sys)) {
# Get the mean and standard deviation for the system at paragraph level
sys_mean_paragraph <- paragraph_data[paragraph_data$system == sys, ]$xwr_mean
sys_sd_paragraph <- paragraph_data[paragraph_data$system == sys, ]$xwr_std
sample2_n_para <- paragraph_data[paragraph_data$system == sys, ]$xwr_observation
# Calculate Welch t-test at paragraph level
# Calculate t-test at paragraph level
se_diff_para <- sqrt((human_sd_paragraph^2 / sample1_n) + (sys_sd_paragraph^2 / sample2_n_para))
t_statistic_para <- abs(human_mean_paragraph - sys_mean_paragraph) / se_diff_para
# Get degrees of freedom using Welch's correction
df_para <- (human_sd_paragraph^2 / sample1_n + sys_sd_paragraph^2 / sample2_n_para)^2 /
(human_sd_paragraph^4 / (sample1_n^2 * (sample1_n - 1)) +
sys_sd_paragraph^4 / (sample2_n_para^2 * (sample2_n_para - 1)))
# Calculate p-value using t-distribution
p_value_para <- 2 * pt(abs(t_statistic_para), df_para, lower.tail = FALSE)
# Add the results to the dataframe
t_test_results_df <<- rbind(t_test_results_df, data.frame(Language = lang_df$lang[1],
System = sys,
Level = "Paragraph",
t_statistic = t_statistic_para,
p_value = p_value_para,
stringsAsFactors = FALSE))
}
# Check if there is data available for the system at sentence level
if (any(sentence_data$system == sys)) {
# calculate the t-test at sentence level
sys_mean_sentence <- sentence_data[sentence_data$system == sys, ]$xwr_mean
sys_sd_sentence <- sentence_data[sentence_data$system == sys, ]$xwr_std
sample2_n_sent <- sentence_data[sentence_data$system == sys, ]$xwr_observation
# Calculate t-test at sentence level
se_diff <- sqrt(((human_sd_paragraph^2)/ sample1_n) + ((sys_sd_sentence^2)/ sample2_n_sent))
t_statistic_sent <- abs(human_mean_paragraph - sys_mean_sentence) / se_diff
# get degrees of freedom using Welch's correction
df_sent <- (human_sd_paragraph^2 / sample1_n + sys_sd_sentence^2 / sample2_n_sent)^2 /
(human_sd_paragraph^4 / (sample1_n^2 * (sample1_n - 1)) +
sys_sd_sentence^4 / (sample2_n_sent^2 * (sample2_n_sent - 1)))
# calculate p-value
p_value_sent <- 2 * pt(abs(t_statistic_sent), df_sent, lower.tail = FALSE)
# Add the results to the dataframe
t_test_results_df <<- rbind(t_test_results_df, data.frame(Language = lang_df$lang[1],
System = sys,
Level = "Sentence",
t_statistic = t_statistic_sent,
p_value = p_value_sent,
stringsAsFactors = FALSE))
}
}
}
# Iterate over each language dataframe in the list_df
for (lang_df in list_df) {
calculate_cohens_d(lang_df)
}
# Print the combined results dataframe
print(results_df)
# Save the results dataframe to a CSV file
write.csv(results_df, file = "../results/cohen_d_effect_size.csv", row.names = FALSE)
# Plot the effect size results
ggplot(results_df, aes(x = System, y = Cohen_d, fill = Level)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~Language, scales = "free") +
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0.5)) +
labs(title = "Cohen's d Effect Size with Hedge's g Correction for Systems' XWR as Compared to Human Values",
x = NULL,
y = NULL,
fill = "Level") +
scale_fill_manual(values = c("Paragraph" = "lightblue", "Sentence" = "gray")) +
theme(legend.position = "bottom") +
theme(legend.title = element_blank()) +
theme(legend.text = element_text(size = 12)) +
theme(axis.text.x = element_text(size = 8)) +
theme(axis.text.y = element_text(size = 8)) +
theme(plot.title = element_text(size = 14)) +
theme(strip.text = element_text(size = 10)) +  # Bold and size 12 subplot titles
theme(legend.key.size = unit(0.5, "cm")) +
theme(legend.key = element_rect(fill = "white", colour = "white")) +
theme(legend.background = element_rect(fill = "white", colour = "white"))
# Save the plot to a file
ggsave("../viz/cohen_d_effect_size.pdf", width = 12, height = 8, units = "in")
# Perform t-test for each language
for (lang_df in list_df) {
calculate_t_test(lang_df)
}
# Print the combined results dataframe
print(t_test_results_df)
# Save the results dataframe to a CSV file
write.csv(t_test_results_df, file = "../results/t_test_results.csv", row.names = FALSE)
# Plot the t-test results
ggplot(t_test_results_df, aes(x = System, y = t_statistic, fill = Level)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~Language, scales = "free") +
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0.5)) +
labs(title = "Welch t-test Results for Systems' XWR as Compared to Human XWR",
x = NULL,
y = NULL,
fill = "Level") +
scale_fill_manual(values = c("Paragraph" = "lightblue", "Sentence" = "gray")) +
theme(legend.position = "bottom") +
theme(legend.title = element_blank()) +
theme(legend.text = element_text(size = 12)) +
theme(axis.text.x = element_text(size = 8)) +
theme(axis.text.y = element_text(size = 8)) +
theme(plot.title = element_text(size = 14)) +
theme(strip.text = element_text(size = 10)) +  # Bold and size 12 subplot titles
theme(legend.key.size = unit(0.5, "cm")) +
theme(legend.key = element_rect(fill = "white", colour = "white")) +
theme(legend.background = element_rect(fill = "white", colour = "white"))
# Save the plot to a file
ggsave("../viz/t_test_results.pdf", width = 12, height = 8, units = "in")
# Plot the p-values
ggplot(t_test_results_df, aes(x = System, y = p_value, fill = Level)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~Language, scales = "free") +
geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +  # Add threshold line
theme_minimal() +
theme(axis.text.x = element_text(hjust = 0.5)) +
labs(title = "Welch t-test p-values for systems' XWR as Compared to Human XWR",
x = NULL,
y = NULL,
fill = "Level") +
scale_fill_manual(values = c("Paragraph" = "lightblue", "Sentence" = "gray")) +
theme(legend.position = "bottom") +
theme(legend.title = element_blank()) +
theme(legend.text = element_text(size = 12)) +
theme(axis.text.x = element_text(size = 8)) +
theme(axis.text.y = element_text(size = 8)) +
theme(plot.title = element_text(size = 14)) +
theme(strip.text = element_text(size = 10)) +  # Bold and size 12 subplot titles
theme(legend.key.size = unit(0.5, "cm")) +
theme(legend.key = element_rect(fill = "white", colour = "white")) +
theme(legend.background = element_rect(fill = "white", colour = "white"))
# Save the plot to a file
ggsave("../viz/p_value_results.pdf", width = 12, height = 8, units = "in")
# close device
dev.off()
setwd("~/Documents/PhD/prompts_n_coherence/prompting")
# Load the necessary library
install.packages("data.table")
install.packages("data.table")
install.packages("ggplot2")
install.packages("ggplot2")
library(data.table)
library(ggplot2)
# Install and load the effsize package
install.packages("effsize")
install.packages("effsize")
library(effsize)
# Read the first CSV file into a data frame
params0000 <- fread("results_pilot/20min_0.0_0.0.csv")
params0010 <- fread("results_pilot/20min_0.0_1.0.csv")
params0700 <- fread("results_pilot/20min_0.7_0.0.csv")
params0710 <- fread("results_pilot/20min_0.7_1.0.csv")
params1000 <- fread("results_pilot/20min_1.0_0.0.csv")
params1010 <- fread("results_pilot/20min_1.0_1.0.csv")
# provide a summary of the data
summary(params0000)
# Plot the data
ggplot(params0000, aes(x=V1, y=V2)) + geom_point() + ggtitle("0.0_0.0") + xlab("Time") + ylab("Distance")
# Premove null values
params0000 <- params0000[complete.cases(params0000),]
# provide a summary of the data
summary(params0000)
# provide a summary of the data
summary(params0000)
# Premove null values
params0000 <- params0000[complete.cases(params0000),]
# provide a summary of the data
summary(params0000)
# each row is a feature, which features use which type of data
str(params0000)
# convert the data to numeric
params0000 <- as.data.frame(lapply(params0000, as.numeric))
# show which type of data is in the data frame. convert to numeric if necessary
str(params0000)
# create a subset of params0000 where "file" == 1
params0000_1 <- params0000[file == 1]
# plot features of params0000_1
ggplot(params0000_1, aes(x = time, y = value, color = as.factor(file))) + geom_point() + facet_wrap(~feature, scales = "free_y")
# Read the first CSV file into a data frame
params0000 <- fread("results_pilot/20min_0.0_0.0.csv", header = TRUE, stringsAsFactors = TRUE)
# Read the first CSV file into a data frame
params0000 <- fread("results_pilot/20min_0.0_0.0.csv", header = TRUE, stringsAsFactors = TRUE)
# create a subset of params0000 where "file" == 1
params0000_1 <- params0000[params0000$file == 1,]
# give summary statistics of the subset
summary(params0000_1)
# plot the subset
ggplot(params0000_1, aes(x = time, y = value, color = factor(file))) + geom_point() + geom_smooth(method = "lm", se = FALSE) + facet_wrap(~variable, scales = "free_y")
View(params0000)
View(params0010)
View(params0000)
# give summary statistics of the subset
summary(params0000_1)
# show distributions of the subset for human, continue, explain, create
ggplot(params0000_1, aes(x=human)) + geom_histogram(binwidth=1, fill="blue", color="black") + ggtitle("Human") + theme_minimal()
ggplot(params0000_1, aes(x=continue)) + geom_histogram(binwidth=1, fill="blue", color="black") + ggtitle("Continue") + theme_minimal()
# show distributions of the columns for human, continue, explain, create
ggplot(params0000_1, aes(x=human)) + geom_histogram(binwidth=1, fill="blue", color="black") + ggtitle("Human")
ggplot(params0000_1, aes(x=continue)) + geom_histogram(binwidth=1, fill="blue", color="black") + ggtitle("Continue")
hist(params0000$human)
# give summary statistics of the subset
summary(params0000_1)
# exclude non-numeric columns
params0000_1 <- params0000_1[, sapply(params0000_1, is.numeric)]
hist(params0000$human)
View(params0000)
# create a subset of params0000 where "file" == 1
params0000_1 <- params0000[params0000$file == 1,]
# give summary statistics of the subset
summary(params0000_1)
# exclude non-numeric columns
params0000_1 <- params0000_1[, sapply(params0000_1, is.numeric)]
# exclude lines with NA
params0000_1 <- na.omit(params0000_1)
# exclude lines with 0 or None
params0000_1 <- params0000_1[params0000_1$human != 0,]
params0000_1 <- params0000_1[params0000_1$human != "",]
hist(params0000$human)
boxplot(params0000$human)
summary(params0000_1)
# Exclude rows with any NA values
params0000_1 <- params0000_1[complete.cases(params0000_1), ]
# Read the first CSV file into a data frame
params0000 <- fread("results_pilot/20min_0.0_0.0.csv", header = TRUE, stringsAsFactors = TRUE)
summary(params0000_1)
# Exclude rows with any NA values
params0000_1 <- params0000_1[complete.cases(params0000_1), ]
# Exclude rows with 0 or empty values in specific columns
params0000_1 <- params0000_1[params0000_1$human != 0 & params0000_1$human != "", ]
params0000_1 <- params0000_1[params0000_1$continue != 0 & params0000_1$continue != "", ]
params0000_1 <- params0000_1[params0000_1$create != 0 & params0000_1$create != "", ]
# Read the CSV file into a dataframe
params0000 <- fread("results_pilot/20min_0.0_0.0.csv", header = TRUE, stringsAsFactors = TRUE)
# Exclude rows with any NA values
params0000 <- params0000[complete.cases(params0000), ]
# Convert empty values to NA
params0000[params0000 == ""] <- NA
# Convert NA values to actual NA
params0000 <- na.omit(params0000)
# Exclude rows with 0 or empty values in specific columns
params0000 <- params0000[params0000$human != 0 & !is.na(params0000$human), ]
params0000 <- params0000[params0000$continue != 0 & !is.na(params0000$continue), ]
params0000 <- params0000[params0000$create != 0 & !is.na(params0000$create), ]
params0000 <- params0000[params0000$explain != 0 & !is.na(params0000$explain), ]
# Visualize distributions for selected columns
hist(params0000$human)
hist(params0000$continue)
hist(params0000$create)
hist(params0000$explain)
# Convert selected columns to numeric
params0000$human <- as.numeric(params0000$human)
params0000$continue <- as.numeric(params0000$continue)
params0000$create <- as.numeric(params0000$create)
params0000$explain <- as.numeric(params0000$explain)
# Visualize distributions for selected columns
hist(params0000$human)
hist(params0000$continue)
hist(params0000$create)
hist(params0000$explain)
# Scatter plot
plot(params0000$human, params0000$continue, main="Scatterplot", xlab="file ", ylab="pos_prop_PART", pch=19, frame=FALSE, col="blue")
View(params0000)
# Read the CSV file into a dataframe
params0000 <- fread("results_pilot/20min_0.0_0.0.csv", header = TRUE, stringsAsFactors = TRUE)
# Exclude rows with any NA values
params0000 <- params0000[complete.cases(params0000), ]
View(params0000)
# Convert empty values to NA
params0000[params0000 == ""] <- NA
# Convert NA values to actual NA
params0000 <- na.omit(params0000)
hist(params0000$rix)
View(params0000)
View(params0710)
View(params1000)
