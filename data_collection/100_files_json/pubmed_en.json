{
    "2021-37065713_985_en.txt": {
        "title": "WITHDRAWN: Sea cucumbers of the Arabian Peninsula and Iran – A review of historical and current research trends",
        "prompt": "Abstract: Sea cucumbers are benthic marine invertebrates with immense ecological and commercial value. Processed sea cucumbers known as “Beche-de-mer” are a delicacy in southeast Asian countries with an ever-increasing demand depleting wild stocks on a global scale. Aquaculture techniques are well developed for commercially important species (eg. Holothuria scabra) to aid in conservation and trade. In the Arabian Peninsula and Iran, where the major land mass is surrounded by marginal seas (Arabian/Persian Gulf, Gulf of Oman, Arabian Sea, Gulf of Aden, and Red Sea), studies on sea cucumbers are rather limited and its economic value is underestimated. Historical and current research trends indicate impoverished diversity (82 species) due to environmental extremes. Artisanal fisheries exist for the sea cucumbers of Iran, Oman, and Saudi Arabia, with Yemen and United Arab Emirates (UAE) playing a key role in collection and export to Asian countries. Stock assessment and data on export indicates depletion of natural stocks in Saudi Arabia and Oman. Aquaculture trials of high value species (H. scabra) were successful in Saudi Arabia, Oman and Iran with prospects for further expansion. Research on ecotoxicological properties and bioactive substances conducted in Iran demonstrates an immense research potential. Molecular phylogeny, biology, use in bioremediation, and characterisation of bioactive compounds were identified as potential gaps in research. Expanding aquaculture operations could revive exports and recuperate damaged stocks through sea ranching. Furthermore, regional cooperation, networking, training, and capacity building could help fill the gaps in sea cucumber research, which will aid in its effective conservation and management.\n\n1. Introduction\n",
        "text": "Sea cucumbers, or Holothurians, are marine invertebrates belonging to the class Holothuroidea in Phylum: Echinodermata, inhabit various environments from shallow coastal waters to deep sea. Sea cucumbers perform several significant ecological functions, as efficient ecosystem engineers by deposit feeding habit reduce organic waste, constantly rework the sediments and facilitate bioturbation [REF]. Sea cucumbers are dried, processed, and sold as a product known as “Beche- de- mer” or “Trepang”, an expensive Chinese delicacy with growing demand [REF]. Globally, sea cucumber wild stocks have declined at alarming rates due to overfishing and poor management policies [REF]. However, aquaculture techniques are well established for commercial species such as Holothuria scabra [REF], which could potentially assist in recovering depleted population and trade in many parts of the world.Research on sea cucumbers of the Arabian Peninsula (AP) and Iran is scant and sporadic. Hence, a detailed and comprehensive review of the studies conducted in this region is warranted. Habitat diversity and prevailing extreme environmental conditions likely drive sea cucumber diversity in this region; however, such correlations remain poorly studied. In this review we focus on the aspects of historical and current research trends in sea cucumbers of the AP (which includes the following countries: Iraq, Kuwait, Saudi Arabia, Bahrain, Qatar, United Arab Emirates (UAE), Oman, and Yemen) and Iran and their marginal seas (Arabian/Persian Gulf (APG), Gulf of Oman, Arabian Sea, Gulf of Aden, and Red Sea), to identify gaps in research and provide recommendations to address future research priorities.The AP is surrounded by marginal seas including the APG to the east, Gulf of Oman and the Arabian Sea to the south, and the Gulf of Aden and the Red Sea to the west. The APG is a young (<6000 years), shallow sea (average depth 30–60 m), connected to the Gulf of Oman and Indian Ocean by the Strait of Hormuz [REF]. The APG has a wide range of sea surface temperature (SST; 12 ○C to > 35 ○C) as well as highly variable salinity levels (>42 to >50 ppt) [REF]. The environmental extremes of the APG influences its relatively impoverished species richness and density of this water body [REF]. Over 200 species of macroalgae and four species of seagrass (Halodule uninervis, Syringodium istaefolium, Halophila ovalis, and Halophila stipulacea) in the APG serve as nursery grounds for several species of invertebrates and vertebrates [REF]. Resilient Corals (40 hard coral and 38 soft coral species) and coral reefs thrive in these extreme conditions [REF]. However, climate change-induced recurrent bleaching events have led to reef degradation [REF]. Furthermore, rapid coastal development in the AP threatens its biodiversity [REF].The Gulf of Oman and the Arabian Sea, which surrounds Oman, consists of deep ocean waters with variable climatic conditions. The southwest summer monsoon triggers upwelling along the coast, which results in movement of the cold, nutrient-rich water to the surface and subsequent increase in primary productivity [REF]. Rich coral reefs and extensive sandy beaches occur along the Omani coast [REF]. In this region, mangrove forests, coral reefs, seagrass, and seaweed communities harbour rich biota [REF]. During the summer, upwelling leads to low temperature creating a dispersal barrier for the benthic fauna known as a “pseudo-high latitude effect” [REF].The Red Sea is also a relatively young sea, extending > 2000 km between the AP and Africa. With an average depth of 500 m, the Red Sea is warm and features relatively elevated salinity levels due to high evaporation rates. Mesoscale eddies play a crucial role in distributing nutrients in the oligotrophic environment of Red Sea [REF]. The world’s most productive and biodiverse tropical biotopes including mangroves, seagrass beds, and coral reefs occur in the Red Sea [REF]. The coral reefs of the Red Sea accounts for 3.8% of the world’s reefs, and are rich in diversity of associated fauna (2710 species) with a high degree of endemism [REF]. The rich biodiversity of the Red Sea reefs attracts tourists for diving generating massive income and creating opportunities for tourism-based coastal development (e.g., beach resorts). However, such activities pose a serious threat to the biodiversity and functionality of this fragile ecosystem [REF]."
    },
    "2021-37305159_826_en.txt": {
        "title": "Improvement method for cervical cancer detection: A comparative analysis",
        "prompt": "Abstract: Cervical cancer is a prevalent and deadly cancer that affects women all over the world. It affects about 0.5 million women anually and results in over 0.3 million fatalities. Diagnosis of this cancer was previously done manually, which could result in false positives or negatives. The researchers are still contemplating how to detect cervical cancer automatically and how to evaluate Pap smear images. Hence, this paper has reviewed several detection methods from the previous researches that has been done before. This paper reviews pre-processing, detection method framework for nucleus detection, and analysis performance of the method selected. There are four methods based on a reviewed technique from previous studies that have been running through the experimental procedure using Matlab, and the dataset used is established Herlev Dataset. The results show that the highest performance assessment metric values obtain from Method 1: Thresholding and Trace region boundaries in a binary image with the values of precision 1.0, sensitivity 98.77%, specificity 98.76%, accuracy 98.77% and PSNR 25.74% for a single type of cell. Meanwhile, the average values of precision were 0.99, sensitivity 90.71%, specificity 96.55%, accuracy 92.91% and PSNR 16.22%. The experimental results are then compared to the existing methods from previous studies. They show that the improvement method is able to detect the nucleus of the cell with higher performance assessment values. On the other hand, the majority of current approaches can be used with either a single or a large number of cervical cancer smear images. This study might persuade other researchers to recognize the value of some of the existing detection techniques and offer a strong approach for developing and implementing new solutions.\n\n1. Introduction\n",
        "text": "Cervical cancer is one of the primary causes of gynecologic cancer and one of the most common and dangerous diseases for women, even though it can be treated if detected early. Cervical cancer is cancer that forms when cells on the cervix grow abnormally. There is a large volume of published studies utilizing the Pap smear test to detect pre-cancer in the uterine cervix [REF]. This type of cancer also remains one of the major public health challenges in several countries, especially countries with low and middle income, in terms of the financial aspect and logistical issues [REF]. Previous studies have reported that this cancer is the fourth most pervasive cancer type, which affects the life of many people worldwide [REF]. A large and growing body of literature has investigated the main cause of cervical cancer, stating that the long-lasting infection with a certain type of human papillomavirus (HPV) is passed from one person to another during sex. Non-human papillomavirus-associated adenocarcinomas (NHPVAs) are uncommon uterine cervix tumors with a deceptive appearance [REF].HPV will affect at least half of all sexually active persons at some point in their lives, but only a small percentage of women will develop cervical cancer. A pap smear test is used to detect cervical cancer in most cases and is known as a widely used screening procedure for cervical cancer. However, in recent years, practitioners have executed this evaluation manually, and the results are still controversial due to the accuracy of the diagnosis in detecting cervical cancer cells. In addition, the evaluation is done using the naked eye to determine the type of cervical cell. Furthermore, due to human error, this manual screening approach has a high rate of false-positive results [REF].However, far too little attention has been paid to the occurrence of cervical cancer that can be effectively reduced with preventive clinical management strategies, including vaccines and regular screening examinations [REF]. It has previously been observed that early diagnosis and classification of cervical lesions greatly boost the chance of successful treatments of patients [REF]. The main objective of the initial diagnosis and classification of cervical cancer is to reduce the mortality rate [REF]. This cancer can be successfully treated with earlier detection. The findings from existing research recognize the critical role played by the screening test in reducing the mortality rate caused by cervical cancer.In the past years, the Pap smear test has attracted much attention and is best known as a preventive approach used in the current medical field for detecting cervical cancer [REF]. This test demands a specialized and labor-intensive analysis of cytological preparations to trace potentially malignant cells from both the internal and external cervix surfaces. The cytopathologist analyzes the microscopic fields by screening for abnormal cells. The use of slide digital cytology imaging to increase cytological diagnosis accuracy could be beneficial. Recent evidence suggests that screening diseases, including cervical cancer, breast cancer, and colorectal cancer, using cell images from slide cells has been widely applied in recent years [REF]. However, poor image quality due to the uneven staining, complex backgrounds and overlapped cell clusters poses a greater challenge in nuclei segmentation [REF].In addition, biomedical signal processing, which entails analyzing, improving, and presenting pictures obtained via x-ray, ultrasound, MRI, and other methods, has the same concept as biomedical image processing. Image processing is a technique for performing operations on an image to improve or extract relevant information. It is one type of signal processing that processes an input of a picture and turns the output maybe into an image or characteristics/features associated with that image. Most recent attention has focused on image processing for classifying cervical cells. However, the nature of the accurate classification of Pap smear images is still in the improvement stage for better performance. It is still one of the challenging tasks in medical image processing, and its performance can be enhanced by extracting and selecting well-defined features and classifiers [REF]. Computer-assisted cervical cancer screening based on automated recognition of cervical cells offers the potential to minimize errors and increase the accuracy of the test when compared to manual screening. Traditional approaches rely heavily on cell segmentation accuracy and discriminative hand-crafted feature extraction [REF]. The purpose of this paper is to review recent research on automated detection methods available for the classification of cervical cancer."
    },
    "2021-37305160_984_en.txt": {
        "title": "Regulation of pathological blood-brain barrier for intracranial enhanced drug delivery and anti-glioblastoma therapeutics",
        "prompt": "Abstract: The blood-brain barrier (BBB) is an essential component in regulating and maintaining the homeostatic microenvironment of the central nervous system (CNS). During the occurrence and development of glioblastoma (GBM), BBB is pathologically destroyed with a marked increase in permeability. Due to the obstruction of the BBB, current strategies for GBM therapeutics still obtain a meager success rate and may lead to systemic toxicity. Moreover, chemotherapy could promote pathological BBB functional restoration, which results in significantly reduced intracerebral transport of therapeutic agents during multiple administrations of GBM and the eventual failure of GBM chemotherapy. The effective delivery of intracerebral drugs still faces severe challenges. However, strategies that regulate the pathological BBB to enhance the transport of therapeutic agents across the barrier may provide new opportunities for the effective and safe treatment of GBM. This article reviews the structure and function of BBB in physiological states, the mechanisms underlying BBB pathological fenestration during the development of GBM, and the therapeutic strategies of GBM based on BBB intervention and medicinal drugs transporting across the BBB.\n\n1. Introduction\n",
        "text": "The leading cause for the failure of tumor chemotherapy is the lack of effective drug delivery in the lesion site, which is still a severe challenge for glioblastoma (GBM) therapy [REF]. The increase in tumor resistance to drugs primarily lies in the use of low-specific and highly cytotoxic therapeutics and fluctuations in plasma drug levels. The blood-brain barrier (BBB) is a specialized endothelial structure that is partially covered by pericytes and almost entirely surrounded by the end feet of astrocytes. The primary function of BBB is to maintain the normal physiological part of the brain and the balance of the central nervous systems (CNS) environment [REF]. It is noteworthy that the lack of fenestration in endothelial cells results in the low permeability of BBB, and tight junctions (TJs) between adjacent brain endothelial cells make the barrier 50–100 times tighter than peripheral microvessels. Except for some special transport channels, almost 100% biological macromolecules and 98% small molecules cannot cross the BBB into the brain, which seriously limits the efficacy of therapeutic agents for brain diseases, including GBM [REF]. Therefore, the critical scientific problem to be urgently solved for treating GBM is to improve the transport efficiency of therapeutics into the brain effectively.Recently, extensive and substantial progress has been made in GBM therapy, including brain receptor-mediated targeted therapy [REF], focused ultrasound opening the BBB [REF] and Borneol for “orifice-opening” of the BBB [REF]. Among them, the preferred strategy for GBM-targeted treatment is receptor-mediated intracerebral transport which shares characteristics of high specificity, affinity and selectivity [REF]. Several studies have described that many kinds of receptors are overexpressed on the BBB, including insulin receptors (IR) [REF], low-density lipoprotein receptors (LDLR) [REF], transferrin receptors (TfR) [REF] and nicotinic acetylcholine receptors (nAChRs) [REF]. And their ligands are usually employed to facilitate receptor-mediated intracerebral delivery of anti-GBM therapeutic drugs. However, the intracerebral transport efficiency of medications is still unsatisfactory. In the author’s crude opinion, some of these reasons are mainly attributed to the fact that current studies on brain-targeted drug delivery systems primarily focus on the interactions between ligands and their receptors, as well as targeted cells and targeted tissues, while ignoring the dynamic changes of BBB function during the development and treatment periods of brain diseases. One or even several treatments of the brain-targeted delivery system may achieve tumor-inhibiting effects in the short term. Still, in the long run, its impact on the restoration of pathological BBB can continue to inhibit the intracerebral delivery efficiency of therapeutic drugs, and lead to the failure of their treatment. In addition, the poor druggability of the vast majority of brain-targeted drug delivery systems limited their clinical application. Therefore, there is still a long way to find safe and effective ways to go to treat brain diseases, and anti-GBM therapy still needs the discovery of new mechanisms, the breakthrough of new technologies and the emergence of new treatments.The pathological process of GBM is likely to be closely related to BBB, which might also play a vital role in the occurrence and treatment of GBM [REF]. Under physiological conditions, BBB is a biological membrane barrier composed of brain microvascular endothelial cells and their TJs, basement membranes and the end feet of glial cells around capillaries [REF]. TJs structure is an essential morphological basis of the BBB, forming the low permeability of the barrier and its high resistance properties [REF]. Under pathological conditions, the occurrences of brain diseases [REF], including GBM, Parkinson’s disease (PD), cerebral ischemia and Alzheimer’s disease (AD), are usually accompanied by pathological impairment of BBB, with decreased TJs proteins and increased BBB permeability.Studies have confirmed that BBB pathological fenestration provides an opening paracellular pathway for the transport of therapeutics across the BBB. Hu et al. have demonstrated that BBB pathological fenestration in GBM provided an extra paracellular route for angiopep-2 modified glycolipid-like nanoparticles transporting across the barrier in vitro and in vivo [REF]. Nevertheless, during multiple dosing treatments for GBM, the first administration with brain-targeted nanotherapeutics would cause pathological BBB restoration, which resulted in poor intracerebral transport efficiency during re-administration and eventual treatment failure. Our previous study has confirmed that the treatment of targeted Ap-CSssSA/P nanoparticles could restore pathological BBB mainly via VEGF downstream signaling pathway inhibition. However, pathological BBB functional recovery seriously reduces the intracerebral transport efficiency of Ap-CSssSA/P nanoparticles, leading to a poor outcome in GBM therapy. Therefore, combined sequential treatment with functionalized Ap-CSssSA/P nanoparticles and BBB permeability regulator SC79, which could reversibly re-open the restored BBB, was proposed to treat GBM. And the strategy finally produced an observable and significantly enhanced anti-GBM effect in vivo, compared to single Ap-CSssSA/P nanoparticles treatment [REF]. Hence, the regulation of pathological BBB for improving the transport of therapeutic agents across the BBB may provide new opportunities for GBM effective treatment."
    },
    "2022-37304622_955_en.txt": {
        "title": "Outcomes of inquiry-based learning in health professions education: a scoping review",
        "prompt": "Abstract: Open inquiry-based learning (IBL) that aims to foster higher-level thinking, is defined by students formulating their own questions and learning through exploration. The present study aimed to summarize the breadth of metrics used to evaluate health professions trainees in open IBL curricula.We conducted a scoping review to identify publications detailing trainee outcomes in open IBL initiatives in health professions education. We queried five databases and included studies which described interventions with five phases of IBL (orientation, conceptualization, investigation, conclusion, and discussion). We completed abstract and full text reviews in duplicate. Data were collated and summarized.From 3030 record, 21 studies were included in the final extraction (k = 0.94), with nine involving physician trainees and twelve involving nursing trainees. Three studies used validated data collection tools to measure student inquiry behavior, and a single study used a validated data collection tool to measure critical thinking abilities. Most studies (n = 11) reported trainee self-reported satisfaction or perceived gain of skills as the primary outcome. All four studies using validated tools reported high scores in inquiry behaviors at the end of the curriculum and results on critical thinking skills were mixed. One study collected serial data, while remaining studies collected pre-post or post-only data.IBL has the potential to cultivate a climate of curiosity among health professions learners. However, studies have relied heavily on subjective outcomes. Limited studies reported standardized measures of inquiry behaviors suggest favorable results. Curriculum innovations using IBL could make use of existing tools to better understand their impact on students’ inquiry-oriented skills.\n\n1. Introduction\n",
        "text": "Health professions trainees in the 21st century have access to an unprecedented amount of open access educational resources that greatly exceeds the organizational capacity of an individual’s mind [REF]. In response, future practitioners must be proficient in knowledge acquisition [REF] The Carnegie Foundation for the Advancement of Teaching highlighted the need to incorporate habits of inquiry and improvement in the 2010 Call for Reform of Medical Education as a learning strategy to optimize proficiency in knowledge acquisition [REF]. To meet the evolving needs of trainees and to foster greater student curiosity as the foundation for learning, inquiry-based learning (IBL) has emerged as an appealing educational strategy.IBL focuses on learner driven acquisition of knowledge through development of inquiries, and hypothesis generation. This differs from problem-based learning in that PBL is focused on learner investigation of teacher provided problems. Comparative to problem-based learning is case based learning wherein learners are provided cases around which to target their investigation and research. Through exclusion of these other teaching methods and focusing only on open inquiry, we attempt to distinguish the specific benefits of pure trainee inquiry learning and consider how this can be incorporated in areas of health profession that involve mature learners such as post-graduate medical education and nursing education.Educational philosopher John Dewey, a prominent education reformist in the early 20th century laid the foundation for inquiry as a central focus of science education, and Jerome Bruner pioneered the inquiry-based instruction in science curricula [REF] in the 1950s. The theory behind IBL is the constructivist, learner-driven active process of knowledge acquisition. Students formulate hypotheses and make observations in order to construct their knowledge [REF]. While descriptions of IBL vary widely in the literature, the framework can be divided into five general phases:[REF]\nOrientation: the topic is introduced, and the student creates a problem statement,Conceptualization: the student develops an open question pertaining to the problem, and generates a hypothesis,Investigation: the student explores or observes, they may even experiment, and interpret their findings,Conclusion: the student reviews the problem, hypothesis, and their interpretation of the findings to consider whether their question has been answered, and,Discussion: the student communicates their findings to others (external) and reflects upon successes and areas for improvement within the inquiry process (internal).\nOrientation: the topic is introduced, and the student creates a problem statement,Conceptualization: the student develops an open question pertaining to the problem, and generates a hypothesis,Investigation: the student explores or observes, they may even experiment, and interpret their findings,Conclusion: the student reviews the problem, hypothesis, and their interpretation of the findings to consider whether their question has been answered, and,Discussion: the student communicates their findings to others (external) and reflects upon successes and areas for improvement within the inquiry process (internal).While the IBL curriculum design has evolved and taken many forms since Bruner’s initial model, the core foundation of student-directed epistemic curiosity has been consistently aimed at encouraging active participation, and improving scientific literacy [REF]. Studies in higher education have found that IBL can hone students’ analytical and critical thinking abilities [REF], and may improve students’ overall academic performance compared to a traditional lecture-based curriculum.7IBL has been variably classified and subdivided in the literature. Aditomo et al. grouped IBL curricula based on assigned tasks that include: scholarly research (students formulate questions and collect empirical data to address them), simplified research (students formulate question but only perform some aspects of data collection or analysis), literature-based research (no empirical data collection), and applied research (similar to simplified, though focused on practical issues and “real-world” problems).8In some cases, IBL has been considered as overlapping or else an umbrella term encompassing problem-based learning (PBL) [REF]. Some have sub-classified IBL based on the roles and responsibilities of teachers and students [REF]:\nStructured inquiry (e.g. PBL) – teachers provide a problem and an outline for addressing it,Guided inquiry – teachers provide questions to stimulate inquiry however students are self-directed in the investigation, conclusion, and discussion, and,Open inquiry – students develop questions themselves, and are self-directed in investigation, conclusion, and discussion.\nStructured inquiry (e.g. PBL) – teachers provide a problem and an outline for addressing it,Guided inquiry – teachers provide questions to stimulate inquiry however students are self-directed in the investigation, conclusion, and discussion, and,Open inquiry – students develop questions themselves, and are self-directed in investigation, conclusion, and discussion.While there have been apparent benefits of IBL in higher education, particularly with regards to fostering inquiry behavior, the outcomes of an IBL curriculum in health professions education are yet to be fully elucidated. In this scoping review, we examine the published literature exploring student outcomes in inquiry-based learning curricula in health professions training. We aim to describe the extent of existing literature in this area, to characterize study designs and outcomes, and to identify gaps in the health professions’ literature where future studies on IBL should focus."
    },
    "2022-37304844_718_en.txt": {
        "title": "Disease-Related Malnutrition in Pediatric Patients with Chronic Disease: A Developing Country Perspective",
        "prompt": "Abstract: Malnutrition is widely known to affect growth in children. There are many studies focusing on malnutrition globally in relation to limited food access; however, there is only limited research on disease-related malnutrition, especially in chronic conditions and particularly in developing countries. This study aims to review articles on the measurement of malnutrition in pediatric chronic disease, especially in developing countries where there are resource limitations in identifying nutritional status in pediatric chronic disease with complex conditions. This state-of-the-art narrative review was conducted through search of literatures through 2 databases, and identified 31 eligible articles published from 1990 to 2021. This study found no uniformity in malnutrition definitions and no consensus regarding screening tools for the identification of the malnutrition risk in these children. In developing countries where resources are limited, instead of focusing on finding the best tools to identify the malnutrition risk, the approach should be directed toward developing systems that work best according to capacity and allow for a combination of anthropometry assessment, clinical evaluation, and observation of feeding access and tolerance on a regular basis.\n\n1. Introduction\n",
        "text": "Pediatric malnutrition is an ongoing burden in developing countries, particularly in Southern Asia, as compared with Latin America and the northern and western regions of Africa. It is globally known to be the most crucial risk factor for morbidity and mortality [REF]. Approximately 1.9 billion people in the Asia and Pacific region have limited access to a healthy diet, and the need to improve the diet quality, especially in children, is critical [REF]. However, when we assess the pathogenesis and pathophysiology of malnutrition, there should be a differentiation between nondisease-related malnutrition, also known as nonillness-related malnutrition, and disease-related or illness-related malnutrition [REF].The first type of malnutrition occurs in relation to environmental factors, such as limited food access or decreased dietary intake, which results in an imbalance relative to nutritional needs, thereby affecting growth, development, and other health aspects. Disease-related malnutrition may progress to acute or chronic disease and occurs because of an energy imbalance due to increased catabolism, decreased appetite, or increased energy losses, which also have detrimental effects on the outcomes [REF].Studies have clearly observed that disease-related malnutrition is associated with an increase in the length of stay of hospitalized children and additional financial burdens. A study in the Netherlands noted the annual medical costs for the country, estimated at €80 million for disease-related malnutrition, which equals 5.5% of the total hospital costs for these hospitalized children [REF]. Disease-related malnutrition, especially in pediatric chronic disease, requires long-term nutrition support to prevent long-term growth faltering and impaired development and also to prevent poor health-related quality of life [REF].The causes of malnutrition in children with chronic disease are multifactorial and are associated with the underlying disease; thus, careful assessment and management are required for chronic disease.This study observed the most common chronic diseases in children, such as congenital heart disease (CHD), chronic kidney disease (CKD), chronic liver disease (CLD), and malignancy, in which their nutritional assessment may be complicated with preceding conditions such as feeding intolerance, water retention, peripheral edema, electrolyte disturbance, tumor masses, and decreases in bone density and fat mass [[REF], [REF], [REF], [REF]]. Indonesia, like other developing countries, also faces malnutrition associated with infectious diseases, such as diarrhea, AIDS, tuberculosis, and parasite infections. However, it is also observed that nutritional deficiency is known to be a predisposing factor for poor immune response, which makes children more susceptible to infection [REF]. The 3 factors undernutrition, infectious diseases, and immune systems form a simple cycle and make their roles interlinked [REF].Our research focused on chronic malnutrition in children with chronic diseases as mentioned above, which has received less attention in developing countries than that on malnutrition caused by infectious diseases. The combination of anthropometry assessment, clinical evaluation, observation on feeding tolerance, and biochemistry analysis should be conducted on a regular basis to ensure adequate nutrition. Lack of uniform definitions, heterogenous screening practices, and the failure to recognize nutrition as part of crucial patient care, however, have contributed to underrecognition of the prevalence of malnutrition in relation to its outcomes [REF]. In developing countries, assessment also depends on the availability of resources and regular monitoring to achieve good outcomes [REF].The initial concept of the study was derived from an expert meeting that consisted of 6 child health professionals, who contributed input and presented current challenges in managing malnutrition in chronically ill children. The purpose of this study was to review articles available on the measurement of malnutrition in pediatric chronic disease, especially in developing countries where resource limitations restrict identification of the nutritional status in pediatric chronic disease with complex conditions."
    },
    "2022-37305016_1292_en.txt": {
        "title": "The effect of Wnt/β-catenin signaling on PD-1/PDL-1 axis in HPV-related cervical cancer",
        "prompt": "Abstract: Infection with high-risk human papillomavirus (HPV), including HPV-16 and HPV-18, is the main cause of malignancies, such as cervical cancer. Viral oncoproteins encoded by HPV are expressed in HPV-positive cancers and associated with the early cancer stages and the transformation of normal cells. The signaling pathways involved in the transformation of normal cells to cancerous form and the subsequently expressed programmed cell death-ligand 1 (PD-L1) on the surface of the transformed cells lead to a disruption in recognition of tumor cells by the immune cell system, including T lymphocytes and dendritic cells which lead to the development of cervical cancer malignancy. These cells also produce modest levels of cytokines during exhaustion, tumor-infiltrating T CD4+ cells with high levels of PD-1 and CD39 release considerable quantities of cytokines. The Wnt/β-catenin signaling pathway, which controls the expression of genes involved in the tumor cells’ markers, is demonstrated to be one of the most potent cancer stimulants. It leads to the evasion of the tumor cells from immune cell detection and ultimately avoids being recognized by dendritic cells or T-cells. PD-L1, as an inhibitory immune checkpoint, is essential for controlling immune system activity by inhibiting T-cells’ inflammatory function. In the present review, we looked into how Wnt/β-catenin affects the expression of PD-L1 and related genes like c-MYC in cancer cells and its role in the development of HPV-induced malignancy. We hypothesized that blocking these pathways could be a potential immunotherapy and cancer prevention method.\n\n1. Introduction\n",
        "text": "Cells determine their function through signaling pathways and communication with other cells and their microenvironment. Due to ligands’ binding to their receptors on the cell surface, protein cascades are activated and consequently affect the level of gene transcription, leading to the conversion of external stimuli into biochemical signals. These signaling pathways control biological effects such as proliferation, differentiation, and death; thus, disrupting these pathways leads to malignancy [REF]. Also, they are owing to changes in some critical cell proliferation and apoptosis-controlling pathways, including phosphatidylinositol 3-kinase (PI3K)/phosphorylation of protein kinase B (AKT), extracellular signal-regulated kinase (ERK)/mitogen-activated protein kinase (MAPK), Notch, Wnt/β-catenin, and epidermal growth factor receptor (EGFR). In addition, a variety of inhibitory immune checkpoint molecules, such as programmed cell death protein 1 (PD-1, CD279) and its ligand PD-L1 (CD274), have been playing a significant role in various types of cancers and chronic viral infections [REF]. For instance, it was demonstrated in a report published on the PD-1/PD-L1 axis in acute viral infections affecting the lower respiratory tract that this pathway impairs the activity of CD8+ T-cells in the human respiratory system. In addition, dendritic cells inhibit T-cell function in acute viral infections by highly expressing PD-L1 on their surfaces [REF]. PD-1, on the other hand, is expressed in lymphocytes, specifically CD8+ and CD4+ T-cells, and is an inducible negative regulator of T-cell activity; as a result, PD-L1 is one of two PD-1 ligands found on antigen-presenting cells (APCs), peripheral tissues, and cancerous cells [REF]. As a result, the interaction of PD-1/PD-L1 leads to the activation of inhibitory signals in T-cells, which eventually leads to the destruction of T-cells, anergy, and reduced function [REF]. PD-1 causes regulatory T-cells to face less apoptosis while increasing the death of some T-cells in lymph nodes. On the surface of CD8+ and CD4+ T-cells, PD-1 is more apparent. The PD-1/PDL1 pathway reduces the activity of T-cells; this inhibitory effect is done by binding PD-1 to its ligands (PD-L1 and PD-L2), which are produced on the surface of cancerous cells and peripheral tissues. The expression of PD-L1 also regulates IFN-γ-secreting T-cells [REF]. Moreover, it has been illustrated that the interaction of PD-1/PD-L1 can increase the transcription of several genes involved in cancer induction. Cancer cells express immune system regulators such as the cluster of differentiation (CD)-47 and PD-L1 to induce immune response resistance. Also, cellular myelocytomatosis (c-MYC) activation is due to CD-47 and PD-L1 expression [REF]. Because c-MYC can directly bind to the CD-47 and PD-L1 gene promoters, inhibiting c-MYC reduces the expression of both genes (CD-47 and PD-L1) and thus improves the anti-tumor immune response [REF]. It is hypothesized that T-cell apoptosis is associated with c-MYC overexpression, a critical target gene of the Wnt/β-catenin signaling pathway [REF]. Additionally, Wnt/β-catenin signaling has been implicated as one of the primary inducers of cancer, accelerating the development of the disease through controlling the tumor immune cycle in various cell types, including dendritic cells, T-cells, and tumor cells [REF]. Some significant regulators of the anti-tumor function of T-cells, especially effective T-cells, T helper cells (Th), and regulatory T-cells (T-reg), are changed functionally by abnormal Wnt/β-catenin signaling [REF]. Besides, the activation of the Wnt/β-catenin signaling pathway is observed during various types of malignancies, such as hepatocellular carcinoma, human immunodeficiency virus (HIV), breast cancer, as well as in HPV-related cancers such as oropharyngeal and cervical cancers, head and neck, anal cancer, and vulvar cancer [REF]. As mentioned above, increased levels of PD-1 expression on the T-cells and APC cells, and also hyper-activation of the Wnt pathway during chronic HPV infection, are positively associated with tumor cell metastasis and HPV-associated cancers such as cervical cancer and cervical intraepithelial neoplasia (CIN) grade [REF]. Besides, inhibition of the Wnt signaling pathway in cancer cells could disrupt the expression of CD-47 and PD-L1, resulting in an increased immune defense against tumors [REF]. In other words, MYC signaling leads to increased expression of CD47 and PD-L1 in tumor cells. Meanwhile, due to CD47-SIRPα interaction, the phagocytosis activity of tumor cells by macrophages and dendritic cells is disturbed. As a result, the lack of activity of the innate immune system may cause an increase in the recruitment of T-cells to the tumor microenvironment. In contrast to PD-L1 expression, it may disrupt in the function of T-cells in the tumor microenvironment. In addition, the Wnt/β-catenin signaling pathway may affect MYC activation [REF].The present study hypothesized that current research concentrates on the Wnt/β-catenin and PD-1 signaling pathways, crucial elements in cancers, particularly cancers related to viruses. These findings may alter the manner that individuals evaluate cancer patients and immunotherapy (Fig. 1).During infection, the PD-1/PD-L1 signaling axis and Wnt play an essential role in developing and maligning epithelial cells. Thus, after presenting the antigen to the T-cells, the DC cells induce signaling pathways in T-cells, resulting in the induction of TCR and the secretion of pro-inflammatory cytokines. Then these cytokines lead to the expression of PD-1 on the surface of T-cells. In this line, Wnt binds to its receptors. The integrated HPV genome regulates these pathways to increase escape and inhibit the immune response against tumor cells by expressing E6 and E7 oncoproteins. Also, these oncoproteins lead to the induction of a series of cellular signaling pathways of Wnt; it eventually conducts the expression of the c-MYC in the target cell nucleus. The c-MYC gene leads to increased expression of PD-L1 and CD47 at the surface of tumor cells. By binding PD-L1 to PD-1 expressed at the surface of T-cells, it leads to apoptosis, anergy, and ultimately inhibition of the immune response vs. tumor cells. Also, INF-γ secreted by T-cells leads to increased PD-L1 expression at the surface of tumor cells."
    },
    "2022-37305018_726_en.txt": {
        "title": "Dissecting the effects of androgen deprivation therapy on cadherin switching in advanced prostate cancer: A molecular perspective",
        "prompt": "Abstract: Prostate cancer is one of the most often diagnosed malignancies in males and its prevalence is rising in both developed and developing countries. Androgen deprivation therapy has been used as a standard treatment approach for advanced prostate cancer for more than 80 years. The primary aim of androgen deprivation therapy is to decrease circulatory androgen and block androgen signaling. Although a partly remediation is accomplished at the beginning of treatment, some cell populations become refractory to androgen deprivation therapy and continue to metastasize. Recent evidences suggest that androgen deprivation therapy may cause cadherin switching, from E-cadherin to N-cadherin, which is the hallmark of epithelial-mesenchymal transition. Diverse direct and indirect mechanisms are involved in this switching and consequently, the cadherin pool changes from E-cadherin to N-cadherin in the epithelial cells. Since E-cadherin represses invasive and migrative behaviors of the tumor cells, the loss of E-cadherin disrupts epithelial tissue structure leading to the release of tumor cells into surrounding tissues and circulation. In this study, we review the androgen deprivation therapy-dependent cadherin switching in advanced prostate cancer with emphasis on its molecular basis especially the transcriptional factors regulated through TFG-β pathway.\n\n1. Introduction\n",
        "text": "Prostate cancer (PCa) is one of the most frequently diagnosed malignancies in males with an increasing prevalence in both developed and developing countries and is one of the major causes of cancer-related death in men [REF]. Although the standard treatment for local PCa is surgery and radiotherapy, some other treatment protocols, such as brachytherapy and androgen deprivation therapy (ADT) especially in patients with advanced PCa are commonly used [REF].The primary purpose of ADT is to reduce circulating androgen levels and block androgen signaling, since androgen has an important role in cancer cell proliferation and consequently disease progression [REF]. Indeed, ADT is a powerful and useful treatment approach that causes a decrease in prostate-specific antigen (PSA) levels and tumor volume. It provides a temporary relief to patients although some patients may experience severe to moderate side effects such as hot flashes and sexual dysfunction during treatment [REF].The response to the ADT may be observed in three phases: early, developing, and late phases (Fig. 1). The early phase is characteristic with decreased level of PSA, tumor regression, and increased quality of life while the late phase is characteristic with castration resistance, relapse, and highly metastatic and aggressive tumors. In the intervening period, which can be considered as a developing phase, homeostatic response of the cells leads to a progressive development of alteration in various cellular signaling mechanisms leading to the late phase response (Fig. 1). The cellular responses include reactivation of androgen receptor (AR) signaling through alternative mechanisms, decreased adhesion, increased motility and invasion, and altered tonic cell survival signaling. Mechanisms involving re-activation of androgen signaling include intra-tumor de novo androgen production, amplification of AR gene, mutations in the ligand-binding domain of AR (AR-LBD), expression of some constitutively active AR isoforms, overexpression of AR co-regulators, and androgen-independent AR activation [REF]. These events lead to a more aggressive phenotype of the disease called castration-resistant PCa (CRPC), generally 24–36 months after the initiation of ADT [REF]. In this stage, metastatic spread is observed in most of the patients and the average life expectancy is 18–24 months afterward [REF].Another altered mechanism is epithelial-mesenchymal transition (EMT), which decreases adhesion and increases motility and invasion. Growing evidence links EMT to ADT in advanced PCa despite the lack of consensus regarding the effect of androgen signaling in the regulation of EMT [REF]. Indeed, initial studies have reported that ADT may induce EMT in advanced PCa at least through the “cadherin switching” mechanism [REF]. These reports led to further investigation, which pointed to an aberrant activation of transforming growth factor β1 (TGF-β) signaling [REF], a multifunctional pathway regulating proliferation and differentiation of cells and cellular signaling molecules involved in cadherin switching [REF]. The term “cadherin switching” generally refers to diminished expression of E-cadherin, encoded by Cdh1, and increased expression of N-cadherin, encoded by Cdh2, in cells [REF]. Cadherin switching is a normal process during embryogenesis and cannot initiate tumorigenesis alone in healthy cells but may confer further migration and increased invasive capabilities on tumor cells [REF]. Decreased Cdh1 expression disrupts epithelial tissue structure causing cells to detach from one another and move freely [REF]. Consistently, decreased expression of Cdh1 depends on cadherin switching, a process closely associated with tumor invasion and metastasis, two common characteristics of advanced cancers [REF]. N-cadherin confers enhanced migratory and invasive abilities on tumor cells, which contrasts the inhibitory role of E-cadherin on migration and invasion [REF].In this study, we aim to review the current literature to dissect the role of ADT on cadherin switching in advanced PCa from a molecular perspective."
    },
    "2022-37305347_1654_en.txt": {
        "title": "Utilization of kinase inhibitors as novel therapeutic drug targets: A review",
        "prompt": "Abstract: Kinase inhibitors are a significant and continuously developing division of target therapeutics. The drug discovery and improvement efforts have examined numerous attempts to target the signaling pathway of kinases. The Kinase inhibitors have been heralded as a game-changer in cancer treatment. For developing kinase inhibitors as a treatment for various non-malignant disorders like auto-immune diseases, is currently undergoing extensive research. It may be beneficial to investigate whether cell-specific kinase inhibitor administration enhances therapeutic efficacy and decreases adverse effects. The goal of the current review is to gain insight into the role of kinase inhibitors in facilitating effective target drug delivery for the treatment of various anti-inflammatory, auto-immune, and anticancer disorders. The aim of this review is also to shed light on drug discovery approaches for kinase inhibitors, their mode of action, and delivery approaches. The variation in the binding of kinases bestows different target approaches in drug design, which can be employed for designing the targeted molecules. Several target sites have been studied, exceeding the design of drugs for various diseases like cancer, Alzheimer’s, rheumatoid arthritis, etc. Diverse delivery approaches have also been studied for the targeted application of kinase inhibitors.\n\n1. Introduction\n",
        "text": "Cellular metabolism, cell cycle regulation, cellular endurance, and differentiation are the essential functions conducted by kinases [REF]. Nowadays, considerable attention has been given to research on kinases. The signaling pathways mediated by protein kinases have been related to a variety of diseases, such as diabetes, inflammation, and cancer [REF]. The kinases are categorized into a wide group lipid kinases, carbohydrates, and protein kinases. According to human genome sequencing, approximately 2% of the human genome codes for protein kinases, which are further classified as families and subfamilies. The major kinases found in mammalian signaling pathways are lipid kinases, tyrosine kinases, serine/threonine kinases, and dual kinases (Ser/Thr and Ty). Protein kinases are enzymes that use the end γ-phosphate group from ATP to phosphorylate serine, tyrosine, or threonine residues in other proteins. Phosphorylation changes target protein activity by controlling signaling pathways by amplification, cellular location, or interaction with other controlling proteins [REF]. The study on human kinases revealed that there are 518 protein kinases grouped into families reliant on their biological functions as well as statistical sequence evaluation [REF].Kinase inhibitors are small molecules that inhibit kinases and are of much use in therapeutic and diagnostic fields. According to recent research, Kinase inhibitors serve as a potential target for treating various diseases like autoimmune disorders, cardiovascular diseases, cancer, and inflammatory disorders [REF]. As of November 24, 2022, the US FDA had approved 72 small molecules that are therapeutic protein kinase inhibitors. All drugs that have been approved by the FDA work best when taken orally, with the exception of temsirolimus, trilaciclib, and netarsudil [REF] (https://brimr.org/protein-kinase-inhibitors/). Novel biologic agents, such as monoclonal antibodies, can also be employed for targeting specific kinases. However, target non-specificity and toxicity are the principal disadvantages associated with kinase inhibitors. Small molecules from natural sources may act together with proteins and function as signaling molecules, which can be further employed for human health. The kinase inhibitors from various plant products help treat several disorders (Table 1).In an in vivo study performed on cancer cell line 85As2 (Gastric cancer cell line), the steroidal alkaloid tomatidine as well as tomatidine rich tomato leaf extract exhibited reduction in proliferation of 85As2 cells as a result of dysregulation of type 1 interferon-stimulated genes [REF]. As a result, several marine natural chemicals have been exercised as lead drug composites that progress in future medicines. Around 22 kinase inhibitors are synthesized from marine bacteria, for instance, Bacillus and Streptomyces species; 16 from marine fungi sources like Penicillium, Aspergillus, etc.; two new kinase inhibitors from deep-sea soft corals like Cladiella australis, Cladiella pachyclados; 14 kinase inhibitors from marine animals like sea cucumber, marine echinoderm, etc.; 10 from marine algae-like cyanobacteria; 42 from marine sponges [REF].Kinase inhibitors are a rapidly growing and important category of target therapeutics. To target kinases signaling, drug discovery and development initiatives have looked at a range of approaches. The role of kinases in drug designing and as delivery systems has been described as follows:Many protein kinases are part of “cascade systems,” where multiple protein kinases sequentially activate each other. A kinase cascade plays a vital role in amplification, diversification of signal effects, and permission for grouping signaling pathways to form networks. Because kinase cascades exist, it is possible to find and produce drugs that are not kinase inhibitors but prevent kinases from being triggered due to the subsequent “upstream” kinase in the cascade through attachment to them. PD98059 is certainly the first kinase inhibitor developed for the Ras-Raf MEK-ERK cascade, which acts through binding to MEK1 and then blocking its stimulation via Raf [REF]. The dual-specificity protein kinases (MEK1/2) that block the RAS-Raf-MEK MAP kinase pathway are trametinib, binimetinib, cobimetinib, and selumetinib. Among these, binimetinib and cobimetinib are used in combination with encorafenib and vemurafenib, respectively for curing melanoma. Selumetinib is employed for Von Recklinghausen disease [REF]. Raf, Src, epidermal growth factor, and breakpoint cluster area are all kinase targets. Early on in the investigation of oncogenic proteins, Abelson’s kinase (bcr-Abl) was discovered. Clinical usage of these inhibitors has also resulted in the growth of drug-resistant malignancies as additional consequences [REF]. The detection alongside subsequent exploitation of numerous configurational positions of kinases is part of the tale of developing these kinase inhibitors. Previously, selectivity had been identified as a problem during the development of kinase inhibitors that bind at the ATP (Adenosine Triphosphate) pocket. As ATP is a cofactor required for the functioning of the kinase, developmental pressure has been applied to keep the binding site of discrete kinases in a general shape and chemical similarity. As a result, the active sites avoiding the direct binding of ATP or kinase conformations with more structural and chemical heterogeneity have been employed. The activation and inactivation modes of kinases allow for the utilization of conformational variability. A rationale based on the structure was also proposed to prevent the formation of drug-resilient mutations. Since the inactive version of a kinase does not require binding with the conjoint substrate ATP, it has been found to have higher structural diversity. A pioneer kinase inhibitor, Imatinib, exhibits binding to the inactive forms of PDGF, c-Kit, and Abl kinases [REF]. Imatinib is used to treat chronic myelogenous leukemia as it causes inhibition of BCR-Abl, an oncogenic tyrosine kinase fusion protein also employed to manage gastrointestinal tumors and myeloproliferative disorders caused by inhibition of c-Kit tyrosine kinase and PDGF receptors, respectively. As a result, oncology happens to be the therapeutic area of choice for the vast majority of drug development efforts for kinase inhibitors [REF]. Most of the small protein kinase inhibitors that have been approved or are on the verge of getting approval for their clinical use, along with the majority of kinase inhibitors currently in the clinical trial phase, intend to target tyrosine kinases and are employed for the treatment of varied cancers. The global market for kinase inhibitors is increasing gradually. Although cancer will likely remain a top priority for kinase medication development for several years, the number of kinase inhibitors in clinical studies for many other disorders has expanded [REF]. For example, Janus Kinase inhibitors such as tofacitinib and ruxolitinib have been approved to treat rheumatoid arthritis and myelofibrosis, respectively [REF]. Inhibition of anaplastic lymphoma kinase (ALK) by six approved molecules, i.e., Alectinib, Brigatinib, Ceritinib, Crizotinib, Entrectinib, and Lorlatinib, has been employed for the treatment of NSCLC (Non-Small Cell Lung Cancer) (link). Targets used for developing kinase inhibitors for diseases other than cancer are described in Fig. 1.WNKs direct the phosphorylation and activation of two interrelated protein kinases: SPAK (STE20/SPS1-related Proline/Alanine-Rich Kinase), OSRK1 (Oxidative Stress Responsive Kinase1), leading to the discovery that they control blood pressure. The unique location of a catalytic lysine residue on WNK isoforms might be of potential use in contrast to many other kinases to build WNK-specific ATP-competitive inhibitors, opening up a new avenue for developing better blood pressure medications. Although it is uncertain which WNK isoform of the four mammalian isoforms is possibly essential to be blocked to lower NCC (Na+/Cl− co-transporter) and NKCC2 (Na+/K+/2Cl− co-transporter-2) activity in the kidney. Additionally, in some studies, WNK4 was a negative controller of WNK1 signaling; signifying that blockage of this isoform might enhance renal salt reassimilation and blood pressure [REF]. Patients with mutations of LRKK2 acquire signs of disease that are identical to both the onset and progression of idiopathic Parkinson’s syndrome. LRKK2 is a multidomain protein with a large GTPase and a kinase domain of 2527 residues. Numerous particular LRKK2 inhibitors have been developed for the treatment of Parkinson’s syndrome. Biological drugs, like adalimumab tend to neutralize TNF-α and have a specific role in treating many inflammatory disorders like Crohn’s disease, psoriatic arthritis, and rheumatoid arthritis. Several Ser/Thr specific protein kinases are involved in the Myeloid cell signaling pathway, which regulates translation, transcription, processing, and secretion of TNF-α along with other pro-inflammatory cytokines. Many serine/threonine-specific protein kinases are involved in these systems, which regulate TNF and other proinflammatory cytokines’ transcription, translation, processing, and secretion. However, it is challenging to develop advanced drugs to target these signaling pathways owing to the complexity in deciding the target protein kinase due to the production of cytokines other than IL-6/12 and TNF-α [REF]."
    },
    "2022-37305348_849_en.txt": {
        "title": "Histone deacetylase inhibitors as a novel therapeutic approach for pheochromocytomas and paragangliomas",
        "prompt": "Abstract: Epigenetic mechanisms, such as DNA methylation and histone modifications (e.g., acetylation and deacetylation), are strongly implicated in the carcinogenesis of various malignancies. During transcription, the expression and functionality of coding gene products are altered following the histone acetylation and deacetylation. These processes are regulated by histone acetyltransferases (HATs) and histone deacetylases (HDACs), respectively. HDAC inhibitors (HDACis) have been developed as promising therapeutic agents, to limit exposure to traditional and toxic chemotherapies and offer more alternatives for some specific malignant diseases with limited options. Mechanistically, these agents affect many intracellular pathways, including cell cycle arrest, apoptosis and differentiation, and their mechanism of action mainly depends on the type of cancer. Currently, five HDACis have been approved for the treatment of several hematological malignancies (e.g., T-cell lymphoma subtypes and multiple myeloma); while, many of them are tested for further therapeutic indications in solid tumors (e.g., colorectal, thyroid, breast, lung and pancreatic cancer). Herein, we review the literature and gather all available evidence, from in vitro and in vivo data to clinical trial results, that recognizes the antitumor activity of HDACis on pheochromocytomas and paragangliomas; and supports their clinical implementation in the treatment of these rare neuroendocrine tumors at metastatic setting.\n\n1. Introduction\n",
        "text": "Pheochromocytomas (PCCs) and paragangliomas (PGLs) are rare neuroendocrine tumors that arise from chromaffin cells and frequently secrete one or more catecholamines. Pheochromocytomas arise from the adrenal medulla, whereas PGLs originate from extra-adrenal sympathetic or parasympathetic ganglia [REF]. The diagnosis may be disregarded during life and be discovered in 0.05%–0.1% of autopsies [REF]. Usually, PCCs are detected by chance (21.1%–57.6%) and constitute approximately 4%–8% of all adrenal incidentalomas [REF]. Updating the previous epidemiological data where 10% of PCCs/PGLs were identified as malignant [REF], all PCCs/PGLs are now considered potentially metastatic [REF], and all patients should be advised for genetic counseling [REF]. Currently, 25%–30% or more of these tumors are attributed to genetic background [REF]; at least 15 PCC/PGL-related genes have been recognized, and 12 syndromes have been described [REF].The diagnosis of PCC/PGL is based on detecting urinary metanephrines [REF]. Following the biochemical diagnosis, CT scanning should be performed [REF]. At the same time, functional imaging should also be used in suspicion of metastatic disease, including positron emission tomography (PET)/CT with various radiotracers [REF] and 123I-metaiodobenzylguanidine (123I-MIBG) scintigraphy, especially to recognize those patients that could also be treated with 131I-MIBG [REF]. After a multidisciplinary team consideration, most PCCs and PGLs can be treated surgically [REF]. Still, for some unfit cases, therapy with 131I-MIBG could also be a reasonable option, if 123I-MIBG scintigraphy is positive [REF]. For metastatic PCCs/PGLs, different combinations of conventional chemotherapy, mainly the regimen cyclophosphamide, vincristine and dacarbazine, have been used for many years [REF], but recently, novel agents, including tyrosine kinase inhibitors [REF], somatostatin analogs [REF], hypoxia-inducible factor (HIF) inhibitors, mTOR inhibitors, histone deacetylase inhibitors (HDACis), DNA-alkylating agents and immune checkpoint inhibitors [REF] are under testing for the treatment of metastatic/unresectable setting of these rare neuroendocrine tumors.Among those approaches, the inhibition of histone deacetylation via HDACis has been entered into the focus of this study. The imbalance between histone acetylation and deacetylation can epigenetically change the expression of tumor suppressor genes and/or proto-oncogenes [REF], that control cancer evolution and progression [REF]. So far, 18 human HDACs have been identified into two families according to the implicated co-factor [REF]. Different classes of HDACs are located in different cellular compartments [REF]. An overview of the classification of human HDACs, their cellular localization and their tissue expression is presented in Table 1 [REF].HDACs are overexpressed in hematologic and solid malignancies, and their inhibition became a promising anti-cancer theory. HDACis include both natural and synthetic compounds. Some HDACis selectively inhibit specific HDAC classes while others are pan-HDAC inhibitors [REF]. HDACis increase acetylation of both histone and non-histone proteins to a significant degree, resulting in cell cycle arrest, cell differentiation, induction of cell death/apoptosis (e.g., oxidative stress generation, disruption of mitosis and mitotic cell death, autophagy, etc.), as well as blocking of angiogenesis [REF]. Fig. 1 depicts the multiple mechanisms of action of HDACis [REF]. The main HDACis under clinical testing, their targeted HDACs, their chemical nature and their approved indications are presented in Table 2 [REF]. The antitumor activity of these agents depends on the specific type and stage of cancer, the characteristics of each patient and the administered dose [REF].Abbreviations: CTCL: Cutaneous T-cell Lymphoma; PTCL: Peripheral T-cell Lymphoma; DLBCL: Diffuse Large B Cell Lymphoma; MM: Multiple Myeloma; AML: Acute Myeloid Leukemia; HCC: Hepatocellular Carcinoma; HL: Hodgkin Lymphoma; NHL: Non-Hodgkin Lymphoma; CML: Chronic Myeloid Leukemia; CLL: Chronic Lymphocytic Leukemia; NSCLC: Non-Small-Cell Lung Carcinoma.In this review, we summarize the research background and the development status of currently tested HDACis for treating metastatic/unresectable PCCs/PGLs and gather all published evidence from in vitro and in vivo studies up to clinical trials, supporting their implementation in oncological practice. The most important clinical trials investigating the use of HDACis in the treatment of PCCs/PGLs and NETs are presented in Table 3."
    },
    "2022-37305350_648_en.txt": {
        "title": "The remodeling roles of lipid metabolism in colorectal cancer cells and immune microenvironment",
        "prompt": "Abstract: Lipid is a key component of plasma membrane, which plays an important role in the regulation of various cell biological behaviors, including cell proliferation, growth, differentiation and intracellular signal transduction. Studies have shown that abnormal lipid metabolism is involved in many malignant processes, including colorectal cancer (CRC). Lipid metabolism in CRC cells can be regulated not only by intracellular signals, but also by various components in the tumor microenvironment, including various cells, cytokines, DNA, RNA, and nutrients including lipids. In contrast, abnormal lipid metabolism provides energy and nutrition support for abnormal malignant growth and distal metastasis of CRC cells. In this review, we highlight the remodeling roles of lipid metabolism crosstalk between the CRC cells and the components of tumor microenvironment.\n\n1. Introduction\n",
        "text": "Colorectal cancer (CRC) is the most common malignant tumor of digestive tract in the world. Its incidence rate ranks the third in the incidence rate of all cancers and the second in the death caused by cancer [REF]. It has been reported that the incidence of CRC is positively correlated with socioeconomic development. By 2030, the number of CRC cases in developed countries is projected to increase to 2.2 million and the number of deaths to 1.1 million [REF]. In recent years, although some progress has been made in the early diagnosis and systemic treatment of CRC, the five-year survival rate of patients with CRC is still only about 50%. This is mainly because CRC often has no obvious symptoms in the early stage, and most CRC patients are diagnosed at a late stage and even have metastases. However, the mechanism of the occurrence, development and invasion of CRC is still not completely clear. Therefore, searching for detection methods and tumor markers for early diagnosis of CRC, and further exploring the exact molecular mechanism of its occurrence, development and invasion are hot topics in current research. Epidemiological studies have found that diet, obesity and diabetes are risk factors for CRC [REF]. A large number of studies have shown that the abnormal lipid metabolism of cells is involved in the occurrence and development of colorectal cancer, and is significantly related to the clinical treatment effect and prognosis [REF].Lipids are hydrophobic macromolecules, which are divided into several groups according to structure of ketoacyl and isoprene: FAs (fatty acids), phospholipids, TGs (cholesterol, triglycerides), sphingolipids, and cholesteryl esters [REF]. Lipids are essential nutrients for cells, acting as the structural components of cell membranes, material transport, energy suppliers, signaling molecules, apoptosis and other aspects [REF]. Abnormal lipid metabolism refers to the abnormal anabolism and catabolism of lipids in the body, resulting in too much or too little lipids in each tissue, thus affecting the body function [REF]. Although normal cells regulate anabolic and catabolic pathways to adapt to changes in nutrient supply, tumor cells can exhibit uncontrolled proliferation even in the presence of nutrient deficiency. The tumor microenvironment is hypoxic, acidic, and nutrient deficient, resulting in metabolic reprogramming of tumor cells and adjacent stromal cells to promote tumor cell survival, proliferation, and metastasis [REF]. Many studies have shown that the abnormal lipid metabolism in cells is significantly related to the induction and metastasis of cancer. At the same time, clinical studies have shown that the abnormal lipid metabolism is closely related to the poor prognosis of CRC patients [REF]. Both malignant progression and accelerated proliferation of CRC cells require more energy, which induces changes in lipid metabolism to allow CRC cells survive. Abnormal lipid metabolism causes changes in various genes and proteins, as well as the dysregulation of cytokines and signaling pathways [REF]. Attentionally, lipid metabolism in CRC cells can be regulated not only by intracellular signals, but also by various components in the tumor microenvironment, including various cells, cytokines, DNA, RNA, and nutrients including lipids [REF].In this review, we discussed the changes of lipid metabolism in colorectal cancer and its role in the genesis and development of colorectal cancer, and also discussed the remodeling of lipid metabolism pathway in CRC cells and tumor microenvironment. The review provides a summary for better understanding and targeting lipid metabolism therapy and improving prognosis."
    },
    "2023-37264452_3665_en.txt": {
        "title": "The controversial effect of smoking and nicotine in SARS-CoV-2 infection",
        "prompt": "Abstract: The effects of nicotine and cigarette smoke in many diseases, notably COVID-19 infection, are being debated more frequently. The current basic data for COVID-19 is increasing and indicating the higher risk of COVID-19 infections in smokers due to the overexpression of corresponding host receptors to viral entry. However, current multi-national epidemiological reports indicate a lower incidence of COVID-19 disease in smokers. Current data indicates that smokers are more susceptible to some diseases and more protective of some other. Interestingly, nicotine is also reported to play a dual role, being both inflammatory and anti-inflammatory. In the present study, we tried to investigate the effect of pure nicotine on various cells involved in COVID-19 infection. We followed an organ-based systematic approach to decipher the effect of nicotine in damaged organs corresponding to COVID-19 pathogenesis (12 related diseases). Considering that the effects of nicotine and cigarette smoke are different from each other, it is necessary to be careful in generalizing the effects of nicotine and cigarette to each other in the conducted researches. The generalization and the undifferentiation of nicotine from smoke is a significant bias. Moreover, different doses of nicotine stimulate different effects (dose-dependent response). In addition to further assessing the role of nicotine in COVID-19 infection and any other cases, a clever assessment of underlying diseases should also be considered to achieve a guideline for health providers and a personalized approach to treatment.\n\n1. Introduction\n",
        "text": "Coronaviruses are a large family of viruses in humans and animals, including Middle East Respiratory Syndrome (MERS-CoV), Severe Acute Respiratory Syndrome (SARS-CoV), and coronavirus disease 2019 (SARS-CoV-2 or COVID-19). Last two decades, several outbreaks of coronaviruses are observed. The SARS outbreak was reported in 2003 with about 8000 cases and more than 700 deaths with an estimated case fatality rate (CFR) of 10% [REF]. According to WHO (Jan 2020), MERS has infected about 2500 cases with more than 800 deaths (CFR ≈ 34.3%) in different countries since 2012 [REF]. The report of WHO showed the novel coronavirus pneumonia, COVID-19, has infected about 177,108,695 confirmed cases with 3,840,223 deaths until 18 June 2021 [REF], and any protective or treatment factor is demanding for health managers.Currently, there are contradictory reports about the effect of smoking on COVID-19 infections. If smokers are protective of COVID-19, new studies could be designed to explore the mechanism of protection. Nevertheless, if smokers are at higher risk, it will be a wake-up call for health providers to adopt separate policies to protect smokers. Currently, we know that smoking is one of the main risk factors for about 20 types and subtypes of cancers, cardiovascular diseases, Chronic Obstructive Pulmonary Disease (COPD), and diabetes. Among the various body parts, the most involvement is directly connected with the respiratory system, digestive system, and nervous system [REF]. According to the 2025 estimation of the World Health Organization (WHO), there will be about 1.1 billion smokers worldwide [REF]. International statistics show that there are about 8 million deaths each year, mostly (80%) in low- and middle-income countries, equivalent to a total economic cost of US$1.4 trillion each year [REF].Currently, it is clear that current and former smokers had a higher percentage among COVID-19 patients in ICU (~ 2.4 times) and mechanical ventilation support, and they had higher mortality rates and more severe cases (~ 1.4 times) [REF]. Even after treatment, exacerbations of COVID-19 are associated with tobacco smoking in the smokers’ group [REF]. Hospitalized smokers have more severe infections and complications, especially pulmonary fibrosis, as the central pathology of SARS, MERS, and COVID-19 [REF]. Like MERS, a higher CFR in hospitalized patients with smoking history is also reported for COVID-19 [REF].Moreover, tobacco smoking is involved in more severe complications of COVID-19 infections [REF]. It has been shown that cigarette smoke upregulates ACE2 in a dose-dependent manner in lung epithelium, including the goblet, club, Clara cells, and alveolar type 2 (AT2) cells of human and rodent lungs, and quitting smoking decreases ACE2 expression (reversible) [REF]. So, smoking history is a predictor of ACE2 expression in the lung cells. The expression of Cathepsin B, which is a protease that activates the spike protein of the virus, is also increased in mice and human cells exposed to cigarette smoke [REF]. So, it seems that the patients’ positive smoking history (current and former smokers) can make them more susceptible to COVID-19. Also, results regarding the effect of smoking on the expression of ACE2 are opposite; besides up-regulation (preprint) [REF], down-regulation [REF] is reported [REF].On the other hand, based on the current epidemiological studies on COVID-19, China [REF], Italia [REF], Europe, and the United States [REF] have reported lower incidence of infected smokers. Tajlil et al. had performed a meta-analysis on 12 reports of COVID-19 epidemiologic data (11,382 cases, including 3,827 cases in 10 Chinese reports and 7,555 cases in two USA reports). They had concluded the significantly lower proportion of hospitalized COVID-19 patients with smoking history [REF]. It should be added that all the current smokers, when entering the hospital or the ICU, are forced to stop nicotine uptake. Besides, a large meta-analysis of over 17,278,392 COVID-19 infected adults showed a lower incidence of infection in smokers [REF], which is concordant to former findings in China [REF], France [REF], and the United States [REF]. However, several studies showed adverse effects of smoking on COVID-19 outcomes such as studies in Kuwait (1096 cases), England (3179 cases), and USA (12,347 cases) [REF]. There is no solid evidence about the protective effect (prevention or treatment) of smoking on COVID-19 infection. The important thing to remember about epidemiological studies is that they have limitations, especially in this field. These limitations include things like the heterogeneity of COPD disease, the heterogeneity of COVID-19 disease, and the restrictions connected to it discussed the characteristics of epidemiological investigations, such as sample sizes and ascertainment bias [REF].In this study, we first tried to systematically show that cigarette smoke effects, although similar in some effects, are different from pure nicotine. Therefore, many experiments on the effect of smoke conclude their results as a probable effect of nicotine as an equivalent univalent. These conclusions can be misleading. Additionally, different cell responses to nicotine are dose-dependent. Considering the prevalence of the COVID-19 epidemic and different published results in this field and the existing judgments, an attempt was made to investigate the role of cigarettes and nicotine in this epidemic disease. Failure to assign this issue can cause misleading and diversion of people and health providers. Therefore, in this study, we have tried to take a closer basic-clinical look at this issue.In the first step, articles containing the MeSH terms of “smoke” and “COVID-19” and “meta-analysis” were investigated only in PubMed (16 Jul 2021) (Fig. 1-a). Meta-analysis studies with more than 10 studies were summarized in Tables 1 and 2.\nFig. 1PRISMA table for (a) smoke and COVID-19 and (b) nicotine and COVID-19.\nPRISMA table for (a) smoke and COVID-19 and (b) nicotine and COVID-19.\nTable 1The impact of smoking on the severity and mortality of COVID-19 patients in the different meta-analysis studies (> 10 studies)NumFirst authorSearch historyNumber of studiesNumber of casesResultRef.1Roengrudee Patanavanich28 April 202019 papers (16 from China, 1 from Korea, and 2 from the United States)11,590 patients:Smoking is a risk factor for COVID-19 progression (OR 1.91).[REF]2Huimei Zhang1 February 2021109 articles (27 USA, 50 china, and 32 other countries)517,020 patients• Smoking elevated the risk of ICU admission and death in patients with COVID-19, but was not relevant to mechanical ventilation.• Former smokers had a risk of progressing COVID-19 severity compared with current smokers. Current smokers were significantly associated with the severity of COVID-19 compared with non-smokers.[REF]3Adinat Umnuaypornlert12 December 202040 studies (19 China, 1 Kuwait, 1 Korea, 1 Mexico, 1 Japan, 2 Spain, 3 Italy, and 12 USA)369,287 patientsSmoking, even current smoking or former smoking, significantly enhances the risk of COVID-19 severity and death.[REF]4Rohin K. ReddyBetween 1 December 2019 and 2 June 202047 studies (32 China, 10 USA, 2 Italy, 1 UK, 2 International)32, 849 patientsCurrent smoking is risk factor for disease progression, severity and mortality in hospitalized patients with COVID-19.[REF]5Qianwen ZhaoBetween December 2019 and 22 March 202011 studies (11 china)2002 patientsCOPD and current smoking could develop severity of COVID-19.[REF]6Tao ZhangBetween 1 January 2020 and 10 April 202016 studies (16 china)1,172 severe patients and 2,803 non-severe patients• The prevalence of former smokers was higher in severe patients as compared to non-severe ones.• The COVID-19 severity could be assessed by radiologic and laboratory findings, and smoking history[REF]7Linwen ZengBetween 1 December 2019 and 2 May 202017 studies (16 China, 1 USA)5,726 confirmed casesSmoking did not enhance the risk of cardiovascular disease in COVID-19 patients in this study (probably due to small sample size, 3 studies, and large heterogeneity).[REF]8Jingyuan XieBetween 1 January 2020 and 18 March 202090 studies (87 China, 1 Australia, 1 Singapore)16,526 patientsMale gender, history of smoking, and comorbidities might affect the prognosis of COVID-19 patients.[REF]9Guiling XiangBetween 1 December 2019 and 10 June 202020 studies15,408 patientsCurrent smoking and elderly patients, aged 60 years old or over, are related with a higher risk of in-hospital death.[REF]10E. H. TaylorBetween 1 January 2020 and 6 December 202058 studies (15 China, 7 USA, 6 Spain, 3 UK, 2 Africa, 25 other countries)44,305 patientsIncreasing age, smoking, pre-existing comorbidities, and the host response to COVID-19 disease were associated with mortality.[REF]11David Simons25 August 202032 studiesNot reportedIn comparison with never smokers, current smokers seem to be at decreased risk of SARSCoV-2 infection whilst former smokers seem to be at elevated risk of hospitalisation, enhanced COVID-19 severity and mortality.[REF]12Angelo Silverio27 April 202045 studies (35 China, 6 USA, and 4 other countries)18,300 patients• Male and smoking did not significantly influence mortality.• Older age and diabetes are related to greater risk of in-hospital mortality in COVID-19 patients[REF]13Saeed ShoarBetween 20 December 2019 and 15 March 202012 studies (12 China)1,845 patientsSmoking and co-morbidities (hypertension, diabetes mellitus, cardiovascular diseases) are associated with COVID-19 mortality.[REF]14Changcheng ShiBetween 1 December 2019 and 29 April 202027 studies (24 China, 2 USA, and 1 Italy)23,860 patientsOlder age, sex (male), smoking (current smokers), pre-existing comorbidities (chronic kidney, respiratory, and cardio-cerebrovascular diseases), some symptoms (dyspnea), and some abnormal laboratory indicators (inflammation and coagulation markers) were associated with COVID-19 mortality.[REF]15Arthur Eumann MesasBetween December 2019 and 27 July 202060 studies in 13 countries (31 China, 13 USA, 16 other countries)51,225 patientsThere was a greater mortality risk from hospital COVID-19 patients for dyspnoea(pooled OR = 2.5), smoking (pooled OR = 1.6) and some comorbidities and laboratory parameters.[REF]16You LiBetween January and May 202040 studies (18 China, 10 USA, 5 Italy, 7 other countries)Not reported• Male gender, older age, obesity, diabetes and chronic kidney diseases were associated with elevated risks for COVID-19 mortality.• There was no elevated risk of mortality for some factors such as COPD, cancer, or current smoker (probably because of limited data on every of these factors)[REF]17Xinyang LiBetween December 2019 and February 202141 studies (30 China, 3 Korea, 2 USA, 6 other countries)21,060 patientsSevere COVID-19 patients were related to older age, male sex, obesity, history of smoking, hypertension, diabetes, coronary heart disease, chronic kidney disease (CKD), cerebrovascular disease, COPD, malignancy, and chronic liver disease.[REF]18Jia LiBetween December 2019 and 14 April 202012 studies (12 China)2,445 patientsSmoking history and comorbidities such as COPD, diabetes, hypertension, coronary heart disease, cerebrovascular diseases, and malignancy were risk factors for severity of COVID-19.[REF]19Zohra S LassiBetween December 2019 and February 202162 studies (from 44 countries of the six continents)31,016 pregnant women• Older pregnant women (> 35 years), obesity, smoking, diabetes and pre-eclampsia could increase the risk of severe COVID-19.• Severe COVID-19 women increased the risk of preterm birth.[REF]20Antonios KaranasosBetween 1 September 2019 and 4 May 202022 studies (20 China, 2 USA)7,171 patientsThe risk of severe COVID-19 was significantly greater among smokers, especially in younger patients without diabetes.[REF]21Shiwei KangBetween 1 Jan and 6 October 202021 studies (17 China, 1 USA, 1 Japan, 1 England, 1 Italy)7,041 patients• Smoking history increased the mortality of COVID-19 patients (OR = 1.91).• Cardiovascular disease enhanced the severity (OR = 2.87) and mortality (OR = 3.05) of COVID-19 patients.[REF]22Ian Huang25 March 2020.23 studies (22 China and 1 Japan)3,099 patients• Age was associated with lymphopenia in COVID-19 patients (lymphopenia was higher in younger patients compared with older ones).• There was no association between lymphopenia and gender, cardiac comorbidity, hypertension, diabetes mellitus, COPD, and smoking.[REF]23Hongjie HouBetween 15 January 2020 and 12 April 202173 articles (30 USA, 6 Italy, 7 England, 6 China, 6 Mexico, 4 Spain, and 14 other countries)863,313 patientsSmoking elevated the mortality risk in COVID-19 patients.[REF]24Askin GülsenBetween December 2019 and 15 April 202016 studies (14 China, 1 USA, 1 unknown)11,322 patientsCurrent smoking was significantly associated with severe COVID-19.[REF]25Stefano Figliozzi24 April 202049 studies (from China, Italy, Spain, France, Germany, Netherlands, Iran, and South Korea, USA)587 790 and 602 234 cases statistics for age and sex.• Older age, male gender, some co-morbidities such as acute cardiac or kidney injury, lymphocytopenia, and some laboratory biomarkers (D-dimer) elevated the risk of mortality in COVID-19 patients.• Smoking was not a predictor of the risk of death, it could increase adverse outcomes in COVID-19 patients.[REF]26Zhaohai Zheng20 March 202013 studies (13 china)3,027 patients• The proportion of male, aged more than 65, smoking patients were statistically significant higher in critical/mortal group in comparison withthe non-critical group.[REF]27Diana C. Sanchez-RamirezBetween 1 January and 15 April 202022 studies (21 China and 1 USA)13,184 patients• The incidence of pulmonary diseases and smoking (current and former smoking) were significantly related to severe COVID-19 outcomes.[REF]28Alqahtani, J. S., et al. (2020)March 24, 202015 studies (14 China, 1 United States)2473 patients• Increased severity (63%) and mortality (60%) in infected COPD patients.• Increased severity in infected current (22%) and ex-smokers (46%).[REF]\nThe impact of smoking on the severity and mortality of COVID-19 patients in the different meta-analysis studies (> 10 studies)• Smoking elevated the risk of ICU admission and death in patients with COVID-19, but was not relevant to mechanical ventilation.• Former smokers had a risk of progressing COVID-19 severity compared with current smokers. Current smokers were significantly associated with the severity of COVID-19 compared with non-smokers.• The prevalence of former smokers was higher in severe patients as compared to non-severe ones.• The COVID-19 severity could be assessed by radiologic and laboratory findings, and smoking history• Male and smoking did not significantly influence mortality.• Older age and diabetes are related to greater risk of in-hospital mortality in COVID-19 patientsThere was a greater mortality risk from hospital COVID-19 patients for dyspnoea(pooled OR = 2.5), smoking (pooled OR = 1.6) and some comorbidities and laboratory parameters.• Male gender, older age, obesity, diabetes and chronic kidney diseases were associated with elevated risks for COVID-19 mortality.• There was no elevated risk of mortality for some factors such as COPD, cancer, or current smoker (probably because of limited data on every of these factors)• Older pregnant women (> 35 years), obesity, smoking, diabetes and pre-eclampsia could increase the risk of severe COVID-19.• Severe COVID-19 women increased the risk of preterm birth.• Smoking history increased the mortality of COVID-19 patients (OR = 1.91).• Cardiovascular disease enhanced the severity (OR = 2.87) and mortality (OR = 3.05) of COVID-19 patients.• Age was associated with lymphopenia in COVID-19 patients (lymphopenia was higher in younger patients compared with older ones).• There was no association between lymphopenia and gender, cardiac comorbidity, hypertension, diabetes mellitus, COPD, and smoking.• Older age, male gender, some co-morbidities such as acute cardiac or kidney injury, lymphocytopenia, and some laboratory biomarkers (D-dimer) elevated the risk of mortality in COVID-19 patients.• Smoking was not a predictor of the risk of death, it could increase adverse outcomes in COVID-19 patients.• The proportion of male, aged more than 65, smoking patients were statistically significant higher in critical/mortal group in comparison withthe non-critical group.• Increased severity (63%) and mortality (60%) in infected COPD patients.• Increased severity in infected current (22%) and ex-smokers (46%).\nTable 2Prevalence of smoking in COVID-19 patients in the different meta-analysis studies (> 10 studies)NumFirst authorSearch historyNumber of studiesNumber of casesResultRef.1Biruk Beletew AbateBetween 1 January 2020 and 27 March 202057 studies (52 china, 1 UK, 1 Italy, 1 Africa, 1 Japan, 1 Korea)221,195 patientsA high prevalence of symptomatic COVID-19 was observed in men than women. One reason for the high prevalence of SARS-Cov-2 in men may be due to excessive cigarette and alcohol use in men.[REF]2Ashkan Baradaran7 April 202033 studies (32 China and 1 Taiwan)9,249 patientsThe most prevalent finding in the confirmed patients with COVID-19 was hypertension, diabetes mellitus, cerebrovascular disease, cardiovascular disease, chronic kidney disease, chronic liver disease, chronic pulmonary disease, malignancy and smoking of the patients.[REF]3Jesus González-Rubio28 April 202018 studies (15 China, 2 USA, and 1 Italy)7,671 patientsThe percentage of hospitalized current smokers was significantly lower than the smoking prevalence in each country.[REF]4Konstantinos Farsalinos25 April 202030 studies (24 China, 4 USA, 1 South Korea, 1 Japan)6,515 patients• Low current smoking prevalence among hospitalized COVID-19 patients.• Odds of adverse outcomes were greater in hospitalized current smokers in comparison with non-current smokers’• Odds of adverse outcomes were lower in hospitalized current smokers in comparison with former smokers.[REF]5Konstantinos Farsalinos1 April 202013 studies (13 China)5,960 patientsThere was a low current smoking prevalence among hospitalized COVID-19 patients compared with smoking prevalence in the general Chinese papulation.[REF]6Kunchok Dorjee31 August 202077 studies (35 China, 18 USA, 10 Europe, and 5 Asia)38,906 patientsSmoking and several diseases such as hypertension, diabetes, and heart disease were higher prevalence among COVID-19 patients compared with the general USA population.[REF]7Francesco Del Sole28 May 202012 studies (11 China and 1 Netherland)2,794 patients• Smoking, male gender and several diseases such as cerebrovascular disease, COPD, cardiovascular disease, diabetes, hypertension were related to severe disease.• Elevated level of some markers such as procalcitonin, D-Dimer and thrombocytopenia predicted severe of disease.[REF]\nPrevalence of smoking in COVID-19 patients in the different meta-analysis studies (> 10 studies)• Low current smoking prevalence among hospitalized COVID-19 patients.• Odds of adverse outcomes were greater in hospitalized current smokers in comparison with non-current smokers’• Odds of adverse outcomes were lower in hospitalized current smokers in comparison with former smokers.• Smoking, male gender and several diseases such as cerebrovascular disease, COPD, cardiovascular disease, diabetes, hypertension were related to severe disease.• Elevated level of some markers such as procalcitonin, D-Dimer and thrombocytopenia predicted severe of disease.Also, articles containing the MeSH terms of “nicotine” and “COVID-19” were systematically searched in the online databases of PubMed, Google Scholar (Fig. 1-b). Then, all the articles were initially evaluated, and the preliminary design for the signaling and metabolic pathway involved in coronavirus infection and smoking/nicotine responses was performed. We performed a systematic search on (SARS-CoV-2 OR COVID-19) in PubMed and manually reviewed all the publications. Subsequently, all related articles were reviewed and included in this review (2021, June). Since the articles related to the study of the effect of nicotine in COVID-19 were very sparse, two other searching strategies regarding the effect of pure nicotine on different (A) cells, including epithelial, fibroblast, endothelial, and dendritic, macrophages, T, and B cells, and (2) organs, including respiratory, nervous, metabolic, cardiovascular, and urogenital systems. Generally, 12 diseases were candidate according to their similarity with the pulmonary and extrapulmonary complications of COVID-19. Also, Finally, the effect of smoking/nicotine on other common coronaviruses (SARS and MERS) was also evaluated and added to the study."
    },
    "2023-37265492_702_en.txt": {
        "title": "Effects of exercise treatment on functional outcome parameters in mid-portion achilles tendinopathy: a systematic review",
        "prompt": "Abstract: Exercise interventions are evident in the treatment of mid-portion Achilles tendinopathy (AT). However, there is still a lack of knowledge concerning the effect of different exercise treatments on improving a specific function (e.g., strength) in this population. Thus, this study aimed to systematically review the effect of exercise treatments on different functional outcomes in mid-portion AT. An electronic database of Pubmed, Web of Science, and Cochrane Central Register of Controlled Trials were searched from inception to 21 February 2023. Studies that investigated changes in plantar flexor function with exercise treatments were considered in mid-portion AT. Only randomized controlled trials (RCTs) and clinical controlled trials (CCTs) were included. Functional outcomes were classified by kinetic (e.g., strength), kinematic [e.g., ankle range of motion (ROM)], and sensorimotor (e.g., balance index) parameters. The types of exercise treatments were classified into eccentric, concentric, and combined (eccentric plus concentric) training modes. Quality assessment was appraised using the Physiotherapy Evidence Database scale for RCTs, and the Joanna Briggs Institute scale for CCTs. The search yielded 2,260 records, and a total of ten studies were included. Due to the heterogeneity of the included studies, a qualitative synthesis was performed. Eccentric training led to improvements in power outcomes (e.g., height of countermovement jump), and in strength outcomes (e.g., peak torque). Concentric training regimens showed moderate enhanced power outcomes. Moreover, one high-quality study showed an improvement in the balance index by eccentric training, whereas the application of concentric training did not. Combined training modalities did not lead to improvements in strength and power outcomes. Plantarflexion and dorsiflexion ROM measures did not show relevant changes by the exercise treatments. In conclusion, eccentric training is evident in improving strength outcomes in AT patients. Moreover, it shows moderate evidence improvements in power and the sensorimotor parameter “balance index”. Concentric training presents moderate evidence in the power outcomes and can therefore be considered as an alternative to improve this function. Kinematic analysis of plantarflexion and dorsiflexion ROM might not be useful in AT people. This study expands the knowledge what types of exercise regimes should be considered to improve the functional outcomes in AT.\n\n1. Introduction\n",
        "text": "Achilles tendinopathy is a type of degenerative tendon disease that causes functional impairment and morbidity [REF]. According to epidemiological data, a prevalence of 2.16 cases per 1,000 patient-years was estimated in the general population and 6.2%–9.5% in athlete population [REF]. According to its anatomical location, it is divided into two primary categories: insertional, which occurs at the calcaneus Achilles tendon junction, and mid-portion, which occurs 2–6 cm proximal to the calcaneus [REF]. The characteristic of mid-portion tendinopathy is diffuse or localized swelling, degenerated tendon morphology and pain caused by repetitive loading without adequate compensation of the plantar flexor muscle function [REF].In consequence, one of the most effective management strategies for mid-portion AT have been proposed to be exercise-based therapy [REF]. To date, several exercise interventions have been reported for AT, such as the “Alfredson protocol” which is also known as heavy eccentric calf training [REF], concentric training [REF], the “Stanish protocol” [REF], and the “Silbernagel protocol” [REF]. These interventions can be classified as three different loading protocols: eccentric, concentric, and combined (eccentric plus concentric) training, respectively. Among those, eccentric loading has emerged as one of the primary conservative approaches for AT rehabilitation over the past twenty years. It is hypothesized to influence the formation in carboxyterminal propeptide of type I collagen, boosting tendon volume and tensile strength [REF]. Eccentric exercises may also have the potential to reduce the neovascularization and related nerve ingrowth being responsible for pain development [REF]. A recent meta-analysis examined the relationship between eccentric exercise treatments and tendon volume in healthy and pathological tendons [REF]. In healthy tendons no significant immediate volume changes following acute exercise interventions were apparent. In pathological tendons, immediate and short-term volume reductions were reported, while no long-term adaptations were found as investigated in one study only. Based on the limited number of studies examining long-term changes in tendon morphology, the proclaimed effect of eccentric training on tendon volume remains inconclusive [REF].Research indicates that both concentric and combined exercise treatments can also be effective in improving plantar flexor functions such as power (e.g., height of countermovement jump) [REF] in the AT population, which is believed to occur through the strengthening of the affected tendon [REF]. Therefore, it remains unclear whether the effects of eccentric loading are different from other types of loadings, such as slow concentric loading [REF]. The primary cause for these similar adaptations may be explained by the “time-under-tension” hypothesis, which states that positive adaptations could be achieved regardless of contraction type, as long as the mechanical strain is performed slowly and heavy enough [REF]. Since the tendinopathic regions can be subjected to mechanical strain that restores normal fibril alignment and cell morphology, different types of loading may have a comparable impact to eccentric exercise.In conclusion, different types of exercise treatments have reported positive findings in varying functional outcomes in addition to eccentric training [REF]. Nevertheless, there is a lack of knowledge concerning the effect of specific exercise treatments for improving specific functional outcomes (e.g., strength) in mid-portion AT. Being able to differentiate the effects of different types of exercises on functional outcomes might allow for more tailored treatment in people with AT. Therefore, the purpose of this systematic review was to analyze the effect of various exercise treatments (eccentric, concentric, combined) on different functional outcomes (strength, power, range of motion, balance) in people with mid-portion AT."
    },
    "2023-37265594_679_en.txt": {
        "title": "Status of food colorants in India: conflicts and prospects",
        "prompt": "Abstract: Food colorants are imperative ingredients for attracting consumers and in deciding their preferences. Here we discuss the current status of natural colorants and synthetic food colorants on the Indian market by appraising the growth of the food colorant market both globally and nationally, based on published case studies on synthetic food colorants (SFCs), rules, and regulations implemented by Food Safety and Standards Authority of India on natural food colorants and SFCs. The substantial lacunae in the research on the impacts of SFCs in the Indian population identified through our literature survey signify the scope and need for appraisal of the issues prevailing in the Indian food colorant market as well as the necessity of renewing the food colorant policies. The illegal use of banned food colorants, the adulteration of natural food colorants, mislabelling of SFCs as natural colorants, and the permitted use of internationally banned food colorants, as well as the unawareness among consumers are serious issues recognized. Appropriate labelling to denote natural food colorants' presence, renewed standards of policy to determine the permitted use of food colorants, comprehensive regulations for the production and use of natural food colorants, stringent rules to constrain the production of toxic SFCs are obligatory to breakdown the dilemma on the Indian food market. Most importantly, awareness and responsiveness should be generated among consumers regarding the illegal use and adulteration of colorants and the need to use natural colorants. We also recommend a logo to designate the presence of natural colorants which will aid the consumers to make the right choice.\n\n1. Introduction\n",
        "text": "Any food product is instinctively scrutinized from a visual sense before deciding on purchasing or consumption. Color remains one of the most prominent visual cues contributing to the sensory aspect of foodstuff. There is significant research underscoring that the color of the food psychologically manipulates impelling the expectation of flavor generated in our brain before tasting the food [REF]. Consumers’ inclination to a particular food item is primarily visual specific to the color of the food which aids the consumers to predict the flavor and taste of the food. Interestingly, the perception of colour is deep-rooted and comes intuitively to human beings. For ages, the colour of fruits, vegetables, and meat has remained a determinant factor to distinguish raw from ripe and fresh from old or spoilt.The implication attached to colour to the food impacted the decision of worldwide food manufacturers to add in a variety of colour additives. The natural colorants derived from natural sources including plants and microbes were used to impart colour to food which was later replaced by synthetic colorants. Natural colorants are of two categories: organic (derived from living sources) and inorganic (gold, silver). Synthetic food colorants are chemicals processed from coal tar compounds and most of them contain dyes from the azo group [REF]. Moreover, natural identical man-made colours like riboflavin are also available [REF].A considerable hike in demand for packaged food fostered the massive use of food colorants. Henceforth, food colorants are added to every packaged and processed food product sold on the markets and almost every food item sold or manufactured by a food industry or a food selling outlet. Additionally, prepared food, e.g. in restaurants and other small food-selling outlets, contains food colouring agents to enhance their visual appeal to entice consumers. However, health hazards reported predominantly in children triggered by these fascinating synthetic colours need to be addressed [REF]. The colossal use of permitted and non-permitted food colorants, their toxicities, and associated adulterations are global concerns with varying gravity in different countries.India is considered a hub of food diversity in terms of the taste, smell, and colour of foods. The Indian food industry is gaining copious profit by marketing different geographical food styles. Bright attractive colours are the distinctiveness of Indian cuisines which aid the growth of the synthetic food colorant (SFC) industry [REF]. Additionally, the market for natural food colorants is on the rise due to an increase in health awareness among consumers [REF]. Besides the potential adverse health effects of SFCs, this hike led to another serious misbranding of natural colorants as synthetic. The misuse of the label ‘natural’ to attract consumers, the adulteration of natural colorants, and the lack of proper legal regulations in the processing of natural colours are serious issues demanding the imperative attention of authorities. There is a pressing need for proper labelling to distinguish the presence of natural colorants as well as to implement systematic standards on the processing and use of natural colorants. We have proposed a label for distinguishing the incorporation of natural colorants in food. Even though India is known for its multi-hued foods and natural resources of colours [REF], hardly a few studies have been published regarding the status of SFCs and natural colorants.Here we discuss the rapid growth of the Indian food colorant market, the reports on side effects, and the associated regulations in a comprehensive manner. Stringent regulations and government-funded research initiatives are required to tackle together this growing menace."
    },
    "2023-37265625_8834_en.txt": {
        "title": "“Blockchain technology in food safety and traceability concern to livestock products”",
        "prompt": "Abstract: Livestock products share more than fifteen percent of total agri-foods traded worldwide. A global increase in food demand has increased the risk to food safety. Improvements in food quality, cold chain transit, and preservation are required for safe livestock products. Though, the food safety and regulation authorities demand complete food traceability from farm to fork, but in traditional supply chain it is ignored by fiddling with the transit paperwork and bill invoices. The process of supply chain reformation and activities linked to food recalls during food safety issues are insanely expensive and challenging. Traceability-driven food supply chain management is likely to implement novel technologies like the Internet of Things (IoT). The capability of the Blockchain era within the food sector is emerging with use cases across different regions, as shown via the growing number of studies. Credibility, efficiency, and safety are all improved when food products can be instantly traced from their point of origin through all points of contact on their way to the consumer. Blockchain assures a tamper-proof and transparent system that allows an innovative business solution, together with smart contracts. However, there are significant difficulties with the implementation of blockchain technology for food traceability. It necessitates more and more training platforms as well as trainers, who can make understanding and operability of this technology easy among ground-level participants and food entities. For the tactical application of this technology, it is essential to comprehend the legal and regulatory framework.\n\n1. Introduction\n",
        "text": "Animal product industrialization has grown speedily, which also leads to development in the scale and scope of animal husbandry and breeding. The world's trade in animal products accounted for sixteen percent of the whole agri-food market. The value of animal products that were traded internationally went from €56 billion in 2000 to €152 billion (in current currency) in 2018 [REF]. Animal products are becoming more and more popular, so large-scale livestock farming and the food chains that support it have become very important. Research have been carried out to analyse a range of manufacturing factors that affect the quality of livestock products, as well as attempts, were made to optimize and boost the quality from all facets. As seen over the previous decade, following food security, it is mandatory to produce adequate safe food. With the increase in the demand for food needed, the threat to food safety is growing and wants extra attention [REF]. World Health Organization [REF] stated that the safeguarding of food quality and safety continuously poses new challenges for the food sector and science and millions of people suffer every year due to contaminated foods. The manufacturing of quality products is one of the features of food security, which dictate methodical research aimed at monitoring the existing quality regulations and standards, initially from grass and fodder production, their processing, animal feeding, animal products manufacturing, transportation, and up to the purchaser's basket. In industrialized countries, food safety is of major concern where consumers are more and more demanding superior quality and safer foods [REF]. The livestock products processing necessitates improvements in food quality and taste, cold chain transportation, and preservation. However, it is complicated to manage the quality of animal products in the traditional supply chain.At the same time, the nuisances raised include various types of food frauds, because fraudsters usually have a high level of cleverness and scientific knowledge, which makes it complicated to blame the perpetrators. Inadvertent contamination and residues in the end products occur repeatedly regardless of established high standards of hygiene and measures for quality assurance [REF]. The global economy faces massive social and economic loss due to mislabelled, misbranded, contaminated, and adulterated food products. The food supply chain, including on-farm rearing, processing, storage, shipping, and retail chain, can be contaminated by microorganisms due to improper management and pose a threat to the contamination of foods [REF]. An expected 600 million humans or nearly one of 10 people on the earth, fall unwell after consuming contaminated food products [REF]. Genomic testing and analysis are appropriate for the detection of meat speciation as well as a genetically modified ingredient or product [REF]. Based on Proteomics, animal species, technological processes or storage-dependent changes can be well understood. Metabolomics analyses, which are generally done by mass spectrometry (MS) or nuclear magnetic resonance (NMR) spectroscopy and iso-topolomics techniques are pertinent for indication of origin or means of manufacture [REF]. However, comparatively huge instrumentation and laboratory infrastructure is mandatory in these testing methods and is usually time-consuming as well as expensive even though some parameters can be rapidly tested on-site with the use of rapid test kits. Such defined analytical procedures are followed in the food industry in addition to that, implement by food regulations authorities [REF]. Additionally, the food regulation and safety standards authorities insist on complete traceability of foods from farm to fork. So far, the intermediates/supervisors can find a way to escape from these claims by manipulating transport documentation and bill invoices. In the international food trade system, the food recall-related work and supply chain reformation process during the food safety issues are shown to be expensive, time-ingesting, and very difficult.The food traceability system assembles, stores and transmits adequate history of livestock products, from the rearing of livestock on a farm up to the final product in the consumer's basket, at all stages inside the Food Supply Chain (FSC) in order to check the product for quality control as well as safety and can be traced forward and backward direction whenever required [REF]. Traceability is well thought-out as a new quality indicator in the food sector. Case-sensitive information storage and handling becomes mandatory in the food industry. Regulations are imposed by food authorities to allow monitoring and identification of all raw materials and substances utilized in food product preparation [REF]. These types of necessities have been incorporated by many FSC participants but a number of them still rely on a non-automated paper-based system. The food sector makes use of a traceability system for the enhancement of FSC and facilitates the traceback for food quality and safety [REF]. Traditional technologies for record-keeping can offer some solutions to the above issues, but that can't clear up all the issues. Therefore, it is strongly desired to introduce a new edge think tank as well as advanced technologies that create possibilities to solve product quality and supply chain problems.Traceability-driven FSC management in the food sector is likely to implement novel technology like the Internet of Things (IoT). IoT-associated applications provide up-to-date info about products as well as contamination facts in the course of production and distribution [REF]. IoT-enabled programs and applicable technology such as Radio Frequency Identification (RFID) possibly will revolutionize the food sector by digitizing facts to be queried and managed in real-time [REF]. The application of blockchain technology has recently experienced a remarkable boom. The capability of the blockchain era within the food sector is only starting to emerge with use cases across different regions, as shown via the growing number of studies, trials, research initiatives, and projects [REF]. Early researchers indicated that blockchain technology has to be used in the food sector for two reasons: a greater efficient food tracing system, which may promptly recognize a source of food infection while a food safety crisis happens; the second is to construct purchaser's faith in food safety via blockchain technology [REF]. Blockchain assures a tamper-proof, transparent, shared, and protected system that can allow an innovative business solution, in particular together with smart contracts [REF]. Blockchain is a virtual and distributed transaction ledger, maintained with the aid of multiple peer-to-peer computing machines that are not depending upon a trusted third party. Every transaction information file (block) is controlled thru unique software program systems that permit the records to be transmitted, processed, saved, and represented in human-readable form [REF]. This article explains blockchain technology, how it works with food safety and traceability systems, and how it has recently been included in the initiatives related to the livestock products sector.In 1982, David Chaum first time proposed a cryptographic concept similar to Blockchain technology [REF]. Haber and Stornetta [REF] further described tamper-proof time-stamping of digital documents or information on a cryptographically secured chain of blocks. Bayer et al. [REF] integrated Merkle trees in this design to improve efficiency by permitting numerous document certifications to be composed into one block.The concept of BCT was imagined by a person (or group of people) in 2008 using the name Satoshi Nakamoto to provide a digital and decentralized public transaction ledger of the crypto-currency: Bitcoin. Nakamoto stepped forward the layout by employing a Hash cash-like technique to timestamp blocks without the involvement of trusted central authorities or the financial sector. More than ten years have passed ever since the release of the virtual currency “Bitcoin: A Peer-to-Peer Electronic Cash System” by using the pseudonymous creator [REF]. Blockchain Technology is coming into recognition as the underlying digital infrastructure on which cryptocurrencies work. A cryptocurrency (e.g. Bitcoin) is a digital currency that can be used in the same way as international or national currency like the USD or Indian rupee. Throughout the last decade, Blockchain has passed through extensive progress, which is a digital currency (Blockchain 1.0), digital economy (Blockchain 2.0), and digital society (Blockchain 3.0).Blockchain is a distributed, decentralized, and jointly verifiable ledger technology consisting of digital records called a block. Blockchain technology (BCT) particularly connects blocks thru cryptographic strategies (Fig. 1). Each data block consists of information regarding a particular system and creates virtual signatures to authenticate the validity of facts and links to the subsequent data block to form the main chain that is referred to as Blockchain [REF]. The information on a blockchain is immutable, which makes it a justifiable technology for industries like finance, cybersecurity, healthcare, food supply chain, and public services [REF]. The BCT makes use of a hash algorithm. The first block will act as the genesis block, additionally known as the block header. The beginner (genesis) block mainly includes the Hash, Mine difficulty; Nonce (number only used once) for mining data, Timestamp, and Merkle tree root records [REF]. Through this, the individual blocks are linked together and protected against manipulations. The blockchain is distributed on the computers of all subscribers (which act as a node) so that a “peer-to-peer network” is obtained. All participants (stakeholders) keep eye on the data and verify the data (consensus principle) to be added to the block at the same time by the addition of a timestamp to the document/information [REF]. The data on the block become tamperproof by the addition of a hash algorithm of the previous block and finally a new hash algorithm generated for that block. In this way, it forms a chain between block to block by hash algorithm. The hash procedure is a one-way code word technique and guarantees that the transaction data/information becomes tamperproof [REF]. If someone changes data/information on the block, it will automatically change the timestamp of the block so that the created hash value will change and data/information turns out to be invalid [REF]. The Merkle tree root on the blockchain accumulates the values of all nodes and these nodes contain the hash value for data/information present on blocks [REF].Fig. 1Blockchain formation.Fig. 1Blockchain formation.It refers to the regulation of permission to the digital register (ledger) and associated services. Based on reads and write control allowed to participants over block data, it is classified into three types.1)Public Blockchain: It enables everyone to access the network. Energy usage is high. Access to the block information is permitted for each node (Participant) on the blockchain network. These digital ledgers are distributed and freely accessible. The contract or transaction can be validated by any node, and information is made publicly available. The public blockchain is very anonymous when it comes to the participant's identity [REF]. The majority of cryptocurrencies commonly use it.2)Private Blockchain: A genuine invitation is a sole way for a participant to function as a node. Data privacy is ensured since only specific nodes on the system could access block data. It is not a completely decentralized system. The controller permits the nodes to view the data. It functions as a centralised, cryptographic-based organisational network, e.g. Multichain [REF]. It is applicable to sectors like the banking system, securities foundations, etc …3)Permissioned Blockchain: Access to over-block data is only permitted by the system's verified and trusted nodes who meet the required standards. The blockchain network is controlled by authorized members, each of whom has a specific role and defined authorization. It combines elements of both public and private blockchains. The likelihood of a Sybil attack is very low [REF]. Most of the industrial sectors like the health-care sector, food sector, education sector, real-estate sector, agricultural sector, etc … can have applicability of permissioned Blockchain [REF].Public Blockchain: It enables everyone to access the network. Energy usage is high. Access to the block information is permitted for each node (Participant) on the blockchain network. These digital ledgers are distributed and freely accessible. The contract or transaction can be validated by any node, and information is made publicly available. The public blockchain is very anonymous when it comes to the participant's identity [REF]. The majority of cryptocurrencies commonly use it.Private Blockchain: A genuine invitation is a sole way for a participant to function as a node. Data privacy is ensured since only specific nodes on the system could access block data. It is not a completely decentralized system. The controller permits the nodes to view the data. It functions as a centralised, cryptographic-based organisational network, e.g. Multichain [REF]. It is applicable to sectors like the banking system, securities foundations, etc …Permissioned Blockchain: Access to over-block data is only permitted by the system's verified and trusted nodes who meet the required standards. The blockchain network is controlled by authorized members, each of whom has a specific role and defined authorization. It combines elements of both public and private blockchains. The likelihood of a Sybil attack is very low [REF]. Most of the industrial sectors like the health-care sector, food sector, education sector, real-estate sector, agricultural sector, etc … can have applicability of permissioned Blockchain [REF].Almost every transaction is distributed via the network of computing machinery that runs a blockchain procedure and is required to be authenticated by all computer nodes [REF]. The consensus algorithm (consensus mechanism) allows a network of computers (nodes) to work together aiming that all contributors obtain identical copies of the distributed database files. The “Proof of Work” (PoW) and “Proof of Stakes” (PoS) are mainly used consensus algorithms in blockchain technology.The well-known cryptocurrency: “Bitcoin” uses a “Proof of Work” (PoW) and it requires computer nodes (miners) to solve the difficulty level of mining before authenticating transactions and adding data/information to block in Blockchain [REF]. The first computer that is able to solve difficulty level (mathematical algorithm) as well as determine the hash and values authenticated by others, receives a number of bitcoins. This practice is repeated every 10 min [REF] (Bohme et al., 2015). The probability to solve mathematical algorithm increases with an increase in numbers and networks of computers (miners) and at the same time, miners guarantee the network's inaccessibility against hackers. The miners compete continuously for computer power that springing up a race between individual miners. So in the PoW mechanism, the requirement of hardware, computer power and energy is very high that making it costly [REF].As an alternative to this, another mechanism that got momentum is “Proof of Stake” (PoS) with the emergence of the Ethereum system in 2015, which is established on a different algorithm that does not necessitate special hardware equipment and higher computing power [REF]. In this mechanism, the validating power is given to individuals/bodies who possess coins within the system and place them “on stake” during transaction authorization. The computing nodes are identified as “validators” rather than “miners” [REF]. To validate blocks and generate hashes, participants are chosen at random, and the stake volume maintained determines the likelihood of the random selection. So this mechanism is resource-saving and secure [REF].Other consensus mechanism includes Proof of Elapsed Time (PoET), Byzantine Fault Tolerance (BFT), Proof of Authority (PoA) [REF] Delegated Proof of Stakes, Steller Consensus Protocol, Ripple, Raft, Proof of Burn, Proof of Personhood, etc … as described in details by Mingxiao et al. [REF], Zubaydi et al. [REF] and Cachin and Vukolic [REF].There are numerous blockchain platforms (i.e Bitcoin, Ethereum, Hyperledger, Agnostic, BigChainDB, Corda, Credits, Multichain, Openchain, Quorum, Stellar and Symbiont Assembly, etc …) working on any one of the consensus mechanisms as described in detail by Cachin and Vukolic [REF]. Ethereum and Hyperledger are the most popular blockchain platforms experimented with or deployed in the groceries/agri-food supply chain and among these two, as time passed from the year 2015 to the year 2020, the Hyperledger blockchain took the lead over Ethereum [REF].Since the inception of blockchain technology, it has increased interest in its application to various sectors because of building up trust among stakeholders [REF]. In the start-up, it was intended to maintain and record financial transactions among participants, and with the technological improvements, the application area for BCT has been broad up internationally [REF]. Globally, more and more attention is given to research in this area by various organizations and countries. The revolutionizing changes applied in this technology have increased its application in digital data authentication and signature, controlling and storing organizational records, IPR and patent tracking and verification, tracing patient health records, enabling smart contracts, real estate management and record of ownership transfer, tracing products through the supply chain from producers to consumers [REF]. Szabo [REF] introduced the concept of smart contracts and now a day, it is vastly applied in this technology under the term Blockchain 2.0, which made tempering or censoring of data impossible. A contract is an agreement having a legal object entered into willingly by two or more bodies/parties. Each of them takes part to create one or more authorized obligations between them. A smart contract is a digitalized computer protocol that allows an agreement to be automatically executed with predefined conditions and authenticates as well as administers the performance of a contract or averts the need for a contractual clause [REF]. A frequently used example for simple understanding is the task of drinks vending machine, which dispense a predefined volume of drink after the required money is entered into the machine [REF]. The application of smart contracts in blockchain technology could be widely used in food supply chain (FSC) management systems to solve various food safety and traceability-related problems. The data interoperability, auditability, transparency, cost-reduction, tracing of products, authenticity and integrity are the important benefits of BCT application with smart contracts in the FSC traceability system [REF].The main area of application of BCT in the food sector concern to traceability system in the supply chain but along with this it can be also valuable for tracking and identification of the point of food fraud and authentication of food safety in the FSC management system from farm to fork. The BCT effectively promotes food safety and purchaser's reliance by timely and tamperproof sharing of data of product just like batch number, location and date of production, food safety certification, real-time hygienic and sanitary condition of production site [REF]. In FSC the food passes through multiple locations starting from production, processing, transportation, distribution, and retail store up to the consumer. The involvement of intermediators can make the food transactions vulnerable to food fraud and costly in above said supply chain [REF] because of improper maintenance of all records about food products from farm to fork. So till date, the current system of FSC is inefficient and unreliable [REF]. The use of various kinds of sensors and digital technologies (i.e. RFID tags, NFC, Automatic Identification and Data Capture (AIDC) systems, mechanical and biological sensors, time-temperature indicators, smartphones, digital signatures, etc …) [REF] can produce real-time information about environmental, production, processing, packaging, storage, transportation and distribution condition of food products. All real-time data records can be transferred to blocks using BCT and smart contracts after being verified and validated by all participants using a consensus process. This serves as an immutable way to deposit data records that are accepted by all participant parties [REF]. The smart contracts in this system can be executed for management of food safety standards, food quality certifications, application of HACCP, Good Agricultural Practices and Good Manufacturing Practices and other standards as and when required at various stages of food transactions [REF], [REF], [REF]. If the required procedure and standards from production to the consumer's basket do not meet, the smart contract will automatically dismiss the food supply process and information is not allowed to enter in Blockchain [REF]. At last, all the information about the product from production to retail store can be available to the consumer by scanning a QR code labelled on the product which increases the consumer's trust and is also useful to manage food quality and safety from farm to fork. The participant parties of the food chain include producers, processors, logistic suppliers, retailers and purchasers. Each one act as a node and every node can see the information on the public service platform without knowing the private details of the parties/bodies [REF]. Each participant's private details can be stored in a permissioned chain whereas the data which should be released is stored in a public chain. This permit parties to access, write and confirm with each other and safeguard participants' privacy to the maximum level possible [REF].The production, processing and handling of foods in a hygienic way to safeguard human health is known as food safety. The growing international trade of foods increases the difficulty to maintain food quality assurance and safety [REF]. Globally, animal food safety and quality are of main concern. The safety issues in animal products are mainly due to the use of hormones, antibiotic residues, adulteration, zoonosis, microbial contamination as well as from animal feed with pesticides or herbicides residues, heavy metals and other contamination [REF]. Drinking contaminated water can cause illness in animals which require veterinary medicines to treat their illness so along with the safety of animal feed, the provision of safe and potable drinking water is also necessary.The handling of animals and temperature directly affect the safety of animal products mainly meat products. Stress due to fluctuation in environmental temperature during the transportation of animals to the slaughterhouse can cause quality problems in meat products after slaughter [REF]. Illness in animals before slaughter, Improper handling of carcass or meat products during processing and packaging leads to microbial contamination and temperature fluctuation increases the microbial load, which causes abnormality in meat products quality [REF]. In the same way, the production, collection, transportation, and processing of milk and milk products also require proper attention. Therefore, real-time monitoring and recording of temperature handling of animals and relative humidity from farm to fork are necessary to support products safety and quality as well as to improve the management, productivity and profitability which can be possible by application of BCT along with sensors as described earlier [REF]. So in the FSC, the stage where standards criteria of process and regulatory and production standards do not match, the parties are informed in a real-time manner to correct the process and further supply chain can be stopped or recall of that particularly affected batch of animal products can be carried out very fast and easily in a time-saving and cost-effective manner that leads to maintaining buyer's faith in safety and quality of product and trust in the company [REF]. The feed provided to animals, administration of veterinary drugs as well as antibiotics and species, breeds and quantity of animals raised and managed by farmers are also necessary to record on the blockchain. As the data on BCT is verified by a consensus mechanism, if the inconsistent and improper or illegal amount of drugs, hormones and antibiotics used by farmers, the smart contract automatically terminates the transaction and data is not documented in Blockchain [REF]. The recording of genomic and proteomic information of farm animals on the blockchain can provide immutable data about whether the animal products produced are from the same animals or different [REF] leads to quality assurance and food safety. Food industries can minimize food frauds by real-time detection and relating outbreaks to their definite cause [REF]. In this way, the distributed ledger technique like blockchain technology permits more determined and feasible control of food safety and quality.Locating product-related information starting from animal rearing up to product dispatch in a retail store in FSC is known as food traceability. It includes data for animal breeding, nutrition, health, production and management on farms, processing and preservation related data of product, packaging and logistic information and finally up to the consumer's basket [REF]. Now a day, the proof of food integrity and transparency is also inquired about by stakeholders as well as purchasers. The real-time tracking of all data can increase the speed of the supply chain and decrease food fraud in all stages of the product's life cycle. But in current, online and centralized monitoring or offline and paper-based traceability systems, data infringement can be possible by supervisors or intermediators of supply chain management. BCT offers an immutable and consensus mechanism-driven online data recording system in the supply chain at all stages of food production which makes the traceability system concrete, irreversible and unchallengeable by any means [REF]. BCT-driven food traceability can be simply illustrated as a three-layer flow (Fig. 2) where the physical flow layer is for the physical food supply chain in which food products go on their way from producer to purchaser. The second flow layer is for a digital record system in which data recording of the physical flow layer is going on simultaneously with the use of various types of sensors, RFID, QR code and other digital technologies as described earlier. The third flow layer is a digital blockchain infrastructure network in which employing a smart contract, the data recorded in the second flow layer is verified by all participant bodies/parties with a consensus algorithm and forms a virtual data block at every stage of the supply chain. This flow layer also follows the physical flow layer and forms a chain of digital data blocks [REF]. If any person tries to hack or change information recorded, the hash value of that particular block changes and that does not permit the altered value to enter into that particular blockchain which means it is an immutable record-keeping platform. The food product on its way to the consumer's basket passes through various points of production and at every point some data is recorded to the blockchain as described below.A.Stockmen: They provide information about the rearing of livestock on a farm, Data in details recorded on blockchain about feeding, breeding, health, production and management practices followed by them and tag number and ownership details of all livestock whether any animal is directly purchased from the market or not. The hygienic and environmental conditions of farm and animal welfare practices can also be recorded.B.Manufacturing parties: The information about plant practices, handling of livestock, the hygienic and sanitary condition of a plant, standards and regulations maintained at the plant, equipment availability, utilization and disinfection procedure followed at the plant, data regarding processing methods, use of packaging technologies, labelling details of every batch of product, etc. The data about the financial transaction between manufacturer and stockmen as well as between a manufacturer and logistic supplier is also recorded on a block.C.Logistic supplier (Distributors): Data regarding transportation method used, shipping details, transit period, and real-time data recorded about temperature, humidity and other product environmental conditions during transportation period. The financial deals between logistic suppliers and authorized dealers should also be recorded.D.Dealers (Wholesalers): Real-time data about storage condition (i.e. time, temperature and humidity) of product up to its supply to retailers. Transportation details can also be recorded. The monetary transaction between dealers and retailers is also recorded.E.Retailers: Data about the current stock of each product, storage details, expiry date, and period at the store up to the consumer's purchase is recorded on the Blockchain.F.Buyer: The buyer scan the QR code labelled on the product to get all the details of the product starting from farm to purchase with the use of an internet connection to have an idea about the quality, safety and reliability of the product purchase.Fig. 2Blockchain technology driven food traceability.Fig. 2Stockmen: They provide information about the rearing of livestock on a farm, Data in details recorded on blockchain about feeding, breeding, health, production and management practices followed by them and tag number and ownership details of all livestock whether any animal is directly purchased from the market or not. The hygienic and environmental conditions of farm and animal welfare practices can also be recorded.Manufacturing parties: The information about plant practices, handling of livestock, the hygienic and sanitary condition of a plant, standards and regulations maintained at the plant, equipment availability, utilization and disinfection procedure followed at the plant, data regarding processing methods, use of packaging technologies, labelling details of every batch of product, etc. The data about the financial transaction between manufacturer and stockmen as well as between a manufacturer and logistic supplier is also recorded on a block.Logistic supplier (Distributors): Data regarding transportation method used, shipping details, transit period, and real-time data recorded about temperature, humidity and other product environmental conditions during transportation period. The financial deals between logistic suppliers and authorized dealers should also be recorded.Dealers (Wholesalers): Real-time data about storage condition (i.e. time, temperature and humidity) of product up to its supply to retailers. Transportation details can also be recorded. The monetary transaction between dealers and retailers is also recorded.Retailers: Data about the current stock of each product, storage details, expiry date, and period at the store up to the consumer's purchase is recorded on the Blockchain.Buyer: The buyer scan the QR code labelled on the product to get all the details of the product starting from farm to purchase with the use of an internet connection to have an idea about the quality, safety and reliability of the product purchase.Blockchain technology driven food traceability.So, a food traceability system on a Blockchain platform ensures tamperproof data collection and real-time monitoring of livestock products in the supply chain and no need to wait for a company or business partner's authorization to access information of products. The possible advantages of blockchain technology in food safety and traceability are briefly illustrated in Fig. 3.Fig. 3Benefits of Blockchain technology in food safety and traceability.Fig. 3Benefits of Blockchain technology in food safety and traceability.Soon after the inception and promotion of BCT, it started to be used in supply chain management systems and there are so many shreds of evidence that prove its beneficial application [REF]. In global supply chain management, this technology is predicted to grow at an annual growth rate of 87% and escalate from $45 million in 2018 to $3314.6 million by 2023 [REF].Globally, a number of large animal products manufacturing food enterprises have already started to incorporate BCT in their food supply chain. First time ever in 2016, the use of BCT in the animal products supply chain was initiated by U.S.-based retailer company Walmart to monitor pork products in China and produce imported to the U.S. via Latin America [REF] (Insights, 2017). It has resulted that the tracing of pork products on their route to the U.S. takes a few minutes related to several days taken in past and also provides all the details of pork from farm to Walmart retail store in the U.S. [REF]. IBM launched its blockchain technology using a software system namely “IBM Food Trust” and it was started to be applied by many big grocery/food companies like Walmart, Nestle and Unilever [REF]. This type of large food sector entities are also involved their suppliers to practice blockchain in their supply of raw materials to the company. The cold-chain monitoring in the supply chain was developed by ZetoChain using the Internet of Things. It was stated that cold-chain problems are identified in real-time and the responsible parties are informed quickly for fast action on the issue. The Zeto tags can be scanned by the purchaser to locate product history [REF]. In the pilot project carried out in 2016 by U.K.-based tech start-up “Provenance”. The company tested “Tuna” fishing and supply to traders, processors, brands, and supermarkets on a blockchain platform. This digital identity also traces the audit info that verifies that fish were caught legitimately and sustainably [REF].Blockchain-based tracing of turkeys from farms to their shopper's stores was developed by Cargill Inc. [REF]. In 2017, the Chinese e-commerce company Jingdong (JD.com) deployed Hyperledger Fabric – an open-source blockchain platform-based supply chain first time in partnership with Kerchin, an Inner Mongolian-based beef manufacturer [REF] where the purchaser can find the history like animals used, nutrition info, food safety test reports and slaughtering related information. In 2018, the primary emphasis of the company is to monitor beef supply from Australia and so JD.com declared a plan to adopt BCT for its meat supply chain [REF]. Similarly, Alibaba, another China-based e-commerce giant, announced a trial program to track global food consignments sold to China, on its online marketplace T-Mall, which were supplied by Australian healthcare supply firm Blackmores and New Zealand dairy product maker Fonterra [REF]. A research project was conducted in Australia to track beef production from origin to consumption with the use of BCT based “BeefLedger” platform [REF] which can be useful to reduce the time required to trace infected stock which ultimately improves efficiency and food safety. BCT-based trading of meat subscription boxes was deployed by The Grass Roots Farmers' Cooperative in 2017 to inform consumers in a trustworthy way about the rearing conditions of their animals. In the pilot project performed in San Francisco, where cartoons of chicken distributed were labelled with QR codes that link to the story of the meat they contain [REF]. Intel presented the Hyperledger Sawtooth blockchain platform's operating model for seafood supply chain traceability in 2017 [REF].The blockchain-based Pacific Islands' tuna fishing traceability project was announced by World Wildlife Foundation (WWF) in 2018. Through this, fishermen can register their fishing on a network platform by RFID and fish scanning which eliminate illegal activities [REF]. The French retailer company, Carrefour announced a plan in 2018 to adopt BCT-directed food traceability for its own branded Filiere Qualite Carrefour (FQC) products like salmon fish, eggs, tomato, chicken, fresh milk, cheese, etc … and planned to expand its application to all FQC products by 2022 as well as indicated that BCT can be a good solution to reduce uncertainty about quality and ingredients by access to detailed information of food product [REF]. In a report about Blockchain's use in the Irish beef supply chain, the American company Deloitte said that smart farming technologies like smart feeding equipment and cow collars could share data with Blockchain to shed light on the cows' diet. Through the use of this technology, we will be able to track the whereabouts of all Irish beef that changes hands. In addition to increasing openness, this action reflects Ireland's dedication to beef safety and quality. The use of smart contracts in trade finance has the potential to boost productivity and cut costs for international trade. Using their smartphones, customers can scan a QR code that provides in-depth information on the product's path to the shelf [REF]. In 2018, a pilot project was deployed by “Perutnina Ptuj” - the largest poultry producer in South-eastern Europe, to provide trustworthy info to their consumers about the rearing condition of poultry, in partnership with OriginTrail: A BCT provider [REF]. In 2019, U.S.-based Bumble Bee foods adopted a seafood traceability system functioning on a blockchain platform in collaboration with German tech company SAP. In this system, a consumer can scan a QR code to access info like products' origins, the size of the catch, the point of capture, shipping history, and trade certification [REF]. To build up a shrimp supply chain and fortify customer trust in the product, Walmart Inc. announced a pilot project in 2019 for end-to-end shrimp traceability from the Indian state of Andhra Pradesh to the U.S. [REF]. Topco Associates, LLC, a leading U.S. food cooperative, employed Envisible's Wholechain traceability technology in 2019 to help member-owner supermarkets trace and highlight seafood origins [REF]. Bumblauskas et al. [REF] have described a case study of public blockchain-based egg supply chain traceability used by Bytable Inc. in partnership with “farmers' hen house” in which consumers can get information about a particular farmer from whom the eggs came to their basket like farm photos, management practices and certifications that followed in egg production system by scanning QR code on an egg carton. Ripe Technologies, a software firm that offers B2B solutions to grocery stores in order to boost their store and omni-channel performance, has partnered with Neogen, a food safety company based in the United States, to strengthen the link between animal genomics, feed use, and food safety. Ripe's president and co-founder Phil Harris has said that systems like blockchain can be used to back up claims of value addition and production labels. Production could be streamlined with the use of additional digital tools like sensors, the IoT, machine learning, and artificial intelligence [REF]. In keeping with the present push of many sectors towards the Internet of Things era, Leme et al. [REF], investigate the usage of a unique infrastructure. They presented a system that is efficient, secure, decentralized, and dispersed, which will modernise the way the cattle sector for beef and milk production operates in predominantly rural areas. They mentioned that while the internet is widely available currently, it is still undeveloped and expensive in rural regions, making widespread deployment unfeasible for most farmers. Surjandari et al. [REF], tested a permissioned blockchain-based system for the halal industry utilising Hyperledger Fabric and raft consensus. The Blockchain Network of Halal Meat Ordering Systems solves traceability issues. This case study proved effective for protecting halal transaction data on the blockchain. The “tampered-proof” feature gives end users transparency into the halal supply chain. To boost trust in the international beef supply chain between Australia's cattle farmers, processors, and Chinese customers, Cao et al. [REF], prototyped a blockchain-based human-machine reconcile method for beef supply chain traceability. BeefLedger, a commercial developer and provider of a blockchain-enabled beef provenance tool built on the Ethereum platform and the proof-of-authority (POA) protocol, commissioned and co-led the case-based design. This Blockchain-credentialed traceability prototype inspires more faith and confidence in Australian beef products among Chinese customers. As a result of their findings, they have decided to further develop, apply, and test the human-machine reconcile mechanism with a wider range of producers, consumers, and supply chain stakeholders such as customs, agri-departments, quarantine, and regulatory authorities. Through integration with TraceX's blockchain-powered traceability technologies, a dairy foods company based in India, “Milk Mantra”, is able to record the digital path of milk in real time, allowing for the creation of reliable consumer brands. Technology makes it possible for all parties in the dairy value chain to share democratic data in a safe and secure way. This addresses consumers' main concern about food safety in the dairy industry [REF]. Dey et al. [REF], developed a FoodSQRBlock (Food Safety Quick Response Block) based on blockchain technology with Google Cloud Platform to emulate an actual food production scenario with milk as a livestock produce example, which digitalizes the information involved in food production and makes it approachable, transparent, and traceable by consumers and producers via QR codes. Similarly, Khanna et al. [REF], proposed a blockchain-based platform with the use of smart contracts, QR technology, and IoT to increase the safety and traceability of the dairy supply chain in India, specifically targeting products like milk, butter, and cheese. Besides the issue of food traceability, this aims to maintain the nutritional quality of dairy products by detecting adulteration and contamination and boosting the efficiency of production. According to them, the framework proved helpful in locating and eliminating adulterated and contaminated food products from a supply chain, which helped build trust between unknown partners and made conflict resolution more manageable. Smart contracts help cut down on the expense of supply chain management by doing away with middlemen. Nonetheless, they emphasised the importance of future research for real-time application, as the interconnection of numerous dairy supply chains operating in different regions may present a complexity for smooth operation. After testing it for more than a year, Dong Nai, Vietnam's largest pork-producing province, put TE-FOOD's traceability system into use all over the country in 2022. The goal of the project is to build a high-tech system for identifying, managing, and tracing animal products and to improve the management and quality control of breeding, slaughtering, transportation, trade, and consumption in the province [REF].Many other experiments and prototypes were proposed to build an IoT and blockchain-based supply chain traceability system for livestock products [[REF], [REF], [REF], [REF], [REF], [REF]]. From these studies, we can deduce that implementing a conceptual framework for animal product supply chain traceability can greatly enhance the efficiency, security, and accountability of supply chain information, boost food quality and safety, and cut down on logistical losses by means of monitoring, tracing, and management. Although the area is still in its infancy and the vast majority of research are still in the design phase, the results of various experiments suggest that a sizeable portion of the effort is already in the implementation or piloting stage. Sendros et al. [REF], noted that just 14% of published studies on agricultural blockchain applications address the potential for use in the production and distribution of livestock products. There are several other projects/initiatives that were deployed on a pilot basis or permanent basis in the agricultural food supply chain other than livestock products [19,50,[REF], [REF], [REF], [REF], [REF], [REF]] that show the very good impact of BCT-driven food supply chain traceability and food safety.The application of blockchain technology in the food supply chain has shown many benefits, but it is necessary to determine the current challenges faced in the application of this technology in the food supply chain. Development of the Blockchain technology is still in a nascent stage, so the availability of Blockchain specialists is less as compared to other software developers. This leads to an increase in financial load for the deployment of this technology as it was noted that a blockchain specialist charges very high fees as compared to other software technologists [REF]. So, it's no longer possible for new entrepreneurs or small and medium-sized businesses (SMEs) to use BCT. The skills and awareness regarding BCT are still deficient worldwide [REF] and in the livestock food supply chain, the ground-level participants are mainly farmers, who do not have proficiency in advanced technologies, which necessitates the increasing number of training platforms as well as trainers, who can make understanding and operability of this technology easy among the ground-level participants and food entities. In the growing phase of this technology, it is required to demonstrate the scalability, speed, and security to deploy it in the food supply chain [REF]. Sometimes, the immutability of data stored on a blockchain can create obstacles to the smooth performance of the food supply chain, as stated by Hald and Kinra [REF].This is the digital ledger technology, which requires a large number of computers and hardware to mine and store data, which leads to an increase in the initial cost of implementation along with the cost of the validation process. However, this challenge can be overcome by using intelligent concepts in a digital framework as described by Cocco et al. [REF]. Globally, the public thought of the BCT as a technology used for cryptocurrencies. The high volatility and large daily changes in the value and market share of cryptocurrencies have a negative psychological effect on BCT's reputation [REF]. The homogeneity of knowledge and understanding among technical experts and policymakers is still a deficit. The legal conduct of smart contracts in a distributed ledger technology (DLT) framework, particularly in relation to disputes such as enforceability, jurisdiction, the applicability of legal principles of contract law, etc., is still an important and vexed question [REF]. The confidentiality of information is still a boundary condition as industries resist sharing their private information with competitors in the market [REF]. The lack of common standards may act as a hurdle to the implementation of BCT in the real-life food sector. So, the harmony and regularisation of different blockchain platforms in the food sector are added challenges.Sustainable, adequate, and safe food supply chain advocacy has also helped bring bioeconomy into the spotlight. The term “bioeconomy” refers to the process by which renewable biological resources are produced and transformed into goods of worth, including waste streams. Livestock production is one of the main production sectors to which bioeconomy would be most closely linked [REF]. In order to make the transition to a bioeconomy, it is important to recognize the myriad of interactions that can arise between various governmental objectives and the food industry as well as consumers, which may be solved with blockchain-driven innovations by providing framework for traceable, verifiable, and complete communication of bio-farming practices to consumers who are increasingly wanting to make eco-friendly consumption choices [REF]. Blockchain technology within the bioeconomy of the livestock products industry in general does not appear to have attracted the attention of researchers as of yet. More work needs to be done in this area of study, and perhaps in the future an in-depth content analysis of the blockchain-based bioeconomy of livestock output will need to be attempted."
    },
    "2023-37265708_973_en.txt": {
        "title": "Cell cycle activation in thyroid hormone-induced apoptosis and stem cell development during  intestinal metamorphosis",
        "prompt": "Abstract: Amphibian metamorphosis resembles mammalian postembryonic development, a period around birth when many organs mature into their adult forms and when plasma thyroid hormone (T3) concentration peaks. T3 plays a causative role for amphibian metamorphosis. This and its independence from maternal influence make metamorphosis of amphibians, particularly anurans such as pseudo-tetraploid Xenopus laevis and its highly related diploid species Xenopus tropicalis, an excellent model to investigate how T3 regulates adult organ development. Studies on intestinal remodeling, a process that involves degeneration of larval epithelium via apoptosis and de novo formation of adult stem cells followed by their proliferation and differentiation to form the adult epithelium, have revealed important molecular insights on T3 regulation of cell fate during development. Here, we review some evidence suggesting that T3-induced activation of cell cycle program is important for T3-induced larval epithelial cell death and de novo formation of adult intestinal stem cells.\n\n1. Introduction\n",
        "text": "The development of vertebrate intestine, like many other organs, takes place in two phases, the initial formation of a neonatal/juvenile form and subsequent maturation into the adult form. This second phase often occurs during postembryonic development, a perinatal period when plasma thyroid hormone (T3) level peaks [REF]. This period corresponds the first 2-3 weeks after birth in mouse and metamorphosis in amphibians such as the highly related anurans pseudo-tetraploid Xenopus laevis and diploid Xenopus tropicalis (Note that due to the conservations between the two species, we will simply refer to both as Xenopus unless specified, although earlier studies on anuran metamorphosis were mainly on Xenopus laevis while more recent ones, particularly gene knockout studies, have been on Xenopus tropicalis). Importantly, maturation of the intestine during this second phase appears to be highly conserved [REF]. For example, the mouse intestine has villi but no crypts, where adult stem cells reside, at birth and develop crypts during the first 3 weeks after birth when T3 levels are high. Similarly, the intestine in a premetamorphic Xenopus tadpole, when there is little or no T3, is also simple in structure, consisting of mostly a single layer of epithelial cells, surrounded by thin layers of connective tissue and muscles (Figure 1A) [REF]. As T3 levels rises after stage 54 (about 4 weeks of age) [REF], metamorphosis begins and larval epithelial cells undergo programmed cell death [REF]. Some larval epithelial cells undergo dedifferentiation during metamorphosis to form clusters of cells that proliferate rapidly and express well-known adult intestinal stem cell markers such as Lgr5 by climax of metamorphosis, e.g., stage 61 (about 6-7 weeks of age) (Figure 1A) [REF]. By the end of metamorphosis or stage 66 (about 2 months after fertilization), these proliferating stem cells differentiate to form a multi-folded epithelium surrounded by elaborate connective tissue and muscles [REF]. In the adult frog, the intestinal stem cells are localized at the bottom of the epithelial fold while cell death occurs mainly at the crest of the fold, similar to those taking place in the crypt-villus unit in adult mammalian intestine [REF].\n(A). Schematic diagram of Xenopus intestinal metamorphosis. Both tadpole and frog intestine are structurally simple, consisting of mainly three tissue layers: inner epithelium, connective tissue, and outer muscle layers. The tadpole intestine is much simpler, with only a single epithelial fold, the typhlosole. In contrast, the frog intestine has multiple epithelial folds with elaborate connective tissue and muscle layers. The major events underlying the change from tadpole to frog intestine during metamorphosis include the apoptosis of essentially all larval epithelial cells, as indicated by circles. Concurrently, the adult epithelial stem cells, with high level expression of known stem cell markers such as Lgr5, are formed de novo through dedifferentiation of some larval epithelial cells and rapidly proliferate at the climax metamorphosis, as indicated by the dots. The connective tissue and muscle cells also develop extensively during metamorphosis. (B). Apoptotic and proliferating cells are non-overlapping epithelial cells during T3-induced intestinal metamorphosis. Premetamorphic Xenopus laevis tadpoles at stage 54 were treated with 10 nM T3 for 0, 3, or 6 days and sacrificed one hour after injection with EdU to label proliferating cells. Intestinal cross-sections were double stained for EdU and by TUNEL for apoptotic cells. A higher magnification of the boxed areas labeled with a’-c’ is shown on the right. The dotted lines depict the epithelium-mesenchyme boundary. Note that apoptosis as shown by the TUNEL signal in the epithelium occurred prior to the appearance of the clusters (islets) of EdU-labeled cells and in distinct epithelial cells during T3 treatment (c’). Arrows point to some apoptotic cells. See [REF] for more details.T3 not only has peak levels during postembryonic development but also plays critical roles during this period, with T3 deficiency causing severely developmental problems in all vertebrates including human [REF]. T3 is both necessary and sufficient for anuran metamorphosis. Thus, preventing the synthesis of endogenous T3 allows Xenopus tadpoles to remain in tadpole form for years while wild type animals typically finish metamorphosis by around 2 months of age [REF]. Conversely, treating premetamorphic Xenopus tadpoles with physiological levels of T3 in the rearing water causes precociously metamorphosis. Making use of the ability to easily manipulate anuran metamorphosis by controlling the availability of T3 to tadpoles or even organ or primary cell cultures and the advancement in genetic technologies, especially gene-editing for knockout studies in the diploid Xenopus tropicalis [REF], we and others have been studying the molecular mechanism by which T3 regulates cell fate and tissue transformation during metamorphosis. Here, we review some recent studies on intestinal remodeling, with an emphasis on the potential role of cell cycle activation in larval epithelial cell death and adult stem cell development."
    },
    "2023-37265769_1286_en.txt": {
        "title": "Jiawei Xiaoyao San in treatment of anxiety disorder and anxiety: A review",
        "prompt": "Abstract: Jiawei Xiaoyao San (JWXYS) has shown excellent clinical efficacy in anxiety disorder, but has not yet attracted widespread attention. The animal experiments, clinical trials and mechanism studies of JWXYS were reviewed in this article, which may provide a reference for developing new anxiolytic drugs based on this prescription. The literature was searched in PubMed and CNKI and the documents written in English or with English abstracts were selected. JWXYS could reduce the anxiety symptoms of patients alone and reduce the adverse reactions when it is used in combination with other drugs in the clinic. In preclinical studies, JWXYS also showed therapeutic effects in reducing anxiety-like behavior. The mechanisms may include improving the hypothalamic–pituitaryadrenal (HPA) axis and hormone disorders, increasing neurotransmitter content, neurogenesis, and regulating the synthesis of related enzymes. This article shows that JWXYS could effectively treat anxiety disorders by regulating the central nervous system. In the future, with the participation of more researchers, it is expected to develop innovative drugs for the treatment of anxiety disorders based on JWXYS.\n\n1. Introduction\n",
        "text": "Anxiety disorder is a common mental illness with persistent fear or anxiety (typically lasting six months or more) and related behavioral disturbances [REF]. For a long time in the past, the incidence of anxiety disorder could not be accurately counted until there is a clear diagnostic criterion. In recent years, demographic data in the UK shows that 3% of the population were eligible to be diagnosed with anxiety disorder, but only 8% of those identified were diagnosed with anxiety disorder finally and received relevant treatment [REF]. In particular, anxiety disorders are considered to be one of the most common psychiatric disorders, affecting 15%−20% of young people [REF]. Corresponding to anxiety disorder, anxiety is a relatively widespread type of human mental illness, that has emotions of dread, apprehension, and impending disaster but not disabling as anxiety disorders [REF]. Anxiety can be adaptive as it is a response to danger or potential threats in the environment [REF]. The statistical results show that the global overall prevalence of major depression is 4.7% [REF], and about 85% of the patients also have symptoms of anxiety [REF]. It is important to note that both anxiety disorder and anxiety can have bad consequences [REF]. It is urgent to find a suitable therapeutic schedule for anxiety disorder and anxiety.At present, the treatment of anxiety disorder is still based on drug intervention and there are a variety of drugs to choose from the clinic. The efficacy and tolerance of drugs are different, which made the drug regimen of different patients has certain differences. The first-line medication is chemical drugs, but the price of drugs is generally high, which is a great expense for many patients [REF]. At present, the main purpose of clinical medicine is to relieve symptoms. A total of 50% improvement on the Hamilton Anxiety Scale (HAMA) or being able to live normally after stopping the drugs rather than completely curing the situation are commonly used as the endpoint indicators [REF]. What's more worrying is that these patients still tend to show symptoms of chronic anxiety in the future [REF]. According to the survey, the lifetime prevalence of generalized anxiety disorder worldwide is about 3.7% [REF]. Therefore, it is necessary to find a cheap and effective anxiolytic drug.Traditional Chinese medicine (TCM) may show unique advantages in the treatment of such psychiatric disorders. Jiawei Xiaoyao San (JWXYS) is a traditional Chinese medicine preparation and its composition has been derived from the Internal Medicine Abstract published in 1529 CE (Fig. 1). It is derived from the drug ‘Xiaoyao San’ in the world's first drug standard Taiping Huimin Hejifang, two Chinese herbal medicines were added on this basis. JWXYS is composed of 10 Chinese herbal medicines: Angelicae Sinensis Radix (Danggui in Chinese), Paeoniae Radix Alba (Baishao in Chinese), Poria (Fuling in Chinese), Atractylodis Macrocephalae Rhizoma (Baizhu in Chinese), Bupleuri Radix (Chaihu in Chinese), Moutan Cortex (Mudanpi in Chinese), Gardeniae Fructus (Zhizi in Chinese), Glycyrrhizae Radix et Rhizome (Gancao in Chinese), and Menthae Haplocalycis Herba (Bohe in Chinese), the auxiliary material is Zingiberis Rhizoma Recens (Shengjiang in Chinese) [REF]. Because of its composition, it is also called Danzhi Xiaoyao San in China. Due to its mood-regulating actions, it can be translated as free and easy wanderer plus [REF]. Because the preparation is initially in powder form, San can also be translated as powder (e.g. Jiawei Xiaoyao Powder). It was called Kami-Shoyo-San or Gami-Shoyo-San in Japan and South Korea, respectively [REF]. A variety of dosage forms are widely used in clinical in China, including mixtures, pills, granules, and capsules. There are powder and tablet forms in Kampo medicines in Japan, which are often used in Kampo hospitals [REF]. In the Chinese Pharmacopoeia, it is used as pills and mixtures. In other drug standards, there are granules, capsules, and tablets. In terms of efficacy, the therapeutic effects of these dosage forms are the same [REF].Fig. 1Origin, composition and studies of Jiawei Xiaoyao San (JWXYS). Xiaoyaosan was recorded in the world's first drug standard Taiping Huimin Hejifang. JWXYS was recorded in the Internal Medicine Abstract published in 1529 CE, which is composed of Xiaoyaosan and two other Chinese herbal medicines. JWXYS is composed of 10 Chinese herbal medicines. JWXYS has shown good anxiolytic effects both in pre-clinical and clinical studies.Origin, composition and studies of Jiawei Xiaoyao San (JWXYS). Xiaoyaosan was recorded in the world's first drug standard Taiping Huimin Hejifang. JWXYS was recorded in the Internal Medicine Abstract published in 1529 CE, which is composed of Xiaoyaosan and two other Chinese herbal medicines. JWXYS is composed of 10 Chinese herbal medicines. JWXYS has shown good anxiolytic effects both in pre-clinical and clinical studies.In most of the clinical research on TCM, JWXYS is not prescription preparation, but a prescription by a doctor. The dosage of the ten drugs in JWXYS is adjusted according to the patient’s symptoms. Some other Chinese medicines may be added according to other symptoms of patients. This is in line with the characteristic of TCM: syndrome differentiation and treatment, similar to the concept of precision medicine in modern medicine, which comprehensively considers the patient's condition, physical condition, living environment, and other factors to provide personalized treatment to the patient. In this review, some of the studies followed the concept of syndrome differentiation and treatment and other herbal medicines to the ten herbal medicines, and we will highlight these studies in subsequent articles.This review summarized the existing research on JWXYS in the treatment of anxiety disorder and anxiety which aims to deepen the understanding of the therapeutic effects of JWXYS."
    },
    "2023-37265795_1262_en.txt": {
        "title": "The role of tumor microenvironment in drug resistance: emerging technologies to unravel breast cancer heterogeneity",
        "prompt": "Abstract: Breast cancer is a highly heterogeneous disease, at both inter- and intra-tumor levels, and this heterogeneity is a crucial determinant of malignant progression and response to treatments. In addition to genetic diversity and plasticity of cancer cells, the tumor microenvironment contributes to tumor heterogeneity shaping the physical and biological surroundings of the tumor. The activity of certain types of immune, endothelial or mesenchymal cells in the microenvironment can change the effectiveness of cancer therapies via a plethora of different mechanisms. Therefore, deciphering the interactions between the distinct cell types, their spatial organization and their specific contribution to tumor growth and drug sensitivity is still a major challenge. Dissecting intra-tumor heterogeneity is currently an urgent need to better define breast cancer biology and to develop therapeutic strategies targeting the microenvironment as helpful tools for combined and personalized treatment. In this review, we analyze the mechanisms by which the tumor microenvironment affects the characteristics of tumor heterogeneity that ultimately result in drug resistance, and we outline state of the art preclinical models and emerging technologies that will be instrumental in unraveling the impact of the tumor microenvironment on resistance to therapies.\n\n1. Introduction\n",
        "text": "Breast cancer (BC) is the second leading cause of cancer death in women. Data from WHO (World Health Organization) reported about 2.3 million new cases and about 685,000 deaths from BC globally [REF]. Similarly, American Cancer Society’s projections for BC incidence in the United States in 2023 (https://www.cancer.org/cancer/breast-cancer/about/how-common-is-breast-cancer.html), estimate about 297,790 new cases of invasive BC in women, about 55,720 new diagnosis of ductal carcinoma in situ (DCIS), and about 43,700 death from this disease. The same statistics indicate for 2023 more than 3.8 million BC survivors in the United States, and 7.8 million worldwide, including both patients currently being treated and making this type of cancer the most prevalent worldwide. The median age at the time of diagnosis is 62 years and a woman’s lifetime risk of acquiring breast cancer in the United States is around 13%, with incidence rates rising by 0.5% annually in recent years. Currently, a woman’s chance of dying from BC is around 2.5%, death rates have been decreased due to improved therapeutic regimens, as well as earlier BC detection through screening programs and increased awareness. However, in recent years, the trend has marginally halted.The breast cancer mass is composed not only by epithelial cancer cells, but also by a plethora of heterogeneous populations coming from the host, including endothelial cells, stromal fibroblasts, and a variety of immune cells that form the so-called tumor microenvironment (TME) [REF]. The TME is a highly complex biological community embedded in a composite matrix of structural proteins constituting the extracellular matrix (ECM), in which immune cells (including macrophages, polymorphonuclear cells, mast cells, natural killer cells, dendritic cells (DCs), and T and B lymphocytes) and non-immune cells (such as endothelial cells and stromal cells) establish subtle interactions with cancer cells. This cellular cross-talk is based on the production of specific soluble (growth factors and cytokines) and insoluble (ECM proteins) molecules, and it determines the tumor’s natural history.BC comprises numerous subtypes that differ genetically, pathologically, and clinically. Indeed, it is currently considered a group of neoplasms originating from mammary gland epithelial cells caused by a variety of genetic alterations, with different disease courses, responses to treatments, and clinical outcomes. This was best exemplified by next-generation sequencing studies depicting comprehensive molecular BC portraits in Cancer Genome Atlas [REF] and identifying more than 1600 likely driver mutations in 93 BC genes [REF]. BC can have distinct molecular profiles from one another, leading to a complex heterogeneity of tumor cell subpopulations within single tumors, between primary tumors and their metastasis, or between independent metastasis, as a consequence of tumor clonal evolution [REF]. In addition to clonal evolution, tumor heterogeneity can occur also at the level of cancer cell plasticity. The capability of BC cells to reprogram their gene expression and change their behavior when triggered by internal or external stimuli coming from surrounding cells and secreted factors, provides dynamic and context-dependent features to tumor heterogeneity [REF]. Moreover, heterogeneity is also modulated by the different composition of the TME, with different ratio between tumor-infiltrating lymphocytes, myeloid cells, macrophages [REF], with the increased presence of cancer-associated fibroblasts (CAFs) [REF] and endothelial cells that controls cancer cell properties. The heterogeneity in components of the BC mass can be either observed between the different BC subtypes, known as inter-tumor heterogeneity, or within the same tumors, known as intra-tumor heterogeneity [REF].Therapeutic approaches are still currently largely based on clinical and pathological BC features, mostly on the presence or absence of targets like the hormone receptors or the Human Epidermal growth factor Receptor 2 (HER2) [REF], and they are not yet tailored to individual patients. In particular, endocrine therapy is expected for hormone-dependent BC patients, targeted therapy with monoclonal antibodies for HER2-positive patients, and chemotherapy for TNBC patients. However, the different mechanisms that contribute to the inter- and intra-tumor heterogeneity are responsible for tumor escape from therapeutic interventions.Drug resistance is among the major obstacles to reach a long-term cure, and overcoming this problem is the biggest challenge in BC research today. Indeed, the heterogeneous pattern of molecular aberrations found in each cancer plays a crucial role in the resistance to anticancer treatment [REF]. The goal of cancer therapy is to target a population of cancer cells within a particular host environment. The pharmacological properties of the therapy, together with intrinsic and acquired molecular features of cancer cells, controlled also by the TME components, dictate the therapy’s efficacy. Unfortunately, despite the clinical management of BC improving every day, the number of patients developing drug-resistant tumors is still high [REF]. The resistance can be already present before the treatment (innate) or appear after the treatment administration (acquired) [REF]. The innate resistance is mainly due to intrinsic tumor heterogeneity: in primary cancer one or more subpopulations (e.g., Cancer Stem Cells) are resistant to the treatments from the beginning; on the contrary, the acquired resistance becomes evident after the therapy. In the clinical setting, innate and acquired resistance may coexist, making the long-term fight against cancer more complex.In BC, standard chemotherapies and targeted therapies have both been extensively correlated to the escape of tumor cells that shape the clonal evolution of tumors, giving rise to drug-resistant subclones [REF]. Moreover, a comparison of the genetic diversity between pre- and post-treatment in tumor specimens indicates the role of therapy in selectively expanding resistant cancer clones that were initially present but at low frequency [REF]. In this context, TME cells play an important role in mediating the drug response and educating the cancer cells to become resistant to the therapy through extensive molecular crosstalk that we will discuss below [REF].We will first describe here what is currently known regarding inter-tumor and intra-tumor heterogeneity and the impact of TME on cancer progression and drug resistance. Moreover we will discuss the up-to-date tools for studying these complex interactions in preclinical models and in patient derived samples in cancer progression and drug resistance. We will present emerging technologies, such as the spatial location of tumor subclones and TME cells within their native spatial context. We will show how the rapid growth of these techniques together with the multi-omics conjoint analysis mode and deep learning network architecture, promise to provide a more comprehensive understanding of cell-to-cell variation within and between individual tumors."
    },
    "2023-37265987_628_en.txt": {
        "title": "Drivers of inappropriate antibiotic use in low- and middle-income countries",
        "prompt": "Abstract: Antimicrobial resistance (AMR) is a global security threat that accounts for about 700 000 deaths annually. Studies have shown that antimicrobial resistance could result in a 2% to 3.5% reduction in global Gross Domestic Product by 2050 and a loss of between 60 and 100 trillion US dollars, worth of economic output resulting in significant and widespread human suffering. Low- and middle-income countries (LMICs) will be worse hit by an unchecked rise of AMR. For example, it is predicted that AMR could kill about 4.1 million people in Africa by 2050 if it is not curbed. Similarly rising rates of AMR will lead to increased treatment costs and an inability to attain universal health coverage, in LMICs with fragile health systems. Sadly, AMR is driven by the inappropriate use of antimicrobials, especially antibiotics. Inappropriate antibiotic use is a pertinent problem in LMICs where regulatory frame works are weak. Inappropriate antibiotic use in LMICs is a multifaceted problem that cuts across clinical and veterinary medicine and agriculture. Therefore, efforts geared at curbing inappropriate antibiotic use in LMICs must identify the factors that drive this problem (i.e. inappropriate antibiotic use) in these countries. A clear knowledge of these factors will guide effective policy and decision making to curb inappropriate antibiotic use and ultimately AMR. The focus of this review is to discuss the factors that drive inappropriate antibiotic use in LMICs.\n\n1. Introduction\n",
        "text": "Antimicrobial resistance (AMR) is a global security threat [REF]. It is driven by the inappropriate use of antimicrobials, particularly antibiotics, in humans, animals and veterinary medicine.1 The inappropriate use of antibiotics also exacts adverse clinical and economic effects on patients [REF]. The adverse economic effects include increased costs of healthcare borne by patients, whereas the adverse clinical effects include increased consumption of antibiotics, increased length of hospital stay, morbidity, mortality and certainly the emergence of AMR [REF]. AMR is considered one of the biggest threats to global health, food security and development [REF]. AMR accounts for an estimated 700 000 deaths per year globally [REF]. If AMR is not curbed, it could result in about 10 million global deaths per year and 4.1 million deaths in Africa by 2050 [REF]. AMR could also have devastating economic impacts as it could result in a 2% to 3.5% reduction in global Gross Domestic Product by 2050, a loss of between 60 and 100 trillion US dollars (US$) of economic output, increased treatment costs (resulting in increased morbidity and mortality in poorly funded fragile health systems in many LMICs) and a failure to achieve universal health coverage and the Sustainable Development Goals (SDGs), particularly in LMICs [REF]. Increased morbidity and mortality (arising from rising rates of AMR) could result in declining labour supply and an additional 28.3 million people being pushed into extreme poverty by 2050 due to high costs of treatment and chronic infections [REF]. AMR related morbidity and mortality in animal husbandry could affect the livelihood of farmers and also threaten food security [REF]. Another danger of AMR is that it results in market failures, low return on investment and disinclination to invest in research and development in new antimicrobial agents, particularly antibiotics [REF]. Rather than invest in developing new antibiotics, multinational pharmaceutical companies are choosing the more profitable option of developing drugs for non-communicable diseases [REF]. Also, the few new antibiotics being developed are expensive and show little benefit compared to existing antibiotics [REF]. For example, the World Health Organization notes that only 2 out of the 12 antibiotics approved since 2017 represent a new class of antibiotics.9 The remainder are derivatives of already existing antibiotics [REF]. Citizens of LMICs would have to bear the unduly high costs of purchasing these newly developed antibiotics since existing and cheaper antibiotics are ineffective [REF]. Sadly, poverty and other factors such as weak antibiotic supply chain management systems deny many citizens of LMICs from accessing both old and new antibiotics [REF]. Globally, the inappropriate use of antibiotics in veterinary medicine and agriculture compounds the problem of AMR as it results in the emergence of antibiotic-resistant bacteria and resistance genes, which can be transmitted to humans through the food chain or through direct contact with animals [REF]. However, in LMICs, the inappropriate use of antibiotics in agriculture is exacerbated by weak or non-existent regulatory frameworks [REF]. The COVID 19 pandemic has aggravated the global problem of inappropriate antibiotic use [REF]. A lack of definite treatment protocols fuelled the irrational prescription and use of antibiotics for COVID 19 patients [REF]. The danger of such irrational use of antibiotics is the future emergence of drug resistant bacteria in the community and healthcare settings [REF]. Although inappropriate antibiotic use is a global problem, the political and socio-cultural peculiarities in many LMICs make it a multifaceted problem [REF]. Therefore, policy formulation and other efforts to curb inappropriate antibiotic use (and AMR) in LMICs must have a precise knowledge of the specific drivers of inappropriate antibiotic use in LMICs [REF]. The focus of this review is to discuss the drivers of inappropriate antibiotic use in LMICs."
    },
    "2023-37265988_794_en.txt": {
        "title": "Exploring the prevalence of antibiotic resistance patterns and drivers of antibiotics resistance of  in livestock and poultry-derived foods: a systematic review and meta-analysis in Bangladesh from 2000 to 2022",
        "prompt": "Abstract: Antimicrobial resistance (AMR) is a severe public health problem that Bangladeshis are dealing with nowadays. However, we wanted to investigate the pooled prevalence of Salmonella and AMR in Salmonella strains isolated from livestock- and poultry-derived foods between 1 January 2000 and 31 August 2022.The metafor and metareg packages in the R programming language were used to conduct all analyses. We used a random-effect or fixed-effect model for pooled prevalence of Salmonella and AMR to Salmonella, depending on the heterogeneity test for each antibiotic. The heterogeneity was examined using stratified analyses, the meta-regression approach and sensitivity analysis.The combined prevalence of Salmonella in livestock and poultry-derived food in Bangladesh is 37%, according to the 12-research considered (95% CI: 23%–52%). According to subgroup analysis, neomycin had the lowest prevalence of resistance (4%, 95% CI: 1%–13%), whereas tetracycline had the highest prevalence of resistance (81%, 95% CI: 53%–98%). According to univariate meta-analysis and correlation analysis, the prevalence of Salmonella increased with the study period (β = 0.0179; 95% CI: 0.0059–0.0298, P = 0.0034; R2 = 46.11%) and without this, none of aforementioned variables was significantly associated with the detected heterogeneity and there was a positive relationship (r = 0.692, P = 0.001) between the Salmonella prevalence and study period.AMR is rising alarmingly in Bangladesh by livestock-derived food consumption. However, monitoring and evaluating antibiotic sensitivity trends and developing effective antibiotic regimens may improve Salmonella infection inhibition and control in Bangladesh. Policymakers should be concerned about food handling practices. Doctors should be concerned when using prescribing antibiotics.\n\n1. Introduction\n",
        "text": "Salmonella is one of the most commonly recognized pathogens that cause gastroenteritis [REF], which results in significant morbidity, mortality and economic loss [REF]. In 2010, the World Health Organization (WHO) reported 153 million cases of non-typhoidal Salmonella (NTS) enteric infections worldwide, of which 56 969 were fatal and 50% were foodborne [REF]. Salmonella was the second foodborne epidemic in some regions’ illness monitoring reports from 2006 to 2010 [REF]. Among the 2600 Salmonella serotypes discovered, NTS serovars such as Typhimurium and Enteritidis are the most common worldwide [REF]. Poultry has been identified as the one cause of human salmonellosis, and avian salmonellosis affects the poultry business and can infect humans when infected poultry meat and eggs are consumed [REF]. Eggs are the principal source of salmonellosis and other foodborne illnesses [REF]. Salmonella that grows in animal farms may contaminate eggs and meat during the slaughtering process before being transmitted to people via the food chain. Indeed, multiple earlier investigations have reported the isolation of Salmonella from animal and human diets [REF]. Human S. Enteritidis is commonly associated with the intake of infected eggs and chicken meat, whereas S. Typhimurium is commonly associated with the consumption of pork, poultry and beef [REF]. In addition, Salmonella enterica serovars have been found in varying concentrations in animal products and by-products worldwide [REF]. The most often-reported serovars connected with human foodborne diseases are Salmonella Typhimurium and Enteritidis [REF]. However, untyped Salmonella of animal origin is becoming more common in Bangladesh [REF]. However, domestic chickens in developing countries live close to humans in urban and rural communities and are frequently housed overnight in the family home [REF]. As soon as a chicken becomes infected, it sheds faeces into the environment. Additionally, interacting with employees in poultry farms and slaughterhouses, the main route of human Salmonella infections, involves contaminated meat and eggs [REF]. One of the primary reasons for animal management is the risk of antimicrobial resistance (AMR) in humans and animals [REF]. AMR is a worldwide public health issue [REF]. AMR can be caused by one of three fundamental processes: (i) antibiotic modification by lowering absorption or enhancing efflux of the antibiotic via their enzymes; (ii) alteration in the antibiotic's target site and (iii) gaining the capacity to break or change the antibiotic [REF]. Several lines of evidence showed that using antimicrobial agents in food animals contributes to the emergence and spread of AMR in foodborne Salmonella [REF]. AMR has recently been a significant issue in treating Salmonella infections [REF]. Salmonella infections in food animals are essential in public health and, in particular, food safety because food products of animal origin are thought to be the most common source of human Salmonella infections [REF]. Contamination by healthy food handlers is also assessed during food processing. In recent years, it has been estimated that animals and their products can account for up to 96% of all Salmonella infections in humans [REF]. AMR is expected to increase by 70% in Asia, posing a national and global threat [REF]. The WHO estimates that Salmonella infections cause 93.8 million instances of gastroenteritis worldwide each year and that Salmonella infections cause 155 000 fatalities [REF]. Moreover, according to a recent Shanghai study, just 1.1% of strains were responsive to all 16 medications, and AMR rates for third and fourth-generation cephalosporins (cefotaxime and cefepime) were 10% and 8.1%, respectively [REF]. Furthermore, according to a study conducted in Guangzhou, annual resistance rates of ampicillin are reasonably consistent. However, resistance rates of NTS to ceftazidime in 2015 (31.43%) were significantly greater than in 2014 (16%). Furthermore, AMR to ampicillin was considerably higher in serotype Typhimurium and Enteritidis isolates than in other serotypes [REF]. Salmonella drug resistance rates to cephalosporin and cefepime were 22.3% and 13.1%, respectively, in four hospitals in Shenzhen,41 which were all higher than the results of earlier Chinese investigations [REF]. This phenomenon demonstrates that the outlook for AMR is bleak. Regarding AMR mechanisms, the corresponding resistance genes are usually found on plasmids, transposons, gene cassettes or variants of the Salmonella genomic islands SGI1 and SGI2 [REF]. However, this study revealed a prevalence of Salmonella in poultry and livestock-derived foods. In this regard, this meta-analytical study will be evidential for assessing the prevalence of Salmonella in livestock and poultry-derived food and AMR in livestock and poultry-derived foods and comprehensively investigate the whole scenario of Bangladesh. We hypothesize that Salmonella pooled prevalence and antibiotic resistance are increasing in Bangladesh."
    },
    "2023-37266026_930_en.txt": {
        "title": "Cross-Frequency Coupling and Intelligent Neuromodulation",
        "prompt": "Abstract: Cross-frequency coupling (CFC) reflects (nonlinear) interactions between signals of different frequencies. Evidence from both patient and healthy participant studies suggests that CFC plays an essential role in neuronal computation, interregional interaction, and disease pathophysiology. The present review discusses methodological advances and challenges in the computation of CFC with particular emphasis on potential solutions to spurious coupling, inferring intrinsic rhythms in a targeted frequency band, and causal interferences. We specifically focus on the literature exploring CFC in the context of cognition/memory tasks, sleep, and neurological disorders, such as Alzheimer's disease, epilepsy, and Parkinson's disease. Furthermore, we highlight the implication of CFC in the context and for the optimization of invasive and noninvasive neuromodulation and rehabilitation. Mainly, CFC could support advancing the understanding of the neurophysiology of cognition and motor control, serve as a biomarker for disease symptoms, and leverage the optimization of therapeutic interventions, e.g., closed-loop brain stimulation. Despite the evident advantages of CFC as an investigative and translational tool in neuroscience, further methodological improvements are required to facilitate practical and correct use in cyborg and bionic systems in the field.\n\n1. Introduction\n",
        "text": "Cyborg and bionic systems (CBS) focus on the integration of organic and biomechatronic components, with the aim of either restoring lost function or normalizing disease symptoms. Examples of such techniques may include brain–computer interfaces or neuromodulation technologies [e.g., deep brain stimulation (DBS)] [REF].Targeting a reliable set of biomarkers is crucial for the development of a useful CBS [REF]. Electrophysiological systems such as the brain or heart generate oscillatory activity over a spectrum of frequencies. System outputs such as movement or cognitive process reflect a complex and nonlinear integration of oscillatory neural population activity [REF]. This can be accessed using a range of approaches including invasive local field potential or electrocorticogram recordings, or non-invasive measures with either electroencephalography (EEG) or magnetoencephalography.Multiple neural oscillations across temporal and spatial scales participate in neural information processing [REF]. In general, low-frequency oscillations are thought to control long-range synchronization, while high-frequency oscillations (HFOs) are believed to be linked to local computation [REF]. The question of how these neural oscillations contribute to top-down neural transmission has raised great interest [REF]. Oscillatory neural activities in multiple frequencies are modulated during a range of tasks (e.g., cognitive tasks) [REF]. Furthermore, brain stimulation techniques that entrain (or alter) oscillatory activity are in turn known to impact task performance [REF]. This has led to the belief that oscillatory neural population activity has a causal impact on behavior [REF]. In keeping with this, it is also becoming increasingly apparent that neurophysiological oscillations may serve as a biomarker for pathophysiological states such as Parkinson's disease [REF].One particular type of oscillatory coupling, known as cross-frequency coupling (CFC), has gained great interest in medicine and neuroscience. CFC characterizes interactions across different frequency rhythms and is modulated during both physiological processing and pathological states, such as spasticity [REF]. CFC denotes the statistical association between the phase, amplitude, or frequency of 2 rhythms [REF]. CFC applied on simultaneous recordings from different cortical areas reveals a coordinated information exchange in cognitive, sensory, and motor events from long distance to local computation [REF]. There are 4 commonly studied types of CFC: phase–amplitude coupling (PAC), amplitude–amplitude coupling (AAC), phase–frequency coupling, and phase–phase coupling [REF].PAC [REF] and AAC [REF] attracted much attention for their association with physiological processing and pathological states. These 2 types of CFCs are depicted in Fig. 1. Figure 1F shows a simulation of PAC, where the activity at 13 Hz is modulated by the phase of a 2.5-Hz wave, forming a nested structure, i.e., XPAC(t) = (xp(t)+1)·sin(2π×13t), where xp(t) = sin(2π×2.5t). On the other hand, AAC refers to when the amplitude of the 13-Hz activity is modulated by the envelope of a 2.5-Hz wave (Fig. 1G). In this case, the form of the resulting signal is XAAC(t) = |H[xa(t)]|·sin(2π×13t), where H[·] represents the Hilbert transform (HT) of a signal xa(t) = V·sin(2π×2.5t), and V is a time-varying variable (Fig. 1B). Both XPAC(t) and XAAC(t) formulated here refer to 13-Hz activities, and both the instantaneous phase (Fig. 1D) and amplitude (Fig. 1E) are derived from the HT of the inputs.Concepts of phase–amplitude coupling (PAC) and amplitude–amplitude coupling (AAC). (A) High-frequency oscillation (13 Hz). (B) Low-frequency oscillation (2.5 Hz). (C) Low-frequency oscillation with varying amplitude modulations. (D) Phase of a 2.5-Hz oscillation. (E) Envelope of xa(t) oscillation. (F) Oscillatory coupling formations of PAC with 2.5-Hz phase shown in (D) modulating 13-Hz amplitude shown in (A). (G) Oscillatory coupling formations of AAC with 2.5-Hz amplitude shown in (C) modulating 13-Hz amplitude shown in (A).This review seeks to offer a comprehensive overview of the latest developments in CFC research, with a special focus on methodologies, neural mechanisms, and potential applications in CBS, clinical interventions in particular. Firstly, the review will commence by defining CFC and summarizing the current state of knowledge regarding its methodological advances. Next, we will summarize the latest studies on CFC in cognitive processes, and various neurological disorders, including but not limited to Alzheimer's disease, epilepsy, and Parkinson's disease, plus discussions over the potential neuromodulation techniques for clinical interventions. Lastly, the review will consider the challenges and opportunities for the integration of CFC technology into CBS, with future trends in this field being highlighted."
    },
    "2023-37266318_791_en.txt": {
        "title": "Toward an Integrated Framework in Health and Human Rights Education",
        "prompt": "Abstract: Global health equity is at a historically tenuous nexus complicated by economic inequality, climate change, mass migration, racialized violence, and global pandemics. Social medicine, collective health, and structural competency are interdisciplinary fields with their own histories and fragmentary implementation in health equity movements situated both locally and globally. In this paper, we review these three fields’ historical backgrounds, theoretical underpinnings, and contemporary contributions to global health equity. We believe that intentional dialogue between these fields could promote a generative discourse rooted in a shared understanding of their historical antecedents and theoretical frameworks. We also propose pedagogical tools grounded within our own critical and transformative pedagogies that offer the prospect of bringing these traditions into greater dialogue for the purpose of actualizing the human right to health.\n\n1. Introduction\n",
        "text": "Health workers concerned about the human right to health have reason to be wary as they observe the world around them. Despite a pandemic-triggered global economy slowdown, human consumption continues to generate dangerous levels of greenhouse gasses, pushing carbon dioxide levels to their highest in three million years [REF]. During the first two years of the COVID-19 pandemic, profit-driven economic systems crowned a new billionaire every 30 hours while pushing one million people toward extreme poverty every 33 hours. Forty of these new billionaires are pharmaceutical executives [REF]. Colonial imprints, white supremacy, and racial capitalism contours and textures both public and private care systems in ways that create patterns of advantage for white-identified and wealthy individuals and disadvantage for (largely poorer) black- and brown-identified individuals in their encounters with COVID-19 [REF]. The consequences of these upstream forces are health inequities experienced as biological pathology. By upstream, we refer to the social, political, and economic contexts that structure society and are “manufacturers of illness,” such as profit-making institutions [REF]. Generative mechanisms in society that positively influence well-being are also examples of upstream social forces, such as equitable transportation, healthy rivers and wetlands, and community practices imbued with an ethos of care. The recalcitrant persistence of health inequities experienced across both local and global contexts has intensified interest in frameworks that claim to diagnose the root causes of these inequities accurately. Prominent among these interdisciplinary fields are social medicine, collective health, and structural competency, all of which also offer a complementary set of prescriptions to remedy inequitable outcomes [REF]. Some argue that the intensified interest at this moment is simply a cyclical rediscovery of historically well-described relationships between oppressive social conditions that structure risk for illness [REF]. Others believe that the intensified interest and energy, pressured by the emergency context of climate catastrophe, ongoing racial injustice, and a global pandemic, signal something different. They hold the hope that disruptive and generative social change will move the global community toward actual fulfillment of article 25 of the Universal Declaration of Human Rights, which states that “everyone has the right to a standard of living adequate for the health and well-being of himself and of his family.” [REF] Social medicine, collective health, and structural competency share core commonalities in their assertion that health is a human right generated not in the biological domain but in the upstream determination of health. This is often akin to the contemporary reference within human rights literature to civil, political, social, cultural, and economic rights. All three of these fields focus on the (re)production of health and systems of care, as well as the political economies that aid or obstruct the realization of public health as a social good. They are each interdisciplinary fields linked with social change projects with broad agendas encompassing the social response to societal ills. All three grew out of academic contexts, and their adherents publish on and organize thematic conferences focused on health justice.Despite their shared ideological interest in health justice and remedying health inequities, practitioners and scholars in the fields of social medicine, collective health, and structural competency engage in minimal dialogue. Possible reasons for the lack of dialogue include their origins in different geographic, linguistic, and historical contexts, territorialism, academic advancement linked to the generation of novel concepts and language, and the belief that new conceptualizations expanding beyond existing theories and actions are required to eliminate health inequities. Whatever the reason, the lack of exchange and engagement diminishes the possibilities of relationship-building, theoretical expansion, imaginative problem-solving, and the collective building of power needed for social change toward health justice. In short, we believe that the lack of dialogue isolates and minimizes the potential for all three to substantively contribute to the movement seeking to ensure health as a human right for all. In this paper, our goal is to ignite intentional dialogue among these three fields by (1) juxtaposing their definitions, geographic and historical journeys, and key frameworks and themes, (2) proposing that transformative pedagogy offers one strategy to foster dialogue, and (3) providing concrete examples of what such pedagogy might look like. We illustrate these points by drawing on our collective experience as a transnational group of social medicine educators and practitioners."
    },
    "2023-37266428_832_en.txt": {
        "title": "cell-wall and antimicrobial peptides: a mission impossible?",
        "prompt": "Abstract: Mycobacterium tuberculosis (Mtb) is one of the most important infectious agents worldwide and causes more than 1.5 million deaths annually. To make matters worse, the drug resistance among Mtb strains has risen substantially in the last few decades. Nowadays, it is not uncommon to find patients infected with Mtb strains that are virtually resistant to all antibiotics, which has led to the urgent search for new molecules and therapies. Over previous decades, several studies have demonstrated the efficiency of antimicrobial peptides to eliminate even multidrug-resistant bacteria, making them outstanding candidates to counterattack this growing health problem. Nevertheless, the complexity of the Mtb cell wall makes us wonder whether antimicrobial peptides can effectively kill this persistent Mycobacterium. In the present review, we explore the complexity of the Mtb cell wall and analyze the effectiveness of antimicrobial peptides to eliminate the bacilli.\n\n1. Introduction\n",
        "text": "The Mycobacterium tuberculosis (Mtb) cell wall is probably the most complex membrane of all bacteria. The architecture of this complex wall extends to a massive core comprised of peptidoglycans covalently attached via a linker unit to a linear galactofuran and to several strands of a highly branched arabinofuran, and mycolic acids. Mycolic acids are oriented perpendicular to the membrane plane and provide a lipid barrier, which is the main feature of this bacteria [REF]. Several immune-evasion mechanisms conferred to Mtb, such as higher hydrophobicity, depend on its cell wall by masking pathogen-associated molecular patterns with phthiocerol dimycocerosates and inhibiting toll-like receptor 2 with sulfoglycolipids [REF]. These strategies inhibit the detection of bacteria and the activation of cytokine responses. They also recruit naive rather than microbicidal macrophages to sites of infection, preventing detection by targeting antigen presentation pathways of the adaptive immune system [REF]. M. tuberculosis uses several components of the cell wall to alter the processing and availability of peptide antigens for MHC class I and MHC class II. Furthermore, these cell wall-derived molecules can inhibit innate immunity processes such as autophagy [REF].During primary infection, the first cells to encounter Mtb are lung epithelial cells (EpCs) and alveolar macrophages which are capable of sensing and mounting an immune response versus Mtb, mainly through a wide variety of antimicrobial peptides. Most of these peptides display conserved properties including amphipathicity, cationicity and a small molecular size (12-50 amino acid residues) [REF]. Whether or not these short cationic peptides can disrupt this thick and impermeable wall will be discussed herein.Although tuberculosis [TB] is a preventable and treatable disease, it remains an important threat to public health. Transcriptomic analysis has shown that MDR-Mtb is capable to modify acetylation/methylation patterns in macrophages and lymphocytes infected, leading to oxidative stress and premature cellular aging, which correlates with higher intracellular survival and dissemination. Besides, some mycobacterial products play epigenetic changes promoting DNA methylation or altering the expression of non-coding RNAs to upregulate the immune response activation [REF].Despite the growing effort of the global health community to eradicate tuberculosis, in 2021 a total of 1.5 million people died from TB. In fact, Mtb was the second leading infectious killer after COVID-19. Due to vaccination and management of the pandemic, TB is profiling to return as the leading cause of mortality. This time, however, it will be even stronger as all healthcare resources were directed to the containment of SARS-Cov2 during the COVID-19 pandemic and important TB control programs, including vaccination, multidrug-resistant surveillance, and direct observed therapy, were ignored (DOT) [REF]. Nowadays, drug-resistant tuberculosis is a major public health challenge, and it poses a significant threat to global efforts to control TB. The treatment of drug-resistant TB is complex, lengthy, and expensive, and it often requires the use of toxic and poorly tolerated drugs. The emergence of extensively drug-resistant TB (XDR-TB) further complicates the situation, as there are limited treatment options available. There is an urgent need for new and more effective therapies to combat drug-resistant TB, and antimicrobial peptides are a feasible option to shorten and improve the conventional therapy to address this pressing public health challenge.\nM. tuberculosis spreads from person to person almost exclusively by aerosolized particles. Nearly 90% of Mtb-infected individuals spontaneously control infection and eliminate mycobacteria [REF], while the remaining percentage of infected individuals contain the bacteria in a granuloma. Interestingly, numerous individuals living in densely populated areas prone to TB seem to be resistant to Mtb infection. They may presumably have immediate elimination of Mtb by innate phagocytes, epithelial cells, soluble antimicrobial molecules, innate invariant T cells, and natural killer cells, located in the mucosa and alveoli of the bronchial pulmonary airway mucosa and alveoli. They appear to have never been Mtb-infected as they are found to be negative for tuberculin skin test reaction and show an absence of granulomas. The resistance of such individuals may suggest the capability of innate immunity as a major natural effector against Mtb [REF].The defining characteristic of Mtb that is atypical of other bacteria is the complexity of its cell wall, which is associated with pathogenesis and provides a barrier against antibiotics and the immune response of the host [REF]."
    },
    "2023-37266441_830_en.txt": {
        "title": "The impact of microbially modified metabolites associated with obesity and bariatric surgery on antitumor immunity",
        "prompt": "Abstract: Obesity is strongly associated with the occurrence and development of many types of cancers. Patients with obesity and cancer present with features of a disordered gut microbiota and metabolism, which may inhibit the physiological immune response to tumors and possibly damage immune cells in the tumor microenvironment. In recent years, bariatric surgery has become increasingly common and is recognized as an effective strategy for long-term weight loss; furthermore, bariatric surgery can induce favorable changes in the gut microbiota. Some studies have found that microbial metabolites, such as short-chain fatty acids (SCFAs), inosine bile acids and spermidine, play an important role in anticancer immunity. In this review, we describe the changes in microbial metabolites initiated by bariatric surgery and discuss the effects of these metabolites on anticancer immunity. This review attempts to clarify the relationship between alterations in microbial metabolites due to bariatric surgery and the effectiveness of cancer treatment. Furthermore, this review seeks to provide strategies for the development of microbial metabolites mimicking the benefits of bariatric surgery with the aim of improving therapeutic outcomes in cancer patients who have not received bariatric surgery.\n\n1. Introduction\n",
        "text": "Cancer is one of the major diseases threatening human life and health. According to GLOBOCAN 2020, there will be approximately 19.3 million new cancer cases and at least 10 million cancer deaths worldwide in 2020 [REF]. It was predicted that by 2022, there would be approximately 4.82 million new cancer cases and at least 3.21 million cancer deaths in China and approximately 2.37 million new cancer cases and at least 0.64 million cancer deaths in the United States [REF]. In many countries, cancer is increasingly becoming the main cause of death. At present, frequently used treatment methods that target tumors include radiotherapy, chemotherapy, surgery, and targeted therapy. In December 2013, Science magazine named tumor immunotherapy one of the top 10 technological breakthroughs of the year [REF]. American scientist James P. Allison and Japanese scientist Tasuku Honjo were awarded the 2018 Nobel Prize in Physiology or Medicine for their discovery that negative immune regulation can be used to treat cancer. The current clinical applications of tumor immunotherapy include immune checkpoint inhibitor therapy, adoptive cellular immunotherapy, and cancer vaccines, among others [REF]. Clinically, tumor immunotherapy has shown clear efficacy in patients with different cancer diagnoses, including long-term remission after treatment [REF]. Moreover, tumor immunotherapy has the potential to cure a small number of patients who develop metastases and were previously thought to be incurable [REF]. Tumor immunotherapy is a novel treatment approach that has rapidly developed in recent years [REF]. Unlike traditional therapy, tumor immunotherapy primarily targets the immune system rather than tumor cells. The immune system prevents cancer progression because of its ability to recognize and destroy abnormal cells. Tumor immunotherapy can eliminate tumor cells by strengthening the body’s innate immune defense and reshaping the immune microenvironment [REF]. Therefore, reactivating the body’s immune system is conducive to killing tumor cells.Although immunotherapy has benefited many patients, these patients remain a minority [REF]. Two studies found that the gut microbiome had a significant impact on cancer immunotherapy in mouse models [REF]. Furthermore, scientists have observed that the impact of the gut microbiome on cancer immunotherapy is variable in cancer patients; specifically, some gut microbiome metabolites can significantly strengthen cancer immunotherapy and, thus, are considered favorable. There is no consensus on what is considered favorable. Researchers from France found that Akkermansia muciniphila was beneficial for immunotherapy against epithelial tumors [REF]. Researchers from North America discovered that the beneficial gut microbes included Collinsella aerofaciens, Enterococcus faecium, and Bifidobacterium longum in patients with metastatic melanoma [REF], while researchers from South America identified Ruminococcaceae as part of a favorable gut microbiome in patients with melanoma [REF]. Furthermore, researchers from China identified Bifidobacterium longum, Prevotella copri, and Alistipes putredinis as beneficial gut microbes for patients with non-small cell lung cancer (NSCLC) receiving immunotherapy [REF]. These results imply that regional differences influence what is considered a favorable microbiome. Therefore, scientists have begun to explore the relationships between microbial metabolites and enhanced cancer immunotherapy.Obesity is defined as body mass index (BMI) ≥ 30 kg/m2 (weight in kilograms divided by height in meters squared) and is the most important risk factor for human health. Unfortunately, the long-term effectiveness of dietary intervention is limited. No effective drugs are currently available for individuals who have morbid obesity. Therefore, bariatric surgery is widely applied and is a high-efficiency treatment option for most patients with obesity. Bariatric surgery is recommended for obese patients with a BMI > 35 to 40 kg/m2 and other comorbidities. Generally, bariatric surgery has become an option for inducing weight loss, and numerous studies have recently shown that it also alters the gut microbiota profile. Indeed, many researchers have reported that alterations in microbial metabolites after bariatric surgery are beneficial in the context of other diseases, such as type 2 diabetes [REF]. However, the impact of the microbial metabolites associated with bariatric surgery on cancer and antitumor immunity requires further investigation.In this review, we introduce and analyze the changes in microbial metabolites caused by bariatric surgery and the effects of the subsequent gut-derived metabolites on anticancer immunity. Furthermore, we attempt to clarify the relationship between the changes in microbial metabolites caused by bariatric surgery and the effectiveness of cancer treatment with the aim of providing strategies for cancer patients with obesity."
    },
    "2023-37266541_1768_en.txt": {
        "title": "Amygdala neurocircuitry at the interface between emotional regulation and narcolepsy with cataplexy",
        "prompt": "Abstract: Narcolepsy is a sleep disorder characterized by chronic and excessive daytime sleepiness, and sudden intrusion of sleep during wakefulness that can fall into two categories: type 1 and type 2. Type 1 narcolepsy in humans is widely believed to be caused as a result of loss of neurons in the brain that contain the key arousal neuropeptide Orexin (Orx; also known as Hypocretin). Patients with type 1 narcolepsy often also present with cataplexy, the sudden paralysis of voluntary muscles which is triggered by strong emotions (e.g., laughter in humans, social play in dogs, and chocolate in rodents). The amygdala is a crucial emotion-processing center of the brain; however, little is known about the role of the amygdala in sleep/wake and narcolepsy with cataplexy. A collection of reports across human functional neuroimaging analyses and rodent behavioral paradigms points toward the amygdala as a critical node linking emotional regulation to cataplexy. Here, we review the existing evidence suggesting a functional role for the amygdala network in narcolepsy, and build upon a framework that describes relevant contributions from the central nucleus of the amygdala (CeA), basolateral amygdala (BLA), and the extended amygdala, including the bed nucleus of stria terminalis (BNST). We propose that detailed examinations of amygdala neurocircuitry controlling transitions between emotional arousal states may substantially advance progress in understanding the etiology of narcolepsy with cataplexy, leading to enhanced treatment opportunities.\n\n1. Introduction\n",
        "text": "The amygdala and accessory regions referred to as the extended amygdala, are key emotional centers in the brain, which have classically been studied in the context of motivated actions, defensive behaviors, and fear learning. Anatomical, molecular, physiological, and behavioral studies have delineated functionally distinct subregions of the amygdala, including the basolateral, central, and medial subregions (BLA, CeA, and MeA, respectively). In addition to the classical amygdala structures, the bed nucleus of the stria terminalis (BNST) is a heterogeneous forebrain region with multiple neuronal subpopulations that has also been characterized as part of the extended amygdala [REF]. Like the CeA, the BNST is composed primarily of neurons that utilize the fast inhibitory neurotransmitter GABA. The vast majority of these GABAergic neurons also synthesize and release varying combinations of several neuropeptide modulators, although such multiplex forms of neural transmission remain largely uncharacterized.Here, our abbreviation of AMY refers to structures of the classical amygdala (BLA, CeA, and MeA), as well as the extended amygdala (BNST), as both are critical for behavioral responding to emotionally salient stimuli and consolidation of emotional memories [REF]. While the AMY has historically been primarily studied in the context of threat avoidance and fear conditioning [REF], extensive research has linked the AMY not only with negative emotional states of anxiety and aversion, but with positive emotional states of approach and reward as well [REF].More recently, experimental manipulations of CeA and BNST neurons also revealed their abilities to potently modulate transitions between sleep and wakefulness [REF]. These sparse reports extend the functions of AMY neurons far beyond their canonical roles as detectors and mediators of affective behavioral states [REF]. In fact, they likely represent only “the tip of the iceberg” in terms of the multifaceted contributions of AMY circuits to arousal regulation, and in particular, the relationships between emotional regulation and arousal (wakefulness) in sleep disorders such as narcolepsy.Narcolepsy is a sleep disorder impacting ~4 million people across the globe that is characterized by excessive daytime sleepiness and abnormal episodes of sleep that intrude upon normal wakefulness [REF]. Human patients with narcolepsy can experience sleep-like episodes at any period throughout the day, often coupled with hypnopompic or hypnagogic hallucinations [REF]. Furthermore, approximately half of people with narcolepsy are estimated to experience symptoms of rapid eye movement (REM) sleep behavior disorder, in which normal muscle atonia during REM fails and they may physically act out the behaviors ongoing in their dreams [REF]. One popular interpretation of narcolepsy is that it is an inappropriate intrusion of REM sleep into wakefulness. Other interpretations exist—for a summary, see Adamantidis et al. (2020). Previous studies in humans and animals have identified genetic markers linked to increased risk of narcolepsy [REF], and the neuroimmune hypothesis of narcolepsy has been described in detail elsewhere [REF]. Those topics are beyond the scope of this article, and we refer the reader to other expert reviews.Categorized into two types, narcolepsy type 1 (NT1) presents with more severe symptoms and is accompanied by cataplexy [REF]. In contrast, narcolepsy type 2 (NT2) has less severe symptoms and is not coupled with cataplexy. For these reasons, our review will focus exclusively on NT1 rather than NT2, although there is scant literature suggesting that lesions of the amygdala can generate symptoms of NT2 [REF].NT1 in humans is strongly associated with the loss of a specific subpopulation of neurons in the lateral hypothalamus (LH) containing the wake-promoting neuropeptide Orexin [REF]. Correspondingly, NT1 patients display lower levels of Orx in the cerebrospinal fluid [REF] as well as of Orx mRNA and peptide in the brain [REF]. The Orx peptide system is critical for stabilizing wakefulness (arousal) and Orx-LH neurons project broadly throughout the brain to exert excitatory physiological effects via signaling of the G-protein-coupled Orexin receptors type 1 and 2 [REF]. Orx activity is essential not only for initiating and maintaining wakefulness, but also for many motivated behaviors such as avoiding threatening stimuli or approaching rewarding stimuli [REF]. NT1 has been identified in other animals as well, with canine NT1 being caused by a mutation in the OX2 receptor gene [REF]. NT1 can also be modeled using genetic manipulations of the Orx system, as discussed later.During episodes of cataplexy, awake and alert subjects can lose muscle tone and eventually experience muscle paralysis and fall into a REM sleep-like state, and these transitions can be detectable by loss of muscle activity on electromyogram recordings (EMG). Frequently, cataplexy is triggered by individuals experiencing powerful positive emotional stimuli, such as laughter or social interactions [REF]. In rare cases, it may be triggered by sexual intercourse (Poryazova et al., 2009a). Occasionally, negative emotional stimuli such as anger or frustration can also initiate cataplexy [REF]. It has been shown in mouse models of narcolepsy that both appetitive/rewarding and aversive stimuli can increase cataplexy [REF].The circuitry underlying the switches between sleep and wake states is complex and involves multiple subregions, pathways, and neurotransmitters. However, given the interpretation of narcolepsy as an intrusion of REM sleep into wakefulness, and that episodes of cataplexy trigger a loss of muscle tone, as seen during REM sleep, it is important to review the circuitry believed to underlie the muscle atonia of REM sleep. During REM, it is believed that activation of the sublaterodorsal tegmental nucleus (SLD, also known as subcoeruleus nucleus) is what maintains muscle atonia. Downstream projections from the SLD ultimately lead to inhibition of motor neurons. During wake, projections from the Orx-expressing LH neurons to REM-regulating regions ultimately inhibit the SLD, allowing normal muscle tone to persist (for a detailed review of this circuitry, see Arrigoni et al. (2016)). In NT1, with loss of Orx neurons in the LH, this tonic inhibition of the SLD is lost, leading to inappropriate intrusion of muscle tone during wake (Figure 1).Summary of the known functional connections between AMY, Orx-LH neurons, REM-off promoting neurons, and the SLD. The thickness of the lines indicates hypothesized strengths and activity levels of connections. Under normal conditions, stimulation of BNST-GABA neurons can trigger wakefulness, and stimulation of CeA-Neurotensin neurons can increase NREM sleep. The BNST and CeA send modulatory neuropeptide (and inhibitory GABAergic) inputs to Orx-LH neurons, as well as inhibitory GABAergic projections to REM-Off regions (including vlPAG, LC, DRN, LPT). BLA neurons send excitatory glutamatergic projections to REM-Off neurons. REM-Off neurons inhibit SLD glutamate neurons that generate muscle atonia such as during REM under normal conditions. In NT1, with loss of Orx, there is an imbalance of excitatory/inhibitory input to REM-off regions. Inhibitory input from AMY to REM-Off neurons may be more pronounced as a consequence, resulting in less disinhibition (excitation) of SLD glutamate neurons. SLD glutamate neurons typically promote muscle atonia, and can lead to cataplexy upon being released from inhibitory control during wakefulness. Figure created using BioRender.com.Given the known role of the AMY in driving affective states, and the well-documented impacts of powerful emotions on sleep and wakefulness [REF], decades of research have sought to elucidate the neural interactions between AMY circuits and Orx-LH neurons in NT1 [REF]. Here, we (1) describe the distinct interactions of AMY subregions with hypothalamic and cortical systems relevant to narcolepsy, and their relationships to sleep regulation. Next, we (2) summarize descriptive studies in human patients that link changes in AMY activity and connectivity to NT1. In the following section, we (3) provide a synopsis of mechanistic studies in animal models of NT1 that causally demonstrate roles for the AMY in cataplexy bouts. Finally, we (4) synthesize these collective findings into an updated circuit model highlighting distinct AMY pathways in discrete contributions to affective regulation and emotional triggers in the context of narcolepsy with cataplexy."
    },
    "2023-37267399_873_en.txt": {
        "title": "Testing persuasive messages about booster doses of COVID-19 vaccines on intention to vaccinate in Australian adults: A randomised controlled trialDecision Letter 0Author response to Decision Letter 0Decision Letter 1Author response to Decision Letter 1Decision Letter 2Acceptance letter",
        "prompt": "Abstract: Achieving high COVID-19 vaccine booster coverage is an ongoing global challenge. Health authorities need evidence about effective communication interventions to improve acceptance and uptake. This study aimed to test effects of persuasive messages about COVID-19 vaccine booster doses on intention to vaccinate amongst eligible adults in Australia.In this online randomised controlled trial, adult participants received one of four intervention messages or a control message. The control message provided information about booster dose eligibility. Intervention messages added to the control message, each using a different persuasive strategy, including: emphasising personal health benefits of booster doses, community health benefits, non-health benefits, and personal agency in choosing vaccination. After the intervention, participants answered items about COVID-19 booster vaccine intention and beliefs. Intervention groups were compared to the control using tests of two proportions; differences of ≥5 percentage points were deemed clinically significant. A sub-group analysis was conducted among hesitant participants.Of the 487 consenting and randomised participants, 442 (90.8%) completed the experiment and were included in the analysis. Participants viewing messages emphasising non-health benefits had the highest intention compared to those who viewed the control message (percentage point diff: 9.0, 95% CI -0.8, 18.8, p = 0.071). Intention was even higher among hesitant individuals in this intervention group compared to the control group (percentage point diff: 15.6, 95% CI -6.0, 37.3, p = 0.150). Conversely, intention was lower among hesitant individuals who viewed messages emphasising personal agency compared to the control group (percentage point diff: -10.8, 95% CI -33.0, 11.4, p = 0.330), although evidence in support of these findings is weak.Health authorities should highlight non-health benefits to encourage COVID-19 vaccine booster uptake but use messages emphasising personal agency with caution. These findings can inform communication message development and strategies to improve COVID-19 vaccine booster uptake.Clinical trial registration: Registered with the Australian New Zealand Clinical Trials Registry (ACTRN12622001404718); trial webpage: https://www.anzctr.org.au/ACTRN12622001404718.aspx\n\n1. Introduction\n",
        "text": "Achieving high COVID-19 vaccine booster coverage is an ongoing global challenge. Health authorities need evidence about effective communication interventions to improve acceptance and uptake. This study aimed to test effects of persuasive messages about COVID-19 vaccine booster doses on intention to vaccinate amongst eligible adults in Australia.COVID-19 vaccination has been critical for controlling the COVID-19 pandemic by protecting vulnerable individuals from severe disease, safeguarding health systems and helping return society to normal functioning [REF]. Booster doses of COVID-19 vaccines (additional doses given after completion of the two-dose primary course) are necessary to provide ongoing immunity to SARS-CoV-2 and offer increased protection against severe disease [REF].Suboptimal uptake of COVID-19 booster doses has occurred globally. In Australia, completion of a primary course of COVID-19 vaccine for individuals ≥16 years is >96%, but booster dose (third dose) coverage has stalled at just over 70% [REF]. In other countries, the United Kingdom (UK) has achieved approximately 70% booster dose coverage in the eligible population [REF], while Europe and the United States (US) have achieved just over 50% and 43% respectively [REF].While many factors can contribute to low vaccine uptake, including both access and acceptance barriers, low motivation may continue to act as a barrier to uptake if not addressed [REF]. This has been the experience with other pandemic vaccines, for example the 2009 H1N1 influenza pandemic vaccine [REF]. There are a range of factors that influence motivation to receive a COVID-19 vaccine. Both personal and collective health benefits have been found to shape acceptance and motivation [REF], building on evidence from strategies to increase vaccine uptake more broadly [REF]. Non-health benefits, i.e. the broader benefits of vaccination beyond direct protection from disease such as ability to travel, have been found to be motivating factors [REF]. Attaching importance to certain moral values, such as individual liberties and the agency or freedom to make personal health decisions, have been linked to hesitancy about COVID-19 vaccines [REF] as well as routine vaccines [REF].Considering these findings, emphasising motivational factors in persuasive messaging could potentially be a successful strategy to support higher intention to receive a COVID-19 vaccine. There is evidence that persuasive messages can support higher intentions towards vaccination in general [REF]. For COVID-19 vaccination specifically, a systematic review has found that messages emphasising personal or community health benefits of vaccination can be effective in supporting higher intentions, although results are mixed [REF]. Three survey experiments in US and UK adults comparing the effect of a range of messages on intention to receive a COVID-19 vaccine found that messages emphasising personal health benefits of vaccination have a larger effect than those emphasising benefits to others [REF]. By contrast, another large survey experiment in US adults found that messages emphasising non-health benefits (such as freedom from public health restrictions) and community benefits were effective. The community messages were the most effective of all, while those emphasising personal health benefits were not more effective than the control [REF]. Evidence of the effectiveness of messaging promoting altruistic behaviour (i.e. behaviour beneficial to the community) on COVID-19 vaccine intention has also been found in other experiments in US and UK adults [REF]. There is limited research investigating the effectiveness of messages emphasising personal agency in making COVID-19 vaccination decisions.Given the current state of evidence, it remains unclear what types of messages have the greatest effect on intention to receive a booster dose of COVID-19 vaccine. This is especially the case in populations that have experienced relatively low risk of encountering COVID-19, combined with restrictive public health measures. These were the conditions experienced in Australia at the time of this experiment, where COVID-19 case numbers were relatively low during the first two years of the pandemic, while public health restrictions, such as border closures and lockdowns, were applied liberally due to the country’s pursuit of a COVID-19 elimination strategy [REF]. Such evidence can inform communications from health authorities and other stakeholders to encourage uptake of COVID-19 booster doses, especially written communications (e.g. emails), which were used to communicate with the public in Australia during the COVID-19 vaccine rollout. This study aimed to compare the effect of persuasive messages on intention to get a booster dose of COVID-19 vaccine in Australian adults. This study is part of a larger study investigating factors influencing COVID-19 vaccine acceptance in Australia and elsewhere [REF] and developing messaging to support acceptance of COVID-19 vaccines in various populations [REF]."
    },
    "2023-37268891_613_en.txt": {
        "title": "How frail is frail in oncology studies? A scoping review",
        "prompt": "Abstract: The frailty index (FI) is one way in which frailty can be quantified. While it is measured as a continuous variable, various cut-off points have been used to categorise older adults as frail or non-frail, and these have largely been validated in the acute care or community settings for older adults without cancer. This review aimed to explore which FI categories have been applied to older adults with cancer and to determine why these categories were selected by study authors.This scoping review searched Medline, EMBASE, Cochrane, CINAHL, and Web of Science databases for studies which measured and categorised an FI in adults with cancer. Of the 1994 screened, 41 were eligible for inclusion. Data including oncological setting, FI categories, and the references or rationale for categorisation were extracted and analysed.The FI score used to categorise participants as frail ranged from 0.06 to 0.35, with 0.35 being the most frequently used, followed by 0.25 and 0.20. The rationale for FI categories was provided in most studies but was not always relevant. Three of the included studies using an FI > 0.35 to define frailty were frequently referenced as the rationale for subsequent studies, however, the original rationale for this categorisation was unclear. Few studies sought to determine or validate optimum FI categorises in this population.There is significant variability in how studies have categorised the FI in older adults with cancer. An FI ≥ 0.35 to categorise frailty was used most frequently, however an FI in this range has often represented at least moderate to severe frailty in other highly-cited studies. These findings contrast with a scoping review of highly-cited studies categorising FI in older adults without cancer, where an FI ≥ 0.25 was most common. Maintaining the FI as a continuous variable is likely to be beneficial until further validation studies determine optimum FI categories in this population. Differences in how the FI has been categorised, and indeed how older adults have been labelled as ‘frail’, limits our ability to synthesise results and to understand the impact of frailty in cancer care.The online version contains supplementary material available at 10.1186/s12885-023-10933-z.\n\n1. Introduction\n",
        "text": "Frailty is a dynamic state of diminished physiological reserve and increased vulnerability to adverse events. It has been recognised as a prevalent and important consideration in the individualised management of older adults with cancer [REF]. Routine screening for geriatric conditions has been recommended for all adults over 65 or 70 years of age with a new cancer diagnosis [REF]. In contrast to older adults in the community-dwelling or acute care settings, those living with cancer face the additional acute stressors of cancer symptoms and potential treatment-related toxicities. Frailty has significant implications for not only understanding the underlying health status of a potentially-vulnerable individual with cancer, but also in influencing oncological treatment decisions and discourse, and tailoring non-oncological interventions or supports. Clinicians try to determine those who are too frail for treatment, those who require modified treatment or additional supports, and those who are deemed fit for standard therapy. However, there is currently no consensus regarding the optimum frailty screening or measurement tool in this population.The frailty index (FI) is one way in which frailty can be quantified [REF]. The FI conceptualises frailty as a multi-dimensional risk state which can be measured by the number, rather than the nature, of health problems. An FI is calculated as a proportion of deficits using a well-defined method [REF] e.g., someone with 6 deficits out of 40 counted has an FI of 0.15. As a continuous variable, ranging from zero (most robust) to a theoretical maximum of one (most frail), the FI affords great precision in risk stratification by capturing frailty gradations. In a scoping review of FI in the community and acute care settings, an FI ≥ 0.25 was the most frequently used score to diagnose people as frail, however this was used in less than half of the identified studies [REF]. This score was derived from work by Rockwood and colleagues, which demonstrated that FI = 0.25 had construct and predictive validity to categorise community-dwelling adults as frail or non-frail [REF]. It correlated with the crossing point between robust and frail groups according to Fried et al.’s phenotype model of frailty [REF], another well validated yet conceptually distinct definition of frailty in older persons [REF], and was predictive of institutionalisation and death. It also presented the crossing point between Clinical Frailty Scale (CFS) ‘apparently vulnerable’ (mean FI = 0.22) and ‘mildly frail’ (mean FI = 0.27) [REF].However, little is known regarding the validity of FI categories in the context of cancer, and variation in who is deemed frail may be used to determine trial eligibility or treatment allocation [REF], and referral for additional assessments or supports [REF]. It is therefore important to understand how the FI has been categorised in oncology literature, and to understand the rationale for these decisions [REF].The objectives of this scoping review were: (i) to evaluate which FI categories (FI scores and labels) have been used in an oncology setting; and (ii) to identify why these categories were selected by the study authors."
    },
    "2023-37268953_1302_en.txt": {
        "title": "Preschool- and childcare center-based interventions to increase fruit and vegetable intake in preschool children in the United States: a systematic review of effectiveness and behavior change techniques",
        "prompt": "Abstract: Fruit and vegetable (FV) consumption in children in the United States (US) is very low. Adequate FV consumption is required for proper development during childhood, and dietary habits are established during preschool-age and tend to persist into adulthood. As most U.S. preschool-aged children attend childcare or preschool, this may be an opportune time and setting to conduct interventions to improve FV intake. These interventions should be based in theory and use behavior change techniques (BCTs) to explain mechanisms for expected change. To date, no published reviews have examined the effectiveness of childcare- or preschool-based FV interventions in preschoolers and their use of theoretical frameworks and BCTs.This systematic review was completed adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. Inclusion criteria were randomized controlled trials (RCTs) published between 2012 and 2022 of interventions to improve diet or FV intake in preschoolers (aged 2–5 years) in childcare or preschool-settings. A search of four databases was conducted between in September 2022 using search terms pertaining to the study’s primary aim (FV consumption), age group (preschool-aged), settings (US childcare or preschool settings), and study design (RCT). Additional criteria were objective measures of FV consumption or skin carotenoids, as a proxy for FV intake. Included studies were narratively synthesized based on intervention type, measured effect, and use of theory and BCTs.The search resulted in six studies that reported on nine interventions. Overall, six interventions increased FV intake, of which five used nutrition education and one manipulated the feeding environment. Among the three interventions with no measured effect, two manipulated the feeding environment and one used peer modeling. Effective studies used at least three BCTs, though no pattern was observed between use of theory or BCTs and intervention effect.While several studies have shown promising results, the limited number of studies identified in this review highlights key gaps in this field: there is a need for studies to test FV interventions in US childcare settings that use objective measures of FV intake, directly compare intervention components and BCTs, are theory-based, and assess long-term behavior change.The online version contains supplementary material available at 10.1186/s12966-023-01472-8.\n\n1. Introduction\n",
        "text": "Promoting fruit and vegetable (FV) intake in children is critical to support proper brain and body development [REF] and to establish healthy dietary habits that persist over the life course [REF]. Despite the long-standing explicit guidelines and evidence regarding the importance of FV intake, average FV consumption amongst all US children (2–18 years old) remains low: only 40% and 7% meet the recommended intake for fruits and vegetables, respectively [REF]. The current average intake for children is only 0.9 cup equivalents of each (60% of recommended), with consumption levels that decline with age and are restricted in variety relative to recommended guidelines [REF]. Thus, in support of public health, it is important to develop strategies to improve FV intake in children.Preschool age children begin developing their own dietary habits by gaining autonomy over their food choice [REF], evident in the decline in FV intake as children transition from preschool age to school age [REF]. Thus, preschool age (2–5 years old) may be an optimal time for a dietary intervention to promote FV intake throughout the lifespan. However, there are key gaps and inconsistencies present in diet research that will be discussed below.Consideration of the measurement tool in diet research is particularly important in young children because they are unable to accurately report their own intake [REF]. Parent-reported measures, while most frequently used in research [REF], have been repeatedly shown to be subjective and prone to recall and reporting bias [REF]; this is especially problematic when parents are asked to report on periods of time for which they are not directly responsible for child feeding, such as during childcare hours [REF]. As such, use of objective observation measures is critical with this age group [REF]. An additional objective measure specific to FV intake is the skin carotenoid level [REF]. When FVs are consumed, the carotenoids in the FVs are absorbed and then deposited in various tissues including the skin [REF], which can be quantified using reflection spectroscopy [REF] to objectively measure FV intake within the previous two to four weeks [REF]. Due to the discrepancies associated with the different techniques used to measure dietary intake, reviews should distinguish between studies with subjective and objective measures to adequately evaluate the validity of a large proportion of this body of work.The setting of dietary interventions in children is another significant consideration. In 2019, nearly two-thirds (64%) of 3–5 year old children in the US were enrolled in childcare or preschool with 64.7% of all enrollments being full-time [REF]. Thus, childcare services have a large influence on children’s development in the US [REF], and this may be an efficient avenue to effectively influence children’s behavior, namely dietary behavior [REF]. In evaluating intervention effectiveness, it is therefore vital to consider the setting of the interventions to avoid extrapolating evidence for intervention effectiveness to other contexts.Behavior change techniques (BCTs) are the intervention components regarded as the “active ingredients” within behavior change interventions. It is often helpful to examine the use of BCTs within interventions aimed at changing dietary behaviors to assess the mechanism by which interventions may be effective at causing the behavior change [REF]. Given the heterogeneity in intervention techniques used in dietary interventions, evaluation of BCTs in reviews and meta-analyses may provide important insight on the underlying intervention components that may be at play [REF].Use of theory in developing an intervention is another metric that can be examined within interventions that target behavior change. This metric evaluates how exactly a specific theory and its concepts are utilized to tailor intervention techniques and components and allows for a better understanding of why an intervention is effective or ineffective. This knowledge may then be evaluated in systematic reviews and applied to the refinement of an interventions to better target the tenets of the theory [REF].A recent systematic literature review by Hodder et al. [REF] of FV intake in children five years and younger identified 80 trials reporting a large variety of interventions to promote FV intake in preschool-aged children globally. This review included interventions conducted in all settings and using various intervention methods, though it did not distinguish between subjective and objective measurements of food intake or evaluate BCTs and use of theory in the included studies. The main implications drawn from these findings is that more pragmatic studies are needed to test the interventions, and that these interventions should be based on BCTs and theoretical frameworks that may explain the mechanism by which these interventions may change children’s dietary behaviors related to FV intake [REF].In 2012, Hendrie et al. [REF] published a systematic review to explore the use of BCTs in home- and school-based interventions for the prevention of childhood obesity involving children and parents. More specifically, they compared the number and type of BCTs used in effective and ineffective studies and reported that effective studies incorporated more BCTs than ineffective studies (median of 10 versus 6.5). However, this review was not specific to interventions aimed at improving FV intake, so we are unable to determine whether these findings are applicable to FV interventions specifically.In 2017, Hendrie et al. [REF] used a similar technique to evaluate interventions to improve vegetable intake in children and found that the BCTs “Repeated exposure”, “Provision of staff training,” and “Planning for social support or change,” were associated with effective behavior change. This review only examined interventions conducted in home or community settings and, therefore, these results may not be extrapolated to interventions conducted in other settings, such as childcare centers [REF]. Similar to the review by Hodder et al., these two reviews are limited by their inclusion of subjective measures of dietary change, and lack of evaluation of use of theory within the included studies.To our knowledge, there are currently no published reviews that evaluate both the effectiveness of FV interventions specifically in childcare- or preschool-based settings in the US, and the use of theoretical frameworks and BCTs within these studies. This is a critical gap as comparing different types of interventions, exploring whether use of theory and BCTs moderate effectiveness, and summarizing the level of evidence is critically needed to develop effective interventions in the future. Therefore, the purpose of this review was to systematically identify published randomized controlled trials (RCTs) evaluating childcare- or preschool-based interventions designed to increase objectively measured intake of fruits, vegetables, or both, in preschool children (aged 2–5 years) in the US and to summarize their methods and results. This review also aimed to identify the use of theoretical models and BCTs in each study and to assess their effectiveness in improving FV intake."
    },
    "2023-37269002_827_en.txt": {
        "title": "Self-locking stand-alone cage versus cage-plate fixation in monosegmental anterior cervical discectomy and fusion with a minimum 2-year follow-up: a systematic review and meta-analysis",
        "prompt": "Abstract: Currently, self-locking stand-alone cages (SSC) are commonly applied in anterior cervical discectomy and fusion (ACDF), as are cage-plate constructs (CPC). However, it remains controversial concerning the long-term effectiveness of both apparatuses. Our purpose is to compare long-term effectiveness of SSC with CPC in monosegmental ACDF.Four electronic databases were queried to identify studies comparing SSC versus CPC in monosegmental ACDF. The meta-analysis was carried out with the use of the Stata MP 17.0 software package.Ten trials with 979 patients were included. Compared to CPC, SSC significantly reduced operative time, intraoperative blood loss, duration of hospitalisation, cervical Cobb angle at final follow-up, 1-month postoperative dysphagia rate, and incidence of adjacent segment degeneration (ASD) at final follow-up. No significant difference was found regarding 1-month postoperative cervical Cobb angle, JOA scores, NDI scores, fusion rate and cage subsidence rate at final follow-up.Both devices achieved similar long-term effectiveness in monosegmental ACDF regarding JOA scores, NDI scores, fusion rate and cage subsidence rate. SSC had significant advantages over CPC in reducing surgical duration, intraoperative bleeding, duration of hospitalisation, as well as rates of dysphagia and ASD after surgery. Therefore, SSC is a better option than CPC in monosegmental ACDF. However, SSC is inferior to CPC in maintaining cervical curvature at long-term follow-up. Whether radiological changes affect clinical symptoms needs confirmation in trials with longer follow-up.The online version contains supplementary material available at 10.1186/s13018-023-03885-4.\n\n1. Introduction\n",
        "text": "With rapid economic development, changes in lifestyles and increasing work pressure, the number of people suffering from neck pain is increasing continuously. It was reported that approximately 220 million people around the world were affected by neck pain, which brought a huge economic burden to society [REF]. Neck pain in adults is usually attributed to degenerative cervical spondylosis (DCS), which in severe cases can develop into spinal cord dysfunction. DCS is a series of clinical syndromes resulting from damage to the spinal cord, nerves and blood vessels caused by degeneration of cervical intervertebral discs and subsequent changes. This debilitating disorder can be treated by conservative means in the early stages. However, once conservative treatment is no longer effective or symptoms continue to worsen, the compressed spinal cord is in danger of irreversible damage. To relieve nerve compression and improve symptoms, surgical treatment should be performed as soon as possible [REF].Currently, anterior cervical discectomy and fusion (ACDF) is widely recognised a gold standard for treating DCS. Through the use of anterior plate fixation, traditional ACDF can provide immediate cervical spinal stability, direct sufficient and effective nerve decompression, normalize intervertebral height, and reconstruct cervical physiological curvature, as well as reducing the incidence of pseudoarthrosis. The placement of a titanium plate can provide additional stability to the operated segment, preventing collapse of the interbody fusion device and the formation of a kyphotic deformity [REF]. However, intraoperative placement of the plate requires as much exposure of the surgical field as possible, which is usually accompanied by complications, such as anterior soft tissue injury, postoperative dysphagia and degeneration of adjacent segments [REF]. Lu et al. attributed postoperative dysphagia to soft tissue oedema at the surgical location, surrounding hematoma, esophageal injury, and tissue adhesion around the plate [REF]. Cage-plate construct (CPC) alters the normal biomechanical state of the cervical spine, which leads to loss of motion at the operated segment, concentrated stress loads on adjacent segments, and increased abnormal activity, thereby accelerating adjacent segment degeneration (ASD) [REF].With the intention of reducing the risk of the complications mentioned above, a novel self-locking stand-alone cage (SSC) based on the zero-notch design concept came into being. It was designed to allow self-locking screws to be inserted through the fusion device into the vertebral body of the adjacent segments without plate fixation. The key difference between SSC and CPC is that in the process of operation, there is no need to expose a large anatomical area, which significantly reduces the surgical injury and the formation of postoperative scars. SSCs can effectively avoid invasion of important structures located in the front of the cervical spine, thereby reducing the incidence of postoperative complications such as hematoma, ASD and postoperative dysphagia [REF]. Studies have shown that SSC has the similar clinical efficacy compared with cage-plate fixation [REF]. It can effectively obtain good stability of fused segments, thereby ensuring eventual bone fusion. Nevertheless, it faces controversy in terms of maintaining the physiological curvature of cervical spine, preventing the subsidence of fusion devices and so on [REF].Some scholars conducted meta-analyses and further compared the efficacy and complications between SSC and CPC in treating DCS, yet the findings remained diverse [REF]. Limitations existed among these previous meta-analyses, including one fact that some papers involved different surgical segments and different lengths of follow-up. Currently, the superiority of SSC over CPC in terms of short- and medium-term efficacy and safety has been confirmed in numerous meta-analyses. Still, some meta-analysis results are not clear when it comes to comparing the long-term outcomes of SSC versus CPC [REF]. One explanation for this phenomenon is that variations in surgical segments and follow-up lengths may influence the results of the meta-analyses. Up to now, no specific meta-analysis has been carried out to investigate the long-term benefits and complications between SSC and CPC in monosegmental ACDF. Therefore, our aim is to evaluate the long-term effectiveness and security of SSC compared to traditional cage-plate fixation in the treatment of monosegmental ACDF with a follow-up length of ≥ 2 years, in attempt to provide convincing evidence for clinicians to make clinical decisions."
    },
    "2023-37269003_904_en.txt": {
        "title": "Non-adherence and non-persistence to intravitreal anti-vascular endothelial growth factor (anti-VEGF) therapy: a systematic review and meta-analysis",
        "prompt": "Abstract: Intravitreal anti-vascular endothelial growth factor (anti-VEGF) injections play a key role in treating a range of macular diseases. The effectiveness of these therapies is dependent on patients’ adherence (the extent to which a patient takes their medicines as per agreed recommendations from the healthcare provider) and persistence (continuation of the treatment for the prescribed duration) to their prescribed treatment regimens. The aim of this systematic review was to demonstrate the need for further investigation into the prevalence of, and factors contributing to, patient-led non-adherence and non-persistence, thus facilitating improved clinical outcomes.Systematic searches were conducted in Google Scholar, Web of Science, PubMed, MEDLINE, and the Cochrane Library. Studies in English conducted before February 2023 that reported the level of, and/or barriers to, non-adherence or non-persistence to intravitreal anti-VEGF ocular disease therapy were included. Duplicate papers, literature reviews, expert opinion articles, case studies, and case series were excluded following screening by two independent authors.Data from a total of 409,215 patients across 52 studies were analysed. Treatment regimens included pro re nata, monthly and treat-and-extend protocols; study durations ranged from 4 months to 8 years. Of the 52 studies, 22 included a breakdown of reasons for patient non-adherence/non-persistence. Patient-led non-adherence varied between 17.5 and 35.0% depending on the definition used. Overall pooled prevalence of patient-led treatment non-persistence was 30.0% (P = 0.000). Reasons for non-adherence/non-persistence included dissatisfaction with treatment results (29.9%), financial burden (19%), older age/comorbidities (15.5%), difficulty booking appointments (8.5%), travel distance/social isolation (7.9%), lack of time (5.8%), satisfaction with the perceived improvement in their condition (4.4%), fear of injection (4.0%), loss of motivation (4.0%), apathy towards eyesight (2.5%), dissatisfaction with facilities 2.3%, and discomfort/pain (0.3%). Three studies found non-adherence rates between 51.6 and 68.8% during the COVID-19 pandemic, in part due to fear of exposure to COVID-19 and difficulties travelling during lockdown.Results suggest high levels of patient-led non-adherence/non-persistence to anti-VEGF therapy, mostly due to dissatisfaction with treatment results, a combination of comorbidities, loss of motivation and the burden of travel. This study provides key information on prevalence and factors contributing to non-adherence/non-persistence in anti-VEGF treatment for macular diseases, aiding identification of at-risk individuals to improve real-world visual outcomes. Improvements in the literature can be achieved by establishing uniform definitions and standard timescales for what constitutes non-adherence/non-persistence.PROSPERO CRD42020216205.\n\n1. Introduction\n",
        "text": "Therapies that inhibit vascular endothelial growth factor (VEGF), ‘anti-VEGFs’, play a key role in reducing angiogenesis and vascular permeability [REF] with the aim to prevent sight loss in ocular diseases, including neovascular age-related macular degeneration (nAMD), diabetic macular oedema (DMO), macular oedema caused by retinal vein occlusion and myopic choroidal neovascularisation (myopic CNV). Currently, anti-VEGF treatments are exclusively administered to patients via intraocular injections for local retinal delivery of the drug. Randomised controlled trials (RCTs) and real-world studies have identified that anti-VEGF treatments, including Ranibizumab, Bevacizumab and Aflibercept [REF] impart visual improvement to up to 40% of patients with nAMD, DMO and macular oedema and to about half of patients with myopic CNV [REF].Anti-VEGF treatment regimens can differ between patients with most patients requiring continual or even indefinite treatments. Patients typically receiving either proactive or pro re nata (PRN) approaches. Proactive treatment protocols involve regular anti-VEGF injections at fixed intervals, usually monthly. Prevention of further sight loss is still dependent on regular monitoring and patient adherence to their treatment regimens [REF]. Previous studies have identified failure in adherence of patients to their anti-VEGF treatment regimens or follow up visits, with various reasons reported. These reasons include the frequency of required visits, difficulty in attending clinical and follow-up appointments, financial limitations, pain, disbelief in the benefit of the treatment, and refusal of continuance of treatment due to associated comorbid conditions [REF].The World Health Organization (WHO) defines adherence to long-term therapy as ‘the extent to which a person’s behaviour—taking medication, following a diet, and/or executing lifestyle changes—corresponds with agreed recommendations from a healthcare provider’ [REF]. There are some differences to the definitions for ‘adherence’ and ‘persistence’ in the literature. Non-adherence, in patients receiving anti-VEGF therapies, would involve deviating from their prescribed therapeutic regimen. Persistence would typically define the duration of continuation with therapy [REF], and non-persistence most often refers to patients choosing to stop their medication against the prescriber’s recommendation. Non-persistence definitions reported in patients on anti-VEGF treatments would typically include ‘discontinuation of therapy’ and ‘loss to follow up’, whereas non-adherence could refer to ‘missed appointments,’ ‘irregular attendance’ or ‘gaps in treatment’.Medication non-adherence may occur at different points in a patient’s decision-making process. It may occur at the outset of their therapy or at some point during their therapy. Previous studies have reported a variation in the rates of therapy discontinuation (non-persistence) of anti-VEGF treatment in diseases such as nAMD to be approximately 42% [REF] and 50% [REF], with factors such as patients’ level of awareness of their disease and treatment affecting compliance to therapy. Similarly, the adherence to treatment regimen with anti VEGF therapy improves the clinical outcomes in patients with nAMD, DMO and CNV [REF].Recently, the coronavirus (COVID-19) pandemic has impacted patient-led adherence to intravitreal injections [REF]. The governments around the world imposed strict measures to prevent the spread of the disease. This included stay-at-home advisories and a reduction in non-urgent care [REF]. The consensus among retinal disease experts was that for neovascular AMD, retinal vein occlusion and diabetic retinopathy patients, anti-VEGF injection regimens should continue during lockdowns or curtailed non-urgent ophthalmic services [REF]. Regardless, from a patient perspective, fear of infection, difficulty travelling, and COVID-19 infection within a household were likely to have had an impact on attendance to appointments [REF]. It is therefore crucial to quantify levels of non-adherence during the pandemic to inform future practices to minimise disruption to essential ophthalmic care.Recent systematic reviews have investigated patient non-adherence and non-persistence to anti-VEGF treatment regimens in nAMD and DMO specifically [REF]; all identify a need for further investigation in this understudied area. In particular, there is a need to investigate reasons for non-adherence and non-persistence, rates of attendance for follow-ups and to determine strategies to tackle these challenges of under-treatment and reduce the burden of ‘sight-threatening’ chronic eye diseases for patients and healthcare providers. The aim of this systematic review and meta-analysis was to investigate the prevalence of patient-led non-adherence/non-persistence to intravitreal anti-VEGF therapy, and the barriers/reasons associated with non-adherence/non-persistence in different disease states."
    },
    "2023-37269528_673_en.txt": {
        "title": "Iron-Related Genes and Proteins in Mesenchymal Stem Cell Detection and Therapy",
        "prompt": "Abstract: Mesenchymal stem cells (MSCs) are located in various tissues of the body. These cells exhibit regenerative and reparative properties, which makes them highly valuable for cell-based therapy. Despite this, majority of MSC-related studies remain to be translated for regular clinical use. This is partly because there are methodical challenges in pre-administration MSC labelling, post-administration detection and tracking of cells, and in retention of maximal therapeutic potential in-vivo. This calls for exploration of alternative or adjunctive approaches that would enable better detection of transplanted MSCs via non-invasive methods and enhance MSC therapeutic potential in-vivo. Interestingly, these attributes have been demonstrated by some iron-related genes and proteins.Accordingly, this unique forward-looking article integrates the apparently distinct fields of iron metabolism and MSC biology, and reviews the utility of iron-related genes and iron-related proteins in facilitating MSC detection and therapy, respectively. Effects of genetic overexpression of the iron-related proteins ferritin, transferrin receptor-1 and MagA in MSCs and their utilisation as reporter genes for improving MSC detection in-vivo are critically evaluated. In addition, the beneficial effects of the iron chelator deferoxamine and the iron-related proteins haem oxygenase-1, lipocalin-2, lactoferrin, bone morphogenetic protein-2 and hepcidin in enhancing MSC therapeutics are highlighted with the consequent intracellular alterations in MSCs. This review aims to inform both regenerative and translational medicine. It can aid in formulating better methodical approaches that will improve, complement, or provide alternatives to the current pre-transplantation MSC labelling procedures, and enhance MSC detection or augment the post-transplantation MSC therapeutic potential.\n\n1. Introduction\n",
        "text": "Iron in essential for various intracellular activities, and stem cells would be no exception. For example, ribonucleotide reductase is an enzyme that facilitates DNA synthesis and repair, and iron is a cofactor for this enzyme. Also, iron is essential for mitochondrial respiration. It is used in the synthesis of haem and [Fe-S] clusters; specifically, cytochrome c that not only participates in the electron transport chain, but also has a role in apoptosis [REF]. Cytochrome P450 are a group of enzymes that utilise haem (iron) as a cofactor, and these enzymes play an important role in detoxification/metabolism of drugs [REF]. The enzyme catalase possesses haem groups (containing iron) and this enzyme is an important anti-oxidant as it converts hydrogen peroxide to water and oxygen, and thereby prevents/reduces cell damage by free radicals [REF]. Amongst specific examples of the involvement of iron at cellular level include the incorporation of iron within haemoglobin in maturing erythrocytes, and thereby aiding in oxygen transport throughout the body. Iron is also a part of myoglobin found in skeletal and cardiac muscle tissue.In a pathological context, specifically pertaining to stem cells, iron has been found to maintain cancer stem cells [REF] and iron loading has been found to inhibit self-renewal of human pluripotent stem cells [REF]. Also, iron and iron-related proteins play a role in Mesenchymal stem cell (MSC) biology. This includes the role of iron loading on cellular components, processes and signalling pathways of the MSCs [REF].MSCs are the most widely researched stem cell types because of their ability to support several physiological processes in the body and their exuberant reparative and regenerative properties. Located in various body tissues, these cells not only show multilineage differentiation but also secrete immune and trophic factors that stimulate endogenous repair mechanisms at the target site. Furthermore, MSCs show tropism towards tumour and inflammation/injury [REF]. Expectedly, these cells have shown promising results in several in-vitro, pre-clinical and clinical trials for a wide range of pathologies including COVID-19 [REF]Despite their therapeutic potential, MSCs are not frequently used in clinical settings for amelioration of pathological conditions. Amongst the many reasons for this are the challenges encountered in pre-transplantation MSC labelling and post-transplantation MSC detection via non-invasive methods like the Magnetic Resonance Imaging (MRI). Nanoparticles including iron oxide nanoparticles have been used to enhance MSC detection, but their usage is confounded by various challenges [REF]. Evidently, these processes involve multistep and complex approaches that are yet to be perfected.Therefore, it is extremely important to search for effective and non-toxic approaches that not only preserve MSC functionality during extraction and in-vitro cultivation stages but also permit the detection of transplanted MSCs non-invasively and help retain and/or enhance their reparative and regenerative potential in-vivo.Iron-related genes and proteins have shown the potential to support many of these pre-requisites for a successful MSC therapy. Thus, this review compiles and critically evaluates the usage of the iron-related genes (genes of ferritin, transferrin receptor-1 and MagA) as reporter genes because their encoded proteins allow cellular iron accumulation that eases in-vivo MSC detection and tracking via MRI. The review also addresses the roles of deferoxamine (iron chelator) and the iron-related proteins haem oxygenase-1, lipocalin-2, lactoferrin, bone morphogenetic protein-2 (BMP-2) and hepcidin in preserving MSC characteristics in-vitro and in-vivo, and in enhancing MSC therapeutics."
    },
    "2023-37270471_819_en.txt": {
        "title": "Instrument–assisted soft tissue mobilization versus myofascial release therapy in treatment of chronic neck pain: a randomized clinical trial",
        "prompt": "Abstract: The purpose of this study was to investigate the effect of instrument-assisted soft tissue mobilization (IASTM) versus myofascial release therapy (MRT) on college students with chronic mechanical neck pain (CMNP).Thirty-three college students with a mean age of 21.33 ± 0.98 involved in distance learning due to the Corona Virus 2019 (COVID-19) restriction were randomized to receive either IASTM on the upper trapezius and levator scapulae muscles or MRT. Researchers measured their pain with a visual analog scale (VAS), function with neck disability index (NDI), and pain pressure threshold (PPT) with a pressure algometer. The subjects received eight therapy sessions over four weeks and outcome measures were assessed pre and post-intervention. The study was registered as a clinical trial on clinicaltrials.gov (registration number: NCT05213871).Unpaired t-test showed no statistical significance between the two groups post-intervention regarding improvement in pain, function, and PPT (p > 0.05).This study showed insignificant differences between groups. However, we did not use a control group, indicating that the improvement in outcomes may not have been caused by the intervention.Quasi-experimental two groups pre-posttest clinical trial.Therapy, level 2b.\n\n1. Introduction\n",
        "text": "Neck pain is one of the most frequently encountered disorders in clinical settings [REF] and is often difficult to diagnose and treat [REF]. Neck pain continues to increase in the general population and specific subgroups worldwide [REF]. Published data on the prevalence is variable, but it is estimated that 22–70% of the general population will experience pain at some point in life [REF].Patients with chronic mechanical neck pain (CMNP) are present with a wide range of symptoms ranging from mild pain and minimal functional limitation to complete disability [REF]. Therefore, it has great socio-economic and negative health impacts [REF].Among the identified risk factors for the development of CMNP are a long history of neck pain, worrisome attitude, poor quality of life, and less vitality. The same clinical practice guidelines and its updated revision identified female gender and prior history of neck pain as predisposing factors for the development of a new onset of neck pain. In addition, there is low to moderate evidence that high job demands, history of smoking, low social/work support, and history of low back pain are risk factors for the development of neck pain in general [REF].The international classification of functioning, disability, and health (ICF) endorses functional terminologies in describing health conditions. Therefore, the clinical practice guidelines linked to the ICF classify patients with neck pain into four categories: neck pain with mobility deficits, neck pain with headache, neck pain with movement coordination deficits, and neck pain with radiated upper extremity pain. Each category is presented with clinical findings specific to that category2,3.There is strong evidence that young individual patients with a duration of symptoms less than 12 weeks can be diagnosed with neck pain and mobility deficits when they are presented with symptoms isolated to the neck and have a limited cervical range of motion (ROM) [REF]. Moreover, the revised clinical practice guidelines of neck pain [REF] identified the patients with a presentation of the following symptoms as having neck pain with mobility deficits: central and/or unilateral neck pain, limitation in cervical ROM with reproduction of familiar symptoms, associated referred shoulder or upper extremity pain.College students can be a risk population for developing CMNP because of the long hours spent studying in front of computer screens [REF]. This can be also triggered by sustained posture and abnormal cervical spine mechanics with tenderness on palpation [REF]. Additional clinical examination findings of patients with neck pain and mobility deficits include limited cervical ROM, neck pain reproduced at the end of active and passive ROM, restricted cervical and thoracic segmental mobility, associated scapular/thoracic segments pain, and strength deficits in subacute or chronic neck pain [REF].Instrument-assisted soft tissue mobilization (IASTM) has gained wide attention as a relatively new technique in the treatment of muscular tightness and pain. Originally described by Cyriax in 1982, this technique can be performed using different tools. The IASTM uses the same concept by applying an adapted pressure on the tight structures using different-shaped stainless-steel tools with beveled edges to conform to different anatomical structures. Although there is some empirical evidence for its use [REF], its effect has not yet been investigated in subjects with CMNP to the authors’ knowledge.Myofascial treatment is an emerging treatment in different musculoskeletal conditions although its clinical benefits is still not clearly understood [REF]. Myofascial release therapy (MRT) aims at restoring the normal length of a tight structure with the target goal of decreasing pain and improving function. Since patients with neck pain are usually presented with myofascial trigger points (MTrPs), MRT can be an effective treatment technique [REF].Since there is a research gap in understanding the effect of IASTM and myofascial MRT in college students with CMNP, the authors of this study were interested in building up evidence for their use. During Coronavirus time, college students were ordered to stay at home to stop spreading infection and as a mitigation strategy. Thus, they had to use the computer for long hours. Therefore, the purpose of this study was to compare the effect of the IASTM technique and MRT on college students studying using distance learning and having CMNP. Researchers hypothesized that there will be no statistically significant difference between the effect of IASTM and MRT on improving pain, function, and/or improving pressure pain threshold."
    },
    "2023-37270546_726_en.txt": {
        "title": "Characteristics of Kenyan women using HIV PrEP enrolled in a randomized trial on doxycycline postexposure prophylaxis for sexually transmitted infection prevention",
        "prompt": "Abstract: The global incidence of sexually transmitted infections (STIs) has been rapidly increasing over the past decade, with more than one million curable STIs being acquired daily. Young women in sub-Saharan Africa have a high prevalence and incidence of both curable STIs and HIV. The use of doxycycline as a prophylaxis to prevent STIs is promising; however, clinical trials, to date, have only been conducted among men who have sex with men (MSM) in high-income settings. We describe the characteristics of participants enrolled in the first trial to determine the efficacy of doxycycline post-exposure prophylaxis (PEP) to reduce STI incidence among women taking daily, oral HIV pre-exposure prophylaxis (PrEP).This is an open-label 1:1 randomized clinical trial on the efficacy of doxycycline PEP compared with standard of care (e.g., quarterly STI screening and treatment) to reduce incident bacterial STIs – Neisseria gonorrhoeae, Chlamydia trachomatis, and Treponema pallidum – among Kenyan women aged ≥18 and ≤30 years. All were also taking HIV pre-exposure prophylaxis (PrEP). We describe the baseline characteristics, STI prevalence, and STI risk perception of participants.Between February 2020 and November 2021, 449 women were enrolled. The median age was 24 years (IQR 21–27), the majority were never married (66.1%), 370 women (82.4%) reported having a primary sex partner, and 33% had sex with new partners in the three months prior to enrolment. Two-thirds (67.5%, 268 women) did not use condoms, 36.7% reported transactional sex, and 43.2% suspected their male partners of having sex with other women. Slightly less than half (45.9%, 206 women) were recently concerned about being exposed to an STI. The prevalence of STIs was 17.9%, with C. trachomatis accounting for the majority of infections. Perceived risk of STIs was not associated with the detection of an STI.Young cisgender women using HIV PrEP in Kenya and enrolled in a trial of doxycycline postexposure prophylaxis had a high prevalence of curable STIs and represent a target population for an STI prevention intervention.\n\n1. Introduction\n",
        "text": "The global incidence of sexually transmitted infections (STIs) has been rapidly increasing over the past decade, with more than one million curable STIs being acquired daily. Young women in sub-Saharan Africa have a high prevalence and incidence of both curable STIs and HIV. The use of doxycycline as a prophylaxis to prevent STIs is promising; however, clinical trials, to date, have only been conducted among men who have sex with men (MSM) in high-income settings. We describe the characteristics of participants enrolled in the first trial to determine the efficacy of doxycycline post-exposure prophylaxis (PEP) to reduce STI incidence among women taking daily, oral HIV pre-exposure prophylaxis (PrEP).Global trends reveal a rapid increase in the incidence of sexually transmitted infections (STIs) over the past decade, with more than one million curable STIs acquired daily [REF]. In 2020, the World Health Organization (WHO) estimated 374 million new infections of four curable STIs, Chlamydia trachomatis, Neisseria gonorrhoeae, Treponema pallidum, and Trichomonas vaginalis [REF]. Young women in sub-Saharan Africa face a high prevalence of curable STIs and HIV [REF] and limited data from HIV PrEP trials suggests high incidence rates [REF]. STIs can severely affect mortality and morbidity for cisgender women by causing conditions such as tubal infertility, chronic pelvic pain, pelvic inflammatory disease, ectopic pregnancy, post-partum endometriosis, adverse neonatal outcomes, and an increase in susceptibility to HIV [REF]. Women are more biologically predisposed to complications from STIs than men [REF]. Several studies conducted in sub-Saharan Africa reveal higher STI prevalence among younger women compared to their age-matched male peers and older women [REF]. In the region, the cultural, economic, and social marginalization of women contributes to the risk of HIV and STIs [REF], in part by rendering the negotiation of preventive measures such as condom use, abstinence, and partner notification ineffective [REF].Taking antibiotics following sexual exposure to prevent bacterial STIs places preventive care in the hands of the user. Interventions that are individually controlled are greatly needed, especially for women, and the use of doxycycline as a post-exposure prophylaxis (PEP) has been proposed as a novel STI prevention strategy [REF]. Doxycycline is already standardly used as prophylaxis to prevent infections such as malaria, lyme, and leptospirosis [REF]. A recent open-label clinical trial of doxycycline PEP among men who have sex with men (MSM) who were using HIV pre-exposure prophylaxis (PrEP) in France found a 47% relative reduction in bacterial STIs overall and a greater reduction specifically for C. trachomatis (70%) and T. pallidum (73%) [REF]. Doxycycline PEP was well-tolerated in that study [REF]. Several clinical trials of doxycycline as PEP or PrEP among MSM are ongoing worldwide to test this initial finding.Studies among young African women have evaluated the association between HIV incidence and perception of HIV risk with disparate results [REF]. STI risk perception and the prevalence of STIs among women receiving HIV prevention care, which is limited to syndromic management of STIs and daily oral PrEP, have not yet been described. Although women disproportionately bear the burden of adverse sequelae of curable STIs, trials on doxycycline PEP in this population have not yet been completed. The doxycycline postexposure prophylaxis (dPEP) Trial is an open-label, randomized clinical trial evaluating the efficacy of doxycycline PEP for STI prevention (C. trachomatis, N. gonorrhoeae, and T. pallidum) in Kisumu, Kenya, and is the first study to assess the efficacy of doxycycline PEP in cisgender women. In this paper, we describe the baseline characteristics of the dPEP Trial population."
    },
    "2023-37273535_1078_en.txt": {
        "title": "Quiz-style online training tool helps to learn birdsong identification and support citizen science",
        "prompt": "Abstract: Citizen science is an important approach to monitoring for biodiversity conservation because it allows for data acquisition or analysis on a scale that is not possible for researchers alone. In citizen science projects, the use of online training is increasing to improve such skills. However, the effectiveness of quiz-style online training, assumed to be efficient to enhance participants’ skills, has not been evaluated adequately on species identification for citizen science biodiversity monitoring projects. Memory mechanisms in adaptive learning were hypothesized to guide the development of quiz-based online training tools for learning birdsong identification and for improving interest in birds and natural environments. To examine the hypothesis, we developed a quiz-style online training tool called TORI-TORE. We experimentally applied TORI-TORE in Fukushima, Japan, and examined its effectiveness for bird identification training using test scores and questionnaires to determine participants’ attitudes in a randomized control trial. We obtained the following key results: (1) TORI-TORE had positive effects on test scores and trainees’ attitudes toward birds. (2) Adaptive training, in which questions focused preferentially on unmastered bird species based on the answer history of individual trainees inspired by adaptive learning, unexpectedly led to lower scores and satisfaction in TORI-TORE. (3) Focusing on species that are relatively easy to remember, short lag times between training and testing, and long question intervals positively affected scores. While there is room for improvement, we expect TORI-TORE to contribute to online capacity building and to increase interest in natural environments.\n\n1. Introduction\n",
        "text": "For biodiversity conservation, it is necessary to monitor changes in the natural environment and ecosystems over large spatial and temporal scales. Monitoring efforts often focus on indicator species or groups, such as birds [REF]. Among bird monitoring methods, sound recordings are often used owing to the ability to identify species, even when they cannot be easily seen in the field [REF].Citizen science, which refers to public participation in scientific research, can be used to obtain data over long periods and at large spatial scales (Cohn, 2008; Silvertown, 2009; Bonney et al., 2009a). This approach can be used to analyze sound recordings of birds [REF]. Training to improve identification skills is expected to increase the data quantity and quality [REF] and to improve the participants’ interest in the natural environment [REF]. System for automated birdsong recognition has been developed considerably [REF], and a library of sounds annotated by citizens will contribute to further improving accuracy of automated birdsong recognition.In citizen science projects, the use of online training is increasing [REF], particularly during the COVID-19 pandemic [REF]. Online training is characterized by low costs and easy accessibility [REF]. However, few studies have examined the effectiveness of online training for species identification in citizen science projects [REF]. Quiz-style training is more efficient than simple memorization due to the testing effect, in which recalling information strengthens memory more than simply writing or listening to the information [REF]. Online quizzes have been used for short-term birdsong identification training, including Bird Song Hero [REF], Photo + Sound Quiz [REF], Larkwire [REF], Bird Research Birdsong Quiz [REF]. However, in existing quiz-based online training, participants repeatedly and/or randomly listen to full-length sound sources, irrespective of their levels of proficiency in identifying bird songs, or they have to customize the training content (select the set of songs) themselves. Therefore, more efficient and user-friendly quiz-based training is needed.Adaptive learning involves tailoring learning content, feedback, and interfaces to individual users [REF], and the personalization is often automatic. Recently, the effectiveness of adaptive learning has been evaluated in various fields [REF]. In adaptive learning, memory mechanisms are important elements [REF] and have been a longstanding research topic. The forgetting curve hypotheses that Ebbinghaus (1885) proposed are the first theories revealed by experiments in the study of memory mechanisms. The forgetting curve can be explained as follows: the most rapid increase in memory occurs after the first learning, the content learned is exponentially forgotten after learning, repeated learning at intervals (repetition learning and spaced learning) increases the amount of time that memory can be retained, the more information that came in immediately before, the more it is retained in short-term memory, and the more it can be remembered (the recency effect) [REF]. Learning effectiveness of repetition learning and spaced learning is thought to be improved by setting appropriate learning intervals, as revealed by meta-analyses [REF]. The recency effect is that when asked to recall a list of items in any order, people tend to recall from the end of the list and tend to recall those items best [REF]. The memory mechanisms are qualitatively applicable in a quiz-style online training tool on birds through controlling order of bird species in the quiz, but there is no a priori information that quantitatively optimizes the number of quiz training questions, the time from quiz to test, or interval between questions. Understanding how theories relate to training effectiveness may help guide the development of online training tools for learning bird identification from recorded songs and for improving interest in birds and natural environments. We developed a new online training tool, TORI-TORE, consisting of multiple-choice quizzes for improving bird identification skills from recorded bird songs and fostering citizen scientists skilled in birdsong identification. In this study, we (1) compared species identification skills and attitudes toward birds based on pre- and post- test results and training questionnaires, (2) compared test results and attitudes in a randomized controlled trial to reveal if automatically personalized online training (hereinafter, “adaptive training”) inspired by adaptive learning is more effective than conventional training (hereinafter, “baseline training”), and (3) tested the prediction that a large number of quiz training questions, short lag between training and testing, and long question intervals improve species identification test scores based on forgetting curve hypotheses."
    },
    "2023-37274187_1532_en.txt": {
        "title": "as a model to study autophagy in neurodegenerative diseases induced by proteinopathies",
        "prompt": "Abstract: Proteinopathies are a large group of neurodegenerative diseases caused by both genetic and sporadic mutations in particular genes which can lead to alterations of the protein structure and to the formation of aggregates, especially toxic for neurons. Autophagy is a key mechanism for clearing those aggregates and its function has been strongly associated with the ubiquitin-proteasome system (UPS), hence mutations in both pathways have been associated with the onset of neurodegenerative diseases, particularly those induced by protein misfolding and accumulation of aggregates. Many crucial discoveries regarding the molecular and cellular events underlying the role of autophagy in these diseases have come from studies using Drosophila models. Indeed, despite the physiological and morphological differences between the fly and the human brain, most of the biochemical and molecular aspects regulating protein homeostasis, including autophagy, are conserved between the two species.In this review, we will provide an overview of the most common neurodegenerative proteinopathies, which include PolyQ diseases (Huntington’s disease, Spinocerebellar ataxia 1, 2, and 3), Amyotrophic Lateral Sclerosis (C9orf72, SOD1, TDP-43, FUS), Alzheimer’s disease (APP, Tau) Parkinson’s disease (a-syn, parkin and PINK1, LRRK2) and prion diseases, highlighting the studies using Drosophila that have contributed to understanding the conserved mechanisms and elucidating the role of autophagy in these diseases.\n\n1. Introduction\n",
        "text": "Drosophila melanogaster is an established model organism for developmental studies and due to the remarkable conservation of the signaling regulating autophagy, it has been used to better understand the relationship of this catabolic pathway with the genetic conditions that in humans are responsible of a class of neuronal diseases called proteinopathies (PPs). Autophagy is a key cellular pathway that, together with the ubiquitin-proteasome system (UPS), controls protein homeostasis by degrading misfolded proteins or exhausted organelles otherwise detrimental to the cells [REF]. Autophagy and UPS are closely linked, in fact protein ubiquitination is a key step for the cargo recognition by the autophagic receptors and alterations in one pathway may affect the activity of the other [REF]. Both pathways are crucial for cell survival particularly in neurons where their perturbation causes age-associated disorders including neurodegenerative diseases [REF]. In this review, we will illustrate the contribution of Drosophila studies to the understanding of the role of autophagy in PPs induced by mutations in genes responsible for the most common neurodegenerative diseases (summarized in Table 1). Furthermore, we will discuss how flies could be used to further improve our understanding of the mechanisms that control these diseases, particularly those that are linked to mutations in genes that are physiologically involved in the control of the autophagic-proteostatic pathway.Drosophila models of human proteinopathies discussed in this article, their principal mechanism, and components of pathways (modifiers) that can either suppress on enhance the toxic phenotype.The length of the CAG correlates with progressive motor decline and neuronal death [REF]Kinetic of aggregate formation [REF] Huntingtin role in autophagy [REF]PolyQ oligomers forms “de novo” aggregates and increase their original size by directly using the endogenous prion-forming protein Rnq1 in its amyloid-like prion conformation [REF] Endogenous dhtt [REF] TOR inhibition [REF]Chaperones [REF]Histone deacetylase [REF] Antioxidant pathway [REF]deubiquitinating enzymes [REF] Glutamine metabolism [REF]Rab5 [REF]; PSA [REF]Compounds that target mHTT to autophagosomes [REF]Retinal degeneration and loss of interneurons projections [REF] Reduced dendritic neurons arborization [REF] Role of chaperones respect to polyQ containing proteins [REF]TOR inhibition [REF] Rac-PAK pathway [REF] CHIP and NMNAT [REF]Transglutaminases 5 (Lee et al., 2022b)Ataxin1 phosphorylation on Ser776 [REF] Atx2 is involved in translational control [REF] Aggregate formation and neuronal degeneration, critical for SCA3 [REF] Atx1 promotes Ataxin2 aggregation [REF] Atx3 enhances the aggregation of the mutant ATXN3 [REF] Catalytical activity of Atx3 is necessary for its autoprotective role [REF ]Mutant ATXN3 expression leads to abnormal eye morphology and motility defects [REF] Functional ATXN3 deubiquitination activity [REF] Ubiquitin ligases as Cullins and Praja1 [REF] Antioxidant drugs [REF] Hsc70-4 [REF] Toxicity of different numbers of GGGGCC repeats and formation of RNA foci [REF] Polypeptides containing GR and PR repeats are the most toxic role of C9orf72 in autophagy [REF] overexpression of SIGMAR1 (Lee et al., 2020a)SOD1 is involved in protein misfolding, and it is necessary for neuronal health [REF] SOD1 was found in a complex with Atg9/Beclin1 to control P62/SQSTM1 accumulation [REF] Mutations in SOD1 lead to mitochondrial impairments and non-autonomous signaling from glial to neurons and in MNs [REF] DUB-USP7 [REF] L3MBTL1 and SETD8 [REF] USP7 which reduces SMAD2/TGF-β pathway [REF] TBPH-null mutant is semi-lethal in flies [REF]. Atg7 [REF] HSP67Bc [REF].HEXA-018 treatment [REF] Expression of wild-type FUS and of ALS-related FUS mutations triggers the accumulation of toxic aggregates that inhibits autophagy [REF] FUS and TDP-43 interact to induce neurodegeneration [REF] Mask promotes autophagy by expression of the proton-pumping vacuolar (V)-type ATPases [REF] Inhibition of PI3K/AKT/TOR ameliorate P525L-FUS mutation due to induction of autophagy [REF].Glutathionylation by Glutathione transferase omega 2 promotes FUS degradation [REF] Lowering the expression of Atg1, Atg8a and Atg18, enhances the neuronal toxicity caused by Aβ expressionDysfunctional AEL (autophagy-lysosomal-endosome vesicles) induces amyloid-plaque formationEctopic APP expression leads to aberrant autophagy [REF] NMNAT [REF]. Trx80 [REF] Lowering of autophagy-related genes [REF] Insights in Tau toxicity and autophagy [REF] Ataxin3 deubiquitinase activity [REF] Calpain silencing [REF] CG11070 [REF] Decrease in PTK2 expression (Lee et al., 2022a)Insulin signaling [REF] Impairments in the autophagic flux and in mitophagy [REF].Spermidine [REF] LAMP2A [REF] dDOR [REF] knockdown of inositol-requiring enzyme 1 (IRE1) and Atg7 [REF] Relevance of Parkin and Pink in mitophagy molecular process [REF]. The age-related increase in mitophagy depends on the interaction of parkin and Pink with UPS-15 and − 30 [REF] Mitochondria phenotype in parkin null mutants recapitulates autophagy inhibition in loss of Atg7 [REF] BNIP3L [REF] Downregulation of ANKHD1 [REF] Overexpression of human mutant LRRK2 induces defects in autophagy [REF] dLRRK is essential for functional autophagic flux and vesicle trafficking, also at the synaptic level [REF] Expression of parkin and AMPK activation [REF] Insights in PrP misfolding process [REF] Reversibility of the human PrP-GSS phenotype using inducible system [REF] Transferability of the pathology between flies or from mice to flies [REF] Toxicity of different aminoacids substitutions in PrP protein [REF] Perturbation of TOR signaling-related and cell-cycle related genes expression [REF] Co-localization of PrPSc in large aggregates with p62/SQSTM1 [REF] 4E-BP activity suppresses human PrP-M129 and PrP-V129 mutations [REF] Their link to autophagy is also outlined."
    },
    "2023-37274223_5144_en.txt": {
        "title": "Biological functions of the Olig gene family in brain cancer and therapeutic targeting",
        "prompt": "Abstract: The Olig genes encode members of the basic helix–loop–helix (bHLH) family of transcription factors. Olig1, Olig2, and Olig3 are expressed in both the developing and mature central nervous system (CNS) and regulate cellular specification and differentiation. Over the past decade extensive studies have established functional roles of Olig1 and Olig2 in development as well as in cancer. Olig2 overexpression drives glioma proliferation and resistance to radiation and chemotherapy. In this review, we summarize the biological functions of the Olig family in brain cancer and how targeting Olig family genes may have therapeutic benefit.\n\n1. Introduction\n",
        "text": "The oligodendrocyte lineage transcription factor (Olig) family of proteins, comprised of Olig1, Olig2, and Olig3, are basic helix–loop–helix (bHLH) transcription factors that are essential regulators of neural cell fate and specification [REF]. The Olig genes and the proteins they encode are responsible for the development of oligodendrocytes and neural cells. Initial studies revealed that the Olig genes, primarily Olig1 and Olig2, are expressed by oligodendrocyte precursor cells (OPCs) where Olig1 regulates oligodendrocyte formation and maturation in the brain and Olig2 modulates oligodendrogenesis in the spinal cord [REF]. It was later discovered that the Olig proteins are also involved in neurogenesis. Specifically, Olig2 is distinctly expressed in the progenitors of motor neurons (pMN) domain of the developing spinal cord where motor neurons are generated [REF]. Furthermore, Olig1/2 were also found to generate inhibitory interneurons in the brain [REF]. Recently, GABAergic neurons were shown to be derived in Olig3 lineage cells [REF]. The role of the Olig family in astrocyte specification is not well established. It was shown that Olig1/2 may promote astrocyte differentiation where astrocytes are sequentially transformed after generation of interneurons [REF]. However, other studies determined that Olig-expressing precursors to be negative regulators of astrocytes confirming the cellular fate to be neurons and oligodendrocytes [REF]. The Olig genes are also expressed in cancer and are detailed below.Gliomas are primary brain tumors derived from neuroglial stem cells or progenitor cells as well as lineage restricted precursors [REF]. Roughly 30% of all primary brain tumors are gliomas and 80% are considered malignant [REF]. Traditionally, gliomas are classified based on their histopathological and clinical features established by the World Health Organization [REF]. Over the years, technological advancement has vastly improved classification of gliomas centered on molecular and genomic changes [REF]. Currently, gliomas are divided into six families: (1) adult-type diffuse gliomas; (2) pediatric-type diffuse low-grade gliomas; (3) pediatric-type high-grade gliomas; (4) circumscribed astrocytic gliomas; (5) glioneuronal and neuronal tumors; and (6) ependymomas [REF]. Adult-type diffuse gliomas are further separated into three different groups: (1) isocitrate dehydrogenase (IDH) mutant with 1p/19q co-deletion with primarily oligodendroglial morphology; (2) IDH mutant with 1p/19q non-codeletion with mainly astrocytic histology; and (3) IDH wildtype [REF]. Gliomas are also further categorized based on WHO grades I–IV. Grade I gliomas consists of pilocytic astrocytic astrocytomas and are primarily observed in the cerebellum or brain stem of children. Grade II gliomas are comprised of adult low-grade gliomas and are generally astrocytomas, oligodendrogliomas, mixed oligoastrocytomas, and diffuse astrocytomas. Grade III gliomas are malignant gliomas composed of anaplastic cells. Grade IV gliomas are glioblastoma (GBM) which consists of primary and secondary GBM [REF].Success in prognosis and therapeutics of gliomas is dependent on accurate diagnosis. While there exist a multitude of histological markers to differentiate between the types of gliomas, challenges remain due to gliomas displaying similar morphological characteristics [REF]. Specifically, reliable markers to accurately distinguish between glioma types are lacking due to ambiguous histological features. Differentiating gliomas based on their cellular morphology can also be confusing and biased, resulting in classifications such as oligoastrocytoma or mixed glioma [REF]. Observer variability can also lead to misdiagnosis or underdiagnosis of the correct disease [REF].CNS tumors are heterogenous and their grading based on histological features is notoriously subjective [REF]. With advances in diagnostic technologies, the most recent WHO 2021 classification of tumors of the CNS have adopted key molecular markers and revised grading of astrocytic tumors, oligodendroglial tumors, oligoastrocytomas, glioneuronal tumors, and neuronal tumors [REF]. The summary of Olig expression in gliomas presented in this review is based on WHO CNS tumor classification at the time the studies were conducted. However, we have organized the gliomas, to the best of our ability, centered on the most current 2021 WHO CNS tumor classification. Large datasets have confirmed the expression of Olig2 across all gliomas such as TCGA and others [REF].Adult-type diffuse gliomas are composed of IDH-mutant and 1p/19q-codeleted oligodendrogliomas, IDH-mutant astrocytomas, and IDH-wildtype GBMs, based on histological characteristics and explicit molecular markers. In adults, oligodendrogliomas with IDH mutation and 1p/19q-codeletion also present with mutations in the TERT promoter gene [REF]. Oligodendroglial tumors have generated great interest over the past decade due to its favorable response to chemotherapy [REF] which may be attributed to the concurrent loss of chromosome arms 1p and 19q [REF]. IDH-mutant astrocytomas are now graded as WHO grade II, III, or IV [REF] and also harbor ATRX and TP53 mutations [REF]. GBM is the most common and deadliest primary brain tumor. IDH-wildtype GBM demonstrates alterations in epidermal growth factor receptor (EGFR), and similar to oligodendrogliomas, exhibit TERT promoter mutations [REF].Because oligodendrogliomas arise from oligodendrocytes, it is not surprising that attempts to diagnose oligodendrogliomas have utilized oligodendrocyte markers. Mature oligodendrocyte markers, such as myelin basic protein (MBP) and proteolipid protein (PLP), however, are not expressed at detectable levels in oligodendrogliomas [REF]. Furthermore, immature oligodendrocyte markers, such as the chondroitin sulphate proteoglycan NG2 and platelet-derived growth factor receptor alpha (PDGFR-α), lack specificity [REF] and have been unsuccessful in discerning between glioma types [REF]. Several earlier studies have observed marked Olig2 expression in oligodendrogliomas [REF]. Specifically, anaplastic oligodendrogliomas displayed intense nuclear Olig2 expression [REF]. Morphologically, Olig positive cells were moderately to densely packed, and displayed round and homogeneous nuclei with perinuclear halos [REF], characteristics consistent with oligodendroglial tumors [REF]. Others have also observed an upregulation of both Olig1 and Olig2 in these tumors [REF]. For example, one study found an astounding 87% (26/30) and 93% (28/30) of oligodendroglial samples were positive for Olig1 and Olig2, respectively [REF]. Furthermore, the strong expression of Olig1 and Olig2 was shown to be correlated to WHO classification with their expression increasing incrementally from grades I to III [REF]. However, one report did note varied expression of Olig1 and Olig2. Here, the authors found 3 grade III oligodendrogliomas did not express either Olig1 or Olig2 while another 3 grade III oligodendrogliomas expressed Olig1 only [REF].Compared to oligodendrogliomas, Olig expression in astrocytomas and GBMs has been inconsistent and varied. Generally, low levels of Olig1 and Olig2 have been observed [REF] with weak Olig2 intensity in the nuclei [REF]. In one report, low Olig1 expression was detected along with a marked upregulation of Olig2 [REF], while another study found an upregulation of both Olig1 and Olig2, although the sample size was small [REF]. In another study astrocytomas were found to exhibit only weak or moderate Olig expression [REF]. Olig expression was not detected in a case of grade III astrocytoma [REF]. GBMs also displayed varying Olig2 expression. While one study rarely observed Olig2 in GBM [REF], another study demonstrated lower mean transcript levels of Olig1 and Olig2 [REF]. In one rare case of GBM, upregulation of both Olig1 and Olig2 were observed [REF]. Interestingly, in a separate study, Olig2 protein levels were upregulated in all cases of GBM and appeared nuclear [REF].While diffuse high-grade gliomas (HGGs) are more common in adults, pediatric HGGs present with similar histopathological features and devastating prognosis [REF]. Pediatric diffuse HGGs can arise from various regions in the brain but most develop as diffuse intrinsic pontine glioma (DIPG) which occurs in the brainstem [REF] during a restricted window of childhood [REF]. DIPGs are the most common brainstem tumors in children with a median of survival of less than 1 year from diagnosis [REF]. Histopathologically, DIPG hosts a spectrum of features that is consistent with diffuse and anaplastic astrocytomas and GBMs [REF]. Because DIPG appears during development, neural stem cells (NSCs) and neural progenitor cells (NPCs), which are actively proliferating and differentiating, are highly impacted during disease progression [REF]. Olig proteins are critical players in cellular specification and differentiation during development [REF]. Their expression in DIPGs have been investigated. Not surprising, a large number of cells in the pons were found to be positive for Olig2 with a subset of these cells also co-expressing Sox2 and Nestin [REF], markers of not only CNS embryogenesis [REF], but also tumorigenesis [REF].Circumscribed astrocytic gliomas are astrocytic neoplasms with circumscribed growth [REF]. Pilocytic astrocytoma (PA) is a type of circumscribed astrocytic glioma and is considered a low-grade glioma (LGG). PAs occur mostly in children and young adolescent but can be observed in older patients as well. This brain tumor is commonly observed in the cerebellum, spinal cord, and optic pathways, but can occur anywhere in the brain [REF]. Histologically, PAs display the classical biphasic pattern which is composed of compact areas containing Rosenthal fibers and loose microcystic areas [REF].Olig expression in PAs have been conflicting. Some studies have found low to moderate expression of Olig1 and Olig2 [REF] while others have reported high expression of these genes [REF]. One study observed greater immunoreactivity of Olig1 (97%; 62/64) compared to Olig2 (75%; 48/64; [REF]). Diffuse staining patterns of Olig2 were observed [REF] and similar to oligodendrogliomas, Olig immunoreactivity was found localized to the nuclei [REF]. Interestingly, double immunolabeling of Ki67 and Olig2 showed that most proliferating cells were also positive for Olig2, however, Ki67+ cells embodied a small portion of Olig2 expressing cells as PAs are LGGs and have a low rate of proliferation [REF].Glioneuronal tumors (GNTs) are exceptionally rare neoplasms composed of both mixed neuronal and glial cells. The majority of GNTs are classified as grade I and are associated with seizures [REF]. The pathological aspects of GNTs remain unclear however, case reports have found Olig2 commonly expressed in these tumors and thus lean more toward oligodendrogliomas. Three subtypes of GNTs that demonstrate Olig2 upregulation are dysembryoplastic neuroepithelial tumors [REF], papillary glioneuronal tumors [REF], and rosette-forming glioneuronal tumors [REF].DNTs are highly heterogenous with varying morphological features. Histologically, these tumors display nuclear atypia, mitosis, endothelial proliferation, or increased cell density, however, these appearances provide no prognostic value [REF]. DNTs are also subtyped as simple or complex which displays oligodendroglia-like cells (OLCs) and floating neurons [REF]. With these hallmarks, the definition of DNTs remain controversial. DNTs were found to be more similar to oligodendrogliomas rather than a glioneuronal tumor. In this same study 88% of OLCs were diffusely Olig2+ and 10% of these cells also colocalized with galectin3 in the nuclei of OLCs. Few OLCs were positive for PDGFRα and did not exhibit 1p/19q codeletion. Additionally, NeuN+ and Olig2+ cells were mutually exclusive, further suggesting that DNTs are clear glial tumors rather than glioneuronal tumors [REF].Similarly, Olig2 expressing cells were also found in PGNTs suggesting that these tumors may be oligodendroglial or oligodendroglial-like. Histologically, PGNTs exhibit two distinct architectures: (1) pseudopapillary structures surrounded by (2) compact regions consisting of neuronal elements under different maturation stages [REF].Since the discovery of a small proportion of clonogenic progenitors in acute myeloid leukemia [REF], the existence of tumor initiating cells or cancer stem cells (CSCs) in several types of cancers were investigated. Thus arose the cancer stem cell hypothesis which states that malignant tumors are driven and sustained by a group of cells with stem cell properties of unlimited capacity for self-renewal and the ability to differentiate into any cell type [REF]. While the cancer stem cell hypothesis was adopted by many, it remains highly controversial [REF]. Even with increasing evidence of CSCs, possibly the most debated aspect of the hypothesis is the mere existence of CSCs, as they only contribute to a small fraction of the tumor [REF]. Therefore, it remains unclear from where CSCs originated and, as a result, identifying a cell of origin in gliomas, such as GBMs, has been explored.GBMs are the most common and aggressive primary malignant brain tumors. They exhibit a high degree of heterogeneity resulting in molecular subtypes of classical, mesenchymal, and proneural [REF]. Evidence of brain tumor initiating cells in vivo [REF] have led to studies identifying glioma stem cells (GSCs) and better understanding of their properties. While markers such as CD133, CD15, L1CAM, CD49f, and SOX2 have been shown to be enriched in GSCs [REF] they are not exclusive to GSCs. Identification of more specific markers of GSCs could increase detection for developing targeted therapies. In one study, comparative analysis demonstrated Olig2 as the most specific GBM stem cell marker [REF]. Similar to previous findings [REF], Olig2 immunoreactivity was observed in all cases of GBM [REF] and was primarily nuclear with rare cases exhibiting cytoplasmic Olig2 staining. Further analysis revealed higher expression of Olig2 in secondary GBM compared to primary GBMs [REF]. Secondary GBMs evolve from diffuse astrocytomas and have frequent TP53 and ATRX mutations [REF] which are also commonly observed in IDH-mutant astrocytomas [REF]. Because secondary GBMs were removed from the 2021 WHO CNS tumor classification, it is plausible that they are more closely related to IDH-mutant astrocytomas. Re-characterization of Olig2 in “secondary GBM” samples is therefore necessary for thorough understanding of glioma pathogenesis.Despite cell lineage studies, it is unclear why Olig2 is enhanced in oligodendrogliomas and astrocytomas [REF]. Olig2 is critical during CNS development. It is known for its role in oligodendrocyte and neuron specification and maturation and may also fulfill a potential function in astrocyte differentiation [REF]. In the postnatal brain, Olig2 functions as a repressor of neuronal lineages to direct subventricular zone (SVZ) progenitor cells toward astrocytic and oligodendrocytic fates [REF]. In fact, overexpression of Olig2 in the SVZ increases the number of highly migratory OPCs to differentiate into mature oligodendrocytes [REF]. Neural stem cells (NSCs) in the SVZ, also known as Type B cells, are a subpopulation of GFAP positive astrocytes that give rise to neurons, astrocytes, oligodendrocytes, and NG2+ OPCs [REF]. Interestingly, these Type B cells, along with some Type C (transit-amplifying) cells, also express Olig2 [REF], indicating possible cells of origin for astrocytomas and oligodendrogliomas.Olig2 dysregulation in gliomas suggests that it is required for glioma growth and formation. Below we describe how Olig2 may be activated during cancer progression. Additionally, we explore how Olig2 drives gliomagenesis and whether it serves an oncogenic function.Throughout CNS development, NSCs and NPCs transform into distinct cell types in a spatiotemporal manner. A central function of Olig2 is to direct cell fate and specification, particularly into oligodendrocytes and neurons, in distinct regions of the brain and spinal cord during development [REF]. Olig2 is induced by Sonic hedgehog [REF] where its pathways are known to regulate cellular patterning and cell fates [REF]. The interplay between Shh and fibroblast growth factor (FGF) promotes Olig2 transcription [REF]. Increasing evidence has associated Shh signaling pathway with CNS tumors, however its relationship with Olig2 in gliomas is only beginning to be elucidated.Several lines of evidence have associated Shh signaling with gliomas. For example, overexpression of Shh was observed in CD133+ cells and accelerated tumor growth while inhibition of Shh or shRNA knockdown of Shh delayed tumor growth and downregulated Ptch1 and Gli1 [REF]. Shh is activated via binding to the Ptch1 receptor while Gli1 is transcriptionally induced by Shh signaling [REF]. Aberrant activation of Gli1 [REF] and mutations in Ptch1 [REF] are correlated with various cancers. In another study, expression of Shh and Ptch1 levels were significantly higher in brainstem astrocytomas compared to supratentorial astrocytomas [REF]. Increased levels of Notch receptors and its ligands were observed in astrogliomas and GBMs. Interestingly, glioma cell lines expressing the active form of Notch1 proliferated faster than those that did not [REF]. Furthermore, overexpression of Notch1 further increased formation of Nestin+ neurosphere colonies [REF] and its expression in GBM cells [REF]. Similarly, overexpression of Notch1, its ligands, and downstream targets (Hes1 and Hes2) have been detected in GBM. Notch activation has also been shown to contribute to Ras-mediated transformation of glial cells to glioma growth, proliferation, and survival [REF].Because Olig2 activity is regulated by Shh [REF], it is plausible that increased levels of Olig2 in gliomas are contributed by Shh deregulation. Recently, Olig2 was shown to behave as an oncogenic activator in Shh medulloblastoma [REF], a malignant pediatric brain tumor characterized by activation of Shh signaling [REF]. Olig2+ progenitors were identified as the rapidly dividing Type C cells at the onset of tumorigenesis. Surprisingly, a substantial increase in Olig2+ progenitors was found in recurrent Shh-MB indicating that Olig2+ progenitors are reactivated during recurrence or metastasis. Finally, enhanced Olig2+ expression was also detected in Shh-MB and was significantly correlated with decreased survival.Studies have also illustrated Olig2 participation in positive feedback loops with the EGFR receptor tyrosine kinase [REF]. EGFR signaling is known to activate the oncogenic PI3K-AKT–mTOR and RAS–RAF–MEK–ERK pathways [REF]. Exposure to EGF leads to proliferation of Olig2+ type C cells [REF] and inhibition of EGFR signaling results in Olig2 depletion indicating that EGFR signaling is responsible for sustaining Olig2 expression in progenitor cells [REF]. Furthermore, Olig2 directly targets EGFR [REF] and overexpression of Olig2 leads to significant upregulation of EGFR and transcripts [REF]. Additionally, phosphorylated Olig2 leads to differentially regulated genes associated with RTKs [REF].Gene network analysis has identified potential roles of Olig2 involvement in gliomas (Figures 1, 2). One such network entails cell cycle regulation [REF]. p53 is a tumor suppressor gene that functions in growth arrest and apoptosis in response to cellular stress. An effector of p53 and cell cycle inhibitor is p21 [REF]. Chromatin immunoprecipitation (ChIP) analysis demonstrated that p21 is a direct target of Olig2 repression in NPCs and gliomas [REF]. Malignant gliomas that are resistant to radiation and genotoxic drugs are associated with reduced p53 functions as a result of Olig2 expression. However, in the absence of Olig2, even attenuated p53 functions were shown to be sufficient to activate radiation-induced apoptosis and growth arrest. Olig2 opposes p53 functions by suppressing acetylation of p53. Therefore, Olig2 acts as post-translational modifier of p53 to repress its downstream biological activities [REF].Gene targets of Olig2. Highlighted yellow are genes that can be bound by olig2 in the promoter-TSS region, light brown highlighted genes can be bound in more distant area before gene sequence.Signaling effects of Olig2. Highlighted yellow are genes that can be bound by olig2 in the promoter-TSS region, light brown highlighted genes can be bound in more distant area before gene sequence.It appears that Olig2 may be an actionable drug target as multiple gliomas express high levels of Olig2. Additionally, several studies utilizing transgenic mouse models showed that ablation of Olig2 delayed tumor growth and improved survival [REF]. Therefore, pharmacological inhibition of Olig2 may be therapeutically beneficial in treating gliomas.In collaboration with Curtana Pharmaceuticals (San Diego, CA), we generated an orally bioavailable small molecule (397 kD) Olig2 inhibitor, CT-179, the first drug targeting bHLH transcription factors for cancer treatment. Our preliminary findings suggest that CT-179 prevents Olig2 homodimerization and strongly inhibited cellular growth and induced apoptosis of Olig2+ cells [REF]. Moreover, CT-179 disrupts the cell cycle, ultimately resulting in mitotic catastrophe at the prometaphase. Treatment with CT-179 in tumor-bearing mice resulted in a reduction of Olig2+ cells and markedly improved survival outcome [REF]. Recent preliminary findings indicated that CT-179 also decreased Shh signaling and prolonged event-free survival in a mouse model of medulloblastoma [REF].Olig2 exhibits a dichotomous function. It displays a pro-neural function by promoting motor neuron differentiation as well as an anti-neural role by participating in generation of oligodendrocytes [REF]. Phosphorylation of Olig2 has been shown to regulate neuronal-glial cellular fate switch. Specifically, Olig2 was shown to be phosphorylated at serine 147 (S147) during motor neuron production [REF]. Additionally, triple serine motif phosphorylation sites (S10, S13, S14) were shown to control proliferative functions of Olig2 [REF]. In fact, phosphorylated Olig2 exhibits pro-mitotic and anti-p53 functions [REF]. Thus, targeting Olig2 or protein kinase inhibitors (PKIs) may have therapeutic effects against gliomas. Certainly, we observed that treatment with CT-179 decreased Olig2 phosphorylation in a mouse model of medulloblastoma [REF] which may enable p53-mediated apoptosis [REF] and improve outcomes."
    },
    "2023-37275361_833_en.txt": {
        "title": "Barriers to overcoming immunotherapy resistance in glioblastoma",
        "prompt": "Abstract: Glioblastoma multiforme (GBM) is the most common malignant primary brain tumor, known for its poor prognosis and high recurrence rate. Current standard of care includes surgical resection followed by combined radiotherapy and chemotherapy. Although immunotherapies have yielded promising results in hematological malignancies, their successful application in GBM remains limited due to a host of immunosuppressive factors unique to GBM. As a result of these roadblocks, research efforts have focused on utilizing combinatorial immunotherapies that target networks of immune processes in GBM with promising results in both preclinical and clinical trials, although limitations in overcoming the immunosuppressive factors within GBM remain. In this review, we aim to discuss the intrinsic and adaptive immune resistance unique to GBM and to summarize the current evidence and outcomes of engineered and non-engineered treatments targeted at overcoming GBM resistance to immunotherapy. Additionally, we aim to highlight the most promising strategies of targeted GBM immunotherapy combinatorial treatments and the insights that may directly improve the current patient prognosis and clinical care.\n\n1. Introduction\n",
        "text": "Glioblastoma (GBM) is the most common malignant primary central nervous system (CNS) tumor in the United States, representing 14.3% of all tumors, 49.1% of malignant tumors, and 58.4% of gliomas [REF]. While the 5-year survival rate for all malignant brain tumors combined is 36%, the unique intrinsic and adaptive immune resistance that characterizes GBM translates to an even lower 5-year survival rate of 5.7% in a DCVax control population and 5% in TTF EF-14 trial, with a median survival of 14.7–16.5 months in DCVax and TTF trial control [REF]. The standard of care for newly diagnosed GBM includes maximal surgical resection followed by adjuvant combinatorial chemotherapy and radiation and subsequent temodar combined with temodar-tumor treating fields [REF]. Almost all patients (~90%) experience tumor recurrence and there is no established standard of care for recurrent glioblastoma (rGBM) other than supportive and palliative care [REF]. Although repeating radiotherapy and chemotherapy or the use of anti-angiogenic drugs such as bevacizumab remain options for certain patients, the 2-year survival for rGBM remains at 26% [REF].There is significant focus on creating novel multimodal therapies to target the unique biological characteristics and immunosuppressive factors unique to GBM [REF]. Recently, clinical trials have shown moderate improvement in median overall survival of 20.9 months from 16.0 months within Tumor-Treating Fields plus temozolomide chemotherapy treatment (TTFields-temozolomide) compared to temozolomide alone, respectively [REF]. Targeted immune therapies, most notably: signaling pathway inhibitors, checkpoint inhibitors, and Chimeric antigen receptor (CAR)-T cell therapy, have significantly improved the treatment of various hematologic malignancies, with over 50 approved therapies within the last decade [REF]. These immune therapies harness the patient’s own immune response to target specific tumor cells. Recent studies have focused on applying the success of immune therapies within GBM populations; a recent single patient case report demonstrated significant GBM tumor regression following IL13Rα2-targeted chimeric antigen receptor (CAR)–engineered T cells therapy administration, with subsequent host-immune response increases sustaining 7.5 months post-treatment [REF].Aside from CAR-T cells, a variety of other immune strategies have been employed in GBM. A multitude of immune checkpoint inhibitor trials have been attempted but have thus far failed to move the needle on survival [REF]. Single and multi-epitope peptide vaccines have in general not impacted survival, but the surviving long peptide vaccine (SurVaxM) has had an impact in recurrent GBM in early phase trials and is moving toward phase 3 trials in malignant glioma [REF]. Most recently, the strategy of personalized dendritic cell vaccine has completed a Phase 3 trial with modest improvements in survival vs. an external control group [REF]. Although these reports illustrates potential, the widespread success of immunotherapies within hematologic and solid malignancies has failed to translate to larger GBM trials, and to date no immunotherapies have been approved for glioblastoma [REF]. The lack of success in clinical trials to improve the SOC for glioblastoma underlines the importance of further understanding the intrinsic and adaptive factors of GBM that encapsulate the aggressive nature of this tumor.There are several reasons that the therapies applied in hematological malignancies have not translated to the same level of success in GBM. GBM’s many immunosuppressive properties can be divided into both intrinsic and adaptive factors. Intrinsic factors of GBM include multiple areas of immunosuppression through intratumoral heterogeneity (ITH), qualitative and quantitative T-cell immune dysfunction [REF], tumor-mediated immune sequestering of T-cells within the bone marrow [REF], and the immune distinct microenvironment of the central nervous system [REF]. Adaptive factors of GBM include plasticity of Glioblastoma stem-like cells (GSCs), selection of resistant intratumoral populations, effect of concurrent steroid treatments (Dexamethasone) on immunotherapy [REF] and adaptive genomic and epigenomic changes [REF] in recurrent glioblastoma that ultimately increase lethality. In this paper we discuss in detail the most recent advancements to the understanding of these intrinsic and adaptive factors of immunotherapy resistance within GBM. Additionally, we will discuss the latest engineered medicines, such as CD47, CSF1R, CD73, COX2, CCL2, IL6, and GITR inhibitors, and non-engineered medicines that possess the potential to overcome these clinical obstacles and improve prognosis for patients."
    },
    "2023-37275856_645_en.txt": {
        "title": "Gasdermin D-mediated pyroptosis: mechanisms, diseases, and inhibitors",
        "prompt": "Abstract: Gasdermin D (GSDMD)-mediated pyroptosis and downstream inflammation are important self-protection mechanisms against stimuli and infections. Hosts can defend against intracellular bacterial infections by inducing cell pyroptosis, which triggers the clearance of pathogens. However, pyroptosis is a double-edged sword. Numerous studies have revealed the relationship between abnormal GSDMD activation and various inflammatory diseases, including sepsis, coronavirus disease 2019 (COVID-19), neurodegenerative diseases, nonalcoholic steatohepatitis (NASH), inflammatory bowel disease (IBD), and malignant tumors. GSDMD, a key pyroptosis-executing protein, is linked to inflammatory signal transduction, activation of various inflammasomes, and the release of downstream inflammatory cytokines. Thus, inhibiting GSDMD activation is considered an effective strategy for treating related inflammatory diseases. The study of the mechanism of GSDMD activation, the formation of GSDMD membrane pores, and the regulatory strategy of GSDMD-mediated pyroptosis is currently a hot topic. Moreover, studies of the structure of caspase-GSDMD complexes and more in-depth molecular mechanisms provide multiple strategies for the development of GSDMD inhibitors. This review will mainly discuss the structures of GSDMD and GSDMD pores, activation pathways, GSDMD-mediated diseases, and the development of GSDMD inhibitors.\n\n1. Introduction\n",
        "text": "The term “pyroptosis” was first proposed in 2001 and consists of “pyro” and “ptosis”, which represent the features of inflammation (fire or fever) and programmed cell death (falling), respectively [REF]. Due to the discovery of diverse pyroptosis-executing proteins, pyroptosis has been redefined as a form of programmed necrosis mediated by gasdermin proteins with the characteristics of cell swelling, membrane rupture, and the release of cellular contents [REF]. Gasdermins belong to the pore-forming protein family and consist of six gasdermin proteins, including gasdermins A-E and DFNB59 [REF]. Among them, DFNB59 is not associated with the formation of membrane pores and pyroptosis [REF]. With the exception of DFNB59, the members of the gasdermin family have two domains: an N-terminal domain and a C-terminal domain linked by a flexible peptide. Upon activation, the cleaved N-terminal domain is responsible for inducing pyroptosis [REF]. This review focuses on GSDMD, the most extensively studied executive pyroptosis-executing protein with the clearest mechanism. More detailed discussions of other members of the gasdermin family can be found in other reviews [REF].GSDMD serves as a direct substrate of inflammatory caspases, including caspase-1/4/5 and murine caspase-11, and is cleaved into an active N-terminal domain (GSDMD-NT) upon canonical or noncanonical inflammasome activation induced by exogenous stimuli or endogenous injuries [REF]. Recent studies have demonstrated that caspase-8, which is responsible for apoptosis, is also involved in GSDMD activation [REF]. GSDMD-NT oligomerizes and forms membrane pores, leading to the release of inflammatory factors such as IL-1β and IL-18, as well as non-selective ion fluxes [REF]. Massive membrane pore formation compromises membrane integrity, causing lytic cell death and the release of cytoplasmic contents that amplify inflammatory signals.While GSDMD-induced pyroptosis has been shown to protect the host against bacterial infection [REF], numerous studies have demonstrated that abnormal GSDMD activation causes severe inflammatory cascades such as disruption of ionic homeostasis, organelle dysfunction, cell lysis, and sustained release of inflammatory cytokines. Abnormal GSDMD activation can cause persistent inflammation, which has been implicated in various inflammatory diseases, including ischemic stroke [REF], familial Mediterranean fever (FMF) [REF], neonatal-onset multisystem inflammatory disease (NOMID) [REF], experimental autoimmune encephalitis (EAE) [REF], sepsis [REF], nonalcoholic fatty liver disease (NAFLD) [REF], cancer [REF], human immunodeficiency virus (HIV) infection [REF], neurodegenerative diseases such as Alzheimer’s disease (AD) [REF] and Parkinson’s disease (PD) [REF]. Moreover, cell pyroptosis has been shown to play an important role in a series of clinical symptoms caused by severe acute respiratory syndrome-related coronavirus 2 (SARS-CoV-2) infection. Excessive inflammation and cytokine storm are the main causes of tissue damage and organ failure in COVID-19. GSDMD serves as a regulatory protein upstream of cytokine storm and is thus a promising target for the treatment of COVID-19 [REF]. Given the important role of GSDMD in pyroptosis and inflammatory disorders, we summarize the structure and activation mechanisms of GSDMD and focus on the discovery of GSDMD inhibitors in this review."
    },
    "2023-37275952_950_en.txt": {
        "title": "Applications of Metformin in Dentistry—A review",
        "prompt": "Abstract: Metformin is a versatile drug with numerous medical uses. It is known primarily as an anti-hyperglycemic drug that has become the main oral blood-glucose-lowering medication for managing type 2 diabetes mellitus globally. Its use has been reported in a variety of oral conditions and dentistry in general. Recent clinical trials have indicated the effectiveness of adjunct topical application of metformin in improving the periodontal parameters of patients with diabetes and periodontitis. Additionally, studies have suggested that metformin stimulates odontogenic differentiation and mineral synthesis of stem cells in the tooth pulp. Metformin also stimulates osteoblast proliferation, decreases osteoclast activity and exerts regenerative effects on periodontal bone, thus making it a viable candidate for periodontal regeneration. Metformin monotherapy significantly enhances osseointegration of endosseous implants and has been reported to have anti-cancer effects on oral squamous cell carcinoma by impeding tumor progression. Animal studies have indicated that metformin improves orthodontic tooth movement and resists orthodontic appliance corrosion. This narrative review aims to provide a current summary of research highlighting the prospective uses of metformin in dentistry.\n\n1. Introduction\n",
        "text": "Metformin (1, 1-dimethyl biguanide; MF) is a renowned drug with versatile utility. MF is an orally administered anti-hyperglycemic drug and is the preferred first-line drug for lowering blood glucose levels in patients with type 2 diabetes mellitus (T2DM)[REF]. MF, a second-generation biguanide, is extracted from the French lilac (Galega officinalis). In the Middle Ages, this plant extract was used to decrease blood sugar, relieve intense urination and alleviate symptoms of T2DM [REF]. In 1922, guanidine was discovered as an active ingredient in G. officinalis that lowers blood glucose levels.3 MF was overshadowed by phenformin and buformin—other guanide-derived drugs that were initially deemed more effective in treating T2DM. However, because of the high toxicity of the other medications, MF became the gold standard drug for managing high glucose levels in patients with diabetes, owing to its benign, nontoxic nature [REF]. The drug was approved for treating hyperglycemia in 1958 in the United Kingdom, in 1972 in Canada and in 1995 in the United States [REF]. MF is the most prescribed drug for T2DM management worldwide and is used by more than 120 million people [REF]. The molecular structure of MF is illustrated in Figure 1. As an orally administered drug, MF decreases glucose levels in the blood by inhibiting gluconeogenesis in the liver [REF]. This decrease is achieved by inhibition of intracellular binding of calcium in hepatocyte mitochondria, thus decreasing gluconeogenesis [REF]. Moreover, MF is the principal therapeutic drug for the treatment T2DM in obese patients [REF]. Beyond its antidiabetic effects, MF has antilipidemic, hepatoprotective, anti-neoplastic, cardioprotective and anti-obesity effects [REF]. It has various applications including the management of acne, hirsutism and polycystic ovarian syndrome, and as a chemo-preventive agent for neoplastic conditions [REF]. After oral administration, MF is gradually absorbed from the small intestine. The bioavailability of the drug has found to be approximately 50–60%, and the plasma half-life has been calculated to be 1.5–4 h [REF]. Approximately 30–50% of an orally consumed dose of MF is excreted in the urine within the first 24 h, whereas 30% is unchanged and excreted via the feces [REF]. Importantly, MF can pass through the placenta, and fetal concentrations of MF are often lower than maternal concentrations of MF. The pharmacokinetics of MF in pregnant women is influenced by the high glomerular filtration rate. The plasma concentrations of MF during pregnancy are therefore lower than those in non-pregnant women [REF]. MF exerts anti-obesity effects by decreasing appetite and increasing secretion of glucagon-like peptide-1 (GLP-1). The anti-hyperglycemic effect of MF decreases intestinal carbohydrate absorption (decreased postprandial hyperglycemia). The inhibition of hepatic gluconeogenesis occurs by halting of the Krebs cycle and oxidative phosphorylation after activation of AMP-activated protein kinase (AMPK) [REF]. The promotion of glucose transport in skeletal muscle stimulated by insulin enhances the function of glucose transporter type 4 (GLUT-4) and increases the non-oxidative glucose disposal in skeletal muscle [REF]. Additionally, MF exerts anti-lipidemic effects through increasing free fatty acid esterification and inhibiting lipolysis in adipose tissue. Furthermore, the action of MF protects β-cells against toxicity and lipotoxicity of glucose and preserves β-cell secretory capacity, thereby slowing the progression to severe diabetes [REF]. During long term use of MF, the decrease in hepatic insulin resistance and beneficial effects on lipid levels have hepatoprotective effects [REF]. MF also has indirect and direct anti-neoplastic effects. It exerts an indirect effect via decreasing insulin resistance and insulin-like growth factor 1 (IGF-1) levels. In contrast, MF exerts direct anti-neoplastic effects through AMPK-associated and AMPK-independent cellular pathways [REF]. The beneficial cardioprotective effects arise from the combined effects of a decrease in weight and an ameliorated lipid profile after long-term use of MF [REF]. Recent research has revealed that this drug also affects bone metabolism, influencing osteoblast and osteoclast differentiation via stimulating osteoprotegerin (OPG), and decreasing receptor activator of nuclear factor κB ligand (RANKL) expression [REF]. The mode of action of MF relevant to dentistry is shown in Figure 2. Note that metformin exerts its effects by (a) enhancing osteogenesis, (b) enhancing angiogenesis, (c) affecting the AMPK pathway and (d) acting in a concentration-dependent manner. (e) The clinical applications of metformin."
    },
    "2023-37275982_1057_en.txt": {
        "title": "Delayed sleep–wake phase disorder and its related sleep behaviors in the young generation",
        "prompt": "Abstract: Delayed sleep–wake phase disorder (DSWPD) is a sleep disorder in which the habitual sleep–wake timing is delayed, resulting in difficulty in falling asleep and waking up at the desired time. Patients with DSWPD frequently experience fatigue, impaired concentration, sleep deprivation during weekdays, and problems of absenteeism, which may be further complicated by depressive symptoms. DSWPD is typically prevalent during adolescence and young adulthood. Although there are no studies comparing internationally, the prevalence of DSWPD is estimated to be approximately 3% with little racial differences between Caucasians and Asians. The presence of this disorder is associated with various physiological, genetic and psychological as well as behavioral factors. Furthermore, social factors are also involved in the mechanism of DSWPD. Recently, delayed sleep phase and prolonged sleep duration in the young generation have been reported during the period of COVID-19 pandemic-related behavioral restrictions. This phenomenon raises a concern about the risk of a mismatch between their sleep–wake phase and social life that may lead to the development of DSWPD after the removal of these restrictions. Although the typical feature of DSWPD is a delay in circadian rhythms, individuals with DSWPD without having misalignment of objectively measured circadian rhythm markers account for approximately 40% of the cases, wherein the psychological and behavioral characteristics of young people, such as truancy and academic or social troubles, are largely involved in the mechanism of this disorder. Recent studies have shown that DSWPD is frequently comorbid with psychiatric disorders, particularly mood and neurodevelopmental disorders, both of which have a bidirectional association with the pathophysiology of DSWPD. Additionally, patients with DSWPD have a strong tendency toward neuroticism and anxiety, which may result in the aggravation of insomnia symptoms. Therefore, future studies should address the effectiveness of cognitive-behavioral approaches in addition to chronobiological approaches in the treatment of DSWPD.\n\n1. Introduction\n",
        "text": "Circadian rhythm, which runs for approximately 24 h, is present in all animate beings and acts as a regulatory mechanism that promotes optimal adaptation to various biological activities, including not only sleep and wakefulness but also various biological activities such as feeding, reproduction, and social activities [REF]. Normally, when circadian rhythms are synchronized with the light–dark cycle of the external world, melatonin is secreted as light exposure diminishes during the night and sleep is initiated during the downward phase of the core body temperature (CBT). Among the circadian rhythm markers, dim light melatonin onset (DLMO) and CBT rhythms show a phase relationship with the 24-h cycle. The relationship between the circadian rhythm markers (i.e., DLMO or CBT rhythm) and sleep timing (i.e., sleep onset or offset) is called the “phase angle” of circadian entrainment. Both Earth’s rotation and social activity rhythms run on the 24-h cycle, but the endogenous circadian rhythm in humans may be typically a little longer than 24 h. The length of the intrinsic circadian period is called “tau.” Although the length of tau varies from individual to individual, the mean period of tau is reported to be 24.15 h (standard deviation 0.2 h) [REF], with the length of tau of women being approximately 6 min shorter than that of men [REF]. Endogenous circadian rhythms are entrained to follow a 24-h cycle of the external world by various zeitgebers, which are the external factors that serve as cues for entrainment. Among these, light stimulation at a certain time of the day alters the firing rate of neurons in the brain’s suprachiasmatic nucleus (SCN; the command center of the circadian clock), activates the molecular signaling pathway, and alters the transcription of clock genes that determine the phase of the circadian cycle by regulating the rhythm of endogenous melatonin secretion [REF]. The effect of circadian phase resetting through melatonin and photic stimulation in the SCN follows the phase-response curve (PRC). In the PRC, if light is applied before the minimal point of CBT (CBTmin) appears, the melatonin secretion phase is likely to be delayed, whereas if light is applied after CBTmin, the phase is likely to advance [REF]. In a normal lifestyle, evening light delays the circadian clock by delaying the sleep onset timing, whereas morning light advances the circadian clock. Light exposure is the most important entrainment factor, but other zeitgebers, such as exercise, mealtime, and social activities also contribute to circadian rhythm entrainment. However, the entrainment ability of these factors is weaker than that of light [REF].Many adolescents and young adults worldwide exhibit a delayed sleep pattern, which can be considered a disorder when it significantly affects important areas of an individual’s functioning. Delayed Sleep–Wake Phase Disorder (DSWPD) [REF] is a circadian rhythm disorder in which the delay of sleep phase causes difficulty in falling asleep and waking up at a desired time, resulting in daytime dysfunction. This disorder is associated with multiple factors including specific biological traits, socio-psychological backgrounds, sleep hygiene problems, and comorbid psychiatric diseases. DSWPD is prevalent during adolescence and young adulthood. The establishment of treatment strategies against this disorder is therefore important because the incidence of DSWPD during these critical developmental stages can damage an individual’s future prospects. However, the pathological mechanism of DSWPD as well as many aspects such as the adequate classification, assessment, and treatment strategy of subgroups based on patients’ backgrounds, psychosocial characteristics, and physiological findings remain unclear. Furthermore, recent changes in the light environment surrounding adolescents have increased the likelihood of eveningness chronotype possibly leading to the development of DSWPD [REF]. A significant number of young people do not meet the diagnostic criteria of DSWPD but have delayed sleep phases (DSP) [REF]. Social jetlag (SJL) is a new concept that refers to the difference in sleep timing between weekdays and rest days has also been proposed as a concern regarding circadian rhythm related sleep hygiene in young people [REF].The development of DSWPD reflects the multifaceted interaction between social schedules, timing of exposure to light and dark, genetic factors, homeostatic pressure on sleep, and the circadian system. The extent to which a combination of any of these factors is impaired is difficult to identify in a clinical setting. Moreover, approximately 40% [REF] of patients with DSWPD have normal timing of melatonin secretion profile (the most important marker of circadian rhythm) even though their sleep–wake schedule is clearly delayed. This subgroup of DSWPD without circadian misalignment is termed circadian-entrained DSWPD and occurs based on the psychological and behavioral backgrounds in addition to biological factors in the young generation [REF]. Recent studies have also shown that DSWPD is frequently comorbid with some psychiatric disorders, particularly neurodevelopmental and mood disorders, both of which have a bidirectional association with the pathophysiology of DSWPD. In 2015, the American Academy of Sleep Medicine (AASM) published revised guidelines for the treatment of circadian rhythm sleep–wake disorders (CRSWDs), including DSWPD. However, additional treatment methods for circadian-entrained DSWPD and DSWPD complicated by psychiatric disorders should be established.In this review, we describe the physiological and psycho-behavioral backgrounds of circadian-entrained and non-entrained DSWPD in young people, as well as the relationship between psychiatric disorders and DSWPD. Furthermore, we discuss the current problems and future development of the treatment of this disorder based on the results of psychological and psychiatric assessments."
    },
    "2023-37277773_1675_en.txt": {
        "title": "Health-seeking behaviour regarding schistosomiasis treatment in the absence of a mass drug administration (MDA) program: the case of endemic communities along Lake Albert in Western Uganda",
        "prompt": "Abstract: Schistosomiasis poses a serious public health problem and a social challenge affecting over 240 million people, the majority of whom live in sub-Saharan Africa. The World Health Organization (WHO) recommends praziquantel (PZQ) drug treatment through regular mass drug administration (MDA) accompanied by social mobilisation and health education and sensitisation. With social mobilisation and health education and sensitisation, there is bound to be increased demand for the PZQ, especially in the case of endemic communities. However, it is not clear where communities go for PZQ treatment in the absence of PZQ MDA. We explored the health-seeking behaviours regarding schistosomiasis treatment among communities along Lake Albert in Western Uganda when MDA had delayed, to inform a review of the implementation policy for the achievement of the WHO’s 2030 target of 75% coverage and uptake.We conducted a community-based qualitative study in Kagadi and Ntoroko, an endemic community in January and February 2020. We interviewed 12 individuals: local leaders, village health teams, and health workers, and conducted 28 focus group discussion sessions with 251 purposively selected community members. The audio recordings of the data were transcribed and analyzed using a thematic analysis model.Generally, participants seldom seek medication for schistosomiasis-related signs and symptoms from government hospitals and health centres II, III and IV. Instead, they rely on community volunteers such as VHTs, private facilities, such as clinics and drug shops nearby, or traditional sources (e.g. witch doctors and herbalists). Results show that factors influencing people to seek treatment from sources other than the government are: the absence of PZQ drugs in the government health facility; health workers’ negative attitude towards patients; long distances to the government hospitals and health facilities; poor and inaccessible roads; medication-related costs; and negative perceptions of the PZQ drug.Availability and accessibility of PZQ seem to be a big challenge. PZQ uptake is further hampered by health systems and community-related and socio-cultural factors. Thus there is a need to bring schistosomiasis drug treatment and services closer to endemic communities, stock nearby facilities with PZQ and encourage endemic communities to take the drug. Contextualised awareness-raising campaigns are needed to debunk myths and misconceptions surrounding the drug.The online version contains supplementary material available at 10.1186/s12889-023-16020-z.\n\n1. Introduction\n",
        "text": "Schistosomiasis poses a serious public health problem and a social challenge affecting over 240 million people, the majority of whom live in sub-Saharan Africa. The World Health Organization (WHO) recommends praziquantel (PZQ) drug treatment through regular mass drug administration (MDA) accompanied by social mobilisation and health education and sensitisation. With social mobilisation and health education and sensitisation, there is bound to be increased demand for the PZQ, especially in the case of endemic communities. However, it is not clear where communities go for PZQ treatment in the absence of PZQ MDA. We explored the health-seeking behaviours regarding schistosomiasis treatment among communities along Lake Albert in Western Uganda when MDA had delayed, to inform a review of the implementation policy for the achievement of the WHO’s 2030 target of 75% coverage and uptake.Schistosomiasis is one of the neglected tropical diseases (NTD) that affect a significant proportion of the world’s population and is most prevalent in sub-Saharan Africa [REF]. Of all the species, Schistosoma mansoni (intestinal schistosomiasis) is the most common in Uganda [REF]. Current control and prevention measures primarily include preventive chemotherapy through mass drug administration (MDA) of PZQ for school-going children aged 5–14 years old and adults in certain situations, supplemented with improvement in water, sanitation and hygiene (WASH), health education and sensitization, and snail control [REF]. The WHO categorizes schistosomiasis infection endemic areas into three levels: low for areas below 10%; moderate for those between 10 and 49%; and high for areas from 50% and above [REF]. The WHO further recommends that PZQ be administered at least once every 2 years, once a year, and twice a year for low-, moderate-, and high-endemic areas, respectively. Although it is hoped that these interventions can facilitate the achievement of the WHO’s target of 75% MDA coverage by 2030, in Uganda’s case, this is far from attainment. This is evidenced by low uptake among school-going children of 28.2% in 2011 to 48.9% in 2012 and among adult communities of 48.8% in 2016 [REF]. To ensure optimum outcomes and sustainability of PZQ uptake, there is a need to improve the MDA implementation strategy to realize increased drug uptake [REF]. Moreover, individuals who experience schistosomiasis-related signs and symptoms must always seek PZQ treatment. Some of the signs and symptoms are skin rash, fever, head and body aches, breathing difficulties, diarrhoea and constipation, blood in the feces and swollen bellies [REF]. While liver fibrosis, intestinal ulcers, high blood pressure, stunted growth, cognitive impairment in children, and infertility in women appear in advanced stages but are difficult to diagnose [REF]. Most of these signs and symptoms show up at different stages after infection, some of which may trigger health-seeking by some community members.Health-seeking behaviour refers to “any activity undertaken by individuals who perceive themselves to have a health problem or to be ill, to find an appropriate remedy” [REF]. Studies conducted on health-seeking behaviour in many parts of the world have suggested that people seek treatment from either biomedical or traditional healthcare sources or both [REF]. In Southwest Ethiopia, for instance, most of the urban communities (80.7%) sought health services from modern sources in contrast to rural areas at 48.1% [REF]. Determinants of health-seeking also vary across space, time, and different population groups [REF]. Factors such as age, sex, education level, marital status, socioeconomic status, social networks, ethnic beliefs, and attitudes among others, have been found to influence the decisions to seek treatment from different sources [REF]. Disease-related factors, such as cause and type of illness, acute or chronic illness, and severe or trivial, also influence individuals’ health-seeking behaviour [REF]. The above findings suggest that different populations seek medication for certain illness conditions from various sources, but it is not clear whether this is the same for schistosomiasis-related signs and symptoms.Studies on health-seeking regarding schistosomiasis treatment with PZQ have equally been conducted showing varied findings. For instance, in the Kiri community of Adamawa state- Nigeria, findings showed that a significant proportion of the participants usually bought drugs from the pharmacy or drug shops for schistosomiasis-related signs and symptoms, whereas some got medication from herbalists and the majority never sought medication at all [REF]. Likewise in Tanzania, some participants sought medication for schistosomiasis treatment from both traditional and modern healthcare sources, although modern medicine was taken to be the most effective [REF]. Findings from studies in Eastern Uganda and Zanzibar, also showed that some participants did not receive PZQ drug treatment because either they were absent or unaware of the MDA program, were busy, feared side effects, or were pregnant [REF]. Similarly, in Ethiopia, a study on health-seeking for different NTDs showed that gender disparities in healthcare access and utilization were attributed to inequalities and power dynamics between men and women regarding the decision to seek treatment and its financing [REF]. Furthermore, studies show that people with biomedical knowledge of transmission and who perceive the symptoms as severe and acute, tend to seek treatment from modern sources [REF]. Lack of health workers and limited drug supplies have also been found to hinder the communities along Lake Victoria in Eastern Uganda from seeking treatment for schistosomiasis-related signs and symptoms from a health facility [REF]. This finding was similar to studies in Brazil, the Philippines, and Nigeria where communities reported the following barriers to seeking treatment from a facility: the long distance from the nearest health facility, transport costs, poor road networks, and lack of health information [REF]. Whereas these studies have been conducted on health-seeking regarding the treatment of schistosomiasis-related signs and symptoms, they have mainly focused on PZQ of MDA but did not provide an insight into where the populations turn to in the absence of the MDA program.In Uganda, the PZQ-MDA program has been implemented for almost 20 years now since it was first introduced in 2002 [REF]. The PZQ drugs are procured with support from development partners such as the schistosomiasis control initiative (SCI), ARISE and ASCEND, and are managed by the vector control division of the Ministry of Health and delivered to the districts [REF]. From the district, the vector control officers together with the district health team distribute them to the communities for VHTs to administer to adult community members and children of five years and above but not going to school, and for teachers to give to school-going children in schools [REF]. Before the MDA is implemented, social mobilisation (SM) and health education and sensitization are conducted by the Ministry of Health and donors, that is the development partners [REF]. Social mobilisation refers to activities conducted to influence a large number of individuals to take certain actions for the benefit of the community as a whole [REF]. In the context of MDA of PZQ, social mobilisation is those activities conducted by the district health team, VHTs, village chairpersons and the development partners, aimed at raising awareness and informing the target communities to participate in the program [REF]. Consequently, there has been a steady increase in the uptake of PZQ over the years [REF]. SM, in particular, is likely to lead to further increased demand for PZQ drugs. Moreover, endemic communities (especially those that live along the lake shores) that continue to engage in risky water, sanitation, and hygiene (WASH) practices [REF], must receive constant treatment to contain the spread of the disease and control morbidity levels. However, in the absence of MDA, it is not clear what such communities do when they need the PZQ. Understanding where the communities go for treatment and the reasons that drive their actions is important to inform policy recommendations for improved NTD control and ultimately eliminate the disease. To investigate this gap in current knowledge and understanding, we conducted a community-based qualitative study among communities in Western Uganda where MDA treatment had not been conducted for more than a year, to explore their health-seeking behaviour regarding schistosomiasis treatment. Specifically, we assessed (1) the communities’ sources of health care services for the treatment of schistosomiasis-related signs and symptoms and, (2) the factors determining their choices of health-seeking regarding schistosomiasis-related signs and symptoms in the absence of the MDA program, to inform government and partners on how to improve praziquantel drug uptake."
    },
    "2023-37277812_1107_en.txt": {
        "title": "Demographic dividend-favorable policy environment in two pre-dividend African nations: review of national policies and prospects for policy amendments in Nigeria and Tanzania",
        "prompt": "Abstract: In collaboration with local partners, we reviewed 18 national policy documents across two sub-Saharan African countries identified as pre-dividend nations by the World Bank in 2017: Nigeria and Tanzania. Our aim was to assess national policies in pre-dividend countries and to determine whether national strategies were primed to capitalize on changing demographic structures, maximally attain the demographic dividend, and augment socio-economic growth.We conducted policy reviews by focusing on five key sectors of the Gates Institute Demographic Dividend Framework: Family Planning, Maternal and Child Health, Education, Women’s Empowerment, and Labor Market. This framework was developed as a tool for countries to apply targeted policies for accelerating the demographic dividend based on their demographic structure. For each component we used a comprehensive list of indicators, defined via a systematic literature review, through which we assessed national policies aimed at maximizing the demographic dividend.Between the two countries, we observed persistent gaps in policies targeting family planning. Although more comprehensive, policies addressing maternal and child health, education, women’s empowerment, and labor market still lagged in their specificity and measurability. We identified specific policy amendments and alternatives that Nigeria and Tanzania could consider to mitigate these gaps. We also stress the importance of designing measurable policy initiatives across sectors.Based on these recommendations, as Nigeria, Tanzania, and other pre-dividend nations start experiencing rapid demographic changes, they may consider implementing routine policy reviews to strengthen policies across the five key sectors and harness the benefits of a demographic dividend.The online version contains supplementary material available at 10.1186/s12889-023-15690-z.\n\n1. Introduction\n",
        "text": "In 2017, the African Union committed to “Harnessing the Demographic Dividend Through Investments in Youth”, prompting leaders across the continent to develop multisectoral policy roadmaps and action plans for attaining the demographic dividend [REF]. The demographic dividend presents countries with the opportunity to accelerate economic growth and maximize per capita gains [REF]. These benefits come to fruition because of a demographic transition initiated by declines in national fertility and infant and child mortality rates. These changes work in tandem to modify the population age structure, limiting the number of child dependents and increasing the number of working age adults, resulting in a lowered dependency ratio and a larger labor force [REF]. With a larger workforce, countries have the potential to boost their economic output and work to impart social and economic change.However, the demographic transition and subsequent dividends in economic growth are not automatic. Indeed the extent to which countries benefit from this demographic transition is highly dependent on the existence of policies that establish optimal conditions for capturing the full returns of the demographic dividend [REF]. Moreover, countries have a limited window of opportunity to act on maximizing the potential of the demographic dividend. Reher [REF] estimated the window of opportunity was open for 100 years for Spain and Sweden and only 10 to 30 years for a subset of developing countries. Eventually, the demographic age structure will shift again when the increased adult population ages and enters an older, less-productive cohort that relies on their savings and retirement funds [REF]. In conjunction with smaller birth cohorts due to fertility declines, the dependency ratio will subsequently rise again, but will be characterized by the need to care for elderly populations. Evidence suggests that ensuring demographic dividend-favorable policies are in place, preferably before entering the demographic transition, can assist countries to fully harness the benefits of the demographic transition. However, most of this evidence comes from Southeast Asian and European countries with limited knowledge from African countries [REF].Bloom and colleagues have previously described that investments in health, education, and the labor-market are key to capitalizing on the demographic transition [REF]. More specifically, key policy areas should: expand family planning and reproductive health to facilitate rapid fertility declines; improve maternal and child health and nutrition to reduce child mortality; increase educational attainment to facilitate a strong labor force and productivity; and invest in the labor and economic sectors to promote job creation and economic growth [REF]. Less well-documented are the specific policy investments and strategies that countries should enact to create a demographic dividend-favorable environment.The Population Reference Bureau (PRB) recently conducted a literature review to identify specific policy inputs conducive to achieving the demographic dividend [REF]. This review provided evidence that policy interventions like legalizing contraception, establishing family planning service delivery and outreach, investing in disease prevention and public health, offering free public education, reinvesting gross domestic product (GDP) growth in working-age populations, developing open trade policies, and encouraging foreign direct investment, among others, can facilitate attainment of the demographic dividend. Similarly, the World Bank reviewed policy documents for policies that would catalyze and accelerate a demographic transition among eight West African countries [REF]. Their review documented achievements and opportunities for improvement in allocating financial resources, stakeholder support, institutional building and strengthening, and results monitoring to fast-track the demographic dividend.Although knowledge of evidence-based policies to realize the demographic dividend exists, the extent to which they are incorporated into national planning is not well understood. Discerning the degree to which these policies are embedded within policy frameworks requires a comprehensive review of the national policy landscape. This intensive process can be accomplished through an exhaustive review of national strategic frameworks and sector-specific policy documents. However, the scope and relevance of these policies may vary depending on which phase of the demographic transition a country is experiencing. For instance, most countries in sub-Saharan Africa are in the pre-demographic dividend phase, with total fertility rate (TFR) greater than four children per woman, or the early-demographic dividend phase, whereby TFR is below four children [REF].Evidence suggests that pre-dividend countries should prioritize human development outcomes, with a goal towards accelerating the fertility decline, creating a population age structure with fewer dependents, and assisting a growing proportion of the population that is working age to develop the necessary skills to enter the labor market in the future. Conversely, early-dividend countries should prioritize accelerating job creation to ensure that the growing share of the working-age population remains employed to promote economic growth. The labor market must develop productive jobs to appropriately match labor supply with a growing labor demand. Regardless of demographic transition stage, countries adopt a multisectoral and integrated approach to ensure that the right supportive policies are in place to catalyze the demographic transition and translate it into concrete economic growth.While studies have suggested specific policy enactments for countries to capitalize on changing demographic structures, these suggestions were mostly generated from the experiences in Southeast Asian and European countries. Furthermore, there are no demographic dividend-favorable policies specific to stages of the demographic transition in African countries in the existing literature. Using the Demographic Dividend Framework, developed by the Gates Institute, we aimed to review the policy landscape of two pre-dividend African nations as case studies, and to identify existing evidence-based policies conducive to fostering a demographic dividend-favorable environment in the pre-dividend context of each country. We also assess existing national policies based on their degree of comprehensiveness and identify gaps. We subsequently make recommendations regarding policy amendments that both African nations could undertake to capitalize on their changing demographic structures to attain a demographic dividend, and boost their economies."
    },
    "2023-37278272_907_en.txt": {
        "title": "Progress in Guillain–Barré syndrome immunotherapy—A narrative review of new strategies in recent years",
        "prompt": "Abstract: Guillain – Barré syndrome (GBS) is an immune-mediated neuropathy, the pathology of which is not clear. Both cellular and humoral immunity are involved in the occurrence of the disease, and molecular mimicry is currently the most widely recognized pathogenesis. Intravenous immunoglobulin (IVIg) and plasma exchange (PE) have been proven to be effective in improving the prognosis of patients with GBS, but there has been no progress in the treatment of the disease or strategies to improve the prognosis. New treatment strategies for GBS are mostly immunotherapies, including treatment against antibodies, complement pathways, immune cells and cytokines. Some of the new strategies are being investigated in clinical trials, but none of them have been approved for the treatment of GBS. Here, we summarized the current therapies for GBS, and new immunotherapies for GBS according to pathogenesis.\n\n1. Introduction\n",
        "text": "Guillain–Barré syndrome (GBS) is an immune-mediated neuropathy that is the most common cause of acute flaccid paralysis and affects approximately 100,000 people per year worldwide. Antecedent events are often found 4 weeks before clinical syndromes appear in GBS patients, such as surgery or infection and Campylobacter jejuni, Haemophilus influenzae, cytomegalovirus, Zika virus and Japanese encephalitis virus are widely discussed [REF]. Depending on the different sites of the immune response, GBS is generally divided into demyelinating and axonal subtypes. Acute inflammatory demyelinating polyradiculoneuropathy (AIDP) is characterized by the demyelination of peripheral nerves and infiltration of inflammatory cells, with subsequent axonal damage. Antibodies can be detected on Schwann cells, and AIDP is the most common subtype in Western countries [REF]. Antigenic epitopes of bacteria and viruses are presented to T cells by activated macrophages, causing the cross reactivity of T cells. Activated T cells promote the release of cytokines and free radicals, disrupt the blood nerve barrier and damage myelin, ultimately leading to acute demyelination syndrome [REF]. Experimental autoimmune neuritis (EAN) is an animal model of AIDP that uses myelin epitopes P0 or P2 as major antigens to induce T-cell-mediated neuritis [REF]. Acute motor axonal neuropathy (AMAN) is the second most common subtype of GBS. AMAN presents as primary axonal injury with antibody and membrane attack complex (MAC) deposition in nodes of Ranvier without obvious inflammatory cell infiltration or demyelination [REF]. Antibodies related to AMAN include anti-GM1 and anti-GD1a antibodies [REF]. In AMAN patients related to Campylobacter jejuni, the gangliosides of the peripheral nerves are similar in structure to the lipo-oligosaccharides of Campylobacter jejuni. This suggests that the pathogenic mechanism of AMAN may be the cross-reaction of homologous epitopes between bacterial lipo-oligosaccharides and peripheral motor axon gangliosides [REF]. Miller-Fisher syndrome (MFS) is a variation of GBS, and the clinical features of MFS are facial muscle weakness and ataxia. Most MFS patients have anti-GQ1b antibodies, implying a potential role for ganglioside antibodies in disease pathogenesis or as reliable diagnostic biomarkers [REF]. Anti-GQ1b antibodies have been proven to activate complement at the neuromuscular junction in vitro, and complement activation is thought to be the primary pathogenic mechanism of MFS.25Inflammatory cells and inflammatory factors play an important role in the pathogenesis of GBS. Macrophages play a dual role in the pathogenesis of GBS. Proinflammatory macrophages (M1) and anti-inflammatory macrophages (M2) play a decisive role in the initiation and development of GBS and EAN. M1 macrophages can promote the destruction of the blood-nerve barrier, induce the production of cytokines and chemokines, promote Th1 polarization, and eventually lead to demyelination of peripheral nerves. M2 macrophages play a protective role in the course of disease; they can promote T-cell apoptosis, remove myelin and axon fragments, inhibit inflammation, and promote the regeneration of axons and myelin [REF]. T-cell subtypes are also involved in the pathogenesis of GBS. The imbalance between Th1 and Th2 responses contributes to the pathogenesis of GBS [REF]. Th1 responses are thought to provoke disease by activating and recruiting macrophages to sites in peripheral nerves, subsequently leading to nerve damage induced by the direct action of macrophages or by toxic and inflammatory substances released in situ. On the other hand, the Th2 response acts as a suppressor and regulator of the Th1 pathway and therefore may have the resolving effect observed in the recovery phase of GBS and EAN [REF]. Recently, IL-17 and Th17 cells have been found to play an important role in many immune diseases, and Treg cells have a suppressive effect on inflammation [REF]. In the acute phase of the clinical course of GBS, the number and ratio of CD4+CD25+ T cells decrease, but this decrease is reversible [REF]. Another study found that regulatory T cells (Tregs) from GBS patients and Tregs from healthy controls showed equal expression of FoxP-3 mRNA, and their ability to suppress the proliferation and cytokine secretion of CD4+ effector T cells was unimpaired in GBS patients [REF]. Furthermore, adoptive infusion of autologous CD4+CD25+ Treg cells can reduce inflammatory cell infiltration of the sciatic nerve in EAN rats [REF]. Cytokines are small active proteins secreted by immune cells and some nonimmune cells that have several functions, such as regulating cell growth and differentiation, modulating the immune response, and participating in the inflammatory response. Proinflammatory cytokines such as interleukin (IL)-1β, IL-6, IL-12, tumor necrosis factor (TNF)-α, interferon (IFN)-γ, etc., damage myelin by recruiting effector cells to peripheral nerves and promoting the in situ release of toxic substances. Anti-inflammatory cytokines, such as IL-4 and IL-10, inhibit disease progression or promote myelin repair by exerting anti-inflammatory effects [REF]. Along with more studies on the pathogenesis of GBS, new strategies targeting at different points in the treatment of the disease have emerged. In this narrative review, we summarize the new approaches of classical therapies and new strategies in both animal models and clinical practice to identify potential therapies for GBS patients."
    },
    "2023-37278852_637_en.txt": {
        "title": "Microneedles: materials, fabrication, and biomedical applications",
        "prompt": "Abstract: The microneedles have attracted great interests for a wide range of transdermal biomedical applications, such as biosensing and drug delivery, due to the advantages of being painless, semi-invasive, and sustainable. The ongoing challenges are the materials and fabrication methods of the microneedles in order to obtain a specific shape, configuration and function of the microneedles to achieve a target biomedical application. Here, this review would introduce the types of materials of the microneedles firstly. The hardness, Young's modulus, geometric structure, processability, biocompatibility and degradability of the microneedles are explored as well. Then, the fabrication methods for the solid and hollow microneedles in recent years are reviewed in detail, and the advantages and disadvantages of each process are analyzed and compared. Finally, the biomedical applications of the microneedles are reviewed, including biosensing, drug delivery, body fluid extraction, and nerve stimulation. It is expected that this work provides the fundamental knowledge for developing new microneedle devices, as well as the applications in a variety of biomedical fields.\n\n1. Introduction\n",
        "text": "Microneedles have attracted increasing interest in recent years in biomedical areas, such as drug delivery, biosensing, and fluid extraction [REF]. The tips of the microneedles can penetrate the epidermis layer of skin to arrive at the top layer of the dermis that is above the blood capillaries. Thus, the microneedles would not touch the nerve endings to further avoid bleeding and pain [REF]. The microchannels can be formed on the skin by the microneedles where the molecules can pass through, or the microneedles can bring the devices, such as the electrodes to get the biomedical signals [REF]. For drug administration, the microneedle approach has the advantages of avoiding the effect of the gastrointestinal digestion, accelerating the drug absorption, prolonging the effective time, and patient self-administered without needing doctors [REF]. For biosensing, the microneedle approach provides possible continuous monitoring of the biomarkers in interstitial fluid [REF].A microneedle usually has a microscale height and width, in the shape of a cone or a pyramid, and usually, a patch of microneedle arrays sticks to the skin. The concept of transdermal delivery by using microneedles was first proposed by Zhao et al. in 2020, but due to the limitations of micromachining technology, it was not until the 1990s that microneedles were mass produced. The earliest use of hollow needles was by Mcallister et al., 150 μm micron needles made of silicon in the 1980s [REF].Several types of microneedles have been fabricated, including solid microneedles, coated microneedles, soluble microneedles, and hollow microneedles. The microneedle materials can be silicon [REF], metals, biomaterials, and polymers [REF]. A variety of fabrication methods have been explored, including photolithography with selective etching, machining with chemical etching, machining with micro-milling, laser cutter, 3d printing, soft lithography, micro-injection molding, hot embossing, electrohydrodynamic atomization, laser machining, electric-discharge machining with laser machining, soft lithography, drawing lithography, centrifugal lithography, and droplet-borne air blowing [REF].This review would focus particularly on the recent progress of the materials, fabrication methods, and biomedical applications of microneedles. The study would introduce the types of materials for constructing the microneedles, their fabrication, and related biomedical applications in drug delivery, biosensing, and body fluid extraction. Most of the published reviews on microneedles mainly reviewed the application of microneedles in drug delivery [REF]. In recent years, the potential of microneedles in the field of biosensors has attracted great attention because of its painless and slightly invasive, such as the sensor based on microneedles for biomarkers of human tissue fluid. This paper also summarizes other applications on body fluid extraction and nerves. In addition, due to the high precision of the structure of the microneedle, there are great difficulties in the preparation of the microneedle. At present, other reviews lacked discussion about the fabrication methods of the microneedle [REF]. This study summarized almost all the existing processing technologies of the microneedle that can serve as the fundamentals for future study of the microneedle devices."
    },
    "2023-37280028_793_en.txt": {
        "title": "Association between domain-specific sedentary behaviour and endometrial cancer: a systematic review and meta-analysis",
        "prompt": "Abstract: Sedentary behaviour is associated with increased cancer risk. We aim to assess the associations of domain-specific and total sedentary behaviour with risk of endometrial cancer, with additional attention paid to potential differences in adjustment strategy for obesity and physical activity.A systematic review and meta-analysis was conducted in accordance with the guidelines of Preferred Reporting Items for Systematic Reviews (PRISMA) and Meta-Analyses and the Meta-analysis of Observational Studies in Epidemiology (MOOSE).PubMed, Embase and MEDLINE databases were searched up to 28 February 2023, supplemented by grey literature searches.Observational human studies evaluating the association between sedentary behaviour and endometrial cancer.Two reviewers extracted data and conducted the quality assessment based on Newcastle-Ottawa Scale (NOS) independently. We used a random-effects model with inverse variance approach to pool the estimates. The extent of heterogeneity was quantified with the I2 statistics.Sixteen studies were included in the systematic review. Fourteen studies involving 882 686 participants were included in the meta-analysis. The pooled relative risks (RRs) for high versus low level of overall sedentary behaviour was 1.28 (95% CI: 1.14 to 1.43; I2=34.8%). The increased risk regarding specific domains was 1.22 (95% CI: 1.09 to 1.37; I2=13.4%, n=10) for occupational domain, 1.34 (95% CI: 0.98 to 1.83; I2=53.7%, n=6) for leisure-time domain and 1.55 (95% CI: 1.27 to 1.89; I2=0.0%, n=2) for total sedentary behaviour. Larger pooled RRs were observed among studies with adjustment for physical activity and studies without adjustment for body mass index.Higher levels of sedentary behaviour, total and occupational sedentary behaviour in particular, increase the risk of endometrial cancer. Future studies are needed to verify domain-specific associations based on objective quantification of sedentary behaviour, as well as the interaction of physical activity, adiposity and sedentary time on endometrial cancer.\n\n1. Introduction\n",
        "text": "According to the updated global cancer burden estimates from Global Cancer Statistics 2020 [REF], endometrial cancer ranks the sixth most common cancer in women worldwide, and the most common gynaecologic cancer in several developed regions, including North America and Eastern and Northern Europe. A worrying trend is that, since the late 1990s, the incidence of endometrial cancer has rapidly increased in several developing countries during urbanisation, including some Asian countries (Japan, Singapore, China and the Philippines) and South Africa [REF]. It is suggested that this phenomenon may be explained, at least partly, by changing environmental and lifestyle risk factors in these regions, such as the epidemic of obesity, lack of physical activity and long-time sitting [REF]. Although obesity is a known risk factor for endometrial cancer, the association between sedentary behaviour and endometrial cancer remains largely unclear. Sedentary behaviour includes sitting, reclining or lying behaviour characterised by low energy expenditure [REF]. During the past decades, technological innovation has influenced how people work and spend leisure-time, and has led to inevitably prolonged sitting time, particularly for desk-based office work and screen-based recreation. According to the WHO Guidelines on Physical Activity and Sedentary Behaviour (2020), long sedentary time is associated with various deleterious health outcomes, including all-cause mortality, cardiovascular diseases, obesity and more recently total cancer morbidity [REF].Three previous meta-analyses investigating the association between sedentary behaviour and several types of cancers [REF], reported a 28–36% increased risk of endometrial cancer among individuals with higher levels of sedentary behaviour by summarising 3–11 individual studies. However, some evidence on sedentary behaviour and endometrial cancer has not yet been included in existing review and meta-analyses, the level of evidence for cancer-specific incidence remains unclear [REF]. Given inconsistent results reported, an up-to-date review of current evidence is in urgent need to clarify the association between sedentary behaviour and endometrial cancer risk.No distinction in domains of sedentary behaviour is a likely source of the discrepancy in previous findings. The WHO Guidelines 2020 has operationalised the definition of sedentary behaviour to further include self-reported sitting that can be assessed in various domains (including leisure-time and occupational domain) and total sedentary behaviour [REF]. Meanwhile, the association with adverse health outcomes may differ in certain domains of sedentary behaviour [REF]. It is increasingly recognised that confounding factors may vary greatly across domains of sedentary behaviour, and contribute to varied associations with health-related outcomes [REF]. For example, while occupational sedentary behaviour is related to education and socioeconomic variables, leisure-time sedentary behaviour is likely linked to lifestyle factors such as diet and obesity [REF]. Moreover, these two domains are often inversely correlated to physical activity. However, current evidence has been derived mostly from studies that have broadly categorised sedentary behaviour according to the level of sitting time involved [REF]. Domain-specific analyses, taking account of variability in study characteristics, may help to further clarify the investigated association and to refine the prevention strategy of endometrial cancer.Besides, the complex interplay within lifestyle factors, including obesity, physical activity and sedentary behaviour, needs to be taken into consideration within the context. Obesity is a known risk factor for endometrial cancer, with a clear dose-response relationship (the higher the body mass index (BMI), the greater the risk), detailed documented by the International Agency for Research on Cancer (IARC) working group [REF]. Given that prolonged sitting is likely to be related with high BMI, obesity thus may be a potential mediator linking sedentary behaviour to cancer incidence. Under this circumstance, studies adjusting for BMI as a confounding factor may attenuate the true effects of sedentary behaviour when evaluating its impacts on endometrial cancer. A few studies have probably recognised this issue and provided results without and with additional adjustment for BMI [REF]. In addition, although less evidence presented, similar concerns have been raised with regard to physical activity, which has a potential protective effect on cancer risk [REF].In this systematic review and meta-analysis, the primary aim was to analyse comprehensively the existing studies of the associations between domain-specific (occupational and leisure-time) and total sedentary behaviour and endometrial cancer risk, with additional attention paid to potential difference of the findings related to different adjustment strategies for BMI and physical activity."
    },
    "2023-37280033_774_en.txt": {
        "title": "Risk factors and associated factors for calcaneal apophysitis (Sever’s disease): a systematic review",
        "prompt": "Abstract: The aim of this review was to identify the potential intrinsic and extrinsic risk factors (RFs), associated factors (AFs) and consequences of developing calcaneal apophysitis (CA).Systematic review.Cochrane Library, Embase, Medline Ovid, PubMed, Web of Science and Evidence, searched from inception to April 2021.We included cohort, case–control and cross-sectional studies that were conducted in patients younger than 18 years who were exposed to RFs or who presented with factors associated with developing CA. Studies in languages other than English or Spanish were excluded.Two reviewers worked independently to evaluate the risk of bias of included studies. The Newcastle–Ottawa Scale (adapted version) was used.A total of 736 studies were identified and 11 observational studies fully met the inclusion criteria, including 1265 participants with a mean age of 10.72 years. Four studies identified extrinsic factors, 10 identified intrinsic factors and three identified both. The extrinsic and intrinsic RFs, AFs and consequences of CA include limitation of ankle dorsiflexion, foot alignment, stiffness and mobility of the midfoot, plantar pressures and ground reaction force, body mass index, age, gender, presence of other osteochondroses and practice of sport. The risk of bias varied, being either moderate or low.Regarding the factors and consequences associated with CA (Sever’s disease), ankle dorsiflexion limitation is the most frequent intrinsic factor studied, followed by peak plantar pressures and foot malalignment. However, disagreements between the investigators of the included studies were found; in some cases, there is a lack of unanimity between different studies as to which factors are considered to be RFs, AFs and consequences.CRD42021246366.\n\n1. Introduction\n",
        "text": "Calcaneal apophysitis (CA) or Sever’s disease is the most common cause of heel pain in children and young people, showing an incidence of 3.7 per 1000 patients [REF]. It is a common musculoskeletal disease, representing between 2% and 16% of the consultations in sports clinics 3 4 and 5.8%–22.7% of repetitive stress injuries in children [REF]. The condition is most common in girls between 7 and 12 years of age and in boys between 8 and 15 years of age [REF], showing a peak in both genders between 10 and 12 years of age [REF].CA was described by Dr Sever in 1912, due to sporadic continuous pain in the posterior area of the calcaneus bone in paediatric patients [REF]. Histological studies define CA as inflammation or bone oedema due to a stress fracture in the secondary growth centre of the calcaneus, which is not related to necrosis [REF]. The inflammation is due to the traction forces from the triceps surae muscle on the CA, which is composed of hyaline cartilage [REF]. The repetition of these traction forces, combined with the vertical ground reaction force on the calcaneus, generates bending forces on the medial side of the CA. Because the number of chondrocytes is increased in the CA during the development of children, the CA is not able to tolerate these bending forces [REF]. The gender incidence of this condition is controversial. A previous study by Orava and Virtanen13 concluded that the CA incidence ratio was 10:1, being more common in boys than girls. However, Micheli and Ireland [REF], McKenzie et al5 and Kvist and Heinonem9 concluded that the ratio was 3:1, 2:1 and 1:1 in their respective studies.CA is a unilateral condition in most cases [REF], which may be related to the dominant leg. However, it has also been described as a bilateral condition in some cases [REF]. The diagnosis of CA is mainly based on its signs and symptoms, but it can also be confirmed with complementary tests, such as X-ray imaging. The pain is mainly located in the posterior and plantar side of the CA9 16 17 or in the Achilles tendon insertion [REF]. Various treatments have been described in the literature and the most frequently used include: stretching and lengthening of the triceps surae muscle, application of ice, restriction or limitation of physical activity, rest, topical non-steroidal anti-inflammatory drugs, taping, and the use of foot orthoses and heel pads [REF]. Among the causes of CA, a rapid period of maturation due to accelerated growth that causes muscle–tendon imbalances has been described. Muscle development is slower than bone development, which causes muscular tension [REF]. Obesity is considered a cause of a valgus deformity of the knees and ankles, which increases the risk of CA [REF]. The distribution of body weight between the right and left feet in children with CA is higher on the affected lower limb, maintaining the highest peaks of pressure in the feet [REF]. Another cause is overuse due to repetitive impacts that generate repetitive micro-traumas, in movements such as jumping or running [REF]. This factor can be worsened by footwear that is inadequate or without cushioning [REF]. Some sports are associated with the disease, such as athletics, football, basketball, ballet and tennis [REF], as well as any running activity practised on hard surfaces [REF]. The functional limitation of the dorsiflexion movement of the ankle joint, which can be due to a limitation of the triceps surae muscle or the hamstring muscles [REF], may cause the disease. Furthermore, 95% of children with CA present with biomechanical alterations [REF]. Some authors have described the flat foot with valgus calcaneus as the most common alteration in children with CA [REF]. However, other authors5 conclude that 50% of the subjects presented a varus calcaneal deviation, which, in dynamic conditions, presents as a pronated foot. The two other main causes of CA that were identified in the literature were clubfoot1 29 and pes cavus [REF].Knowledge of the risk factors (RFs) related to CA helps to provide important information for the prevention and early treatment of this common pathology in the paediatric population. Therefore, the aim was to conduct a systematic review to identify potentially intrinsic and extrinsic RFs associated with developing CA, as well as the consequences of the condition, as described by the evidence."
    },
    "2023-37280524_620_en.txt": {
        "title": "Interaction of lncRNAs with mTOR in colorectal cancer: a systematic review",
        "prompt": "Abstract: Colorectal cancer (CRC) is the third most widespread cancer and the fourth leading lethal disease among different societies. It is thought that CRC accounts for about 10% of all newly diagnosed cancer cases with high-rate mortality. lncRNAs, belonging to non-coding RNAs, are involved in varied cell bioactivities. Emerging data have confirmed a significant alteration in lncRNA transcription under anaplastic conditions. This systematic review aimed to assess the possible influence of abnormal mTOR-associated lncRNAs in the tumorigenesis of colorectal tissue. In this study, the PRISMA guideline was utilized based on the systematic investigation of published articles from seven databases. Of the 200 entries, 24 articles met inclusion criteria and were used for subsequent analyses. Of note, 23 lncRNAs were prioritized in association with the mTOR signaling pathway with up-regulation (79.16%) and down-regulation (20.84%) trends. Based on the obtained data, mTOR can be stimulated or inhibited during CRC by the alteration of several lncRNAs. Determining the dynamic activity of mTOR and relevant signaling pathways via lncRNAs can help us progress novel molecular therapeutics and medications.\n\n1. Introduction\n",
        "text": "According to the released statistics in 2019, colorectal cancer (CRC) is the third foremost pervasive malignancy in cancer patients. It is projected that CRC causes over 1.8 million newly diagnosed cases with an approximate annual death of 900,000 [REF]. Due to recent progress in cancer therapy, the survival rate for CRC patients has been dramatically improved. Despite these advances, the therapeutic outcome under progressive CRC conditions is suboptimal based on a five-year survival rate in 12% of CRC cases [REF]. Molecular investigations have indicated that the incidence and development of CRC is an intricate process with the involvement of exogenous and endogenous variables [REF]. For instance, recent investigations in molecular pathological epidemiology revealed a close association between dietary and lifestyle factors with the risk of CRC. It is suggested that smoking, alcohol drinking, processed meat, genetic predisposition, and some therapeutic agents such as aspirin can increase the chance of CRC in human [REF]. Hypermutation has been detected by large-scale sequencing in CRC samples, especially in association with substantial microsatellite instability (MSI) caused by hypermethylation and suppression of the MLH1 gene [REF]. Likewise, APC, TP53, SMAD4, PIK3CA, and KRAS are the candidate genes most often mutated [REF]. To date, numerous attempts have been done to recognize the molecular processes and signaling pathways implicated in CRC development and progression [REF].The mammalian (or mechanistic) target of rapamycin (mTOR) is a critical constituent of a signaling pathway that controls varied cell activities such as progression and proliferation, metabolism, motility, phenotype acquisition, and angiogenesis [REF]. mTOR is a member of the phosphoinositide 3-kinase-related kinases family, which has a substantial impact on CRC [REF]. Regarding the central role of mTOR in CRC pathophysiology, future studies should focus on the elucidation of mTOR activity in CRC cases.Several studies have shown that several long non-coding RNAs (lncRNAs) have a role in the regulation of the mTOR signaling pathway [REF]. lncRNAs are transcripts that include more than 200 nucleotides and lack protein-coding capabilities [REF]. Some of the lncRNAs may be transcribed by RNA polymerase II with comparable features to messenger RNAs (mRNAs) [REF]. Transcribed or spliced RNAs from lncRNAs can alter the activity of several genes at multiple levels such as transcription, translation, and protein modification [REF]. Following the progression of CRC, oncogenic lncRNAs can stimulate tumor activity, while other lncRNAs with a role as tumor suppressors inhibit tumor activity [REF]. Data suggested that overexpression of certain lncRNAs is allied with poor prognosis and metastatic behavior in CRC patients [REF]. To this end, therapies targeting lncRNAs may be potential approaches in CRC patients [REF]. RNA sequencing data from the TCGA dataset have indicated that about 200 lncRNAs expressed differentially in CRC patients [REF]. In particular, mTOR is a potential target that is affected by lncRNAs in CRC cases. Whether and how lncRNAs influence mTOR has been the subject of the area. In this systematic review, mTOR-associated lncRNAs were highlighted using literature database potentials to find possible correlations between lncRNA expression and mTOR regulation in CRC cases."
    },
    "2023-37280602_1106_en.txt": {
        "title": "Defining indicators for the scoping stage of health impact assessment to evaluate tobacco control policy in the city of Beijing",
        "prompt": "Abstract: Beijing initiated the nation’s most comprehensive tobacco control program that adheres to the WHO Framework Convention on Tobacco Control. This study aimed to identify a set of indicators for the scoping of an Health Impact Assessment (HIA) to assess this policy.This study used a modified Delphi process. It proposed a tobacco control health impact framework based on the Driving forces- Pressure- State- Exposure- Effect- Action model and the Determinants of Health Theory. After a review of current surveillance system and literature, a working group of 13 experts with multidisciplinary background was established to formulate indicator evaluation criteria and conduct indicator scoring. Each indicator was scored by experts according to four evaluation criteria chosen. Indicators that obtained a total score above 80% and with standard error less than 5 were selected as the final set of indicators. Kendall’s coefficient of concordance was calculated.Twenty-three out of 36 indicators were selected. Smoking prevalence, mortality rate, hospital admission rate, tobacco consumption and hospital admission fees of smoking related diseases achieved more than 90% of total scores and ranked as the top five. Kendall’s concordance coefficient was 0.218 for all indicators. For all model composition, Kendall’s concordance coefficients were statistically significant.This study identified a set of twenty-three indicators for scoping of HIA of a comprehensive tobacco control policy in Beijing based on a tobacco control health impact conceptual framework. The set of indicators achieved high scores and statistically significant consistency and has great potential to promote the evaluation of tobacco control policy in a global city. Further study might use the set of indicators for HIA on tobacco control policy to analyze empirical data.\n\n1. Introduction\n",
        "text": "Beijing initiated the nation’s most comprehensive tobacco control program that adheres to the WHO Framework Convention on Tobacco Control. This study aimed to identify a set of indicators for the scoping of an Health Impact Assessment (HIA) to assess this policy.China is the world’s biggest tobacco consumer, with an estimated consumption that exceeds that of the 39 countries combined [REF]. In 2018, there were more than 300 million smokers in China, with a smoking prevalence of 50.5% for men and 2.1% for women [REF]. According to estimates, 68.1% of the surveyed of nonsmokers in China experience SHS at least once each day [REF]. In China, smoking contributed to roughly 2 million fatalities in 2017 [REF]. China signed the Framework Convention on Tobacco Control (FCTC) of the World Health Organization (WHO) in 2005, and it went into effect in 2006. However, it has taken a little while to fulfill the responsibilities [REF]. Due to China’s tobacco monopoly, the FCTC has made only modest national progress [REF], and there is no smoking ban in public places [REF].As seen by the 21 cities in China that have passed smoke-free regulations in recent years, there is a rising bottom-up approach to tobacco control legislation in that country [REF], including Beijing. For the smoke-free Olympics, Beijing implemented a smoking ban in 11 different types of public spaces in 2008 [REF]. Beijing residents’ smoking rate has fallen by 1.5% as a result of the regulation [REF] however, the smoking phenomenon quickly returned to its previous status after the Olympics [REF]. Beijing implemented the Beijing Municipal Tobacco Control Regulation in June 2015, seven years later [REF]. The National Tobacco Tax Reform was started in May 2015 almost simultaneously [REF], which, by the middle of 2015, has created the Beijing comprehensive tobacco control policy. Six MPOWER measures, including Monitoring tobacco use, Protect people from tobacco smoking, Offer help to quit tobacco, Warning about the dangers of tobacco, Enforcing tobacco advertising, promotion & sponsorship, Raising taxes on tobacco, as recommended by WHO, make up Beijing’s comprehensive tobacco control policy. These include a ban on smoking in all indoor and four outdoor public places, a higher tobacco tax, a ban on tobacco advertising, promotion, and sponsorship, the establishment of a cessation support system, and a media campaign [REF]. In 2015, this was the nation’s most successful tobacco control strategy that followed the WHO standard.[REF]. The introduction of the 2015 policy have decreased the amount of cigarettes sold per person between 2015 and 2017 by 1388.2 sticks [REF]. From 2014 to 2017, there was a further decline in smoking prevalence and SHS exposure, with decreases of 20.3% and 25.6%, respectively [REF]. After the adoption of smoke control measures, 18,137 (26.7%) hospital admissions for stroke decreased [REF] and 5581 (17.5%) hospital admissions for chronic obstructive pulmonary disorders [REF] were probably avoided in Beijing for 25 months.Health Impact Assessment (HIA) is a combination of procedures, methods and tools by which a policy, program or project may be judged as to its potential effects on the health of a population, and the distribution of those effects within the population [REF]. HIA has been used extensively to assess the health impacts of major national policies and inform the policy making process [REF]. Basic steps for carrying out an HIA include screening, scoping, appraisal, reporting, and monitoring. Screening means making a quick mapping of whether there are potential linkages between the policy, program or project and health, and what different aspects of health they might affect. Scoping is intended to identify how the HIA will be carried out and to set the boundaries for the assessment. Appraisal means rapid or in-depth assessment of health impacts using available evidence – who will be affected, what is the baseline, what is the prediction, significance and mitigation. Reporting contains conclusions and recommendations to remove or mitigate negative impacts on health or to enhance positive impacts. Monitoring is where appropriate, to monitor actual impacts on health to enhance existing evidence. The existing scoping approach lace a clear, systematic method of selection of indicators in HIA [REF].Costa et al. applied the HIA methodology on the Portuguese law on Smoking Prevention and Tobacco Control including indicators: rates of total and premature tobacco associated mortality, standardized mortality rate for all tobacco related diseases, the number of hospital admissions for ischaemic heart disease and cerebrovascular disease, the number of patients diagnosed with chronic obstructive pulmonary disease (COPD), smoking prevalence and the total number of smoking cessation consultations [REF]. Though it measured the change of some indicators after a tobacco control policy, the selection procedure of indicators was not mentioned and whether these indicators were comprehensive remained unknown. The WHO recommended four essential indicators [REF] for measuring the effect of the tobacco control legislation on “outcomes”: mortality, tobacco consumption, smoking prevalence and tobacco control policies. But it focused on outcome evaluation without including process evaluation and only four indicators were recommended. While Beijing’s tobacco control policy employed many local measures, such as tobacco control complaint, which generated a series of process indicators, so we feel there was a need for a comprehensive set of indicators to measure the impact of tobacco control policy in Beijing based on a scientific selection procedure.This study aimed to identify a set of indicators for the scoping of an HIA to assess Beijing’s comprehensive tobacco control policy and serve as a reference for future HIAs on tobacco control policy in China and perhaps in other developing countries. Results of HIA would be summarized in further publication."
    },
    "2023-37280687_1139_en.txt": {
        "title": "Emerging role of the RNA-editing enzyme ADAR1 in stem cell fate and function",
        "prompt": "Abstract: Stem cells are critical for organism development and the maintenance of tissue homeostasis. Recent studies focusing on RNA editing have indicated how this mark controls stem cell fate and function in both normal and malignant states. RNA editing is mainly mediated by adenosine deaminase acting on RNA 1 (ADAR1). The RNA editing enzyme ADAR1 converts adenosine in a double-stranded RNA (dsRNA) substrate into inosine. ADAR1 is a multifunctional protein that regulate physiological processes including embryonic development, cell differentiation, and immune regulation, and even apply to the development of gene editing technologies. In this review, we summarize the structure and function of ADAR1 with a focus on how it can mediate distinct functions in stem cell self-renewal and differentiation. Targeting ADAR1 has emerged as a potential novel therapeutic strategy in both normal and dysregulated stem cell contexts.\n\n1. Introduction\n",
        "text": "Stem cells have self-renewal and multidirectional differentiation potential. According to the different developmental stages, they are divided into embryonic stem cells (ESCs) and adult stem cells (ASCs) [REF]. Stem cell therapy has prominent application prospects. For example, allogenic hematopoietic stem cell (HSC) transplantation can rebuild the patients’ hematopoietic and immune system, and mesenchymal stem cells (MSCs) can reduce transplantation rejection and severe COVID-19-induced lung injury [REF]. Under the accumulation of gene mutations, cell fusion, chromosomal mutations, etc., mechanisms regulating the self-renewal and differentiation potential of the stem cells are compromised on, which then drive tumor growth from the malignant transformation of stem cells. Numerous studies have shown that the pluripotency, differentiation ability and cell reprogramming of stem cells are regulated by various transcription factors, such as Oct4, Sox2, Nanog, Klf4 and c-Myc [REF]. However, recent studies have found that posttranscriptional modifications are critical for regulating different cellular processes and stem cell fate [REF]. Posttranscriptional modifications have been increasingly demonstrated to be important for both RNA biosynthesis and degradation, including N6-methyladenosine (m6A), N1-methyladenosine (m1A), inosine (I), pseudouridine (Ψ), 5-methylcytosine (m5C), 5-hydroxymethylcytosine (5-hmC), N6,2’-O-dimethyladenosine (m6Am), and 7-methylguanosine (m7G) [REF]. The modification of mRNA widely affects key biological processes, such as its folding [REF], translation [REF] and transport [REF]. In recent years, A-to-I editing, which is the most common type of RNA editing in animals, has gradually attracted attention. It is widely involved in a variety of gene regulatory mechanisms at the transcriptional and posttranscriptional levels, including alteration of sequences coding for amino acids at the transcriptome level and mRNA splicing, mRNA stability and circular RNA formation. Its dysregulation drives aberrant transcription and translation programs that promote cancer occurrence and progression [REF].As an enzyme acting on double-stranded RNA (dsRNA), the ADAR protein family was discovered in 1987 [REF]. The ADAR family has three different members: ADAR1, ADAR2 and ADAR3. ADAR1 and ADAR2 have enzymatic activities that explain the existence of RNA editing in different tissues [REF], while ADAR3 is only present in brain tissue and has no enzymatic activity, which may compete for dsRNA substrates and thus act as an RNA editing inhibitor in the brain [REF]. ADAR1, a specific adenosine deaminase, binds to dsRNA and converts adenosine (A) to inosine (I) after RNA transcription, which is known as A-to-I editing [REF]. It contains three isoforms, p150, p110 and p80 [REF]. P150 has been reported to be involved in the regulation of type I interferon signaling [REF]. The IFN-inducible p150 heterodimer of ADAR1 contains a Zα structural domain that recognizes RNA with an alternative left-handed double helix structure, termed Z-RNA. Heterozygous ADAR1 mutations in the Zα structural domain cause type I IFN-mediated disorders in humans and mice. Mutations in the ADAR1 gene are associated with Aicardi-Goutières syndrome (AGS) and Dyschromatosis symmetrica hereditaria (DSH), where AGS manifests mainly as neurological lesions associated with chronic activation of type I interferon (IFN) [REF]. Analysis of ADAR1 mutations in AGS patients showed 11 mutations, with 8 amino acid substitutions located in the catalytic domain with a significant increase in the production of interferon-α [REF]. Like AGS, DSH is characterized by exceeding 130 ADAR1 mutations. Both DSH and AGS share a common mutation, Gly1007Arg. This is also the only missense mutation that completely eliminates the editing activity of ADAR1 [REF]. Moreover, ADAR1 blocks endogenous Z-RNA-dependent activation of Z-DNA binding protein 1(ZBP1) in response to pathogenic type I IFN, suggesting that ZBP1 may be a key molecule in type I interferon disease caused by ADAR1 mutations [REF].ADAR1 has been found to affect immune cell functions, for instance, mediating early T-cell development [REF] and T-cell immune tolerance and preventing colitis [REF]. ADAR1 is involved in regulating macrophage function and maintaining the balance of DC cell subsets [REF].In addition to its classical role in triggering adaptive immunity, there has been a growing report about the effects of ADAR1 on stem cells. A study found that knockout of ADAR1 leads to lethality or premature death of mouse embryos, suggesting that RNA editing regulation has vital biological significance [REF]. ADAR1 promotes leukemia stem cell (LSC) self-renewal via let-7 pri-microRNA editing [REF] ADAR1 knockdown also reduces the self-renewal ability of blast crisis leukemia stem cell (BC-LSC) in RAG2+ γc+ mice. These data show that ADAR1 reprogram malignant progenitor cells to drive leukemia progression [REF]. By the way, ADAR1 plays an important role in the survival and maintenance of intestinal stem cells and intestinal homeostasis by inhibiting endoplasmic reticulum (ER) stress and interferon (IFN) signal transduction. ADAR1 is highly expressed in Lgr5+ cells, and its absence in adult mice leads to rapid apoptosis and loss of these active circulating stem cells in the small intestine and colon [REF]. In addition, the Wnt/β-catenin pathway can be triggered by ADAR1, thereby affecting the regulation of the proliferation of malignant hematopoietic stem cells [REF]. However, the specific mechanism of ADAR1 as a new target for the clinical treatment of stem cell-related diseases is not yet known.In this context, we will discuss the protein structure and biological function of ADAR1, and its potential role in stem cell. In addition, we describe some theoretically feasible treatment strategies for stem cells related diseases based on ADAR1 function in this review."
    },
    "2023-37283420_623_en.txt": {
        "title": "Methodologies used by Nursing professionals in the production of educational videos: An integrative review",
        "prompt": "Abstract: to evaluate the diverse scientific evidence on the methodologies used by Nursing professionals in the production of educational videos.an integrative review. The search for primary studies was carried out in the CINAHL, LILACS and MEDLINE/PubMed databases. The sample consisted of 19 research studies. The methodological quality of the studies included was assessed using a tool proposed by the Johns Hopkins Nursing Evidence-Based Practice and the results were analyzed in a descriptive form.the methodological stages used for the process to elaborate and make the videos include pre-production, production and post-production. The studies reveal that, for the most part, the stages were properly applied and/or described by the authors, in addition to contemplating the method adopted. However, in 14 studies there was no use of a methodological framework to ensure rigor in their conduction and in 11 presented validation by the target audience.the synthesis of knowledge showed that there is still a need for attention for the construction of educational videos regarding the methodological framework and validation by the target population. The rigorous execution of the methodological procedures necessary for the development of educational videos, aiming to encourage the acquisition of essential skills for the creation of high-quality teaching materials.\n\n1. Introduction\n",
        "text": "Using educational technologies to mediate health professionals’ education process regarding health interventions expands access to information, eases the teaching-learning process, promotes knowledge dissemination and causes changes in health care, through devices that favor positive perspectives in the health standards[REF].In the Nursing education context, educational videos (EVs) have been widely used as a rich, interesting and complex tool that contributes to the promotion of education[REF]. When properly developed, EVs can serve as a solid foundation to support understanding and effective reflection. However, their production requires special precautions in relation to the structuring and organization of all the information. This educational tool is used in different ways in teaching-learning environments to present motivational concepts or experiences, convey information and other applications[REF].It is also noted that access to EVs, to be made available on digital platforms, contributes to reaching the target audience regardless of their geographic location and time, and enables Nursing professionals to update on the theme developed, with the possibility of positive repercussions on the assistance provided[REF].Several methods have been used for the development of educational videos (EVs) and three stages are generally adopted, namely: pre-production (preparation and validation of a script and storyboard), production (video recording and editing) and post-production (evaluation of the video by the target audience)[REF].When properly created and structured, following the methodological steps for their elaboration, EVs can become a powerful tool for building knowledge and improving the care practice. In addition to that, due to their potential as an appealing visual communication medium, EVs have aroused great interest in users[REF].A study whose objective was to validate a video script and storyboard for an educational intervention on Nursing care aimed at syphilis prevention and management showed that the educational material produced can contribute for Nursing professionals to better understand the issues that involved the occurrence, prevention, diagnosis and treatment of syphilis[REF].In addition to that, it provided users of health services with the choice of infection prevention methods, as well as with the perception of the benefits of self-care, with regard to changing behaviors and to safe sexual practices[REF]. Other studies[REF] showed that using videos contributed to advancing knowledge in Nursing and to acquiring practical skills by Nursing students and professionals, in addition to methodologically supporting the development of other EVs in the health area. In this context, studies that develop and validate EVs on different Nursing procedures are relevant both for education and for health care. This is because they allow Nursing professionals to incorporate validated educational technologies, such as videos, in order to promote permanent and continuing education[REF].Considering the methodological rigor required for the construction and elaboration of EVs, in some cases, the scientific production related to the theme has shown failures in fulfilling the methodological steps, which exerts negative impacts on the quality of the materials produced. In this sense, this review is justified by the contribution to Nursing professionals’ knowledge, through the presentation of diverse information about the methodologies used in the production of EVs. This approach aims at favoring the creation of safe and good quality educational materials, promoting the dissemination of scientific knowledge in the Nursing area.Therefore, the objective of this study is to evaluate the diverse scientific evidence on the methodologies used by Nursing professionals in the production of educational videos."
    },
    "2023-37283586_945_en.txt": {
        "title": "The role of noncoding genetic variants in cardiomyopathy",
        "prompt": "Abstract: Cardiomyopathies remain one of the leading causes of morbidity and mortality worldwide. Environmental risk factors and genetic predisposition account for most cardiomyopathy cases. As with all complex diseases, there are significant challenges in the interpretation of the molecular mechanisms underlying cardiomyopathy-associated genetic variants. Given the technical improvements and reduced costs of DNA sequence technologies, an increasing number of patients are now undergoing genetic testing, resulting in a continuously expanding list of novel mutations. However, many patients carry noncoding genetic variants, and although emerging evidence supports their contribution to cardiac disease, their role in cardiomyopathies remains largely understudied. In this review, we summarize published studies reporting on the association of different types of noncoding variants with various types of cardiomyopathies. We focus on variants within transcriptional enhancers, promoters, intronic sites, and untranslated regions that are likely associated with cardiac disease. Given the broad nature of this topic, we provide an overview of studies that are relatively recent and have sufficient evidence to support a significant degree of causality. We believe that more research with additional validation of noncoding genetic variants will provide further mechanistic insights on the development of cardiac disease, and noncoding variants will be increasingly incorporated in future genetic screening tests.\n\n1. Introduction\n",
        "text": "Cardiomyopathies are disorders of the myocardium caused by genetic and environmental factors that eventually result in impaired cardiac function and heart failure [REF]. Depending on the specific effects in the function and morphology of the heart, and the isolated presence of arrhythmias, cardiomyopathies are divided into dilated, hypertrophic, restrictive, and arrhythmogenic [REF]. Dilated cardiomyopathy (DCM) is the most common cardiomyopathy affecting 1 in 250 individuals, followed by hypertrophic cardiomyopathy (HCM), which affects 1 in 500, and arrhythmogenic cardiomyopathy (ACM) encountered 1 in 5,000, while the prevalence of restrictive cardiomyopathy is even less common [REF]. About 30%–50% of cardiomyopathies are heritable, and the different types can have variable phenotypes, prognosis and causal mutations [REF]. HCM is primarily a disease of the sarcomere, as in up to 60% of patients, a pathogenic or likely pathogenic variant is detected in sarcomeric genes [REF]. Beta-myosin heavy chain (MYH7) and myosin binding protein C3 (MYBPC3) are the most frequently affected genes, encoding for proteins of the thick sarcomeric filaments, and patients tend to exhibit disease onset in their forties. Other commonly affected genes in the thin filaments of the sarcomere are cardiac Troponin I (TNNI3) and cardiac Troponin T (TNNT2) [REF]. In contrast to HCM, the causative genes in DCM are functionally diverse. Titin (TTN) mutations represent 12%–25% of DCM patients and Lamin (LMNA) genetic variants represent the second most common mutations in DCM patients [REF]. Other genes that are associated with DCM are MYH7, TNNT2, Tropomyosin 1 (TPM1), Desmoplakin (DSP), RNA binding motif protein 20 (RBM20), and sodium voltage-gated channel alpha subunit 5 (SCN5A) [REF]. In arrhythmogenic cardiomyopathy (ACM), most pathogenic variants are in genes encoding desmosomal proteins such as Plakoglobin (JUP) [REF], DSP [REF], Plakophilin-2 (PKP2), Desmoglein-2 (DSG2) and Desmocollin-2 (DSC2) [REF]. Finally, inherited restrictive cardiomyopathies are caused by mutations in sarcomeric genes such as cardiac troponin I, and less commonly by mutations in Desmin (DES) and Filamin C (FLNC) [REF]. It is worth noting that although cardiomyopathies are classified based on phenotypes manifested in the general population, the pathogenic mechanisms and phenotypic features among the various types of cardiomyopathies can overlap to a significant degree.With the advancement of next generation sequencing and genome wide association studies (GWAS), our understanding of the genetic basis of cardiomyopathies has significantly improved. Multiple GWAS have identified susceptibility loci and variants associated with different types of cardiomyopathies [REF]. Most rare disease causal variants have been found within the coding region of the genome [REF]. For example, TTN coding variants usually lead to gene truncations and are viewed as the leading genetic causes in DCM patients [REF]. Contrarily, MYBPC3 truncating and MYH7 missense variants are the most pathogenic HCM mutations detected in next generation sequencing research studies [REF]. Although definitive causative genetic mutations have been identified for familial cardiomyopathies, in over half of the cases targeted genetic screening tests do not identify a contributing variant. This is because most of the current clinical genetic screening tests and earlier research studies relied heavily on whole exome sequencing (WES) or targeted sequencing of coding regions [REF]. Another explanation regarding the lack of focus in noncoding variants is that even in large meta-analyses, the power of variant detection is limited by variant frequency and penetrance, and lack of systemic interpretation. However, recent evidence from whole genome sequencing (WGS) supports a strong association between genetic variants within noncoding regions and cardiomyopathies [REF]. Also, emerging evidence corroborates the role of noncoding regulatory regions, where disruption of transcription factor binding sites within enhancers or promoters can alter the 3D chromatin structure and reduce target gene expression, which can be critical for disease [REF]. Similarly, based on other studies variants within intronic or untranslated regions (UTRs) could also be involved in the pathogenesis of cardiomyopathies [REF]. Furthermore, according to ClinVar, a significant percentage of non-coding variants in splice sites (∼60%) and UTRs (∼5%), are classified as pathogenic or likely pathogenic (www.ncbi.nlm.nih.gov/clinvar). In this review we will provide an overview of the role of noncoding genetic variants and their association with cardiomyopathies. We will specifically focus on variants within promoter, enhancer, untranslated, splice and intronic regions (Figure 1), where there is sufficient evidence to support a strong association with cardiac disease.Schematic illustration of noncoding parts in the eukaryotic genome: enhancer, promoter, 5′ UTR, intron, and 3′ UTR."
    },
    "2023-37283751_772_en.txt": {
        "title": "Soluble biomarkers to predict clinical outcomes in non-small cell lung cancer treated by immune checkpoints inhibitors",
        "prompt": "Abstract: Lung cancer remains the first cause of cancer-related death despite many therapeutic innovations, including immune checkpoint inhibitors (ICI). ICI are now well used in daily practice at late metastatic stages and locally advanced stages after a chemo-radiation. ICI are also emerging in the peri-operative context. However, all patients do not benefit from ICI and even suffer from additional immune side effects. A current challenge remains to identify patients eligible for ICI and benefiting from these drugs. Currently, the prediction of ICI response is only supported by Programmed death-ligand 1 (PD-L1) tumor expression with perfectible results and limitations inherent to tumor-biopsy specimen analysis. Here, we reviewed alternative markers based on liquid biopsy and focused on the most promising biomarkers to modify clinical practice, including non-tumoral blood cell count such as absolute neutrophil counts, platelet to lymphocyte ratio, neutrophil to lymphocyte ratio, and derived neutrophil to lymphocyte ratio. We also discussed soluble-derived immune checkpoint-related products such as sPD-L1, circulating tumor cells (detection, count, and marker expression), and circulating tumor DNA-related products. Finally, we explored perspectives for liquid biopsies in the immune landscape and discussed how they could be implemented into lung cancer management with a potential biological–driven decision.\n\n1. Introduction\n",
        "text": "Lung cancer represents the first cause of cancer‐related deaths worldwide with over 1.5 million deaths in 2018 and an incidence superior to 2 million (11.6%), largely represented by non-small cell lung cancer (NSCLC) [REF]. Lung cancer is diagnosed at a locally advanced or metastatic stage in most cases, leading to no curative options and poor outcomes [REF]. In recent decades, many innovative strategies have been designed, namely tyrosine kinase inhibitors (TKIs) targeting oncogenic drivers or immunotherapies [REF]. On the one hand, personalized medicine based on molecular targetable alterations has emerged from proof of concept to current clinical applications with restricted indications to a sub-population [REF]. On the other hand, immune checkpoint inhibitors (ICI) are now largely employed but obtain various response rates with fewer than 40% of responders among a population selected on programmed death-ligand 1 (PD-L1) expression [REF].Many biomarkers have been investigated through the last decades to improve clinical cancer management and patient outcomes. First, biomarkers designed to predict better, and longer responses have been proposed, such as PD-L1. PD-L1 expression in tumor biopsy is the strategy that allows identifying a subpopulation of patients benefiting from ICI. For example, patients with a high PD-L1 tumor proportion score (TPS ≥ 50%) benefit from ICI in first-line (vs platinum-based chemotherapy) [REF]. However, resistance and relapse fatally occur in most cases. Consequently, global age-standardized 5-year survival remains within the range of 10-20% and a limited increase of up to 5% has been observed [REF], arguing the need to further refine and improve clinical lung cancer management. Therefore, other approaches have been explored in plasma or total blood. Soluble biomarkers have the advantages to allow real-time monitoring, repeatable, and easily feasible at every step of lung cancer (from the diagnosis throughout the progression of the disease) including non-evaluable radiographic diseases, named biological minimal residual diseases (MRD) [REF]. Liquid biopsy is now even integrated into clinical practice to research and/or monitor oncogenic addiction under TKI treatment [REF]. Circulating tumor-derived products are various and offer wide potential applications, especially in the ICI field [REF]. Inflammation-related biomarkers are particularly promising since inflammation is associated with a worse prognosis in solid tumors due to its effect on the immune modulation, into both tumor cells and its microenvironment, influencing disease-related outcomes [REF]. These biomarkers include immunoregulatory cells, soluble mediators, and a panel of features including absolute neutrophil, eosinophil, lymphocyte counts, or ratios [REF]. To date, no soluble biomarker has yet been approved and validated for the management of lung cancer patients, despite important recent technical advances. In this context, there is an emerging interest to identify one to predict ICI benefit, overcoming limitations due to tissue-based analysis [REF].Numerous serum-based biomarkers have already been explored or are currently under investigation. Among the most promising, the blood cell count of neutrophils, lymphocytes, and platelets have been associated with ICI efficacy with potential prognostic value [REF]. Other promising serum-based biomarkers include soluble PD-L1 (sPD-L1) [REF], circulating tumor cells (CTCs) [REF], blood tumor mutational burden (bTMB), or circulating tumor DNA (ctDNA) [REF]."
    },
    "2023-37283899_1232_en.txt": {
        "title": "Phylogeography of sharks and rays: a global review based on life history traits and biogeographic partitions",
        "prompt": "Abstract: Considerable research exists on the life history traits, evolutionary history, and environmental factors that shape the population genetic structure of marine organisms, including sharks and rays. Conservation concerns are particularly strong for this group as they are highly susceptible to anthropogenic stressors due to a combination of life history traits including late maturity and low fecundity. Here, we provide a review and synthesis of the global phylogeography of sharks and rays. We examined existing data for 40 species of sharks belonging to 17 genera and 19 species of rays belonging to 11 genera. Median joining haplotype networks were constructed for each species for the mtDNA cytochrome C oxidase subunit I (COI), and an Analysis of Molecular Variance (AMOVA) was conducted to understand patterns of genetic diversity and structure across the three major ocean basins—the Indian, Atlantic and Pacific Oceans. Haplotype networks showed very shallow coalescence in most species, a finding previously reported for marine teleosts. Star topologies were predominant among sharks while complex mutational topologies predominated among rays, a finding we attribute to extremely limited dispersal in the early life history of rays. Population structuring varied amongst species groups, apparently due to differences in life history traits including reproductive philopatry, site fidelity, pelagic habitat, migratory habits, and dispersal ability. In comparison to reef-associated and demersal species, pelagic and semi pelagic species showed lower levels of structure between and within ocean basins. As expected, there is variation between taxa and groups, but there are also some broad patterns that can guide management and conservation strategies.\n\n1. Introduction\n",
        "text": "Many marine organisms are characterized by very large distribution ranges, a finding often attributed to the lack of physical barriers. Allen (2008) estimated that the average range of a teleost (bony) reef fish in the Indo–Pacific is 9 million km2, roughly the size of China, compared to 350,000 km2 for a typical freshwater fish range [REF]. The spatial scales of population structure and dispersal in marine ecosystems are also much larger than in terrestrial and freshwater environments [REF]. Long-distance colonisation and range expansion, both facilitated and constrained by oceanographic and geographic processes, have shaped the distribution and genetic architecture of marine fishes [REF].The patterns of genetic variation within species are linked to geographical processes that give rise to sub-divided populations, as indicated by quantifiable factors such as genetic connectivity and demography. The study of population connectivity is especially pertinent to management practices for commercial exploitation and conservation [REF]. Genetic diversity is also an important axis for species assessment in a conservation context, especially for wide-ranging marine species like sharks [REF].Sharks and rays play a crucial role in the sea by maintaining coastal and oceanic ecosystem structure and function. Large sharks function as top predators while smaller sharks are mesopredators and prey of larger sharks and other oceanic predators [REF]. Unlike teleosts, most elasmobranchs (sharks, rays, and skates) show late sexual maturity, long gestation period, low fecundity, slow growth rate, high level of maternal investment and long-life spans [REF]. These extreme life histories result in elasmobranchs being among the slowest reproducing vertebrates in the ocean and make their populations extremely vulnerable to anthropogenic pressures such as overfishing, habitat modification, pollution and climate change [REF]. The primary cause of declining shark and ray populations is overfishing, as harvest rates exceed their capacity to replenish, and their life history traits render them vulnerable to rapid declines [REF]. More than half of this fishing mortality is due to bycatch [REF]. The expansion of the shark and ray fishing industry is an outcome of declining commercial fish populations (teleosts) and/or stringent restrictions on their capture [REF].Sharks are found in coastal, demersal, and pelagic habitats that are largely limited to continental shelves, although there are a few completely oceanic species like Carcharhinus longimanus (oceanic whitetip shark). Several species in the family Sphyrnidae (hammerheads), Carcharhinus falciformis (silky shark), Galeocerdo cuvier (tiger shark), and Carcharodon carcharias (white shark) migrate between coastal and oceanic waters [REF]. Rays are mostly marine except for a few species in the family Dasyatidae capable of living in low salinity habitats, and members of Potamotrygonidae completely adapted to a life cycle in freshwater [REF]. Like sharks, they occupy a variety of niches with pelagic rays capable of undertaking long migrations [REF]. However, population subdivisions may be more common in rays because of limited dispersal and greater susceptibility to geographical impediments [REF]. Sharks that inhabit coastal waters aggregate for mating and parturition at specific discrete locations which provide protection for juveniles [REF]. The extent of population subdivision and genetic divergence between populations in different geographic regions is directly influenced by such segregation and philopatry [REF] and the dispersal ability of individuals.For example, philopatry to natal sites in blacktip reef sharks (Carcharhinus melanopterus) is a major contributor to genetic structuring within the Indo-Pacific and between islands in French Polynesia, by reducing dispersal [REF]. Bull shark juveniles from nurseries in the Gulf of Mexico and Atlantic showed significant genetic variation in the mitochondrial control region (mtDNA-CR) but were homogenous with nuclear microsatellites indicating male biased dispersal [REF]. Similarly, population structure (using mtNADH sequences) among juvenile bull sharks from 13 nurseries located in rivers around Northern Australia also indicated a strong influence of female reproductive philopatry [REF]. These observed genetic differences support philopatry and indicate a strong role in shaping population separations in bull sharks [REF]. Site fidelity and long-term residency also resulted in fine-scale genetic structuring within reef manta rays (Mobula alfredi) in New Caledonia [REF].Understanding elasmobranch biology and life history is therefore important for evolving species-specific management plans. Their governance poses a challenge as many species of sharks and rays migrate across national boundaries and international waters, and there is little knowledge/information about the migratory habits of transboundary species in international waters [REF]. Presently, conservation measures for sharks and rays are influenced by political boundaries, oceanic expanses, and centres of high demand [REF]. Conservation efforts are under-resourced due to lack of adequate funds, technical capacity and political will to efficiently monitor, control and manage elasmobranch fisheries/trade [REF].Hence, we examined patterns of phylogeography and population structure within and across multiple families of sharks (Carcharhinidae, Cetorhinidae, Hemiscyllium, Odontaspididae, Stegostomatidae, Alopiidae, Rhincodontidae, Sphyrnidae and Lamnidae) and rays (Dasyatidae, Mobulidae, Myliobatidae and Gymnuridae) in relation to their habitat and life history. We explored these patterns by (a) compiling data from a variety of published and unpublished sources, (b) constructing haplotype networks and examining network topology, (c) estimating nucleotide and haplotype diversity, and (d) assessing population genetic structure using AMOVA. In a few species, we report data from a single study, but for many species, our meta-analysis combines data from multiple sources, both published and unpublished, and provides insights from comparisons across genera, and between sharks and rays."
    },
    "2023-37284058_1057_en.txt": {
        "title": "Neuroprotection for Nonarteritic Central Retinal Artery Occlusion: Lessons from Acute Ischemic Stroke",
        "prompt": "Abstract: Nonarteritic central retinal artery occlusion (NA-CRAO) is a variant of acute ischemic stroke (AIS) and is a cause of sudden severe loss of vision. There are guidelines by the American Heart Association and the American Stroke Association for the care of CRAO patients. This review explores the basis of retinal neuroprotection for CRAO and its potential for improving the outcome of NA-CRAO. Recently, there have been significant advances in research into the use of neuroprotection to treat retinal diseases, including retinal detachment, age-related macular degeneration, and inherited retinal diseases. Also, neuroprotective research in AIS has been extensive, and newer drugs tested, including Uric acid, Nerinetide, and Otaplimastat, with promising results. Progress in cerebral neuroprotection after AIS offers hope for retinal neuroprotection after CRAO; and a possibility of extrapolating research findings from AIS into CRAO. Combining neuroprotection and thrombolysis can extend the therapeutic window for NA-CRAO treatment and potentially improve outcomes. Experimented neuroprotection for CRAO includes Angiopoietin (Comp Ang1), KUS 121, Gene therapy (XIAP), and hypothermia. Efforts in the field of neuroprotection for NA-CRAO should focus on better imaging to delineate the penumbra after an acute episode of NA-CRAO (using a combination of high-definition optical coherence angiography and electrophysiology). Also, research should explore details of pathophysiologic mechanisms involved in NA-CRAO, allowing for further neuroprotective intervention, and closing the gap between preclinical and clinical neuroprotection.\n\n1. Introduction\n",
        "text": "In recent years, significant resources have been invested in the research into treating retinovascular diseases, including diabetic retinopathy (DR), retinal vein occlusion (RVO), and macular diseases including age-related macular degeneration (AMD). This effort has led to remarkably improved outcomes for patients suffering from these diseases. This experience has not been replicated for retinal artery occlusion (RAO). The reason for this is multifactorial and includes fewer RAO patients relative to the other retinovascular diseases and the fact that several central retinal artery occlusion (CRAO) patients present for treatment after the theoretical time limit for retinal neuron survival [REF]. Unlike DR, RVO, and AMD, treatment for CRAO must be instituted within a specified time when neurons are still salvageable [REF].CRAO is an ocular variant of acute ischemic stroke (AIS); they share similar predispositions and pathogenetic mechanisms [REF]. CRAO eyes have been classified into four categories based on etiology, permanency of occlusion of the central retinal artery and preexisting collateral supply (cilioretinal artery). The four known types of CRAO are non-arteritic (NA) CRAO, NA-CRAO with cilioretinal artery sparing, transient NA-CRAO, and arteritic CRAO (occurring secondary to giant cell arteritis-GCA). Hayreh has shown that the natural history of these four types of CRAO defers significantly in terms of spontaneous improvements in vision and visual fields after an acute episode. Upto 82% of transient NA-CRAO and 67% of NA-CRAO with cilioretinal sparing will show spontaneous improvements in vision [REF]. The NA-CRAO variant is more commonly encountered in patients [REF]. The pathogenesis of NA-CRAO is thromboembolic. Emboli may occur at the narrowest portion of the central retinal artery (CRA), which is the point at which it penetrates the dura [REF], or thrombi may form within the region of the lamina cribrosa, usually posterior to it [REF]. Vascular occlusion in CRAO can be partial or complete. CRAO can also be permanent or transient, depending on the nature and mobility of the vessel occluding emboli [REF]. Patients who suffer transient CRAO are at increased risk of more permanent NA-CRAO and other ischemic diseases and should be evaluated carefully. In a more significant proportion of patients, when CRAO occurs, it can cause profound and irreversible loss of vision. The reported visual outcome of CRAO using the so-called conservative treatment in most cases is similar to or worse than natural history and unsatisfactory [REF]. Seventeen percent of CRAO patients regain functional vision. In 80% of patients, vision is counting fingers or worse, while in about 50% of patients, only a tiny peripheral island of vision remains [REF]. Neovascularization of the iris may occur in up to 18.2% of cases [REF]. Treatment principles for CRAO are similar to that for AIS and include 1. Acute reperfusion of the CRA, 2. Prevention of ocular complications, and 3. Vascular review to prevent further end-organ ischemia.There are guidelines by the American Heart Association (AHA) and the American Stroke Association (ASA) which guide the management of patients with acute stroke [REF]. Also, the Retinal and Ophthalmic Artery Occlusions Preferred Practice Pattern® of the American Academy of Ophthalmology (AAO) gives detailed instruction on the holistic care of a patient who suffers a CRAO [REF]. In addition, local intra-arterial thrombolysis is used by several departments worldwide for the treatment of NA-CRAO. This practice continues despite the finding that intra-arterial thrombolysis was similar in efficacy to conservative therapy for treating NA-CRAO and resulted in a higher rate of adverse events, in the EAGLE study [REF]. However, significant criticisms and short comings have trailed the EAGLE study design, including that the average time from symptom onset to intra-arterial therapy was >12 hours in the treatment group. In addition, none of the patients received intra-arterial therapy within 4.5 hours of symptom onset [REF].This review explores neuroprotection as an additional strategy alongside already-known strategies for the care of NA-CRAO. The role of neuroprotection is often overlooked in ocular stroke. Instead, revascularization techniques are given the spotlight. This manuscript seeks to redirect our attention to neuroprotection which is an important element of ocular stroke management. The need for a different approach arises from the unfortunate fact that outcomes after CRAO over the past three decades have remained the same. `The lack of randomized controlled studies providing guidance or recommendations for treatment makes the situation worse. To fulfil the need for improved outcomes after NA-CRAO, it should be mentioned that there are two ongoing clinical trials investigating the use of intravenous alteplase. These are a Phase 2 trial (Early Reperfusion Therapy With Intravenous Alteplase for Recovery of VISION in Acute Central Retinal Artery Occlusion (REVISION),) [NCT04965038] [REF], and a Phase 3 trial (Multicenter Study Assessing the Efficacy and Safety of IV THrombolysis (Alteplase) in Patients With acutE Central retInal Artery Occlusion (THEIA)) [NCT03197194] [REF]. These two studies investigate the role of treatment < 4.5 hours after symptom onset in NA-CRAO patients. Similarly, there is a phase 3 study, TENecteplase in Central Retinal Artery Occlusion Study (TenCRAOS) [NCT04526951] [REF], investigating the role of intravenous Tenecteplase for CRAO. These studies hopefully will provide some information and fill the knowledge gap that currently exist on the use of intravenous thrombolysis for treating NA-CRAO. In several cases of CRAO, reperfusion of the retina does occur without significant improvement in vision [REF]. The lack of improvement in vision in CRAO is multifactorial, as elucidated by Jusufovic et al, and a significant contribution is due to the issue of retinal survival time (RST) [REF]. Real-world experience suggests that a substantial proportion of CRAO patients eventually seek care after exceeding the RST."
    },
    "2023-37284204_715_en.txt": {
        "title": "Laparoscopic versus ultrasound-guided transversus abdominis plane block for postoperative pain management in minimally invasive colorectal surgery: a meta-analysis protocol",
        "prompt": "Abstract: Transversus abdominis plane block (TAPB) is now commonly administered for postoperative pain control and reduced opioid consumption in patients undergoing major colorectal surgeries, such as colorectal cancer, diverticular disease, and inflammatory bowel disease resection. However, there remain several controversies about the effectiveness and safety of laparoscopic TAPB compared to ultrasound-guided TAPB. Therefore, the aim of this study is to integrate both direct and indirect comparisons to identify a more effective and safer TAPB approach.Systematic electronic literature surveillance will be performed in the PubMed, Embase, Cochrane Central Register of Controlled Trials (CENTRAL), and ClinicalTrials.gov databases for eligible studies through July 31, 2023. The Cochrane Risk of Bias version 2 (RoB 2) and Risk of Bias in Non-randomized Studies of Interventions (ROBINS-I) tools will be applied to scrutinize the methodological quality of the selected studies. The primary outcomes will include (1) opioid consumption at 24 hours postoperatively and (2) pain scores at 24 hours postoperatively both at rest and at coughing and movement according to the numerical rating scale (NRS). Additionally, the probability of TAPB-related adverse events, overall postoperative 30-day complications, postoperative 30-day ileus, postoperative 30-day surgical site infection, postoperative 7-day nausea and vomiting, and length of stay will be analyzed as secondary outcome measures. The findings will be assessed for robustness through subgroup analyses and sensitivity analyses. Data analyses will be performed using RevMan 5.4.1 and Stata 17.0. P value of less than 0.05 will be defined as statistically significant. The certainty of evidence will be examined via the Grading of Recommendations, Assessment, Development, and Evaluation (GRADE) working group approach.Owing to the nature of the secondary analysis of existing data, no ethical approval will be required. Our meta-analysis will summarize all the available evidence for the effectiveness and safety of TAPB approaches for minimally invasive colorectal surgery. High-quality peer-reviewed publications and presentations at international conferences will facilitate disseminating the results of this study, which are expected to inform future clinical trials and help anesthesiologists and surgeons determine the optimal tailored clinical practice for perioperative pain management.\n\n1. Introduction\n",
        "text": "Transversus abdominis plane block (TAPB) is now commonly administered for postoperative pain control and reduced opioid consumption in patients undergoing major colorectal surgeries, such as colorectal cancer, diverticular disease, and inflammatory bowel disease resection. However, there remain several controversies about the effectiveness and safety of laparoscopic TAPB compared to ultrasound-guided TAPB. Therefore, the aim of this study is to integrate both direct and indirect comparisons to identify a more effective and safer TAPB approach.Recently, minimally invasive surgery (MIS) has been recommended to treat colorectal diseases, such as colorectal cancer, inflammatory bowel disease, and diverticular disease, because of its equivalent efficacy and improved functional recovery [REF]. Compared to traditional protocols, enhanced recovery after surgery (ERAS) pathways could significantly shorten the length of hospital stay (LOS) and reduce the healthcare costs without compromising surgical outcomes [REF]. Acute postoperative pain, however, remains the most common concern of ERAS. Meanwhile, regular administration of opioids is associated with postoperative ileus (POI), postoperative nausea and vomiting (PONV), delayed mobilization, acute urinary retention, and early-term somnolence and delirium [REF]. Despite the increasing popularity of the conception of opioid-sparing multimodal analgesia, consensus on optimal pain management after MIS is lacking.Transversus abdominis plane block (TAPB) as a type of local anesthesia involves the injection of a local anesthetic between the transversus abdominis and internal oblique muscles to infiltrate the segmental nerves at the level of T8-L1 [REF]. TAPB is now commonly employed during laparoscopic colorectal surgery and has proven to be effective in reducing postoperative opioid consumption [REF]. In the ERAS Society Guidelines for Perioperative Care in Elective Colorectal Surgery 2018, the use of TAPB is strongly recommended instead of epidural analgesia in colorectal MIS [REF]. Both laparoscopic (Lap-) and ultrasound-guided (US-) TAPB have allowed to reduce the risk of peritoneal penetration and facilitate the accurate identification of the tissue plane [REF].Currently, training in the use of ultrasonography among anesthesiologists is commonplace in tertiary referral centers [REF]. Characterized by ability to perform dynamic maneuvers and assess long segments of nerves, lack of radiation and contraindications, and portability, ultrasonography is recognized as one of the optimal imaging modalities for peripheral nerves [REF], which contributes to its widespread application in perioperative nerve blocks. Conversely, due to additional human, time, and economic costs, techniques of ultrasound-guided nerve blocks might not be available in the primary hospitals. Furthermore, despite the guidance of ultrasound, procedure-related inadvertent visceral injury still should not be ignored [REF]. Lap-TAPB can seemly be a potential alternative to reduce the waste of healthcare resource. Though visualization of laparoscopy minimizes intraperitoneal injection and visceral injury originated from peritoneal penetration, the precise positioning of the nerves and planes can be compromised by Lap-TAPB compared to US-TAPB [REF].The existing systematic reviews generally aimed to assess the differences between TAPB and no-TAPB locoregional analgesia or placebo control in colorectal surgery [REF]. Focusing on not all colorectal MIS but only laparoscopic colorectal surgery, a recently published meta-analysis cannot provide a convincing conclusion owing to the small sample size (3 studies, 219 patients) [REF]. Above all, there remain several controversies about the effectiveness and safety of Lap-TAPB compared with US-TAPB, and high-quality evidence is needed to guide individualized clinical practice [REF]. We hypothesize that surgeon-performed Lap-TAPB would be non-inferior to anesthesiologist-delivered US-TAPB. To verify this, we conduct the present meta-analysis to compare the effectiveness and safety of the two specific TAPB approaches for postoperative analgesia in colorectal MIS."
    },
    "2023-37284240_823_en.txt": {
        "title": "Technological advances for analyzing the content of organ-on-a-chip by mass spectrometry",
        "prompt": "Abstract: Three-dimensional (3D) cell cultures, including organ-on-a-chip (OOC) devices, offer the possibility to mimic human physiology conditions better than 2D models. The organ-on-a-chip devices have a wide range of applications, including mechanical studies, functional validation, and toxicology investigations. Despite many advances in this field, the major challenge with the use of organ-on-a-chips relies on the lack of online analysis methods preventing the real-time observation of cultured cells. Mass spectrometry is a promising analytical technique for real-time analysis of cell excretes from organ-on-a-chip models. This is due to its high sensitivity, selectivity, and ability to tentatively identify a large variety of unknown compounds, ranging from metabolites, lipids, and peptides to proteins. However, the hyphenation of organ-on-a-chip with MS is largely hampered by the nature of the media used, and the presence of nonvolatile buffers. This in turn stalls the straightforward and online connection of organ-on-a-chip outlet to MS. To overcome this challenge, multiple advances have been made to pre-treat samples right after organ-on-a-chip and just before MS. In this review, we summarised these technological advances and exhaustively evaluated their benefits and shortcomings for successful hyphenation of organ-on-a-chip with MS.\n\n1. Introduction\n",
        "text": "The use of three-dimensional (3D) cell cultures has been growing due to their widespread application ranging from studying drug efficacy and toxicity to creating disease models (Zheng et al., 2016). By using a 3D-culture model, one can mimic the in vivo environment of human physiology more accurately than two-dimensional (2D) models, such as standard cell culture [REF]. Therefore, the quality of the conducted experiments improves. The progress of 3D devices and technologies has advanced to the development of microfluidic chips to capture organ-level function known as organ-on-a-chip (OOC) [REF]. The design of the OOC devices is made to mimic the human cellular microenvironment. It includes the flow of fluids through micro-channels to mimic the vasculature network for providing nutrients and transporting waste and metabolites. The OOC models could also simulate the physical environment of human organs (e.g., lung, gut, and kidney) by mimicking the structural features [REF]. This can eventually facilitate the translation of the in vitro findings to the human condition.The use of OOCs has the potential to make the drug discovery process fast and cheaper (with a cost reduction of up to 26%) [REF]. However, despite many advances, its applicability is hampered by the lack of online detection schemes, allowing the real-time observation of cellular behaviors [REF]. This shortage limits our understanding of cellular mechanism as a function of time and consequently unable us from correcting for the defects in the produced OOC models. The analytical techniques that have been used so far include optical imaging, electrochemical sensors, fluorescence- or label-free assays such as photonic crystal in a total internal reflection [REF], capillary electrophoresis, and mass spectrometry (MS) [REF].Among these techniques, MS offers high sensitivity and specificity to analyze changes in metabolites, proteins, and lipids [REF]. Utilizing MS for the in-situ monitoring of 3D cell systems in OOC can provide insight into the molecular composition of culture media, excreted metabolites, and waste products [REF]. Despite the multiple advantages of MS, it cannot be directly coupled with OOC for online and real-time analysis of molecules of interest (e.g., cytokines, proteins, chemokines) [REF]. This problem occurs mainly due to the presence of cell media in the chambers of OOC, which is rich in salts, nonvolatile buffers, and compounds that can hamper the MS analysis by creating ion suppression [REF]. Currently, offline sample preparation methods are used to normalise chip-to-chip variability or manipulate cell excretes before MS analysis [REF]. However, with these approaches, the time-resolved detection of metabolites is largely reduced, which is an essential factor for unravelling cellular mechanisms.Considering the numerous advantages offered by MS for biomedical applications, this paper focuses mainly on the investigated approaches for direct coupling of OOC with MS. The capabilities and weaknesses of each approach for real-time analysis are reviewed in detail.The review begins with an introduction to OOC and used analytical techniques to evaluate the mimetic tissue models. After a brief discussion on the techniques of phase contrast microscopy, enzyme-linked immunosorbent assays (ELISA), transepithelial electrical resistance (TEER), the review is mainly focused on MS. To this end, MS introduction is followed by hyphenation techniques that bridge OOC with MS, namely, electrophoresis, solid phase extraction, liquid chromatography and droplet-based chips, and their limitations are discussed. This article provides an exhaustive review of relatively new developments that would potentially enable the development of a robust and reliable interface for analyzing OOC content with MS as a rapid, sensitive, and specific analytical technique."
    },
    "2023-37284324_632_en.txt": {
        "title": "A Systematic Review and Meta-Analysis of the Effects of Rehabilitation Using Digital Healthcare on Musculoskeletal Pain and Quality of Life",
        "prompt": "Abstract: Rehabilitation using digital healthcare (DHC) has the potential to enhance the effectiveness of treatment for musculoskeletal disorders (MSDs) and associated pain by improving patient outcomes, while being cost-effective, safe, and measurable. This systematic review and meta-analysis aimed to evaluate the effectiveness of musculoskeletal rehabilitation using DHC. We searched PubMed, Ovid-Embase, Cochrane Library, and PEDro Physiotherapy Evidence Database from inception to October 28, 2022 for controlled clinical trials comparing DHC to conventional rehabilitation. We used a random-effects model for the meta-analysis, pooling the effects of DHC on pain and quality of life (QoL) by calculating standardized mean differences (SMDs) with 95% confidence intervals (CIs) between DHC rehabilitation and conventional rehabilitation (control). Fifty-four studies with 6240 participants met the inclusion criteria. The sample size ranged from 26 to 461, and the average age of the participants ranged from 21.9 to 71.8 years. The majority of the included studies focused on knee or hip joint MSD (n = 23), and the most frequently utilized DHC interventions were mobile applications (n = 26) and virtual or augmented reality (n = 16). Our meta-analysis of pain (n = 45) revealed that pain reduction was greater in DHC rehabilitation than in conventional rehabilitation (SMD: −0.55, 95% CI: −0.74, −0.36), indicating that rehabilitation using DHC has the potential to ameliorate MSD pain. Furthermore, DHC significantly improved health-related QoL and disease-specific QoL (SMD: 0.66, 95% CI: 0.29, 1.03; SMD: −0.44, 95% CI: −0.87, −0.01) compared to conventional rehabilitation. Our findings suggest that DHC offers a practical and flexible rehabilitation alternative for both patients with MSD and healthcare professionals. Nevertheless, further researches are needed to elucidate the underlying mechanisms by which DHC affects patient-reported outcomes, which may vary depending on the type and design of the DHC intervention.\n\n1. Introduction\n",
        "text": "The World Health Organization (WHO) adopted the “Resolution on Digital Health” introduced during the 71st World Health Assembly in May 2018 and recognized the need for digital technology to achieve universal health coverage and sustainable development goals [REF]. As such, digital healthcare (DHC), in which digital technology is converging with healthcare, has become increasingly important. Along with the fourth industrial revolution, changes in demographic structure, such as population aging, and social changes, such as smart device popularization, have stimulated the demand for DHC [REF]. Above all, the coronavirus disease 2019 pandemic has led to dramatic changes in the healthcare sector. Enabling contactless healthcare services, DHC has spread faster than ever.Musculoskeletal disorders (MSDs) are a broad spectrum of chronic diseases affecting bones, joints, and soft tissues [REF]. Musculoskeletal pain is a typical symptom caused by MSD, and especially, low back and neck pain is the leading causative factor of disability worldwide [REF]. Globally, 1.71 billion people suffer from MSDs, which are also the leading cause of disability [REF]. Approximately one-third of the UK population has MSDs, such as arthritis and back pain, making MSD the third–largest burden of the National Health System’s expenditure [REF]. According to a recent study in Korea, where national health insurance is available, one in three Koreans visits healthcare providers for musculoskeletal pain and functional decline; MSD prevalence is also increasing among older patients [REF]. Rehabilitation is essential for patients with MSDs. In 2017, when the WHO launched the “Rehabilitation 2030 initiative”, everyone should be able to rehabilitate at all life cycle stages, especially for musculoskeletal health [REF]. In addition, pain control is a major concern in MSD care and rehabilitation programs. In the MSD rehabilitation program, the specific needs of each patient are considered. An exercise program designed to increase muscle strength, flexibility and mobility is crucial, and through such rehabilitation, the patient can become independent and lead a regular life. Active participation of both patients and their families during this process is essential [REF]. Recently, Austria introduced a secondary preventive program for patients with MSD, aiming to prevent rehospitalization. As part of the appraisal for this program, a large observational study was conducted, and the results indicated that physical inactivity is the factor that is most associated with the need for healthcare services [REF]. Using DHC for health improvement is cost-effective, safe, and measurable [REF]. In particular, rehabilitation for MSD, which requires physical activity (PA), may have great potential when combined with DHC based on the Internet, smartphone applications, augmented reality (AR), and virtual reality (VR). Currently, most smartphones and smartwatches already include accelerometer-based PA trackers, allowing easy access even for older adults unfamiliar with innovative technology [REF]. The development of sensors and wearable devices not only enhances the convenience of systematic health management but also helps healthcare providers access and manage the rehabilitation of patients with MSD in daily life outside hospitals or facilities. In addition, by making the VR and AR content more interactive and engaging, patients can become interested in rehabilitation.DHC can offer a practical and flexible rehabilitation alternative for both patients and healthcare professionals. However, the effectiveness of DHC remains inadequately assessed; in particular, studies suggesting a relationship between MSD management and DHC are difficult to find [REF]. Hence, this systematic review and meta-analysis aimed to evaluate the effectiveness of musculoskeletal rehabilitation using a DHC system."
    },
    "2023-37284439_1381_en.txt": {
        "title": "Smaller size packs a stronger punch - Recent advances in small antibody fragments targeting tumour-associated carbohydrate antigens",
        "prompt": "Abstract: Attached to proteins, lipids, or forming long, complex chains, glycans represent the most versatile post-translational modification in nature and surround all human cells. Unique glycan structures are monitored by the immune system and differentiate self from non-self and healthy from malignant cells. Aberrant glycosylations, termed tumour-associated carbohydrate antigens (TACAs), are a hallmark of cancer and are correlated with all aspects of cancer biology. Therefore, TACAs represent attractive targets for monoclonal antibodies for cancer diagnosis and therapy. However, due to the thick and dense glycocalyx as well as the tumour micro-environment, conventional antibodies often suffer from restricted access and limited effectiveness in vivo. To overcome this issue, many small antibody fragments have come forth, showing similar affinity with better efficiency than their full-length counterparts. Here we review small antibody fragments against specific glycans on tumour cells and highlight their advantages over conventional antibodies.\n\n1. Introduction\n",
        "text": "Glycans are the most complex and structurally diverse biomolecules found on all cells and tissues of the human body. Glycosylation is the primary source of microheterogeneity in proteins, and, with few exceptions, all proteins passing through the ER and Golgi during biosynthesis are modified with O- and/or N-linked glycans. As part of the cellular glycocalyx, glycans are involved in various biological processes, including intracellular transport, cell adhesion, cell-cell, and cell-matrix interactions, as well as signalling [REF]. Hence, they play a crucial role in human physiology and pathology.The assembly of glycans can differ considerably from cell to cell. Although at least 4000 human genes are directly and indirectly involved in glycan synthesis, glycans are not directly encoded in the genome. Genetic defects in the glycosylation machinery often tend to be lethal at the embryonic stage, emphasising the vital role of glycans. The dynamic structure and composition of the glycocalyx are affected by various endogenous factors, such as the expression levels of specific glycosyltransferases and pH in the ER and Golgi. Age and environmental factors, such as lifestyle and diet, have also been shown to affect the glycocalyx. Under normal physiological conditions, a cell's glycan characteristics are primarily conserved, and alterations often reflect and result in cancer [REF]. These aberrant glycan structures can, therefore, be used as glycan biomarkers for diagnostics as well as specific targeting of the cells that carry them.Tumour-associated carbohydrate antigens (TACAs) have received increasing attention over the past decades due to their central role in every aspect of cancer progression and the possibility of targeting them specifically using antibodies (Abs). Carbohydrates, however, are generally poorly immunogenic and often elicit a T-cell-independent response, which fails to create immunological memory. Hence, immunisation of animals using glycans primarily leads to the production of low-affinity Immunoglobulin M (IgM), which does not class switch to Immunoglobulin G (IgG) [REF]. Thus, vaccine development against TACAs is hampered by the absence of T-cell-mediated immunity, which is critical for active cancer immunotherapy. TACAs are often tolerated by the immune system due to the structural similarity of TACAs with healthy cell glycans 3 and the low expression of TACAs in healthy tissue or during prenatal development [REF]. To overcome the poor immunogenicity of glycans in vaccines and therapeutic Ab development, glycans are conjugated to a carrier protein. The generation of glycopeptides in antigen-presenting cells, following their display by the major histocompatibility complex (MHC) system to T-cell receptors, is essential for eliciting a T-cell-dependent immune response and the formation of specific and high-affinity immunoglobulins. To date, production of most anti-glycan monoclonal antibodies (mAbs) involves animal immunisation with glycan-carrier protein conjugates to ensure the essential T-cell-dependent response for the formation of glycan-specific IgGs.Even though immunotherapy targeting glycans faces several hurdles, it also holds a considerable advantage. One of the major problems in combating cancer is drug resistance. Due to drug-imposed selection pressure in combination with the high mutational rate and clonal expansion, cancer cells often acquire drug resistance genetically. Targeting TACAs reduces the chance of antigen escape tremendously, as their synthesis, in contrast to proteins, is neither linear nor template-driven and involves the coordinated activity of numerous enzymes. Furthermore, TACAs are far more abundant than protein tumour antigens; for instance, the highly expressed protein marker HER-2 has about 106 copies per cell, whereas the TACA Thomsen-Friedenreich (TF) has about 107 copies per cell [REF]. TACA expression is often more frequent for given tumour types than protein antigens and identified in a larger percentage of the patients. Targeting TACAs can pave the way to more effective cancer therapies as is demonstrated by the approval of Dinutuximab, a mAb targeting the TACA GD2, for treating neuroblastoma in 2015 [REF].The mAbs used in therapy are typically IgG and are composed of the fragment antigen binding (Fab), and the crystallizable fragment (Fc) (Figure 1A), which interacts with Fc receptors expressed on several immune cells such as natural killer (NK) cells and macrophages, leading to antibody-dependent cellular cytotoxicity (ADCC) or antibody-dependent cell phagocytosis (ADCP). In addition, the Fc fragment can be recognised by components of the complement system and can thereby induce complement-dependent cytotoxicity (CDC). Alternatively, mAbs can be used to deliver a toxic payload in the form of an antibody-drug conjugate (ADC) [REF].The first mAb for therapeutic use was approved by the U.S Food and Drug Administration (FDA) in 1986. In 2021, the FDA approved the 100th mAb product, highlighting their wide success 8 Nevertheless, their large size and high molecular mass of ~150 kDa, prevents Abs from reaching their full potential in therapeutic efficiency, as penetration into thick and dense tissue, as the tumour microenvironment, is often a limiting factor. In addition, the structure, function, and serum half-life of therapeutic mAbs, as well as non-desired adverse effects in treated human patients are due to Fc fragment interactions with the immune system cells and the N-glycans it carries [REF].To overcome these problems, tremendous efforts have been expended to generate smaller antigen-binding fragments, such as Fab, single chain variable fragment (scFv), and single-domain Abs (Figure 1). Ab fragments are produced through enzymatic cleavage from intact mAbs, or by recombinant protein expression systems such as bacteria, which allow cheaper and faster production in large quantities. The fragments are less immunogenic due to the absence of the Fc fragment, and their size enables better access to dense tissue and contributes to enhanced tumour penetration [REF]. Moreover, Ab fragments are readily coupled with different molecules, and their relatively smaller size makes them highly amenable to genetic engineering and the development of multivalent and multispecific tools [REF].However, many recombinant fragments exhibit reduced stability and affinity compared to their parent Ab. Moreover, lying below the glomerular filtration cut-off of approximately 60 kDa, most Ab fragments suffer from fast renal clearance and a half-life ranging from minutes to several hours [REF]. A longer circulating time is desirable for cancer therapy to achieve its full therapeutic efficacy through enhanced biodistribution and increased Ab concentration at the target site. Unfortunately, too-small constructs will be excreted through the kidneys before exerting biological effects. Therefore, several strategies are successfully employed to increase the half-life of Ab fragments, including chemical conjugation to polyethylene glycol (PEG) chains or fusion to additional Ab fragments that bind serum proteins such as albumin [REF]. Nevertheless, a shorter blood circulation time can also be advantageous for tumour imaging, as it lowers background signal compared with the 2-3 weeks of half-life of canonical Abs [REF].While there have been several comprehensive reviews on full Abs targeting TACAs [REF], literature regarding Ab fragments targeting TACAs was not reviewed to date. Herein, we provide a comprehensive overview of the achievements and applications of Ab fragments that target specific TACAs. First, we provide a structural overview of the different Ab fragments. Thereafter, we describe specific TACAs, and the fragments that target them. Engineered Ab fragments that contain Fc segments are out of the scope of this review."
    },
    "2023-37284586_682_en.txt": {
        "title": "Strategies in the synthesis of dibenzo[]heteropines",
        "prompt": "Abstract: The dibenzo[b,f]azepine skeleton is important in the pharmaceutical industry, not only in terms of existing commercial antidepressants, anxiolytics and anticonvulsants, but also in reengineering for other applications. More recently, the potential of the dibenzo[b,f]azepine moiety in organic light emitting diodes and dye-sensitized solar cell dyes has been recognised, while catalysts and molecular organic frameworks with dibenzo[b,f]azepine derived ligands have also been reported. This review provides a brief overview of the different synthetic strategies to dibenzo[b,f]azepines and other dibenzo[b,f]heteropines.\n\n1. Introduction\n",
        "text": "The dibenzo[b,f]azepine (1a) scaffold (Figure 1) is featured in commercial pharmaceuticals [REF] and other lead compounds [REF], ligands [REF] and in materials science with possible applications in organic light emitting diodes (OLEDs) [REF] and dye-sensitized solar cells (DSSCs) [REF].Dibenzo[b,f]azepine (1a), -oxepine (1b) and -thiepine (1c) as examples of dibenzo[b,f]heteropines (1) with the corresponding 10,11-dihydro derivatives (2).Commercial pharmaceutical agents based on dibenzo[b,f]azepine (1a), or the 10,11-dihydro derivative thereof (2a), include imipramine (3) and clomipramine (4) (tricyclic antidepressants) [REF], opipramol (5) (generalized anxiety disorder) [REF] and carbamazepine (6) (seizure disorders) [REF] (Figure 2).Selected pharmaceuticals with the dibenzo[b,f]azepine skeleton.10,11-Dihydrodibenzo[b,f]azepine-based ligand 7 and a methyl analogue thereof are known to form pincer complexes with Pd, Ir, Rh and Ln [REF], whereas a copper(II) wagon wheel complex of 8 was reported in a molecular organic framework (MOF) (Figure 3) [REF].Examples of 10,11-dihydrodibenzo[b,f]azepine-based ligands.4,4'-(5-(Pyridin-2-yl)-10,11-dihydro-5H-dibenzo[b,f]azepine-2,8-diyl)bis(N,N-diphenylaniline) (9) exhibits properties suitable for the use in organic light emitting diodes [REF] whereas dyes 10–12 were found suitable for the use in dye-sensitised solar cells (Figure 4) [REF].The dibenzo[b,f]azepine moiety in dyes with properties suitable for the use in organic light emitting diodes and dye-sensitised solar cells.Though analogous dibenzo[b,f]oxepines 1b, with an oxygen in the heterocyclic ring as opposed to nitrogen in azepines, are known from natural sources (compounds 13–18 as examples) [REF], the application thereof in a clinical setting is limited (Figure 5). Novartis CGP 3466 (19), a propargylamine derivative, showed excellent neuroprotective properties for the treatment of Parkinson’s disease in rat models (Figure 5) [REF]. Unfortunately, the promising preclinical studies of 19 could not be replicated in human trials [REF].Selective bioactive natural products (13–18) containing the dibenzo[b,f]oxepine scaffold and Novartis CGP 3466 (19).For more details on the early synthesis of dibenzo[b,f]azepine (1a), the extensive review prepared by Kricka and Ledwith [REF] in 1974, is recommended. While the review is lacking modern metal catalysis, it is still an excellent work covering early syntheses and properties. An analogous review published by Olivera et al. [REF] covers the topic of dibenzo[b,f]oxepines (1b) up to 2002.Other heteroatoms (e.g., O, N, S, P, B and Si) in the heterocyclic ring result in analogues of dibenzo[b,f]azepines and -oxepines. This group of compounds will thus be broadly referred to as dibenzo[b,f]heteropines (1).The first section of this review will cover the synthesis of dibenzo[b,f]heteropines (1) and 10,11-dihydrodibenzo[b,f]heteropines (2). The following section will briefly touch on functionalisation of the scaffold. While some reports are limited to the introduction of a single heteroatom, e.g., nitrogen in the case of azepines 1a or oxygen in the case of oxepines 1b, some approaches allow for the incorporation of a diverse scope of heteroatoms (e.g., O, N, S, P, B and Si) and may give access to a range of dibenzo[b,f]heteropines 1 using common intermediates [REF]. Therefore, this section will be broadly organised by reaction type responsible for ring closure."
    },
    "2023-37284721_685_en.txt": {
        "title": "Advances in omics research on peanut response to biotic stresses",
        "prompt": "Abstract: Peanut growth, development, and eventual production are constrained by biotic and abiotic stresses resulting in serious economic losses. To understand the response and tolerance mechanism of peanut to biotic and abiotic stresses, high-throughput Omics approaches have been applied in peanut research. Integrated Omics approaches are essential for elucidating the temporal and spatial changes that occur in peanut facing different stresses. The integration of functional genomics with other Omics highlights the relationships between peanut genomes and phenotypes under specific stress conditions. In this review, we focus on research on peanut biotic stresses. Here we review the primary types of biotic stresses that threaten sustainable peanut production, the multi-Omics technologies for peanut research and breeding, and the recent advances in various peanut Omics under biotic stresses, including genomics, transcriptomics, proteomics, metabolomics, miRNAomics, epigenomics and phenomics, for identification of biotic stress-related genes, proteins, metabolites and their networks as well as the development of potential traits. We also discuss the challenges, opportunities, and future directions for peanut Omics under biotic stresses, aiming sustainable food production. The Omics knowledge is instrumental for improving peanut tolerance to cope with various biotic stresses and for meeting the food demands of the exponentially growing global population.\n\n1. Introduction\n",
        "text": "Arachis hypogaea (peanut or groundnut) is among the most important oil and food legumes with annual production of ~46 million tons (http://www.fao.org/faostat/en/#home). It is cultivated in more than 100 countries around the world in tropical and subtropical regions, and is the principal source of digestible protein, cooking oil and vitamins in development and developing regions of Asia, Africa and America for fighting malnutrition and ensuring food security [REF]. Productivity levels of peanut in most of the developing countries have remained low due to several production constraints which include biotic and abiotic stresses. Breeding new cultivars to improve productivity is the best way to meet the needs of the producers, consumers and industry. As an allotetraploid species in the Arachis genus, peanut has extremely low genetic diversity because most of the other species in the genus are diploid [REF]. Peanut is particularly susceptible to a number of pest and pathogens due in part to the lack of gene exchange with its diploid wild ancestors that have resistance genes [REF]. The limited genetic diversity and the tetraploid complexity of cultivated gene pool is a barrier and challenge to create cultivars with broad resistance, excellent quality and high yield [REF]. On the other hand, diploid wild relatives (Arachis spp.) with a larger genetic diversity evolving in a variety of habitats and biotic challenges are significant sources of resistance genes and a rich source of novel alleles that can be introduced into the cultivated species by unconventional method [REF]. Therefore, there is an urgent need to exploit gene resources in diploid species by using Omics methods.Plant genome research has facilitated gene discovery and gene functional elucidation. With Omics, scientists can manage the intricate global biological systems based on advances in Omics technology [REF]. Recent advances in DNA sequencing technology have promoted the rapid development of science and made any other new applications beyond genome sequencing possible [REF]. Particularly, the emergence of next-generation sequencing makes whole-genome resequencing for variant discovery, transcriptional regulatory networks analysis, RNA sequencing analysis (RNA-seq) for transcriptome and noncoding RNAome, quantitative detection analysis (Chip-seq) for epigenome dynamics and DNA-protein interactions become viable applications [REF]. Other techniques, including interactomic analysis for protein-protein interactions, hormonomic analysis for plant hormone signaling, and metabolomic analysis of metabolic products, have been developed [REF]. The omics technologies will help researchers to mine and screen specific genes involved in crop improvement. In addition, integrated network analysis reveals molecular connections between genes and metabolites, boots our understanding the relationships between phenotypic and genotype [REF]. Over the past few decades, advances in genomics, transcriptomics, metabolomics, and proteomic analysis with the development of cutting-edge technologies have greatly facilitated the increase in the study of molecular aspects of peanut-biotic factor interactions. Therefore, different omics-based studies have attempted to decipher the molecular pathways that contribute to crop defenses against diseases and pests. In this review, we mainly retrospect the studies on the basic of vary Omics analyses concentrating mainly on those with relevant data on peanut defense responses and resistance to biotic stresses including insect pests, pathogen and bacteria."
    },
    "2023-37285011_2938_en.txt": {
        "title": "What Do We Know About Depression Among Youth and How Can We Make Progress Toward Improved Understanding and Reducing Distress? A New Hope",
        "prompt": "Abstract: This paper summarizes many findings about depression among children and adolescents. Depression is prevalent, highly distressing, and exerts considerable burden worldwide. Rates surge from childhood through young adulthood and have increased over the last decade. Many risk factors have been identified, and evidence-based interventions exist targeting mostly individual-level changes via psychological or pharmacological means. At the same time, the field appears stuck and has not achieved considerable progress in advancing scientific understanding of depression’s features or delivering interventions to meet the challenge of youth depression’s high and growing prevalence. This paper adopts several positions to address these challenges and move the field forward. First, we emphasize reinvigoration of construct validation approaches that may better characterize youth depression’s phenomenological features and inform more valid and reliable assessments that can enhance scientific understanding and improve interventions for youth depression. To this end, history and philosophical principles affecting depression’s conceptualization and measurement are considered. Second, we suggest expanding the range and targets of treatments and prevention efforts beyond current practice guidelines for evidence-based interventions. This broader suite of interventions includes structural- and system-level change focused at community and societal levels (e.g., evidence-based economic anti-poverty interventions) and personalized interventions with sufficient evidence base. We propose that by focusing on the FORCE (Fundamentals, Openness, Relationships, Constructs, Evidence), youth depression research can provide new hope.\n\n1. Introduction\n",
        "text": "Over the last several decades, a prodigious literature has amassed on depression in children and adolescents. Major and consequential epidemiological findings show that (1) depression exhibits high prevalence and is associated with substantial distress and burden around the world [REF]; (2) rates surge six-fold from childhood through late adolescence with steady, persistent rates throughout adulthood [REF]; and (3) rates are increasing across generations, with current prevalence rates exceeding those seen just 10 years ago [REF]. As such, public policy experts recommend annual screening of depression for individuals ages 12 and above [REF]. In an effort to better understand (and interrupt) the development of depression across childhood and adolescence, researchers have identified numerous risk and resilience factors that prospectively predict depression [REF]. Indicated or selective preventions can reduce the likelihood of future depression for youth1 with elevated symptoms or risk factors (Cuijpers et al., 2021a, 2021b). Moreover, there exist several evidence-based treatments, including psychotherapies and pharmacotherapies, each of which works generally equally well to relieve youth depression [REF]. Table 1 enumerates what we know regarding risk factors for youth depression, and Table 2 summarizes knowledge of evidence-based interventions (treatments and preventions).\n\nThe field has accumulated an impressive corpus of knowledge. At the same time, however, it is an undeniable reality that many young people across the world continue to suffer from and with depression, and there is an urgent and critical need to address this suffering for as many people as possible. Consider global data, for example, which indicate that the age-standardized prevalence of depression increased by 4.2% from 1990 to 2013, whereas the prevalence of anxiety decreased by 0.5% over this same period [REF]. This depression rise has been accompanied by co-occurring increases in rates of treatment; yet, no country included in this global analysis showed diminished depression rates over this time period. Even with many empirically supported treatments, there has been little sustained progress in reducing depression’s burden, or decreasing depression-related distress and suffering since 1980s. What can be done to address clear gaps to reduce the considerable and highly consequential distress and burden associated with youth depression? The purpose of this paper is to revisit and critically interrogate how and what we think we know about youth depression and its interventions. To this end, we review the sociohistorical context in which the phenomena termed “depression” were conceptualized and highlight the ways in which our academic notions and “best practice” assessment instruments both do and do not align with the symptoms and features of this depression construct. In a similar manner, we consider contemporary prevention and treatment strategies and provide rationale for expanding the range and scope of intervention efforts to more efficiently and effectively respond to youth depression and prioritize structural- and systems-level change.Ultimately, we strive to provide A New Hope for advancing progress on youth depression. To this end we take some positions (admittedly ours) for what we believe are directions and priorities that hold promise for both improving the scholarly understanding of youth depression and reducing depression-related distress and burden worldwide. We believe meaningful progress can be made without unduly devoting more time, energy, and limited resources investigating primarily unproven biological and technological solutions [REF] in the hope that some kind of singular breakthrough will meet massive current needs and close the prevalence-intervention gap.As Darth Vader famously said in the original Star Wars: A New Hope (episode IV), “Don’t be too proud of this technological terror you’ve constructed. The ability to destroy a planet, or even a whole system, is insignificant next to the power of the Force.” Our perspective and the main points we emphasize can be summarized by focusing on the power of the FORCE: Fundamentals are essential to ground clear thinking informed by humility, history, and philosophy; Openness is needed to explore new ideas with scientific rigor and transparency; Relationships matter for understanding and intervening in youth depression across all levels in social–ecological systems; Constructs are key in the conceptualization, measurement, and classification of depression; and Evidence must be collected and evaluated, grounded in construct validation with epistemic iteration, to ensure accurate, reliable, reproducible knowledge with scientific and practical utility.In this paper, we have three main goals. First is to provide an overview of what the field knows about depression among youth, via Table 1 for depression risks across ecological levels and Table 2 for interventions. All of this knowledge is grounded in how depression as a construct is currently, and has historically, been conceptualized and measured. Our second goal is to reinvigorate serious academic progress focused on defining and explicating conceptually what depression is among youth as informed by developmental psychopathology. As we summarize in our historical review, necessary and important steps in the construct validation process (content conceptualization; measurement) were minimally engaged in the study of adult depression, and this incomplete conceptual understanding has carried forth in the study of depression among youth. Our final goal is to address immediate needs to reduce the prevalence and distress associated with youth depression. We propose ways for responding to unmet needs of youth at risk for and affected by depression, as well as their families and communities. We begin with an eye toward how we might improve the science of youth depression, with an emphasis on issues of methods, measures, and construct validity. We then propose directions to enhance interventions to alleviate the prevalence and distress of youth depression and suggest efforts that engage multiple ecological systems and stakeholders."
    },
    "2023-37286235_1865_en.txt": {
        "title": "Incorporating resilience when assessing pandemic risk in the Arctic: a case study of Alaska",
        "prompt": "Abstract: The discourse on vulnerability to COVID-19 or any other pandemic is about the susceptibility to the effects of disease outbreaks. Over time, vulnerability has been assessed through various indices calculated using a confluence of societal factors. However, categorising Arctic communities, without considering their socioeconomic, cultural and demographic uniqueness, into the high and low continuum of vulnerability using universal indicators will undoubtedly result in the underestimation of the communities’ capacity to withstand and recover from pandemic exposure. By recognising vulnerability and resilience as two separate but interrelated dimensions, this study reviews the Arctic communities’ ability to cope with pandemic risks. In particular, we have developed a pandemic vulnerability–resilience framework for Alaska to examine the potential community-level risks of COVID-19 or future pandemics. Based on the combined assessment of the vulnerability and resilience indices, we found that not all highly vulnerable census areas and boroughs had experienced COVID-19 epidemiological outcomes with similar severity. The more resilient a census area or borough is, the lower the cumulative death per 100 000 and case fatality ratio in that area. The insight that pandemic risks are the result of the interaction between vulnerability and resilience could help public officials and concerned parties to accurately identify the populations and communities at most risk or with the greatest need, which, in turn, helps in the efficient allocation of resources and services before, during and after a pandemic. A resilience–vulnerability-focused approach described in this paper can be applied to assess the potential effect of COVID-19 and similar future health crises in remote regions or regions with large Indigenous populations in other parts of the world.\n\n1. Introduction\n",
        "text": "A large amount of research on the COVID-19 pandemic around the world emphasises the disproportionate burden of the pandemic among racial/ethnic minorities, the poor, the elderly and people with disabilities, low educational attainment and comorbidities, among others [REF]. The word ‘vulnerable’ is often used to indicate these disproportionately impacted populations. Moreover, COVID-19 has broadened the definition and scope of vulnerability that include not only a population with certain socioeconomic or health characteristics but also the marginalised and disadvantaged communities that are more likely susceptible to overwhelming epidemiological outcomes (i.e., higher case, morbidity and mortality rates) [REF]. Thus, researchers have assessed a variety of communities’ characteristics to identify the groups at most risk during the COVID-19 pandemic and its aftermath [REF]. In other words, the discourse on vulnerability to COVID-19, or any other pandemic, is largely about the susceptibility of an individual or a community to the negative effects of disease outbreaks.In the context of the COVID-19 pandemic, numerous studies have assessed vulnerability through various indices calculated using a confluence of societal factors [REF]. Most of these studies examine the relationship between the vulnerability index and COVID-19 outcomes and discover that highly vulnerable areas have higher COVID-19 incidence and mortality rates. Furthermore, vulnerability indices have also been used as a tool to highlight disparities in COVID-19 vaccination coverage and vaccine hesitancy [REF]. One consistent thread found in these studies is the development or modification of the vulnerability indices using simple to sophisticated statistical methods such as arithmetic mean [REF], percentile ranking [REF], regression analysis [REF], principal component analysis [REF], factor analysis [REF], machine learning algorithms [REF], etc. The common societal indicators used by these studies to develop the indices include age, poverty, race or ethnicity, education, population with comorbidities, overcrowded households, and hospital density, among others.Researchers have also worked to understand and convey the distinct challenges and vulnerabilities the Arctic, including Alaska, already has and could be amplified due to COVID-19 and future pandemics [REF]. This study, likewise, focuses partly on the vulnerability of the Arctic populations with respect to the pandemic. Pre-existing conditions that exacerbate the vulnerability of Arctic residents include geographic barriers that limit access to health services, underdeveloped civic infrastructure, larger populations with underlying medical conditions (such as hypertension, diabetes, heart disease, tuberculosis, hepatitis, obesity), lower socioeconomic status, inadequate housing, poor sanitation, lack of clean water, etc [REF]. Due to these challenges, previous pandemics such as smallpox, cholera, the 1918 influenza, tuberculosis and the 2009 H1N1 influenza had a disproportionate impact on the Arctic and its residents, especially its Indigenous population [REF].Despite their existing vulnerabilities, Arctic communities have persevered through COVID-19 with less dire consequences than the rest of the world. Developed countries such as the USA and the UK had very high cumulative confirmed COVID-19 death rates (i.e., above 315 per 100 000) [REF], which were 2–3 times as high in some Arctic countries and Indigenous communities [REF]. Among 7.5 million Arctic residents [REF], as of 1 December 2022, there were 29 325 confirmed COVID-19 deaths [REF]. Out of these, about 26 000 (i.e., 285 per 100 000) deaths were ascribed to Northern Russia [REF]. Greenland, Faroe Islands, Iceland, Northern Canada and Northern Norway reported just under 60 deaths per 100 000 population, cumulatively [REF]. Alaska and Northern Sweden had over 180 cumulative deaths per 100 000 [REF]. Meanwhile, in Canada and Alaska, the case fatality ratio (CFR) was substantially lower in the Indigenous population compared with the non-Indigenous population [REF]. The lessened socioeconomic and health impacts of COVID-19 across the Arctic and its Indigenous population can largely be attributed to livelihoods based on subsistence activities, traditional practices and knowledge, remoteness, effective vaccination campaigns, proactive community leadership and engagement, self-determination and other factors [REF]. These factors represent the Arctic communities’ strengths, making them remarkably resilient against COVID-19 or potentially other pandemics.To date, studies assessing pandemic risk exposure are limited in scope as a community’s strengths are rarely taken into account, and the combined evaluation of vulnerability and resilience is almost absent. Further, in the literature, resilience is often treated as the simple inverse of vulnerability, that is, communities with lower vulnerability are generally considered highly resilient [REF]. Categorising Arctic communities, without considering their geography, socioeconomic, cultural and demographic uniqueness, into the high and low continuum of vulnerability using the universal societal indicators will undoubtedly result in the underestimation of these communities’ capacity to prevent, withstand and recover from pandemic exposure. For instance, the poverty rate, a universal economic indicator, is likely limited when evaluating Arctic communities’ risk exposure. Family relations and subsistence activities are among the various ways of coping with income limitations in these communities [REF]. Thus, an asset-based approach that refocuses research from community deficiencies to its strengths may provide a better alternative for a comprehensive pandemic risk assessment. This study, by recognising vulnerability and resilience as two separate but interrelated dimensions, evaluates the Arctic communities’ capacity to address current and future pandemic risks. In particular, this study advances a vulnerability–resilience framework that combines the strengths and weaknesses of Alaskan communities into one integrated conceptual model. Based on the framework, this study aims to develop indices of vulnerability and resilience and assess them integratively concerning COVID-19 epidemiological outcomes.The combined assessment of vulnerability and resilience is necessary as this technique accurately identifies the populations and communities at most risk or with the greatest need, which, in turn, helps public officials and local organisations to efficiently allocate resources and services as well as take mitigation and recovery measures customed to local realities. This can be a particularly important tool as future global pandemics from novel viruses will likely continue due to globalisation, human migration, urbanisation and climate change around the world.Remote regions and the world’s margins, such as the Arctic, are rarely found in the focus of global health research. Yet, they present important cases for understanding health disparities, social and environmental determinants of health in general and of the COVID-19 pandemic in particular [REF]. The Arctic, Northern circumpolar region, covers Iceland, Greenland, the Faroe Islands and parts of Canada, Finland, Norway, Russia, Sweden and the USA (Alaska). Common features representing the Arctic Indigenous populations include their distinct cultures, languages and knowledge, traditional livelihoods based on subsistence, and deep connection to their homeland [REF]. Alaska was chosen as a case study to develop and apply the Arctic pandemic vulnerability–resilience framework. The framework incorporates vulnerability and resilience indices because of (a) the availability of data and (b) Alaska’s geography, demography, cultural heritage, population health and socioeconomic conditions are indicative of the other Arctic and remote regions. Thus, the framework and indices developed for Alaska in this study can be replicated and refined for other Arctic or remote regions to assess the risks of COVID-19 or future pandemics.Alaska is the largest state in the USA. It comprises 29 boroughs and census areas inhabited by 733 391 people [REF]. Out of these, an estimated 21.9% are Indigenous (ie, American Indian and Alaska Native in alone or combination) [REF]. Approximately 50% of the Indigenous population of Alaska lives either in remote or Northern regions of the state [REF]. The North and Northwest of Alaska are home to the Inupiaq and St. Lawrence Island Yupik. While Eyak, Tlingit, Haida, Tsimshian, Yup’ik and Cupik, Alutiiq, and Unangax Indigenous peoples live further in the south [REF]. Most Athabaskan People reside in Alaska’s interior [REF]. Research has shown that in the USA, ethnic minorities, including the Indigenous Peoples, and individuals with lower socioeconomic status are more likely to have comorbidities and limited access to healthcare and resources, thereby increasing their susceptibility to diseases [REF]. According to recent study by Ward et al [REF], in Alaska, the risk of death due to COVID-19 for an American Indian and Alaska Native person is 2.9 times higher than for a white individual. Furthermore, Alaska’s population as a whole is relatively vulnerable compared with the rest of the USA in socioeconomic and health indicators. In 2018, Alaska ranked 23rd out of 50 US states in the economic hardship index developed using 6 indicators: unemployment, dependency, education, crowded housing and poverty [REF]. Moreover, around 38.8% of Alaskans have underlying health conditions such as obesity, heart disease, chronic pulmonary obstructive disease, diabetes and chronic kidney disease [REF]. It is broadly acknowledged that the prevalence of these health burdens among Alaska Indigenous Peoples stems from the detrimental effects of colonialism on traditional food systems, healing practices, sense of identity, healthy ways of knowing and access to healthcare [REF]. Therefore, these socioeconomic disadvantages and health disparities, as one of the causes of the disproportionate impact of COVID-19 [REF], are considered in this study while developing the vulnerability index.Even though vulnerable, Alaskan communities have curtailed adverse COVID-19 impacts due to the strong measures taken for its containment at the early stages of the pandemic, remoteness (ie, two-thirds of the land area is unreachable by road or ferry), livelihoods based on subsistence activities and a relatively low concentration of people aged 65 or above (12.52%) compared with the USA (16.3%) [REF]. Further, the Alaska Native Tribal Health Consortium’s COVID-19 awareness programme in different Native languages and a higher vaccination rate have significantly reduced the risk for COVID-19-associated hospitalisation and deaths [REF]. Alaska launched mass vaccination campaigns as early as December 2020 and led the USA in vaccination rates in January 2021 [REF]. By June 2021, over 55% of adults (ie, 16 years old and older) living in Aleutians East Borough, Juneau City and Borough, Skagway Municipality, Sitka City and Borough, Nome Census Area and Yukon-Koyukuk Census Area had completed the primary series of COVID-19 vaccination [REF]. These early higher vaccination rates can largely be accredited to strong vaccine-distribution networks, tribal cultural values that put emphasis on protecting and honouring elders, culturally tailored awareness strategies to overcome vaccine hesitancy, self-governance that allowed the tribal groups to establish their campaigns and priorities, and the door-to-door vaccine outreach to medically fragile individuals [REF]."
    },
    "2023-37286888_625_en.txt": {
        "title": "Frequency of anticancer drug use at the end of life: a scoping review",
        "prompt": "Abstract: Anticancer drug use at the end of life places potential extra burdens on patients and the healthcare system. Previous articles show variability in methods and outcomes; thus, their results are not directly comparable. This scoping review describes the methods and extent of anticancer drug use at end of life.Systematic searches in Medline and Embase were conducted to identify articles reporting anticancer drug use at the end of life.We selected 341 eligible publications, identifying key study features including timing of research, disease status, treatment schedule, treatment type, and treatment characteristics. Among the subset of 69 articles of all cancer types published within the last 5 years, we examined the frequency of anticancer drug use across various end of life periods.This comprehensive description of publications on anticancer drug use at end of life underscores the importance of methodological factors when designing studies and comparing outcomes.The online version contains supplementary material available at 10.1007/s12094-023-03234-1.\n\n1. Introduction\n",
        "text": "Palliative anticancer drug use is reported to prolong survival and relieve disease-related symptoms for some patients with incurable cancer [REF]. Conversely, other studies have reported a marginal impact of anticancer therapy on life expectancy and showed an unclear relationship between the use of this treatment and quality of life [REF]. Furthermore, the potential benefits of using anticancer drugs in terminally ill cancer patients are disputed because cancer therapy might threaten patients’ quality of life and even survival, and also imposes a significant cost on the healthcare system. Moreover, an increasing number of articles claim that near the end of life, anticancer treatment is associated with poorer quality of life [REF] and does not improve survival [REF]. In this review, the term “end of life” is used referring to final days, weeks, months in a person’s life in which it is medically obvious that death is imminent or a terminal moribund state cannot be prevented [REF].Anticancer drug use towards the end of life is highly prevalent worldwide and increasing in some countries [REF]. A single centre study in Korea reported that, the proportion of patients receiving chemotherapy in the last month of life was 25.7% in 2000, 32.7% in 2005, and continued to increase to 44.2% in 2010 [REF]. In the USA, between 1993 and 1996, the proportion of patients receiving chemotherapy within the 2 weeks before death rose from 13.8% to 18.5%, while recent studies reported stagnation in the USA [REF]. The trend and magnitude of end of life anticancer treatment frequency may be explained in part by the rapidly evolving treatment options in oncology, like immunotherapy and newer targeted treatments, which have broadened the therapeutic landscape. Recent reviews on resource utilization in end of life cancer patients show great diversity in settings, methods, and outcomes across publications [REF].These differences are more evident when defining the study population. While in retrospective population-based studies, which include all cancer patients in the analysis, regardless of previous treatments received or stage of cancer, they typically reported a lower frequency of end of life cancer treatment [REF]. Conversely, research studies that enrolled patients with advanced cancer or those who had undergone cancer treatment in the past reported substantially higher frequencies of end of life cancer treatment in the same period [REF].The goal of this article is to explore the methods and outcomes used by the studies, and along these differences, describe the current magnitude of anticancer treatment at end of life. These objectives lend themselves to a scoping review approach because they aim to identify knowledge gaps, clarify concepts, and scope the literature related to cancer treatment at end of life. Scoping reviews are particularly useful for mapping the existing literature in a specific field, allowing researchers to compare the methods and outcomes used by various studies [REF]. In this case, a scoping review can provide a comprehensive overview of the current magnitude of anticancer treatment at end of life, highlighting differences in methods and outcomes across studies. Furthermore, this scoping review can inform decision-making and set research agendas to identify areas that require further investigation and inform future systematic reviews or primary research on this topic [REF]."
    },
    "2023-37286937_755_en.txt": {
        "title": "Prescription of therapeutic exercise in migraine, an evidence-based clinical practice guideline",
        "prompt": "Abstract: The main objective of this clinical practice guideline is to provide a series of recommendations for healthcare and exercise professionals, such as neurologists, physical therapists, and exercise physiologists, regarding exercise prescription for patients with migraine.This guideline was developed following the methodology and procedures recommended in the Appraisal of Guidelines for Research and Evaluation (AGREE). The quality of evidence and strength of recommendations were evaluated with the Scottish Intercollegiate Guidelines Network (SIGN). A systematic literature review was performed and an established appraisal process was employed to rate the quality of relevant scientific research (Grading of Recommendations Assessment, Development, and Evaluation methodology).The evaluation of the current evidence, the elaboration of the grades of recommendation, and their validation show a B grade of recommendation for aerobic exercise, moderate-continuous aerobic exercise, yoga, and exercise and lifestyle recommendations for the improvement of symptoms, disability, and quality of life in patients with migraine. Relaxation techniques, high-intensity interval training, low-intensity continuous aerobic exercise, exercise and relaxation techniques, Tai Chi, and resistance exercise obtained a C grade of recommendation for the improvement of migraine symptoms and disability.The online version contains supplementary material available at 10.1186/s10194-023-01571-8.\n\n1. Introduction\n",
        "text": "Migraine is the second leading cause of disability in the world after low back pain [REF]. It is a neurological condition with a global prevalence of 14.4%, with the peak prevalence and years of life lived with disability occurring between ages 35 and 39 years [REF]. The impact generated on the patient’s various social and family dimensions induces a decline in quality of life and a high economic cost due to work absenteeism, a decrease in work efficiency, and increased healthcare costs [REF].Pharmacological interventions are the most common approaches, typically consisting of the use of non-steroidal anti-inflammatory drugs and triptans for acute management [REF]. Prophylactic drugs, such as topiramate or valproic acid, are recommended for chronic migraine [REF]. Other pharmacological approaches have been developed in recent years, including the human monoclonal antibody erenumab, botulinum toxin, ditans, and gepants, with good results in the reduction of the frequency of migraine and evolution of migraine attacks [REF]. However, the main problem with these interventions is their concomitant adverse effects, such as the increase in blood pressure with triptans use, the risk of allergic reaction with monoclonal antibodies, the transient development of blepharoptosis and muscle weakness produced by the injection of botulinum toxin, and interaction with other drugs [REF].In addition to pharmacological treatment, behavioral change interventions are fundamental in the clinical management of migraine. These treatments include management of stress, sleep, diet, and exercise [REF], of which aerobic exercise and yoga modalities are proposed as preventive alternatives for migraine [REF]. Exercise prescription for migraine improvement appears to be a safe and effective intervention that could decrease migraine symptoms and disability and increase quality of life. Aerobic exercise has been the most studied modality [REF].Migraine and exercise information disseminated on social networks has increased exponentially in recent years; however, the quality of that information is questionable [REF]. Several international scientific societies recommend the practice of exercise as part of the therapeutic approach to migraine. The French Headache Society includes physical exercise as part of the non-pharmacological treatment of migraine headaches [REF]; the Danish Headache Society agrees with a similar recommendation and also includes relaxation and postural exercises [REF]; and the American Headache Society recommends regular exercise as part of the biobehavioral treatment of migraine management and prevention [REF].The recommendations of the various headache societies for treatments involving exercise for migraine have one characteristic in common: exercise is mentioned in a very general way, and the various exercise modalities that can be used for migraine treatment are not mentioned in depth. Current scientific evidence has not yet determined the adequate exercise prescription parameters for patients with migraine. Also, there are still no clinical practice guidelines on exercise prescription for migraine. Therefore, we consider it necessary to develop a guide to help clinicians who treat headaches so they can make better recommendations or provide a more specific exercise prescription.The main objective of this clinical practice guideline is to provide a series of recommendations regarding different exercise modalities that could be effective in the treatment of migraine, and other lifestyle recommendations that could increase the efficacy of exercise interventions, for healthcare and exercise professionals, such as neurologists, physical therapists, and exercise physiologists, so as to better treat patients with migraine. For this proposal, we reviewed the current evidence that shows which exercise interventions improve migraine symptoms (intensity, frequency, and duration), disability, and quality of life. Moreover, the intention of this guideline is to provide parameters of exercise prescription for each exercise modality that could be adapted depending on the patient’s characteristics (e.g. migraine frequency, physical condition, and patient’s preferences). It is not a standard of medical care that determines the exercise intervention approach for migraine treatment. Patients’ clinical presentation, experiences, and expectations, as well as clinicians’ experiences and expertise should guide the exercise prescription based on the best recommendations of the current evidence."
    },
    "2023-37287399_646_en.txt": {
        "title": "The effect of vitamin D supplementation on the gut microbiome in older Australians – Results from analyses of the D-Health Trial",
        "prompt": "Abstract: Observational studies suggest a link between vitamin D and the composition of the gut microbiome, but there is little evidence from randomized controlled trials of vitamin D supplementation. We analyzed data from the D-Health Trial, a randomized, double-blind, placebo-controlled trial. We recruited 21,315 Australians aged 60–84 y and randomized them to 60,000 IU of vitamin D3 or placebo monthly for 5 y. Stool samples were collected from a sample of 835 participants (417 in the placebo and 418 in the vitamin D group) approximately 5 y after randomization. We characterized the gut microbiome using 16S rRNA gene sequencing. We used linear regression to compare alpha diversity indices (i.e. Shannon index (primary outcome), richness, inverse Simpson index), and the ratio of Firmicutes to Bacteroidetes between the two groups. We analyzed between-sample (beta) diversity (i.e. Bray Curtis distance and UniFrac index) using principal coordinate analysis and used PERMANOVA to test for significant clustering according to randomization group. We also assessed the difference in the abundance of the 20 most abundant genera between the two groups using negative binomial regression model with adjustment for multiple testing. Approximately half the participants included in this analysis were women (mean age 69.4 y). Vitamin D supplementation did not alter the Shannon diversity index (mean 3.51 versus 3.52 in the placebo and vitamin D groups, respectively, p = 0.50). Similarly, there was little difference between the groups for other alpha diversity indices, the abundance of different genera, and the Firmicutes-to-Bacteroidetes ratio. We did not observe clustering of bacterial communities according to randomization group. In conlusion, monthly doses of 60,000 IU of vitamin D supplementation for 5 y did not alter the composition of the gut microbiome in older Australians.\n\n1. Introduction\n",
        "text": "Microbiota are communities of microorganisms that co-exist with the host ecosystem in a specific environment. The term microbiome refers to the microbial genome. Current metagenomic sequencing has identified more than 10 million microbial genes in the human gut microbiome [REF]. The gut microbiome plays an important role in health and disease; indeed, it is sometimes referred to as an organ in its own right. Studies have suggested that the gut microbiome influences the interaction between the central and enteric nervous systems (the gut-brain axis) (reviewed by Carabotti et al. [REF]), with effects on health outcomes such as depression. Characteristics of an unhealthy gut microbiome (e.g., dysbiosis) have also been linked to several diseases, such as irritable bowel syndrome, inflammatory bowel disease, diabetes, and cardiovascular disease [REF]. Recent studies suggest a role of the gut microbiome in modulating the functions of the immune system (reviewed by Shreiner et al. and Yamamoto and Jorgensen) [REF], indicating a possible effect on risk or severity of infection. For example, some components or metabolites of gut bacteria (e.g. polysaccharide A and short-chain fatty acids) promote anti-inflammatory responses [REF]. The composition of the gut microbiome is diverse, both within and between individuals, and it is challenging to characterize a healthy gut microbiome. However, it is generally accepted that gut microbial communities with greater diversity, richness, and stability, and a higher relative abundance of species associated with production of short-chain fatty acids are associated with better health outcomes [REF]. There has been increasing interest in the potential effect of vitamin D on the composition of the gut microbiome. The active form of vitamin D (calcitriol, 1,25(OH)2D) binds to macrophages and induces the production of antimicrobial peptides, resulting in bacterial killing [REF]. Rodent studies have shown that vitamin D deficiency or a lack of vitamin D receptor is associated with an increase in both Bacteroidetes and Proteobacteria phyla, which are generally considered less healthy (reviewed by Yamamoto and Jorgensen) [REF]. Vitamin D also helps maintain the physical and functional integrity of the gut mucosal barrier by reducing permeability of epithelial cells and modulating tight-junction proteins [REF], preventing invasion of pathogenic bacterial species.A systematic review of studies in humans found a link between vitamin D and the composition of the gut microbiome as measured by diversity and the abundance of different bacteria [REF]. Subsequent to this review, a non-randomized pre-post interventional study (n = 80) in healthy women found significant increases in gut microbial diversity and the abundance of health-promoting probiotic taxa, Akkermansia and Bifidobacterium, after 12 weeks of supplementation with 50,000 international units (IU) per week of vitamin D.9 We identified only three small randomized controlled trials (RCTs) assessing the effect of vitamin D supplementation on the gut microbiome [REF], two of which were published subsequent to the systematic review. All had a sample size ≤26 and intervention duration ≤16 weeks; one study was conducted in adults with cystic fibrosis [REF], one among overweight or obese adults [REF], and the third in healthy adults [REF]. These studies found some positive effects of vitamin D supplementation or of high versus low dose supplementation on some beneficial bacteria; however, effects on the diversity indices were mixed.In light of the paucity of data from RCTs, we aimed to investigate the effect on the gut microbiome of supplementing older adults with 60,000 IU of vitamin D per month for five y, using a subsample (n = 835) of participants recruited from the large population-based D-Health Trial."
    },
    "2023-37287915_664_en.txt": {
        "title": "Enhanced recovery of postoperative nursing for single-port thoracoscopic surgery in lung cancer patients",
        "prompt": "Abstract: Lung cancer is a common clinical malignant tumor, and the number of new lung cancer patients is increasing year by year. With the advancement of thoracoscopy technology and equipment, the scope of application of minimally invasive surgery has expanded to almost all types of lung cancer resection, making it the mainstream lung cancer resection surgery. Single-port thoracoscopic surgery provides evident advantages in terms of postoperative incision pain since only a single incision is required, and the surgical effect is similar to those of multi-hole thoracoscopic surgery and traditional thoracotomy. Although thoracoscopic surgery can effectively remove tumors, it nevertheless induces variable degrees of stress in lung cancer patients, which eventually limit lung function recovery. Rapid rehabilitation surgery can actively improve the prognosis of patients with different types of cancer and promote early recovery. This article reviews the research progress on rapid rehabilitation nursing in single-port thoracoscopic lung cancer surgery.\n\n1. Introduction\n",
        "text": "Lung cancer is one of the malignant tumors with the highest incidence and mortality rates, making it the greatest threat to human health and life [REF]. Currently, the standard treatment for lung cancer is surgery along with postoperative chemotherapy or radiotherapy [REF]. However, most patients experience varying degrees of deterioration in quality of life after discharge from the hospital due to cardiopulmonary dysfunction, negative psychology, and adverse reactions from chemotherapy and radiotherapy, all of which harm their prognosis after surgery and may even shorten their survival period [REF]. In addition, chemotherapy may trigger hazardous effects in varying degrees [REF]. Patients suffer from long-term pain and are under tremendous psychological stress [REF]. They are physically and psychologically weary. Negative emotions such as anxiety and fear, in turn, affect their treatment efficacy and quality of life. To avoid being trapped in a treatment cycle, it is critical to strengthen nursing intervention during chemotherapy [REF]. Many studies have confirmed that lung cancer patients need additional assistance and care after being discharged from the hospital. Therefore, high-quality, efficient, continuing care should be given to patients to ensure and improve their quality of life after they return home.Surgical resection is currently the most effective and important method for treating early stage lung cancer. Surgical methods are classified as thoracoscopic surgery and thoracoscopic surgery, while thoracoscopic surgery is further classified as multi-hole thoracoscopic surgery and single-port thoracoscopic surgery [REF]. Thoracoscopic surgery has been reported to minimize complications, improve postoperative quality of life, reduce postoperative discomfort, improve lung functions, shorten hospital stay, and hasten the return of patients to normal life [REF]. In the past 20 years, the popularity of minimally invasive techniques, namely video-assisted thoracoscopic surgery (VATS), has been continuously increasing and has been widely used in the treatment of cancer [REF]. Since the first video-assisted thoracoscopic lobectomy was performed in the early 1990s. Previous studies have shown that for early non-small cell lung cancer, video-assisted thoracoscopy has significant benefits over traditional thoracotomy lobectomy, including shorter hospital stay, improved recovery, reduced perioperative complications, and improved long-term survival for selected patients [REF]. Thoracoscopic surgery usually includes 3–4 incisions. With the development of thoracoscopy technology and equipment, thoracoscopic surgery has progressed from being multiple-incision to double-incision surgery, and, finally, to single-incision thoracoscopic surgery, which is also known as single-port thoracoscopic surgery. Table 1 depicts the various lung cancer treatment alternatives.The therapy approaches for lung cancer.In the 1990s, enhanced recovery after surgery (ERAS) was proposed [REF]. This concept optimizes the clinical pathways through evidence-based medicine, reduces traumatic stress, shortens hospital stays, and promotes early organ function recovery [REF]. The concept of ERAS has been progressively expanded in recent years, as have the perioperative nursing measures [REF]. Long-term hunger or dehydration, for example, should be avoided before surgery; carbohydrate beverages should be consumed before surgery, and early feeding and mobilization should be advocated. Unfortunately, there have not been many studies on intraoperative nursing measures that promote rapid postoperative recovery. As a result, this article focuses on the latest advancements in intraoperative nursing related to rapid postoperative recovery, which can promote patients’ early recovery after surgery and reduce the incidence of complications."
    },
    "2023-37287916_1148_en.txt": {
        "title": "Could the tumor-associated microbiota be the new multi-faceted player in the tumor microenvironment?",
        "prompt": "Abstract: Microorganisms have been identified in tumor specimens for over a century. It is only in recent years that tumor-associated microbiota has become a rapidly expanding field. Assessment techniques encompass methods at the frontiers of molecular biology, microbiology, and histology, requiring a transdisciplinary process to carefully decipher this new component of the tumor microenvironment. Due to the low biomass, the study of tumor-associated microbiota poses technical, analytical, biological, and clinical challenges and must be approached as a whole. To date, several studies have begun to shed light on the composition, functions, and clinical relevance of the tumor-associated microbiota. This new piece of the tumor microenvironment puzzle could potentially change the way we think about and treat patients with cancer.\n\n1. Introduction\n",
        "text": "Human body is composed of 30 x 10^12 eukaryote cells governing multiple and coordinated functions [REF]. Although endowed with very diverse and specific functions, the origin and development of eukaryotic cells is one of the most enigmatic processes of evolution. A crucial event in this process was the emergence of the mitochondria, the energy-generating organelles specific to eukaryotic cells. These organelles are thought to have begun to form when a bacterial cell related to the alpha-proteobacteria began living inside an archaeal host cell, of the phylum Lokiarcheaeota [REF]. This event resulted in one organism living inside another (endosymbiosis) and was beneficial to both protagonists, who aligned their interests and evolved in synergy, thus becoming evolutionarily stable [REF]. This prolonged partnership and associated coevolution have thus led to the formation of the eukaryotic cell as we know it today which constitutes the main element of animals, plants, fungi, and protists. This symbiotic event occurred billion years ago [REF] and this is the first of a long series.All plants and animals are home to symbiotic microorganisms whose interactions on the host can be neutral (commensalism), harmful (parasitism) or have beneficial effects (mutualism). These interactions are an integral part of coevolution and dynamically change from one to the other along a continuum depending on exterior and host factors [REF]. In the human body, viruses, bacteria, archaea, nanoarchaea, and eukaryotic microbes (fungi, protozoan, and parasitic helminths) constitute these symbiotic microorganisms and together form the microbiota. The number of microbial cells is roughly equal to the number of human cells (about 40 x 10^12 microbial cells) but their gene count exceeds the human genome’s gene count by ~100-fold [REF]. Host-microorganism interactions occur daily and take place in the skin and genital, respiratory (from the nose to the lung) and gastrointestinal (GI) tracts, which contains, by far, the greatest density and diversity of microorganisms. Long considered to be sterile, microorganisms have also been detected in the urine outside of any clinicopathological situation [REF].In human, ~97% of total microbial cells are bacteria residing in colon and ~2-3% are extracolonic bacteria residing in other body sites (gut, skin, lungs, etc). Therefore, bacteria largely dominate the human microbiome, where only ~0.1-1% are archaea, eukaryotic microbes and viruses in the GI tract [REF]. The microbiota symbiotically contributes to several functions such as trophism, metabolism, barrier function, immunological processes, and signaling to virtually all organs of the body [REF].Beyond the beneficial homeostatic roles, a growing body of evidence suggests that microbes also influence states of health and disease including cancer. Indeed, the International Agency for Research on Cancer classifies 11 microbial agents (7 viruses, 3 parasites, and 1 bacterium) as group 1 human carcinogens and infection-induced cancer accounts for approximately 13% of the global burden of all human cancers [REF]. Furthermore, it is now clear that microorganisms in the gut microbiota may also contribute to the carcinogenesis and prognosis of patients with cancer at the systemic level through mechanisms involving microbiota-derived metabolites, genotoxins and inflammation [REF]. Conversely, tumorigenesis can result from the contraction of anti-tumorigenic bacteria that release anti-proliferative metabolites. An endogenous strain of the mouse microbiota (Faecalibaculum rodentium isolate PB1) and its human counterpart Holdemanella biformis belonging to the Erysipelotrichaceae family, which disappear during the early phases of colorectal tumorigenesis, produce short-fatty acids that block tumor cell proliferation by reducing activation of nuclear factor of activated T cells, cytoplasmic 3 (NFATc3) and calcineurin [REF]. Then, there is growing evidence that micro-organisms, particularly bacteria and fungi residing in the gut, influence the response to chemotherapy, radiotherapy and immunotherapy with immune checkpoint blockers [REF]. Because of its richness, diversity, and the ease of access to samples, the bacterial microbiome of the gut is at the center of research in this area.An oxygen gradient along the longitudinal axis of the gut lumen, from ~6% O2 in the duodenal lumen to ~0.6% O2 in the colonic lumen, is generated and maintained by the host [REF]. Host factors and diet, by setting oxygen and nitrate availability, regulate and direct microbial growth that governs the composition and function of the gut microbiota. Thus, oxygen and nitrate deprivation in the lumen of the large intestine promotes the growth of obligate anaerobic bacteria. In tumors, oxygen levels range from 0.3% to 2.2%, which is 7-fold lower than the normal oxygen level measured in the corresponding normal tissue [REF]. In addition, tumors typically have aberrant, leaky, and irregular vasculature and create a gradient of small molecules (aspartate, ribose etc.) that act as chemoattractants for bacteria [REF]. Tumors consist of an asynchronous and heterogeneous aggregate of malignant cells and host cells, such as immune and stromal cells entangled in an extracellular matrix. Tumor-associated immune cells, fibroblasts and endothelial cells associated are each phenotypically and functionally diverse; therefore, some can promote tumor progression whereas others can exhibit antitumor activity. However, when viewed as a whole, a tumor is an immunosuppressed structure where bacteria might more easily replicate without the clearance mechanisms of macrophages and neutrophils, which normally serve to eliminate them. Altogether, these characteristic (deoxygenation, chemotaxis, chaotic vasculature, immunosuppression) proper to the tumor microenvironment (TME) provide a favorable and attractive niche for microbial growth. Microorganisms have been identified in patient tumors for more than a century, although the magnitude of these microorganisms, their role and their interaction with components of the TME have been incompletely appreciated so far, mainly due to technological limitations.After a brief description of the composition of the tumor-associated microbiota, the potential origin, assessment techniques and clinical relevance of this potential new player in the TME will be discussed."
    },
    "2023-37287967_1175_en.txt": {
        "title": "cGAMP the travelling messenger",
        "prompt": "Abstract: 2’3’-cGAMP is a key molecule in the cGAS-STING pathway. This cyclic dinucleotide is produced by the cytosolic DNA sensor cGAS in response to the presence of aberrant dsDNA in the cytoplasm which is associated with microbial invasion or cellular damage. 2’3’-cGAMP acts as a second messenger and activates STING, the central hub of DNA sensing, to induce type-I interferons and pro-inflammatory cytokines necessary for responses against infection, cancer or cellular stress. Classically, detection of pathogens or danger by pattern recognition receptors (PRR) was thought to signal and induce the production of interferon and pro-inflammatory cytokines in the cell where sensing occurred. These interferon and cytokines then signal in both an autocrine and paracrine manner to induce responses in neighboring cells. Deviating from this dogma, recent studies have identified multiple mechanisms by which 2’3’-cGAMP can travel to neighboring cells where it activates STING independent of DNA sensing by cGAS. This observation is of great importance, as the cGAS-STING pathway is involved in immune responses against microbial invaders and cancer while its dysregulation drives the pathology of a wide range of inflammatory diseases to which antagonists have been elusive. In this review, we describe the fast-paced discoveries of the mechanisms by which 2’3’-cGAMP can be transported. We further highlight the diseases where they are important and detail how this change in perspective can be applied to vaccine design, cancer immunotherapies and treatment of cGAS-STING associated disease.\n\n1. Introduction\n",
        "text": "Cells utilize an arsenal of pattern recognition receptors (PRRs) which recognize structures within invading microbes termed pathogen associated molecular patterns (PAMPs). Viral genomic nucleic acids are a preeminent PAMP during infection which trigger PRR activation, leading to the production of type-I interferons (IFN) and proinflammatory cytokines by infected cells. IFN then acts in both an autocrine and paracrine manner to induce the expression of interferon-stimulated genes (ISGs) in neighboring cells. These ISGs act as restriction factors in bystander cells to limit pathogen replication and spread. Alongside the direct antimicrobial activity of ISGs, the cytokines produced are also crucial to initiate adaptive immune responses. Since its discovery in 2013, the PRR detecting cytosolic double-stranded DNA (dsDNA) - cyclic guanosine-monophosphate-adenosine-monophosphate synthase (cGAS) - has been a focal point within the field. cGAS detects dsDNA indiscriminate of whether it is host or pathogen derived. Instead of detecting structural differences between pathogen and host like classical PRRs (e.g. TLR9), it identifies the presence of dsDNA in the cytosol as a danger signal. Therefore, exposure of “self” dsDNA due to cellular disruption can also be detected and is a known damage associated molecular pattern (DAMP) that can lead to inflammation in sterile settings. For example, the rare but debilitating interferonopathies - Aicardi-Goutières syndrome (AGS), STING associated vasculopathy with onset in infancy (SAVI) and COPA Syndrome - can be caused by chronic activation of the cGAS-STING pathway [REF]. Moreover, in a wide range of more prevalent autoimmune and inflammatory disorders such as systemic lupus erythematosus, polyarthritis and Parkinson’s disease, STING signaling is thought to contribute towards pathology [REF]. These diseases are summarized in Figure 1A. The cGAS-STING pathway is therefore of great therapeutic interest: agonists are sought after for use as adjuvant in vaccines or for cancer immunotherapy; while antagonists could be developed as treatment for inflammatory or auto-immune diseases.The cGAS-STING pathway and disease. (A) Schematic showing diseases where STING signaling is thought to contribute to pathology. (B) The presence of mislocalised or pathogen associated dsDNA in the cytoplasm is detected by cGAS. Upon DNA binding, cGAS undergoes phase separation and generates the cyclic dinucleotide 2’3’-cGAMP required by the central hub of DNA sensing – STING – to translocate from the ER towards ERGIC. Here, STING recruits TBK1 to phosphorylate the master transcriptional regulators of innate immune activation - IRF3 and NF-κB. This results in the production of proinflammatory cytokines, primarily type I IFNs but also IL-6 and TNF-α.Mechanistically, when cGAS binds to dsDNA, its N-terminal domain polymerises, inducing phase separation (a phenomenon where liquids separate from each other forming distinct membrane-less compartments), and it produces the small cyclic di-nucleotide 2’,3’-cyclic-GMP-AMP (hereafter called cGAMP). cGAMP then acts as a second messenger and binds to the central hub of DNA sensing - STING - which translocates from the endoplasmic reticulum (ER) to the Golgi, where it recruits TANK-binding kinase 1 (TBK1). TBK1, in turn, activates the master regulators of innate immune activation: IRF3 and NFkB. IRF3 induces production of type-I IFN while NFkB drives expression of pro-inflammatory cytokines most notably IL-6 and TNF-α (Figure 1B) [REF].Studies published over the last decade challenged the classical model of PRR signaling. Rather than cGAS detecting dsDNA and inducing IFN and cytokine production in the cell where sensing occurred, the second messenger cGAMP can travel to neighboring cells where it activates STING and production of IFN and cytokines without the need for a PRR. Over the last decade, several studies have now described multiple mechanisms by which cGAMP can move between cells. It can travel freely through GAP junctions in juxtaposed cells [REF]. Moreover, viral particles have been shown to incorporate cGAMP derived from the viral producing cells, thereby transmitting cGAMP to the next infected cell [REF]. Both of these mechanisms effectively link the cytosol of one cell to the next and so cGAMP remains topologically contained within a compartment derived from the cell cytosol. More recently a role for cGAMP in the extracellular space has been considered. Exogenous cGAMP, and other similar cyclic dinucleotides (CDNs) produced by bacteria as signaling molecules, were known to trigger IFN-β production by cells in vitro and reduce tumor growth by direct injection in vivo [REF]. However, it remained unclear how these CDNs entered the cytosol to activate STING. This changed between 2019 and 2021 when, in quick succession, five membrane transport proteins were shown to provide a route for CDNs to cross the cell membrane - SLC19A1, SLC46A2, P2X7, LRRC8A:C/E and ABCC1 [REF]. In addition, the antimicrobial defense peptide LL-37 has been reported to bind and chaperone cGAMP across the cell membrane [REF]. These proteins are separated into channels (SLC19A1, SLC46A2 and LRRC8A:C/E), transporters (ABCC1) and pores (P2X7 and LL-37) depending on their mode of action. For simplicity, we will collectively refer to them as cGAMP conduits.The study of how cGAMP is a second messenger that travels between cells constitutes a new field that has recently exploded. In this review, we detail and discuss the current knowledge of the different mechanisms by which cGAMP is transferred between cells summarized in Figure 2. We further highlight the diseases where these mechanisms play a role and the relevance this has to pharmaceutical design suggesting novel targets for innovative therapies.Extracellular 2’3’-cGAMP transport. Schematic showing the mechanisms by which 2’3’-cGAMP can be transferred between cells. (A) Within viral particles. (B) Through gap junctions. (C) Import and export through LRRC8A:C heterodimers which are activated by hypotonicity and Sphingosine-1-phosphate (S1P). (D) Export through ABCC1. (E) Transport through the ATP gated pore P2X7. (F) Import through SLC19A1. (G) Import through SLC46A2. (H) 2’3’-cGAMP shuttling by LL-37. (I) 2’3’-cGAMP hydrolysis by ENPP1."
    },
    "2023-37288152_1333_en.txt": {
        "title": "Artificial Intelligence in Medicine and Dentistry",
        "prompt": "Abstract: Artificial intelligence has been applied in various fields throughout history, but its integration into daily life is more recent. The first applications of AI were primarily in academia and government research institutions, but as technology has advanced, AI has also been applied in industry, commerce, medicine and dentistry.Considering that the possibilities of applying artificial intelligence are developing rapidly and that this field is one of the areas with the greatest increase in the number of newly published articles, the aim of this paper was to provide an overview of the literature and to give an insight into the possibilities of applying artificial intelligence in medicine and dentistry. In addition, the aim was to discuss its advantages and disadvantages.The possibilities of applying artificial intelligence to medicine and dentistry are just being discovered. Artificial intelligence will greatly contribute to developments in medicine and dentistry, as it is a tool that enables development and progress, especially in terms of personalized healthcare that will lead to much better treatment outcomes.\n\n1. Introduction\n",
        "text": "Artificial intelligence has been applied in various fields throughout history, but its integration into daily life is more recent. The first applications of AI were primarily in academia and government research institutions, but as technology has advanced, AI has also been applied in industry, commerce, medicine and dentistry.There is probably no human being in the world that has not, at some point in his or her life, become aware of the limits of his or her physical and/or mental abilities. In the past, these limits often meant life or death because there was no way to go beyond one's limits and simply improve one's abilities. The devices and machines invented and manufactured by man have undoubtedly made everyday life easier, especially in the physical sense. However, to combine the endurance and reliability of machines with intelligence and consciousness as human characteristics was the desire of many inventors, researchers and philosophers who tried to describe the process of human thinking as a mechanical manipulation of symbols. A thinking artificial creation, i.e., a machine that has a kind of meta-consciousness and thinks like a human, is something that captures the imagination. The theoretical foundations of what we now call artificial intelligence (AI) were laid by Alan Turing, Claude E. Shannon, and Norbert Wiener [REF]. Alan M. Turing (1912 – 1954) is an English mathematician who is considered the father of theoretical computer science; Claude E. Shannon (1916 – 2001) is an American mathematician who is known as the father of the so-called \"information theory\"; and Norbert Wiener (1894 - 1964) is an American mathematician and philosopher, the founder of cybernetics. They are responsible for the concept of creating intelligent machines [REF]. However, the concept of AI itself is somewhat younger. The concept of AI dates back to 1956, when a group of researchers participating in an eight-week Dartmouth Summer Research Project on Artificial Intelligence at Dartmouth College in New Hampshire, USA, proposed a research project and set the goal of creating \"thinking machines\" that could mimic human intelligence and behavior. This is widely regarded as the beginning of AI as a formal field of study [REF].In order to better understand the concept of artificial intelligence, it is necessary to clarify the difference between artificial intelligence, deep learning, machine learning and data science, Figure 1. Artificial intelligence, deep learning, machine learning, and data science are related but distinct fields. Artificial intelligence is the broadest field, of which machine learning and deep learning are subsets. Data science uses techniques from all of these fields to gain insights and knowledge from data. Artificial intelligence is a broad field that encompasses a range of techniques and methods aimed at creating intelligent machines that can perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision making, and natural language processing. Artificial intelligence can be divided into several branches, including expert systems, robotics, and natural language processing, to name a few. Deep learning is a subfield of artificial intelligence that uses neural networks inspired by the structure of the human brain to learn from large amounts of data. Deep learning algorithms can automatically identify and extract features from raw data such as images, sounds, and text and use them to make predictions or decisions. Examples of deep learning applications include image recognition, speech recognition, and natural language processing. Machine learning is a subfield of artificial intelligence that focuses on developing algorithms and statistical models that allow computers to learn from data without being explicitly programmed. Machine learning techniques can be supervised (the algorithm learns from labeled data), unsupervised (the algorithm learns from unlabeled data), or semi-supervised (the algorithm learns from a combination of labeled and unlabeled data). Applications of machine learning include recommendation systems, fraud detection, and predictive modeling. Data science is an interdisciplinary field that combines statistical and computational techniques with domain-specific knowledge to gain insights and knowledge from data. Data science encompasses a range of activities including data acquisition, cleaning and pre-processing, exploratory data analysis, statistical modeling, and machine learning. Data science is used in a variety of fields, including healthcare, finance, social media, and e-commerce.Artificial intelligence, deep learning, machine learning and data scienceArtificial intelligence has been applied in various fields throughout history, but its integration into daily life is more recent. The first applications of AI were primarily in academia and government research institutions, but as technology has advanced, AI has also been applied in industry and commerce [REF]. One of the earliest examples of AI in everyday life was the use of expert systems in the 1980s and 1990s. These were computer programs that could mimic the decision-making abilities of a human expert in a particular field, such as medicine or finance. In the 21st century, AI has been increasingly integrated into a variety of consumer products and services. Examples of AI include virtual personal assistants such as Apple's Siri and Amazon's Alexa, and recommendation systems used by companies such as Netflix and Amazon to personalize their customers' experiences. AI is also being used in areas such as self-driving cars, healthcare and finance.Today, we can distinguish three generations of AI [REF]. The first generation is the so-called artificial narrow intelligence (ANI), the second is the artificial general intelligence (AGI) and the third, currently the most advanced generation, is the artificial superintelligence (ASI). From today's perspective, we can safely say that the first generation is already ubiquitous. When we talk about the first generation of AI, we mean, among other things, the facial recognition and tagging technology used by Facebook, for example, virtual voice assistants in our cell phones such as Siri, Alexa or Bixby, the technology developed by Tesla and Google for self-driving cars, and much more. The second generation of AI is expected to think, plan, and solve problems and tasks on its own. The third generation of artificial superintelligence will be a truly self-aware system that could make humans and their input obsolete in certain situations. Although it is a fictional AI-based system on AI, HAL 9000 from Arthur C. Clarke's A Space Odyssey is perhaps the most vivid representation of the capabilities of third-generation artificial intelligence at this time.Considering that the possibilities of applying artificial intelligence are developing rapidly and that, according to PubMed data, this field is one of the areas with the greatest growth in the number of newly published articles [REF], the aim of this paper was to provide an overview of the literature and to give an insight into the possibilities of applying artificial intelligence in medicine and dentistry. The aim was to discuss in details its advantages and disadvantages. For this purpose, the databases Pubmed and Web of science were searched. The keywords artificial intelligence, medicine, healthcare, dentistry, deep learning, and machine learning were used."
    },
    "2023-37288171_799_en.txt": {
        "title": "Digital interventions to moderate alcohol consumption in young people: a Cancer Prevention Europe overview of systematic reviews",
        "prompt": "Abstract: Strategies to reduce alcohol consumption would contribute to substantial health benefits in the population, including reducing cancer risk. The increasing accessibility and applicability of digital technologies make these powerful tools suitable to facilitate changes in behaviour in young people which could then translate into both immediate and long-term improvements to public health.We conducted a review of systematic reviews to assess the available evidence on digital interventions aimed at reducing alcohol consumption in sub-populations of young people [school-aged children, college/university students, young adults only (over 18 years) and both adolescent and young adults (<25 years)].Searches were conducted across relevant databases including KSR Evidence, Cochrane Database of Systematic Reviews (CDSR) and Database of Abstracts of Reviews of Effects (DARE). Records were independently screened by title and abstract and those that met inclusion criteria were obtained for full text screening by two reviewers. Risk of bias (RoB) was assessed with the ROBIS checklist. We employed a narrative analysis. Twenty-seven systematic reviews were included that addressed relevant interventions in one or more of the sub-populations, but those reviews were mostly assessed as low quality. Definitions of “digital intervention” greatly varied across systematic reviews. Available evidence was limited both by sub-population and type of intervention. No reviews reported cancer incidence or influence on cancer related outcomes. In school-aged children eHealth multiple health behaviour change interventions delivered through a variety of digital methods were not effective in preventing or reducing alcohol consumption with no effect on the prevalence of alcohol use [Odds Ratio (OR) = 1.13, 95% CI: 0.95–1.36, review rated low RoB, minimal heterogeneity]. While in adolescents and/or young adults who were identified as risky drinkers, the use of computer or mobile device-based interventions resulted in reduced alcohol consumption when comparing the digital intervention with no/minimal intervention (−13.4 g/week, 95% CI: −19.3 to −7.6, review rated low RoB, moderate to substantial heterogeneity).In University/College students, a range of E-interventions reduced the number of drinks consumed per week compared to assessment only controls although the overall effect was small [standardised mean difference (SMD): −0.15, 95% CI: −0.21 to −0.09]. Web-based personalised feedback interventions demonstrated a small to medium effect on alcohol consumption (SMD: −0.19, 95% CI: −0.27 to −0.11) (review rated high RoB, minimal heterogeneity). In risky drinkers, stand-alone Computerized interventions reduced short (SMD: −0.17, 95% CI: −0.27 to −0.08) and long term (SMD: −0.17, 95% CI: −0.30 to −0.04) alcohol consumption compared to no intervention, while a small effect (SMD: −0.15, 95% CI: −0.25 to −0.06) in favour of computerised assessment and feedback vs. assessment only was observed. No short-term (SMD: −0.10, 95% CI: −0.30 to 0.11) or long-term effect (SMD: −0.11, 95% CI: −0.53 to 0.32) was demonstrated for computerised brief interventions when compared to counsellor based interventions (review rated low RoB, minimal to considerable heterogeneity). In young adults and adolescents, SMS-based interventions did not significantly reduce the quantity of drinks per occasion from baseline (SMD: 0.28, 95% CI: −0.02 to 0.58) or the average number of standard glasses per week (SMD: −0.05, 95% CI: −0.15 to 0.05) but increased the risk of binge drinking episodes (OR = 2.45, 95% CI: 1.32–4.53, review rated high RoB; minimal to substantial heterogeneity). For all results, interpretation has limitations in terms of risk of bias and heterogeneity.Limited evidence suggests some potential for digital interventions, particularly those with feedback, in reducing alcohol consumption in certain sub-populations of younger people. However, this effect is often small, inconsistent or diminishes when only methodologically robust evidence is considered. There is no systematic review evidence that digital interventions reduce cancer incidence through alcohol moderation in young people. To reduce alcohol consumption, a major cancer risk factor, further methodologically robust research is warranted to explore the full potential of digital interventions and to form the basis of evidence based public health initiatives.\n\n1. Introduction\n",
        "text": "Cancer is a leading cause of mortality in all European countries and the impact on individual health and wider society is significant. Studies have shown that nearly 40% of cancer cases are related to known modifiable risk factors, and therefore preventable [REF]. These known main risk factors include (but are not limited to) tobacco and excessive alcohol consumption, consequences of an unhealthy diet, being overweight and being sedentary with insufficient physical activity [REF].Alcohol consumption remains as one of the four leading causes of premature death, and the second leading cause of premature mortality in the World health Organisation (WHO) European region [REF] (Supplementary File S4 for full list of abbreviations). It is well established that there exists a direct relationship between consumption of alcohol and the development of several cancers, such as those of the oral cavity, oropharynx, oesophagus, larynx and liver [REF]. Although the potential negative health effects e.g., increased risk of liver disease, cardiovascular disease, road accidents of alcohol are widely known, it is less well known that it is a risk factor for cancer, and that those who routinely indulge in heavy drinking are more at risk. Younger people in adolescence and during early adulthood are particularly vulnerable to the impact of alcohol consumption in general. Effective strategies to inform and educate younger people about the risks of alcohol consumption, may have considerable positive impact on both current and future health problems, including cancer incidence.A recent WHO report stated that it is expected that most younger people tend to begin drinking alcohol between the ages of 12–16 [REF] with drinking behaviours during adolescence associated with a multitude of physical, psychological, and social problems that can persist into later life. Drinking behaviours amongst university and college students has also been shown to be highly concerning, with risky drinking being common place [REF] and heavy drinking being reported in university students in high income countries [REF]. Increased consumption of alcohol in younger people has been linked to increased alcohol consumption in later adulthood [REF] which in turn has also has been linked to increased risk of cancer [REF].By promoting health-conscious behaviours, and increasing risk awareness around alcohol consumption, young people will be more informed and healthy lifestyle choices can be made. This could lead to substantial public health improvement with reductions in health and social problems associated with alcohol consumption, both now and in the future The increasing popularity, accessibility, and multi-functional use of digital technologies (computer, mobile phone, tablet etc.) make these potential tools to help facilitate communication, education, and risk awareness to elicit protective changes in behaviour, especially among children, adolescents and young adults, who are generally more familiar with new technologies throughout their formative years. young people aged between 10 and 24 years who are approaching adolescence and early adulthood may therefore be particularly suitable as recipients for such digitally delivered interventions. Mobile Health (mHealth) initiatives for instance, have rapidly expanded and are being utilised to deliver public health interventions, especially in the younger population who conduct many of their daily activities using smartphones and have been termed the “phono-sapiens” [REF]. The flexibility of such mHealth platforms provide opportunities for public health specialists to target a large number of people and also monitor people's behaviour in “real-time” [REF], and further emphasise the potential of digital technology in healthcare delivery It has previously been suggested that by addressing interventions for those who demonstrate the riskiest drinking behaviours, the greatest outcomes at the population level can be realised [REF] and we are conscious that such digital technologies may have considerable impact in helping to moderate alcohol consumption in younger people.Considering the negative public health and social impact that alcohol has, including increased risk of alcohol related cancers; alongside the prevalence of hazardous and harmful drinking behaviours in younger populations, we were interested in examining the impact and accessibility of emerging digital technology to moderate drinking behaviours in those younger populations. We reviewed the available systematic review literature with the objectives to ascertain (1) are digital interventions aimed at young people effective in addressing alcohol consumption? and (2) What is the quality and strength of the systematic review evidence?"
    },
    "2023-37288404_557_en.txt": {
        "title": "A study of the effects of blended learning on university students’ critical thinking: A systematic review",
        "prompt": "Abstract: One of the basic, constructive needs of humans, which plays a major part in their development is critical thinking. As education is one of the factors in shaping individuals’ critical thinking, the present study addresses the effects of blended learning and its subcategories on university students’ critical thinking (and its subcategories). The present article is a review study. Data were collected using valid search engines and databases. The keywords which were used included blended learning, integrated learning, blended training, integrated training, critical thinking, critical thinking disposition, and critical thinking skills, as well as the subcategories of blended learning, that is, the flex model, the self-blended model, the enriched virtual model, and the rotation model and its subcategories (the station rotation model, the lab rotation model, the flipped classroom model, and the individual rotation model). The results of 14 sources, out of the selected 15 sources, showed that blended learning and its subcategories, that is, the flex model, the self-blended model, the enriched virtual model, and the rotation model and its subcategories contribute to university students’ critical thinking of disposition and skill. One of the essential skills which must be given more serious attention in learning in the twenty-first century is critical thinking. Having the benefits of both lecturing and e-learning, blended learning is a more effective and practical method for promoting critical thinking in university students.\n\n1. Introduction\n",
        "text": "In today's competitive world, critical thinking is one of the abilities which all individuals must have.[REF] Critical thinking is a vast, comprehensive process that begins with a problem and continues until a solution is found.[REF] Regarded as one of the primary skills in the twenty-first century,[REF] critical thinking is an essential competence in all professional and academic fields.[REF] Critical thinking consists of the two domains of disposition and skill. The importance of creativity is high in order to provide innovative solutions for decision-making and problem solving.[REF] A critical thinker can accurately analyze data to arrive at correct conclusions or use alternative methods to solve problems.[REF] Thus, development and evaluation of critical thinking are significant in teaching and learning.[REF] However, due to information overload and quick advances in technology, the goal of education has moved toward mere transfer of information at the cost of raising intelligent and creative individuals.[REF] Karakoc Najafi et al. concluded in their study that critical thinking skills should be emphasized in university education.[REF]The integration of technology into face-to-face learning has raised great interest. Due to its efficacy in allowing for flexible, timely, and continuing learning, blended learning is regarded as the most effective and popular approach to learning.[REF] As blended learning combines classroom learning with online learning,[REF] it has the benefits of both conventional learning and electronic learning.[REF] Blended learning is regarded as a practical learning model for increasing the skills of learners in the twenty-first century.[REF] This type of learning transforms students from passive learners to active learners who seek knowledge.[REF] Blended learning consists of the subcategories of the flex model, self-blended model, enriched virtual model, and rotation model and its subcategories.[REF] These modern, dynamic methods of learning promote students’ ability to investigate and identify their own learning needs, to perform critical thinking, to play an active role in their learning process, to have better initiative in different situations, and to improve their problem solving skills.[REF] According to Wahyuni, blended learning had a significant impact on improving students’ critical thinking skills.[REF] However, the results of the study of Hajrezayi et al. showed that the contribution of blended learning to students’ critical thinking was not significant.[REF] Harrington et al. suggested that there was need for more research into the effects of the flipped classroom model (one of the subcategories of blended learning) on students’ critical thinking and problem-solving skills.[REF] Teaching critical thinking is the most important effort that should be made in nursing education.[REF] Some experts believe that education is just teaching thinking to the learner. On the other hand, there is a need to review current educational strategies and making more use of active learning strategies has been repeatedly emphasized.[REF] Accordingly, in the present study, the researchers conducted an extensive systematic review of previous studies of the effects of blended learning and its subcategories on critical thinking (and its subcategories) which are among the essential skills in the twenty-first century."
    },
    "2023-37288405_624_en.txt": {
        "title": "Prevalence of viral hepatitis infection in India: A systematic review and meta-analysis",
        "prompt": "Abstract: Nowadays, Viral Hepatitis can be comparable to the big three communicable diseases: tuberculosis, HIV/AIDS, and malarial infections. The main purpose of this study was to summarize the prevalence of viral Hepatitis in India from peer-reviewed articles published from February 2000 to February 2021.We conducted a systematic search on Science Direct, Scopus, Medline, PubMed, Web of Science, Google Scholar, and other open access journals. We evaluated all relevant papers that looked into the prevalence of viral Hepatitis systematically. Finally, 28 studies on viral Hepatitis published from February 2000 to February 2021 have been selected. These studies have been conducted across the northern, southern, central, eastern, and western regions of India.Twenty-eight full-text publications were obtained and evaluated consisting of 45,608 research participants. Hepatitis A was found to range from 2.1% to 52.5%. Hepatitis B was found in a wide range of individuals, ranging from 0.87% to 21.4% of the population. Hepatitis C was found to range from 0.57% to 53.7%. The majority of the children were affected by hepatitis A, and 47.4% of third-trimester pregnant mothers were affected by hepatitis E. Diabetes, hospital admission, history of jaundice, history of surgeries, and heterosexual contact were the leading modes of acquiring HBV and HCV infections. As a result of its great magnitude, this disease poses a severe threat to the national healthcare system.Effective public health measures are urgently needed to minimize the burden of viral Hepatitis and eliminate the disease.\n\n1. Introduction\n",
        "text": "Viral Hepatitis is one of the major global public health issues, and every year millions of individuals suffer from it.[REF] Viral Hepatitis caused 1.34 million fatalities worldwide in 2015, with the majority of viral Hepatitis deaths owing to chronic liver disease or primary liver cancer (mortality due to cirrhosis: 720,000; hepatocellular carcinoma (HCC): 470,000). The death rate from viral Hepatitis has continued to climb over time.[REF] Nowadays, viral Hepatitis can be comparable to the big three communicable diseases: tuberculosis, HIV/AIDS, and malarial infections.[REF] Viral Hepatitis can be caused by any of the known five hepatotropic viruses, namely Hepatitis A Virus (HAV), Hepatitis B Virus (HBV), Hepatitis C Virus (HCV), Hepatitis D Virus (HDV), and Hepatitis E Virus (HEV).[REF] Based on the 2017 global Hepatitis report, a large number of individuals do not have access to screening and treatment for Hepatitis, consequently leading to Chronic Liver Disease (CLD) and cancer mortality due to Hepatitis.[REF] The World Health Organization (WHO) is urging countries to take quick action to increase the knowledge, diagnosis/testing, and treatment services for Hepatitis. According to a WHO report, one in twenty individuals are living with viral Hepatitis.HAV and HEV are enterically transmitted pathogens that produce sporadic infections as well as outbreaks of Acute Viral Hepatitis (AVH).[REF] HAV is a single-stranded RNA virus and it is mostly transmitted through the fecal-oral pathway.[REF] In India, HAV infection is common among children, and it generally leads to mild anicteric Hepatitis. The majority of children under the age of two (85%) and nearly half of those aged two to five years (50%) have nonspecific symptoms and are usually anicteric.[REF] HAV infection disease severity increases with the patient's age and the prevalence of existing chronic liver diseases.[REF]Chronic HBV affects 240 million persons worldwide and chronic HCV affects 130–150 million.[REF] Chronic Hepatitis affects approximately 400 million individuals worldwide, with the Asia-Pacific area serving as the hub of the epidemic.[REF] HBV and HCV are mainly transmitted through the parenteral route and are known to cause chronic Hepatitis, which can progress to serious consequences such as liver cirrhosis and HCC.[REF]HBV and HCV infections are more alike, which include distribution, hepatotropism, disease transmission, and at last leading to chronic infection which may end in liver cirrhosis and HCC.[REF] HBV and HCV infections do not have a standard of care due to the individual category of infected subjects so it is difficult to cure.[REF] Thus, studying the magnitude becomes a crucial component in prevention of HBV and HCV. The current review aimed to summarize the prevalence of viral Hepatitis in India from peer-reviewed articles published from February 2000 to February 2021.To study the overall prevalence of viral Hepatitis in India from peer-reviewed articles published from February 2000 to February 2021.To review the determinant factors of viral Hepatitis in the Indian region with the included article.Hepatitis: According to Centers for Disease Control and Prevention (CDC) “Hepatitis is an inflammatory disease of the liver caused.”Funnel plot: “A funnel plot is a simple scatter plot of the intervention effect estimates from individual studies against some measure of each study's size or precision.”[REF] Funnel plot is a graphical representation used to detect systematic heterogeneity and publication bias."
    },
    "2023-37289839_887_en.txt": {
        "title": "Transactional sex among women in Sub-Saharan Africa: A systematic review and meta-analysisDecision Letter 0Author response to Decision Letter 0Decision Letter 1Author response to Decision Letter 1Decision Letter 2Author response to Decision Letter 2Decision Letter 3Acceptance letter",
        "prompt": "Abstract: Transactional sex is casual sex between two people to receive material incentives in exchange for sexual favors. Transactional sex is associated with negative consequences, which increase the risk of sexually transmitted diseases, including HIV/AIDS, unintended pregnancy, unsafe abortion, and physiological trauma. In Sub-Saharan Africa, several primary studies have been conducted in various countries to examine the prevalence and associated factors of transactional sex among women. These studies had great discrepancies and inconsistent results. Hence, this systematic review and meta-analysis aimed to synthesize the pooled prevalence of the practice of transactional sex among women and its associated factors in Sub-Saharan Africa.Data source: PubMed, Google Scholar, HINARI, the Cochrane Library, and grey literature were searched from March 6 to April 24, 2022, and included studies conducted from 2000 to 2022. The pooled prevalence of transactional sex and associated factors was estimated using Random Effect Model. Stata (version 16.0) was used to analyze the data. The I-squared statistic, a funnel plot, and Egger’s test were used to check for heterogeneity and publication bias, respectively. A subgroup analysis was done based on the study years, source of data, sample sizes, and geographical location.The pooled prevalence of transactional sex among women in Sub-Saharan Africa was 12.55% (9.59%–15.52%). Early sexual debut (OR = 2.58, 95% CI: 1.56, 4.27), substance abuse (OR = 4.62, 95% CI: 2.62, 8.08), history of sexual experience (OR = 4.87, 95% CI: 2.37, 10.02), physical violence abuse (OR = 6.70, 95% CI: 3.32, 13.53), orphanhood (OR = 2.10, 95% CI: 1.27, 3.47), and sexual violence abuse (OR = 3.76, 95% CI: 1.08, 13.05) were significantly associated with transactional sex.The prevalence of transactional sex among women in sub-Saharan Africa was high. Alcohol consumption, substance abuse, early sex debuts, having a history of sexual experiences, physical violence, and sexual violence increased the practice of transactional sex.\n\n1. Introduction\n",
        "text": "Transactional sex is casual sex between two people to receive material incentives in exchange for sexual favors. Transactional sex is associated with negative consequences, which increase the risk of sexually transmitted diseases, including HIV/AIDS, unintended pregnancy, unsafe abortion, and physiological trauma. In Sub-Saharan Africa, several primary studies have been conducted in various countries to examine the prevalence and associated factors of transactional sex among women. These studies had great discrepancies and inconsistent results. Hence, this systematic review and meta-analysis aimed to synthesize the pooled prevalence of the practice of transactional sex among women and its associated factors in Sub-Saharan Africa.Transactional sex is defined as a sexual act (s) that is structured by the implicit assumption that sex is exchanged for a variety of instrumental supports such as educational expenses, transportation, a place to sleep, clothing, material items, or money. It reveals that socioeconomic factors have a great role in establishing exchange-based sexual relationships in many countries with high HIV prevalence. Commonly, in many countries, men provide and women receive material rewards [REF].Globally, an estimated 36.7 million individuals worldwide were infected with the Hunan Immunodeficiency Virus (HIV) epidemic. Young people in sub-Saharan Africa (SSA) are more threatened by the HIV epidemic than young people elsewhere. In total, there were 2.1 million new HIV infections worldwide, with 1.1 million occurring in sub-Saharan Africa [REF]. HIV infection deaths mainly affect the young and productive segments of the community. Among newly infected people in SSA, 40% belong to the age group of 15–24 years, and more than 60% of these infections occurred among young girls [REF].Significant proportions of females have multiple concurrent sexual relationships and engage in risky sex. Because of their risky sexual *practices*, the girls and their sexual partners, including schoolmates, are at risk of HIV infection and other sexually transmitted infections (STIs) [REF]. Among HIV-infected young people in the world, 63% lived in sub-Saharan Africa, and among these, 59% were female. Unprotected sex also puts women at risk of unintended pregnancy, which leads to unsafe abortions [REF]. Sex motivated by financial gain is a serious public health issue, particularly in sub-Saharan Africa [REF].Assessing transactional relationships is still an important aspect of HIV prevention initiatives [REF]. Individual behaviours that harm people’s chances of acquiring sexually transmitted diseases (STDs) and unwanted pregnancies were identified. Among these identified risk behaviours, transactional sex (sex in exchange for money, gifts, benefits, or other monetary rewards) is the main one [REF]. The infection rate of STIs, including HIV, among young women aged 15 to 24 years old, is greater than that of young males (3.6 to 1 ratio) [REF]. Early sexual activity, early pregnancy, unsafe abortions, and the increase in HIV infections have become major concerns in sub-Saharan Africa [REF]. Unwanted pregnancy is the cause of school dropout in girls. School dropout is an additional barrier that women, severely handicapped by parenthood, must face to overcome the longer-term impacts of childbearing [REF].Transactional sex involves engaging in sex for money or gifts in order to increase one’s long-term life chances [REF].In sub-Saharan Africa, cultural and social norms, gender inequality, and harmful traditional *practices*, combined with a lack of access to reproductive health services, a high unemployment rate, and young females from lower-income families, expose young people to a variety of social and economic challenges and encourage them to engage in transactional sex [REF].Transactional sex occurred at a rate of 2% in Niger, 14% in Benin, 14% in Kenya, 27% in Zambia, 31% in Uganda, 5% in Cameroon, and 85–90% in Uganda among sexually active girls who reported ever engaging in sexual relations in exchange for money or gifts in the last 12 months [REF]. Transactional sex is associated with HIV risk factors or behaviours including alcohol use [REF], sexual or physical violence or abuse [REF], inconsistent condom use [REF] and multiple partners [REF].In Sub-Saharan Africa, several primary studies have been conducted in various countries to examine the prevalence and associated factors of transactional sex among women. These studies had great discrepancies and inconsistent results across countries. Hence, this systematic review and meta-analysis aimed to synthesise the pooled prevalence of the *practice* of transactional sex among women and its associated factors in sub-Saharan Africa."
    },
    "2023-37292265_920_en.txt": {
        "title": "Nanostructured surface plasmon resonance sensors: Toward narrow linewidths",
        "prompt": "Abstract: Surface plasmon resonance sensors have found wide applications in optical sensing field due to their excellent sensitivity to the slight refractive index change of surrounding medium. However, the intrinsically high optical losses in metals make it nontrivial to obtain narrow resonance spectra, which greatly limits the performance of surface plasmon resonance sensors. This review first introduces the influence factors of plasmon linewidths of metallic nanostructures. Then, various approaches to achieve narrow resonance linewidths are summarized, including the fabrication of nanostructured surface plasmon resonance sensors supporting surface lattice resonance/plasmonic Fano resonance or coupling with a photonic cavity, the preparation of surface plasmon resonance sensors with ultra-narrow resonators, as well as strategies such as platform-induced modification, alternating different dielectric layers, and the coupling with whispering-gallery-modes. Lastly, the applications and some existing challenges of surface plasmon resonance sensors are discussed. This review aims to provide guidance for the further development of nanostructured surface plasmon resonance sensors.\n\n1. Introduction\n",
        "text": "Under the illumination of resonant wavelength, noble metals (such as Au and Ag) can support the coherent oscillations of electrons in metals. These excitations, called as surface plasmon resonances (SPR), can greatly increase the optical extinction cross section of the nanostructure and produce significant near-field enhancement due to the high localization of electromagnetic field beyond the diffraction limit [REF]. Therefore, noble metal plasmonic nanostructures can be served as SPR sensors by utilizing the high sensitivity of local electromagnetic field to the change of surrounding dielectric environment. In the past decade, benefited from the rapid development of nanofabrication technologies (such as nanosphere lithography and nanoimprint lithography), SPR sensors have found wide applications for the label-free and real-time detection of bio-/chemical species in numerous fields [REF].A typical SPR sensing process is shown in Fig. 1, where the resonance spectrum of a SPR sensor shifts to longer wavelength as the refractive index of surrounding environment increases from n to n+Δn. The sensing performance of various SPR sensors can be evaluated by figure of merit (FOM), which is defined as spectral shift (Δλ) per refractive index unit (RIU) divided by resonance linewidth (full-width at half-maximum, FWHM), namely FOM = Δλ/(Δn × FWHM). A high FOM has therefore become an important indicator for the excellent sensing performance of a SPR sensor [REF]. To improve FOM, there are two main directions: 1) enhancing the near-field intensity of SPR sensors to obtain a large spectral shift per refractive index and 2) decreasing the linewidths of SPR sensors. The former can be realized through the fabrication of SPR sensors with strong near-field enhancement [REF], while the latter requires reducing plasmonic loss of sensor structures [REF]. For the widely used propagating surface plasmon polaritons (SPP) sensors using prism coupling strategy, it is difficult to realize the flexible tuning of optical properties of the sensor, and the existence of optical components in the device makes it difficult to realize miniaturization and portability, which greatly limits their practical applications [REF]. As for localized surface plasmon resonance (LSPR) sensors which are usually based on metallic plasmonic nanoparticles, they do not need to meet momentum matching conditions and the preparation of nanoparticles are relatively simple with low cost [REF]. The resonant frequency of the sensors can also be flexible tuned by adjusting the material, shape and size of plasmonic nanoparticles. Nevertheless, owing to strong radiative damping of metallic plasmonic nanoparticles, LSPR sensors usually exhibit broad resonance spectra, which greatly limit their sensing performance [REF].Fig. 1Illustration of the spectral shift of a SPR sensor as the environmental refractive index changes from n to n+ Δn.Fig. 1Illustration of the spectral shift of a SPR sensor as the environmental refractive index changes from n to n+ Δn.Recent progresses in micro-/nanofabrications have encouraged the development of metallic plasmonic substrates and numerous nanostructured SPR sensors, such as metallic nanohole [REF], nanoring [REF] and nanomushroom arrays [REF], have been proposed for sensing applications. Compared with the conventional SPR sensors based on prism coupling configurations and plasmonic nanoparticles, these nanostructured SPR sensors are easy to integrate with imaging and microfluidic systems. In addition, their plasmonic resonances can be excited directly without additional coupling structures, beneficial for the multiplex and high-throughput sensing analysis and the miniaturization and portability of sensing devices. More importantly, various ingenious and flexible structural designs endow the nanostructured SPR sensors with rich optical properties, providing possibilities to enhance near-field intensities and reduce resonance linewidths and therefore improve FOM. Benefited from these characteristics, nanostructured SPR sensors have been widely used for medical diagnosis [REF], food safety analysis [REF] and environment monitoring [REF], etc.In this review, we aim to introduce effective strategies to improve sensing performance of SPR sensors. We here focus on the current approaches for reducing resonance linewidths instead of the enhancement of near-field intensities. This paper is organized as follows. In section 2, we will introduce the influence factors of plasmon linewidth to facilitate readers of different fields to understand relevant background. In section 3, we focus on the developed strategies for obtaining narrow resonance linewidths. In section 4, the applications of SPR sensors in different fields are reviewed. Lastly, we will discuss in depth the future development of nanostructured SPR sensors."
    },
    "2023-37292276_1069_en.txt": {
        "title": "Knowledge mapping of energy research in the hospitality industry to evaluation research collaboration",
        "prompt": "Abstract: Analyzing collaborations on energy research in the hotel industry has important implications for promoting the research performance in this field. The Web of Science Core Collection from 1984 to 2022 was used to analyze the research contributions and cooperation networks and clusters at three levels: macro (national level), meso (institutional level), and micro (key authors and papers) using a bibliometric approach. The results show the following. (1) The cooperative relationship is the closest between China and the United States. Developed countries in Europe exhibit more academic cooperation. (2) There is a significant regional imbalance in the cooperation between universities. Leading universities rely on their strengths in energy research or hotel management and are often highly productive institutions. (3) The breadth of the authors' cooperation is insufficient. Collaborative research dominated by productive authors tends to focus on practical issues in the local hotel industry. The collaboration between experts from different disciplines benefits from the complementary advantages of these experts. (4) Hotel energy research has evolved from single-disciplinary research in the early days to interdisciplinary research in recent years. This paper provides visualizations of current conditions and deficiencies in existing research collaborations and provides a reference for analyzing the potential of research cooperation.\n\n1. Introduction\n",
        "text": "The hospitality sector is the most energy-intensive part of the tourism industry after the transportation sector [REF]. Hotel buildings are public buildings with high energy consumption due to heating, cooling, lighting, cooking, cleaning, entertainment, and other factors. The hotel industry in most countries and regions globally tends to focus on high-end and luxury features [REF], resulting in high investment, high consumption, and high pollution of high-star hotels [REF]. For example, the annual average energy utilization rates of high-star hotels in Singapore and Portugal are 427 kWh/(m2·[REF] and 446 kWh/(m2·) [REF], respectively, which is 20 times the energy consumption of residential buildings. In Greece, the average energy consumption per meal and bath is 5.5 kWh and 1.66 kWh, respectively [REF]. In Ottawa, the annual average energy intensity of hotels is as high as 612 kWh/(m2·[REF]. Sheng et al. (2018) found that luxury hotels in China consume on average four times more energy than other commercial buildings, highlighting the pressing need for energy-efficient operations in this sector [REF]. This has prompted scholars from various research fields to delve into the issue of hotel energy consumption. For instance, Shao et al. (2020) conducted a study on hotel building energy consumption by developing a support vector machine energy consumption prediction model [REF], while Todorović et al. (2020) explored the subject system in bivalent operation through the basic and advanced configurations of a heat pump system assisted with a gas boiler [REF].The rapid development of the hotel industry has caused a nonlinear growth in energy consumption, especially fossil energy consumption, and increased localized greenhouse gas emissions, which have negative impacts on the climate, environment, and economic growth. For instance, Oluseyi et al. (2016) found a strong correlation between energy consumption per unit room and CO2 emission levels in the hotel industry [REF]. Moreover, Sofer & Potchter (2006) pointed out that the urban heat island effect is more significant around the dense hotel belt than in other areas [REF].Various countries and regions have taken measures to reduce the hotel industry's excessive dependence on energy and alleviate the resulting environmental pressure. For instance, low-carbon hotels and green hotels have been advocated by and established in many countries [REF]. In academia, researchers have been advocating energy conservation, emission reduction, and green performance in the hotel business since the 1970s. Although the specific relationship between the total carbon emissions and the performance of hotels is unclear, there is no denying that a relationship exists [REF]. In this context, the concept of green consumption has been increasingly endorsed by tourists [REF].Most research has focused on the following aspects. (1) The composition of hotel energy and major energy consumption sectors [[REF], [REF], [REF]]. (2) Factors affecting hotel energy consumption [REF]. (3) Establishing a benchmark for hotel energy consumption [[REF], [REF], [REF]]. Early studies on hotel energy consumption were mostly case studies to demonstrate the performance of buildings and the technical level of the equipment. In recent years, an increasing number of studies have shown that interdisciplinary research has become the main research trend. The existing studies provide evidence of international cooperation in the area of energy efficiency technologies for hotel industry Furthermore, interdisciplinary studies in energy research in the hotel industry combine knowledge from various fields including engineering [REF], environmental science [REF], business and economics [22,[REF], [REF], [REF]]. The aim is to find ways to reduce the energy consumption in hotels and increase their energy efficiency.Collaboration is essential for energy research in the hospitality industry as it allows researchers and practitioners to combine their expertise and resources to solve complex energy-related issues. Collaboration promotes the sharing of information and best practices, leading to new insights that can be applied to real-world problems. Research collaborations assure in-depth investigations, resulting in robust findings with broader applications. Evaluating research collaborations by analyzing the knowledge mapping of energy research in hospitality is vital to understand the current state, identify knowledge gaps, and focus efforts on the most pressing issues. Assessing research collaborations helps researchers identify best practices and challenges, ultimately enhancing collaboration and research quality.Interdisciplinary research drives innovation. Interdisciplinary research refers to the collaboration of researchers across different fields of study to solve complex problems that cannot be solved using a single discipline [REF]. This collaboration brings together perspectives, methods, and theories from various disciplines, which leads to innovation in research and the development of new ideas, technologies and approaches [REF]. Since research is a complex social undertaking, it relies on collaboration between researchers. Furthermore, cooperation has a substantial influence on knowledge exchange and innovation. Research cooperation has become an important aspect of bibliometrics due to an increase in bibliometric studies [REF]. However, there are very few reports on collaborative research on hotel energy-related issues. Therefore, this paper uses bibliometric and visualization methods to explore research cooperation among countries, institutions, and authors to understand the current organization of academic activities in energy research in the hotel industry. The citation networks of highly cited papers were also analyzed to clarify how knowledge is transferred and developed.The remainder of this paper is organized as follows. The methodology and data are described in Section 2. Section 3 presents the results and discussions. The conclusions are drawn in Section 4."
    },
    "2023-37293433_725_en.txt": {
        "title": "Preclinical orofacial pain assays and measures and chronic primary orofacial pain research: where we are and where we need to go",
        "prompt": "Abstract: Chronic primary orofacial pain (OFP) conditions such as painful temporomandibular disorders (pTMDs; i.e., myofascial pain and arthralgia), idiopathic trigeminal neuralgia (TN), and burning mouth syndrome (BMS) are seemingly idiopathic, but evidence support complex and multifactorial etiology and pathophysiology. Important fragments of this complex array of factors have been identified over the years largely with the help of preclinical studies. However, findings have yet to translate into better pain care for chronic OFP patients. The need to develop preclinical assays that better simulate the etiology, pathophysiology, and clinical symptoms of OFP patients and to assess OFP measures consistent with their clinical symptoms is a challenge that needs to be overcome to support this translation process. In this review, we describe rodent assays and OFP pain measures that can be used in support of chronic primary OFP research, in specific pTMDs, TN, and BMS. We discuss their suitability and limitations considering the current knowledge of the etiology and pathophysiology of these conditions and suggest possible future directions. Our goal is to foster the development of innovative animal models with greater translatability and potential to lead to better care for patients living with chronic primary OFP.\n\n1. Introduction\n",
        "text": "Orofacial pain (OFP) is that localized below the orbito-meatal line, above the neck and anterior to the ears, including pain in structures of the oral cavity. According to the latest OFP classification, OFP can be acute, when it lasts for less than 3 months, episodic, when it occurs on fewer than 15 days per month whether or not for more than 3 months, or chronic, when it persists for more than 3 months and is present on at least 15 days per month [REF]. This distinction is important because chronic OFP is often accompanied by reduced quality of life, sleep and psychological disturbances and disability [REF] that require different management and it has less favorable prognosis. The estimated prevalence of chronic OFP in the general population is of 10% [REF]. OFP can be additionally classified as primary, when its etiology is unknown (e.g., chronic primary temporomandibular joint pain and idiopathic trigeminal neuralgia), or secondary, when it has an identifiable cause (e.g., temporomandibular joint pain attributed to arthritis and trigeminal neuralgia attributed to multiple sclerosis). In other words, chronic primary OFP conditions can be understood as their own disease, while chronic secondary pain syndromes represent symptoms of other underlying conditions or diseases [REF]. Some of the most challenging types of chronic primary OFP to manage include painful temporomandibular disorders (pTMDs), idiopathic trigeminal neuralgia (TN), and burning mouth syndrome (BMS). These seemingly idiopathic conditions have no cure and treatment is mostly palliative, geared toward pain management and coping strategies. Often, patients living chronic pain, including pTMDs, TN, and BMS, must endure significant biological, psychosocial, and economic burdens that also affect their families, and society as a whole [REF].The most common types of primary pTMDs are myofascial pain and temporomandibular joint pain [REF]. These conditions have a prevalence of about 5% that is greater in women than in men [REF] and their hallmark is spontaneous masticatory muscle and/or temporomandibular joint pain that is exacerbated upon jaw function. TN has a lifetime prevalence of 0.16%–0.3% and also affects more women than men (1.5:1) [REF]. TN is characterized by recurrent severe paroxysmal pain restricted to the territory of the trigeminal nerves (which innervate the orofacial region) lasting from a fraction of a second up to 2 min. TN pain is described as electric shock-like, stabbing, or sharp, and is triggered by innocuous stimuli (e.g., washing the face, eating, and brushing the teeth). BMS has a prevalence of 2.5%–5% in the general population that rises to 14% among post-menopausal women. BMS is characterized by a spontaneous burning sensation that most commonly affects the tongue, lips, and hard and soft palates and may be accompanied by an alteration of taste, a stinging sensation, dryness, and atypical odontalgia [REF].Despite considerable scientific advances over the past decades [REF], the need for greater understanding of the pathophysiology underlying these primary OFP conditions persists because treatment remains suboptimal. In this scenario, pre-clinical research has been and will continue to be an important and irreplaceable means of facilitating scientific advancement. Furthermore, we are currently experiencing a boom of both pain neuroimaging and genetic studies that find crucial complementation in animal research [REF].The goal of this review is to describe the currently available rodent assays (injury models) and OFP measures (behavior) that can be used in support of chronic primary OFP research, in specific pTMDs, TN, and BMS. We will discuss their applicability and challenges considering our current knowledge of the etiology and pathophysiology of these conditions and possible future directions. Previous reviews have focused on other conditions linked to OFP, including migraine/headaches [REF] and oral cancer [REF]."
    },
    "2023-37293541_10731_en.txt": {
        "title": "Programmable RNA editing with endogenous ADAR enzymes – a feasible option for the treatment of inherited retinal disease?",
        "prompt": "Abstract: RNA editing holds great promise for the therapeutic correction of pathogenic, single nucleotide variants (SNV) in the human transcriptome since it does not risk creating permanent off-targets edits in the genome and has the potential for innovative delivery options. Adenine deaminases acting on RNA (ADAR) enzymes catalyse the most widespread form of posttranscriptional RNA editing in humans and their ability to hydrolytically deaminate adenosine to inosine in double stranded RNA (dsRNA) has been harnessed to change pathogenic single nucleotide variants (SNVs) in the human genome on a transcriptional level. Until now, the most promising target editing rates have been achieved by exogenous delivery of the catalytically active ADAR deaminase domain (ADARDD) fused to an RNA binding protein. While it has been shown that endogenous ADARs can be recruited to a defined target site with the sole help of an ADAR-recruiting guide RNA, thus freeing up packaging space, decreasing the chance of an immune response against a foreign protein, and decreasing transcriptome-wide off-target effects, this approach has been limited by a low editing efficiency. Through the recent development of novel circular ADAR-recruiting guide RNAs as well as the optimisation of ADAR-recruiting antisense oligonucleotides, RNA editing with endogenous ADAR is now showing promising target editing efficiency in vitro and in vivo. A target editing efficiency comparable to RNA editing with exogenous ADAR was shown both in wild-type and disease mouse models as well as in wild-type non-human primates (NHP) immediately following and up to 6 weeks after application. With these encouraging results, RNA editing with endogenous ADAR has the potential to present an attractive option for the treatment of inherited retinal diseases (IRDs), a field where gene replacement therapy has been established as safe and efficacious, but where an unmet need still exists for genes that exceed the packaging capacity of an adeno associated virus (AAV) or are expressed in more than one retinal isoform. This review aims to give an overview of the recent developments in the field of RNA editing with endogenous ADAR and assess its applicability for the field of treatment of IRD.\n\n1. Introduction\n",
        "text": "The eye has been at the forefront of innovative genetic therapies due to its relative accessibility, ease of drug administration that precludes systemic administration of therapeutics as well as the availability of functional outcome measures for therapeutic evaluation. Inherited retinal diseases (IRDs) in particular have been at the forefront of diseases for which gene therapies are currently being developed. There is an imperative need for therapies to cure IRDs: they have a world-wide prevalence of about 1 in 2000, affect more than two million people and cause significant economic as well as and psychological burden [REF]. While gene replacement therapy has proven to be a safe and effective therapy for IRDs, with an approved treatment for RPE-65-associated IRD now available and others being evaluated in clinical trial [REF], an unmet therapeutic need remains for genes too large to fit into the 4.7 kB packaging constraints of an Adeno-associated virus (AAV) (e.g., ABCA4 or USH2A), the gold standard delivery vehicle for most gene replacement therapies, or genes that are expressed as multiple isoforms in the human retina (e.g., CRB1) [REF].RNA therapeutics work by reversibly changing the transcriptomic sequence without inducing permanent genomic changes, thus eschewing the risk of udesired, permanent genomic off-target effects of DNA editing techniques [REF]. RNA therapeutics can broadly be classified into antisense oligonucleotides (ASOs), RNA interference (RNAi) and RNA editing therapies. ASOs are currently under clinical investigation for treatment of several IRDs, such as CEP290- and USH2A-associated IRDs [REF]. RNAi therapeutics for IRDs are being developed in preclinical stages with the primary therapeutic target of autosomal dominant RP (adRP), where a dual RNAi suppression strategy using artificial mirtrons, atypical RNA interference effectors spliced from transcripts as short introns, is coupled with a gene replacement strategy [REF]. For a comprehensive review on RNA therapeutics and their use in the eye the review by Kumar et al. (2022) is recommended. The third RNA therapeutic is RNA editing, which is derived from an essential post-transcriptional modification in humans. The most common form of RNA editing in humans involves the hydrolytic deamination of adenosine (A) to inosine (I) at the C6 position (A-to-I editing) by a family of enzymes aptly named Adenosine deaminase acting on RNA (ADARs) [REF]. The catalytically active ADAR enzymes, ADAR1 and ADAR2, share a C-terminal deaminase domain (ADARDD) and have multiple double stranded RNA binding domains (dsRBD), with which they bind to double stranded RNA (dsRNA) and edit tens of thousands to millions of sites in the human transcriptome [REF]. When A-to-I deamination takes place in coding sequences, inosine is biochemically read as guanine by the translational and splicing machinery due to their structural similarity and inosine is paired with cytosine during translation [REF]. While only the minority of A-to-I editing takes place in coding sequences, it is a powerful and essential tool for proteome diversification, particularly in neurons, where A-to-I editing mediated protein recoding modulates receptor and ion channel properties [REF]. RNA editing harnesses ADAR’s recoding ability and uses it as a programmable tool to correct pathogenic single nucleotide variants (SNVs).Therapeutic RNA editing can broadly be divided into approaches utilising exogenous or endogenous ADAR as catalytically active deamination effectors. The first strategy facilitates editing by overexpression of the either full-length ADAR protein or, more commonly, overexpression of the catalytically active deaminase domain fused to an intermediate RNA binding protein such as deactivated Cas13b (dCas13b) [REF]. The second strategy aims to harness the cell’s native, endogenously expressed ADAR enzymes and recruit these for deamination. Both exogenous and endogenous ADAR editing approaches require a programmable guide RNA (gRNA) that is antisense to the target sequence and marks the target adenosine with an A-C mismatch. The gRNA and the target RNA sequence create a dsRNA that serves as a substrate for the ADAR deamination. For all exogenous ADAR approaches harnessing an intermediate binding protein, the gRNA carries a recruitment sequence that binds the intermediary protein. For approaches harnessing the endogenous ADAR or recruiting an exogenously expressed full-length ADAR, the gRNA either carries an ADAR recruitment sequence or simply relies on the dsRNA structure to recruit ADAR. Like other genome editing approaches, the central challenge is to design a system that achieves deamination with high on-target editing efficiency and specificity for the target adenosine, while minimising bystander and transcriptome-wide off-target edits. RNA editing with exogeneous ADAR has exhibited high on-target editing efficiency but has been hampered with high transcriptome-wide off-target editing. Therapeutic endogenous ADAR RNA editing has up until now been unable to compete with exogenous RNA editing due to low on-target efficiency. However, due to exciting developments in the endogenous RNA editing field which succeed in stabilising the gRNA by circularisation or chemical modification, endogenous ADAR editing might be approaching viability in an in vivo, therapeutic clinical setting. Whether this is relevant for ocular gene editing will depend on the ADAR-tissue expression as well as finding a suitable vehicle for delivery.To unlock the utility of ADARs as an RNA editing toolset, editing must be achieved with a high on-target efficiency and specificity which precludes off-target editing in the form of bystander editing and transcriptome-wide off-target editing. For editing with endogenous ADAR, the central challenges are improving on-target editing efficiency while limiting bystander editing in the gRNA:target RNA duplex, while the central challenge for exogenous ADAR editing is restricting promiscuous transcriptome-wide off target editing and achieving target specificity while retaining high on target editing rates.This review aims to give an outline of the history and recent developments in the field of RNA editing with endogenous ADARs and discuss the potential for treatment of IRDs. Since an understanding of the structure, function and role of ADAR enzymes is essential to understanding the design, challenges and strategies in the RNA editing field, an overview of ADAR enzymes will proceed the discussion on RNA editing with endogenous ADAR.Three members of the ADAR family are encoded in the mammalian genome and are highly conserved across species [REF]: ADAR1, ADAR2 and ADAR3 [REF]. ADAR1 is expressed as two separate isoforms: the full-length ADAR1p150, which is under the control of an interferon alpha (IFN-alpha) inducible promotor and the shorter ADAR1p110 isoform, which is constitutively expressed [REF]. ADARs from all characterised species have two main structural motifs: several double stranded RNA Binding Domains (dsRBD) and a single Deaminase Domain (DD) [REF]. Each dsRBD is approximately 65 amino acids (aa) in length and makes direct contact with dsRNA, recognising higher order structures within dsRNA. While dsRBDs bind perfectly matched duplex RNA, they can also bind imperfectly matched structures with bulges, hairpins, and mismatches with high affinity [REF]. Both ADAR1 isoforms and ADAR2 have a single deaminase domain (DD) at their carboxy (C-) terminus that form the enzyme’s catalytic centre. ADAR1 has unique Z-DNA binding domains (ZBD), that have been shown to bind both Z-DNA and Z-RNA, but whose function remains poorly understood [REF]. The full-length ADAR1p150 isoform contains two ZBDs, ZBD alpha and beta. ZBD alpha domain is only present in ADAR1p150, whereas the shorter ADAR1p110 only has one ZBD, Z beta, at its N-terminus. Both ADAR1 isoforms contain a Nuclear Localisation Signal (NLS) in the third dsRBD, but only ADAR1p150 contains a Nuclear Export Signal (NES), which is located in the ZBD alpha domain. ADAR2 contains a NLS, located toward the N-terminus. ADARp150 can shuttle from the nucleus to the cytoplasm, but in accordance with the NES found in its ZBD beta, ADAR1p150 is detected mainly in the cytoplasm [REF]. ADARp110 and ADAR2 are expressed predominantly in the nucleolus and nucleus [REF]. ADAR1p110 is expressed ubiquitously at high levels and is therefore responsible for the majority of editing activity, particularly in repetitive sequences in noncoding regions, which are particularly suited for dsRNA formation due to their inherent ability to base pair with themselves [REF]. While ADAR2 expression is ubiquitous, it is generally expressed at much lower levels than ADAR1. The exception to this is ADAR2 expression in the brain, bladder, and lung, which exhibit high levels of ADAR2 expression (The Human Protein Atlas1).Overview of structure and function of ADAR enzymes. (A) ADARp110 and ADAR2 are expressed predominantly in the nucleolus and nucleus, while ADAR1p150 is expressed in the cytoplasm. ADAR1p110 is expressed ubiquitously at high levels and is primarily responsible for editing in noncoding regions. ADAR1 expression is ubiquitous, whereas ADAR2 expression is highest in the brain, bladder, and lung. (B) ADAR enzymes hydrolytically deaminate Adenosine to Inosine in dsRNA. Inosine is biochemically read as Guanosine.The presence of ADAR2 and ADAR1 protein expression levels in the neuronal cell types of the retina has not yet been comprehensively analysed, but immunohistochemical evidence points toward ADAR2 expression in the retinal ganglion cell (RGC) layer [REF]. It is also unclear how much protein ADAR protein expression is needed to induce therapeutic editing in the retina. In concordance with its expression profile, ADAR2’s protein recoding ability is particularly important in the neuronal system [REF]. ADAR3 has a catalytically inactive deaminase domain, its expression is restricted to the brain and is thought play an inhibitory role of RNA editing by the catalytically active ADARs [REF]. Due to its lack of A-to-I editing, it will not be the focus of this review.In general, ADARs mediate A-to-I editing in both coding regions, where editing can lead to proteome diversification through codon changes and induction of alternative splicing, and in non-coding regions, where one of the many functions comprises the modulation of the innate immune response [REF]. Certain transcript targets are edited exclusively by ADAR1 or ADAR2, whereas other sites can be edited by both (Nishikura, 2016, tables 1 and 2) [REF]. ADAR2 is the primary, but not the exclusive editor of protein coding sequences in mammals [REF]. Through this A-to-I mediated alterations of coding sequences and splicing sequences, ADARs are capable of creating different protein isoforms as well as altering and regulating gene expression at an RNA level. Two of the most well-characterised ADAR editing sites are the R/G and the Q/R site of the pre-mRNA encoding the subunits of the a-amino-3-hydroxyl-5-methyl-4-isoxazole-propionate (AMPA)- subtype of ionotropic glutamate receptor (GRIA2, also known as GluR2) [REF]. Both ADAR1 and ADAR2 can edit the R/G site [REF] and this sequence is of particular importance in RNA editing because of its use as a physiological recruitment sequence for endogenous or full-length exogenous ADAR in the gRNA design [REF]. The ADAR dsRBDs bind to the pre-mRNA exon/intron border at the R/G site of the GluR2 receptor and results in highly specific ADAR-mediated editing at this site [REF]. Mouse knockout studies suggest that the GluR2 Q/R recoding site is ADAR2’s only essential target [REF], where it converts the CAG (Glutamine, Q) codon to CGG (arginine, R) at the Q/R site of the GRIA2 subunit. Adar2-null mice experience neuronal death due to an excess influx of calcium, which leads to frequent epileptic seizures and death several weeks after birth [REF]. This severe phenotype is fully rescued by introducing an allele containing an arginine (R) codon for the Q/R site in knockout mice. Other physiologically important mammalian genes that undergo recoding type editing that alters their protein function is the G-protein-coupled serotonin receptor 5-HTR2C, the voltage-gated potassium channel Kv1.1 and the alpha-3 subunit of GABAA receptor [REF].While the recoding of proteins is of most interest when harnessing ADARs for a therapeutic design, it should be stated that vast majority of native A-to-I editing takes place in the non-coding region of the human genome such as introns, untranslated regions and non-coding RNA. Within the non-coding region, editing mostly takes place within mobile elements such as Alu [part of the class of short interspersed elements (SINE)] and long interspersed elements LINEs [REF]. ADAR1 is recognised as the primary editor of these sequences [REF]. Over 99% of the editing are detected in Alu repeats [REF], and similarly to the GluR2 motif, Alu elements have therefore been used as recruiting domains for endogenous ADAR [REF]. Perhaps the most prominent function of ADAR1 is the suppression of the activation of the innate immune response to endogenous ADAR [REF]. A-to-I editing of long, dsRNA motifs disrupt the perfect pairing of the repetitive RNA transcripts and hinders melanoma differentiation-associated gene 5 (MDA-5) binding and the downstream activation of the interferon induced innate immune response [REF]. Murine studies showed the embryonic lethality of Adar1 knockout mice, likely due to an aberrant interferon response and subsequent stress induced apoptosis of liver haematopoietic cells [REF]. In a human knockout neuronal progenitor cell line, MDA5 (dsRNA sensor)-dependent spontaneous interferon production, Protein kinase R (PKR) activation and cell death was observed [REF]. In humans, mutations in ADAR1 have been shown to cause type I interferonopathies such as Aicardi-Goutières-Syndrome [REF]. ADAR1 has many other important physiological functions, such as the altering of miRNA maturation and targeting as well as promoting genome diversification, and roles in human disease pathogenesis. For excellent reviews on this topic, as well the details of ADAR editing in coding and non-coding regions, refer to Nishikura, (2016) and Song et al., (2022).It has yet to be fully understood which characteristics determine whether an adenosine within dsRNA is deaminated, but it is thought that ADAR exhibits a sequence preference for particular RNA motifs [REF] while the secondary structure of the dsRNA substrate confers editing selectivity [REF].ADAR preferentially deaminates adenosines that occur in an A-C mismatch over those that occur as A-A or A-G mismatches or in an A-U pairing [REF]. These characteristics are commonly used to confer target specificity in a gRNA when an A-C mismatch is used to mark the adenosine. On the other hand, this preference is used to avoid bystander editing in a long, gRNA:targetRNA duplex by creating an A-to-G mismatch across from adenosines vulnerable to bystander edits [REF].Although ADARs do not exhibit strict sequence specificity, the enzymes do exhibit a preference for certain neighbouring nucleotides. The most pronounced preference is seen for the base flanking the 5′ end of the target adenosine. Here, both ADAR1 and ADAR2 favour Uridine (U) as a neighbouring base, followed by A, C and G (U > A > C > G). While the nature of the 3′ neighbouring base is thought to be less important, both ADARs prefer guanosine (G) at this position. More specifically, ADAR1 prefers G > C ≈ A > U and ADAR2 exhibits a slightly different order of preference: G > C > U ≈ A [REF]. These preferences make the stop codon RNA motif 5′-TAG-3′ a preferential editing site, whereas a particularly difficult RNA motif to edit would be one with a 5′ neighbouring G (5’-GAN-3′).Beyond sequence-specific preferences, ADAR-dsRNA interactions rely on the structure of the RNA substrate to confer target editing specificity. Generally, ADAR acts on both inter- and intramolecular dsRNA of >20 bp in length [REF]. Long (>100 bp), perfectly matched dsRNAs have been found to be non-selectively deaminated and exhibit an editing rate of 50–60%, whereas shorter dsRNA structures are edited more selectively (editing rate < 10%) indicating that the secondary structure of dsRNA may dictate editing site selectivity [REF]. In general, imperfectly paired dsRNA which is periodically interrupted with mismatches or loops of at least 6 bp exhibited a much more selective deamination than perfectly paired dsRNA, since internal loops are equivalent to helix termini for ADAR1 [REF]. The non-specific editing of long stretches of dsRNA might be explained by the presence of dsRBD which are thought to bind to dsRNA in a sequence independent manner [REF]. Thus, the insertion of internal loops into long stretches of dsRNA are another strategy to decrease the bystander deamination events that occur in particular when using guide RNAs with a long specificity domain within the dsRNA substrate and confer selectivity in dsRNA deamination [REF].Since many RNA strategies employ only the deaminase domain of ADAR, it is important to note that ADARDD alone also has targeting capacities [REF]. However, our understanding of the features controlling ADAR target recognition is incomplete and a precise prediction of editing sites is not currently possible [REF]. Once the target adenosine has been identified, a base-flipping mechanism enables the target adenosine to access the enzyme’s catalytic site. The ADAR base flipping loop approaches the RNA duplex from the minor groove side and flips the adenosine out of the RNA double helix and into the enzyme’s catalytic pocket, where the hydrolytic A-to-I deamination reaction takes place at the C6 position [REF]. Structural studies have revealed the presence of inositol hexakisphosphate (INsP6) in the enzyme core and close to the catalytic centre [REF]. The deaminase domain can function as an independent catalytic unit in the absence of the dsRBD. The space vacated by the reactive base is stabilised by the intercalation of the E488 amino acid side chain. This amino acid (among others) can be mutated to induce a hyperactive ADAR variant widely employed in RNA editing [REF].In a landmark study almost 30 years ago Woolf et al. showed the potential of endogenous ADAR recruitment for therapeutic RNA editing when they demonstrated A-to-I editing in a Xenopus embryo microinjected with a pre-assembled duplex consisting of a target RNA hybridised to a 52 nt long, unstructured gRNA [REF]. Two central hurdles of editing with endogenous ADAR, namely low on target efficiency and bystander edits, were already apparent in this landmark study.In a crucial step toward harnessing endogenous ADAR for RNA editing, Wettengel et al. (2017) designed a gRNA for the recruitment of full-length ADAR in 2017. While this strategy still required the overexpression of full-length ADAR2 under the strong CMV promotor, either from a plasmid or from an ADAR2 expressing cell line, it did not rely on an intermediary protein to mediate gRNA binding to ADAR and thus laid the groundwork for recruiting endogenous ADAR.The ADAR2-recruiting gRNA was composed of two parts: (1) an antisense sequence complementary to the target RNA substrate located at the 3′ end and (2) the R/G GluR2 ADAR-recruiting domain at the 5′ end. The antisense region contained a C mismatch opposite the target A, and the optimal length was determined to be 18–20 nt. Lengthening of the antisense region beyond this point led to declining editing rates as well as bystander edits.In an eGFP – reporter plasmid with a premature stop codon, on-target editing rates up to 50% were observed, which increased to 65% when the ADAR2 expression was integrated in the host genome of HEK293T cells under the CMV promotor. When editing components as well as ADAR2 were given at high concentrations, massive off target effects were seen and a gRNA lacking the R/G motif was able to elicit editing. These effects were decreased when lowering the concentrations of all transfected components, indicating that dosing is a central question when trying to achieve desired editing while lowering bystander edits. Furthermore, the authors showed editing of several endogenous housekeeping gene transcripts with editing rates ranging from 10 to 35% in 12 out of 13 target sites. Interestingly enough, one site in beta actin consistently achieved 0% editing, indicating the target variability in editing rates. Apart from showing editing of several endogenous housekeeping genes, a 10% correction of the PINK W437X nonsense mutation, a functional rescue of cellular phenotype was also shown. In a longitudinal analysis of editing, peak editing was observed at 48 h after transfection which remained constant until 72 h and then started to decline at 96 h.In a follow-up study by Heep et al. (2017) the same gRNA construct was shown to be able to recruit the two inducible as well as the constitutively expressed isoforms of ADAR1 in an ADAR1 expressing cell line. The gRNA design was also optimised to minimise auto-editing of adenine within the R/G GluR2 hairpin by substituting A/U base pairs with C/G base pairs.Using a very similar gRNA design to Wettengel et al. (2017) and Fukuda et al. (2017) also showed that an ADAR-recruiting gRNA (AD-gRNA) design based on the secondary structure of GluR2 (GRIA2) pre-mRNA, was capable of recruiting ADAR2 in an ADAR2 overexpressing cell line.In 2019, Katrekar et al. used the design of the ADAR-recruiting gRNA (named adRNA after ADAR-recruiting gRNA) described in Wettengel et al. (2017) and Fukuda et al. (2017) and compared delivery of the adRNA alone with the delivery of the adRNA together with full-length exogenous ADAR2 [both wild-type and hyperactive ADAR2 (E488Q)]. Several factors were varied: the length of the antisense region (20, 60, and 100 nt), the number of GluR2 recruiting domains (0, 1, and 2) as well as the position of the A-C mismatch. Several important observations were gleaned from this study: when elongating the adRNA antisense region to 60 bp or more, administration of the adRNA alone (without exogenous ADAR2 overexpression) resulted in editing an endogenously expressed RABA7 transcript in HEK293T cells through the recruitment of endogenous ADAR. Even when the GluR2 recruitment domains were removed from the adRNA, the adRNA elicited editing. An A-C mismatch placed at the centre of the antisense region exhibited the most effective editing. While the RNA editing through adRNA recruitment of endogenous ADAR exhibited a target editing efficiency lower than the RNA editing achieved with overexpressed wild type and hyperactive ADAR2 (E488Q), this was the first indication that endogenous ADAR was able to be recruited by an ADAR-recruiting gRNA, with and without the ADAR-recruiting domain R/G GluR2. These results held true in vivo, when different adRNA designs were tested on a G > A point mutation in a sparse fur ash (spfash) mouse model of ornithine transcarbamylase (OTC) deficiency. One month after retroorbital injection adRNA alone resulted in low but significant RNA editing yields in the liver. Nevertheless, the highest edited fraction came from adRNA delivery together with the hyperactive ADAR2 mutations (E488Q) (4.6–33.8% RNA editing).Building on this work, Qu et al. developed LEAPER (leveraging endogenous ADAR for programmable editing of RNA), an endogenous ADAR-recruiting gRNA (arRNA), consisting of a long, single arRNA without ADAR-recruiting sequences or chemical modifications. During the optimisation of LEAPER, several design principles were established: similar to Katrekar et al. which required an adRNA of at least 60 bp for endogenous ADAR recruitment, a minimum length of at least 71 nt was required for efficient endogenous ADAR recruitment and subsequent editing. The length of the arRNA correlated positively with the target editing efficiency, with arRNA counting 111 and 151 nt being used in experiments in standard and primary cell lines. A mismatch position placed in the middle of the arRNA lead to the highest editing efficiency and the preferred 5′ neighbouring base was 5’U. The standard cell lines all lacked ADAR2 expression, indicating that ADAR1 was being recruited for editing. But in a knockout cell line, editing was rescued by the addition of ADAR2 as well as ADAR1 isoforms, indicating that LEAPER can recruit ADAR1 and ADAR2. In addition to a testing in a reporter plasmid assay, where arRNA achieved a target efficiency of approximately 13%, LEAPER was tested on an array of standard and primary cell lines, showing a wide range of target editing rates. The highest editing rates were shown in the primary bronchial epithelial cells targeting transcripts of the endogenous PPIB gene with editing rates >80%. When targeting this same transcript with lentiviral transduction in HEK293T cells, the editing rates dropped to 6% at 6 days post transduction. This efficiency increased to 20% when chemical modifications were introduced in the form of 2-O-methyl and phosphorothioate backbone modifications at either end of the arRNA. This highlights the wide range of editing rates in human primary cell lines, that are dependent on target sequence, delivery mechanism and endogenous ADAR expression in the target tissue.Due to the length of the antisense domain, As occurring in the target sequence were promiscuously deaminated, although the extent of this varied between targets. The highest bystander editing was seen in the KRAS gene transcript, where one-third of As were subject to bystander editing. In an attempt to curb this bystander editing, A to G mismatches were placed across from the potential target As. Clinical relevancy of LEAPER was shown by editing the tumour suppressor gene transcript TP53 as well as a nonsense mutation in the IUDA gene transcript in a primary cell line isolated from a Hurler syndrome patient. Editing rates were modest but nevertheless significant. Expression levels of targeted transcripts were monitored to rule out possible RNA interference (RNAi) effect of the arRNA.In the same year, Merkle et al. chemically modified their previously described [REF] ADAR2-recruiting gRNA to develop the ADAR-recruiting antisense oligonucleotide construct RESTORE (recruiting endogenous ADAR to specific transcripts for oligonucleotide-mediated RNA editing). Chemical modifications included selective phosphorothioate (PS) backbone stabilisation as well as 2’-O-methyl (2’-OMe) modifications and were intended to stabilise the gRNA and thus enable more efficient recruitment of endogenous ADAR without the need for lengthening the antisense domain subsequent increase of probability for bystander editing. The RESTORE construct consists of a short (20 or 40 nt), programmable antisense specificity domain with an invariant ADAR-recruitment domain based on the naturally occurring R/G motif of the GluR2 subunit, a well-characterised editing target for both ADAR1 and ADAR2 [REF]. While the genetically encoded gRNA described in Wettengel et al. (2017) was only able to recruit overexpressed ADAR, the heavily chemically modified ASO achieved significant editing rates in a range of standard and primary cell lines. RESTORE editing relied primarily/heavily on the ADARp150 isoform, since editing rates consistently improved by 2- or 3-fold after IFN-alpha treatment. The two best performing ASO versions, ASOv9.5 and ASOv25, both contained a fully chemically modified R/G GluR2 domain and a chemically modified antisense domain either 18 bp (ASOv9.5) or 40 bp (ASOv25) in length. In ASOv25, locked nucleic acids (LNA) were also included at the 3′ end of the antisense region. ASO9.5 showed 19 and 32% editing in a panel of human standard cells and human primary cell lines, respectively. Since editing relied heavily on ADARp150, IFN-alpha treatment increased these editing rates by 2- or 3-fold. In HeLa cells, ASOv25 showed higher editing of 26% without IFN-alpha and 43% with IFN-alpha. In primary cell lines with the 5′ stop codon of the endogenously expressed GAPDH as a target, editing levels ranged between 9 and 27%, with IFN alpha treatment increasing the target rate by two-fold. To illustrate therapeutic potential of RESTORE in vitro, the E342K missense mutation (PiZZ mutation) in the SERPINA gene, the most common cause for alpha1 antitrypsin deficiency, was targeted. In a HeLa cell line expressing mutated SERPINA, cDNA editing rates of 10% were found without IFN-alpha, which increased to 20% with IFN-alpha treatment. When targeting the Tyr701 site in the 5′ UAU codon of the endogenous STAT1 in HeLa and primary cells, the highest editing yields were achieved with IFN-alpha induction and ranged from 20 to 30%. Neither LEAPER nor RESTORE were tested in vivo.In 2022, the Stafforst group further modified their original gRNA design and created CLUSTER, which retained the basic design principle of RESTORE, but achieved recruitment of endogenous ADAR without chemical modification. While the short, 20 nt antisense target specific domain as well as the invariant GluR2 recruitment domain remained, a series of single stranded RNA recruitment sequences (RS), a “cluster,” were added to the gRNA design. These RS bind to the target mRNA in various regions distal to the target site and distal to each other. These RS were 7–20 nt in length and could be flexibly chosen to exclude adenines that would otherwise be prone to bystander editing. The spacer regions between the RS binding sites were generally between 10 and 460 nt. The Stafforst group provides an open-source bioinformatics tool named recruitment cluster finder (RCF) to enable users a customised CLUSTER gRNA design. This strategy allowed for an increased binding affinity of the gRNA to the target sequence by adding RS while retaining the minimised chance of bystander editing that occurs when using a short specificity domain. Another important difference to RESTORE was the type of ADAR recruited. RESTORE preferentially recruited ADARp150 for editing and thus required IFN-alpha induction, but CLUSTER preferentially recruited the ubiquitously expressed ADARp110 and was also able to show recruitment of ADAR2. In contrast to LEAPER, CLUSTER was shown to benefit from the GluR2 ADAR-recruiting domain, but not from an extension of the specificity domain, so the specificity domain was kept at 20 nt to avoid extraneous bystander editing. At least 2 RS were needed to enable significant editing through the recruitment of endogenous ADAR. A luciferase reporter assay in HeLa cells was used for CLUSTER gRNA optimisation and testing of ADAR-specific recruitment. In HeLa cells using transfection of disease relevant transcripts and CLUSTER gRNA, editing yields between 3 and 61% without bystander edits were observed. When comparing CLUSTER to LEAPER, LEAPER outperformed CLUSTER by 20% on the BMPR target transcript, but CLUSTER yielded higher on target efficiencies for the mIDUA target transcript. In contrast to LEAPER, which showed high bystander editing rates up to 50%, CLUSTER did not elicit any detectable bystander edits. Coding and non-coding regions of endogenous transcripts with varying levels of expression were targeted in HEK293Ft cells and yielded on target rates of 19–44% without bystander editing. Encouragingly, editing efficiencies were similar for targets in the ORF and the 3’UTR. Fibroblasts taken from a Hurler patient were treated with CLUSTER gRNA in the form of chemically stabilised ASO. Editing rates of 24% were seen as well as an increase in IDUA enzyme activity. Off-target effects were investigated and found to be mainly located in repetitive Alu sequences with three novel exonic off target effects also observed. Of note, Adenovirus (AV) not AAV was used. Finally, two different CLUSTER designs were tested in wild-type C57/BL6 mice by co-delivering the dual luciferase reporter plasmid with a CLUSTER gRNA into the liver by hydrodynamic tail vein injection. Both gRNAs had a similar design with a 20 nt target specificity domain, an invariant GluR2 ADAR-recruiting sequence and 3 × RS. The length of the RS varied between the two constructs with the first having 15-, 13-, and 11 nt and the second gRNA having 20-, 15-, and 15 nt. 72 h post-injection, the first CLUSTER gRNA demonstrated 5% editing rate both in luminescence detection and Sanger sequencing while the second gRNA demonstrated a 10% editing rate.In 2022, two groups utilised the elegant method of RNA circularisation, developed by Litke and Jaffrey (2019) for therapeutic RNA delivery, and applied it to the ADAR-recruiting guide RNA with the idea of enabling a more stable, long-lasting gRNA expression and thus allowing for more efficient on-target editing. Litke based the Tornado (Twister-optimised RNA for durable overexpression) expression system on the endogenous mechanism of RNA circularization in a subset of intron-containing tRNAs, in which a tRNA specific endonuclease (TSEN) cleaves the intronic sequence, thus creating an exonic and intronic sequence that both have unique 5’hydroxy and 2′3’-cyclic phosphate ends. The nearly ubiquitous RNA ligase RtcB uses these unique 5′-and 3′-prime RNA ends to form a mature tRNA from the exonic sequence and circularises the intronic sequence. To produce ribozyme-assisted circular RNA (racRNA), a genetically encoded RNA sequence is flanked by autocatalytic twister ribozymes, which mimic processing by the endogenous TSEN and rapidly self-cleave to produce the unique 5′ and 3′ ends required for circularisation. A short, 19-nucleotide stem was created through RNA hybridisation and this stem placed the unique 5′ hydroxyl and 2′-3′ phosphate 3′ ends near each other to facilitate recognition and ligation by RtcB RNA ligase (Figure 2). Since ADAR-recruiting gRNA are usually transcribed from a polIII promoter such as U6, they do not have a 5′ prime cap and polyA tail, which makes them prone to rapid degradation by cellular exonucleases. The ring structure of circular RNA protects against exonuclease-mediated degradation and thus confers circular RNA with a high stability and consequently, a longer half-life [REF]. The ribozyme-assisted circularisation of RNA was harnessed by Yi et al. (2022) and Katrekar et al. (2022) with the hypothesis that increasing the half-life of the gRNA would lead to both an increased target efficiency as well as an increase in the editing duration.Circularisation of endogenous ADAR using autocatalytic twister ribozymes. Twister ribozymes rapidly self-cleave after transcription has occurred. RNA hybridization brings the unique 5’ and 3’ ends in close proximity for the ligation by the ubiquitously expressed RNA ligase RtbC.Yi et al. based their circularised ADAR-recruiting gRNA (circ-arRNA) on their previously described arRNA, LEAPER. Like in the original design, the target specific antisense region of LEPAER2.0 was 151 nt long, with the A-C mismatch placed at the centre of the sequence and no chemical modifications (Figure 3). To increase editing efficiencies for target sequences that showed no or even reduced benefit from the circ-arRNA (as was the case for MALAT1 and KRAS), a 50 nt, flexible polyAC linker (AC50) was included to increase the structural flexibility of the circ-arRNA. This circ-arRNA_AC50 showed increased editing efficiencies across all target sites. Editing efficiency was tested for 20 sites across 9 endogenous target genes in HEK293T cells. On average, editing efficiency increased by 2.3 for circ-arRNA and by 3.1 for cir-arRNA_AC50 across all target sites when compared to the linear arRNA LEAPER design. Sustained editing >13 days was shown when targeting the PPIA transcript in HEK293T cells via transfection. When transduced with AAV containing cir-arRNA, HEK293T, primary human hepatocyte cell line and cerebral organoids showed long term, sustained editing over 9 days, whereas the linear counterparts did not elicit significant editing. KRAS was one of two sites that exhibited initial editing efficiencies below that of linear arRNA. Yet over 7 days, editing rates with linear arRNA fell precipitously, while circ-arRNA exhibited continuous, stable editing, resulting in more efficient cumulative editing by circ-arRNA. Similarly, when targeting a reporter plasmid, the expression of arRNA was still detectable at high levels on day 21, whereas the linear arRNA counterpart was not detectable at day 21. While circ-arRNAs produced sustained, high editing efficiencies, they also exhibited pronounced bystander editing along the length of their antisense region. To reduce these bystander edits, the base flipping mechanism required to access ADARs catalytic centre was found to be hindered by deleting the Us opposite the adenosines prone to bystander editing. Different numbers of base deletions were strategically placed across the antisense regions. The cir-arRNA with 14 deletions in the antisense region as well as a flexible AC50 linker was shown to eliminate all bystander edits while retaining a 60% on-target editing efficiency across 8 target sites of the endogenous PPIA transcript. A cir-arRNA with a AC50 linker and 4 deletions opposite unwanted adenosines showed a 70% editing rate in a clinically relevant p53 nonsense mutation as well a restoration of full-length protein expression in p53−/− HEK293T cells. Of note, even though the editing rate of cir-arRNA_AC50 (not carrying deletions) was much higher than that of the construct with the deletions, the functional recovery was higher despite the lower editing rate, likely due to the low rate of bystander editing.Overview and comparison of the utilisation of circularised guide RNAs. Both circular gRNA are generated with the help of autocatalytically active twister ribozymes and vary in the length of their antisense domain. To improve target efficiency, the antisense region can be lengthened, or the “dose” of the gRNA increased, or the structure of the gRNA made more flexible by including a linker sequence. To decrease bystander editing, deletions, loops and A-G mismatches are used to disrupt perfectly paired gRNA. Circular gRNA can be delivered as DNA or RNA.To optimise their previously described adRNA, Katrekar et al. undertook a screening of linear adRNA of 100 and 200 nt with and without different ADAR-recruiting domains (GluR2 and Alu recruiting domains as well as the stabilising U6 + 27 cassette) as well as circular adRNA (cadRNA) in HEK293T cells using the 3’ UTR of the RABA7 transcript as a target. Once again, a positive correlation was seen between gRNA length and on-target editing efficiency, with a linear arRNA of 200 bp resulting in a 1.6-fold increase in on-target editing efficiency compared to the linear 100 bp adRNA counterpart. While addition of ADAR recruitment domains resulted in an only marginal increase in editing efficiency compared to the simple 100 bp linear guide RNA, addition of the stabilising U6 + 27 cassette resulted in a 2-fold increase in target editing and circularisation of the adRNA resulted in a 3.5-fold on-target editing improvement over the linear adRNA. ADAR1 recruitment was confirmed by knockdown of ADAR1 and concomitant abrogation of editing efficiency. To decrease bystander editing, Katrekar et al. employed two strategies (Figure 3): (1) A-G mismatches were introduced opposite all non-target adenosines (cadRNA.bulges) and (2) Introduction of 8 bp loops. While the cadRNA bulges eliminated bystander editing, they also led to a 50% drop in the on-target editing efficiency. By contrast, inclusion of 8 bp loops positioned 5 bp upstream and 30 bp downstream and every 15 bp after that along the antisense domain substantially reduced bystander editing while retaining on target efficiency. CadRNA was tested in vitro for its ability to target the coding sequence and 3’ UTR sequences of eight transcripts in HEK293T cells. Twenty-four hours after transfection, robust editing up to 40% was shown in almost all target transcripts with the exception of TARBDP, which showed very little editing. The highest editing (90%) was shown when K562 cells were electroporated with genetically encoded cadRNA targeting the 3’UTR of the RABA7 transcript. These editing rates were similarly high (70% editing) when electroporating the cell line with in vitro transcribed RNA (IVT RNA).Both Yi et al. and Katrekar et al. investigated transcriptome-wide off-target editing of cadRNA/circ-arRNA and compared these with transcriptome- wide off-target editing of exogenously delivered ADARs. In Yi et al. 17 off-target transcriptome wide edits were seen using circ-arRNA, whereas overexpression of exogenous ADAR led to >16,000 transcriptome-wide off-target effects. Katrekar et al. found similar results: With enzyme overexpression 10^3–10^4 off targets edits were routinely observed, this decreased by 2–3-fold when using cadRNAs. Both groups noted cadRNA and circ-arRNA did not lead to a change in mRNA expression levels, indicating the there was no RNA interference mechanism at play.The in vivo results generated in both these studies were the first to show sustained editing of a disease-causing mutation using endogenous ADAR (Figure 4). Katrekar et al. first targeted the 3’UTR of the endogenous PCSK9 gene transcript in the liver of a wild-type C57BL6J mouse model. The best performing linear gRNA with a U6 + U27 cassette was compared with two different cadRNA, one encoding a single copy of the gRNA, while the other encoded two copies of the gRNA. After 2 weeks, the linear gRNA produced no editing, while the two cadRNA exhibited 11 and 38% editing, respectively. Eight weeks post-injection, the editing rates for the cadRNA containing two copies was even higher at 53%. Both Yi et al. and Katrekar et al. used the same mouse model of Mucopolysaccharidosis (MPS) Type I (Hurler syndrome) with the pathogenic nonsense variant W392X in the Idua gene to show transcript editing efficiency of cadRNA/circ-arRNA. Both studies used a genetically encoded cadRNA/circ-gRNA packaged in scAAV or AAV8, respectively, to target the liver. Yi et al. showed a 10% targeted editing rate at 4 weeks post injection while Katrekar et al. showed a 7–17% correction of the premature stop codon 2 weeks post-injection. On a protein level endpoint both studies detected a decrease in GAG accumulation (Katrekar et al. measured a 33% decrease) as an indirect marker for partial restoration of alpha-L-iduronidase activity (Figure 3).Summary of in-vivo results achieved with circularised and chemically stabilised gRNA. Two wild-type and one disease model were used to evaluate the in-vivo efficacy of endogenous ADAR recruitment. All studies showed sustained on-target editing over a minimum of 2 and a maximum of 8 weeks.In the first study demonstrating endogenous ADAR recruitment in a non-human primate (NHP) animal model, Monian et al. used fully chemically modified, short, 30 nt antisense oligonucleotides (AIMers) for endogenous ADAR recruitment. Optimisation of the chemical modifications of the AIMers showed that in addition to the selectively placed 2′ ribose modifications 2’deoxy-, 2’fluoro and 2’-O-methyl, a phosphorothioate (PS) backbone increased target editing activity. PS backbone chirality also influenced editing activity with left-handed (Sp) PS modifications increasing editing efficiency, while right-handed (Rp) PS chirality decreased editing efficiency. Modifications that preferentially enhanced ADAR1 rather than ADAR2 were selected since ADAR1 is expressed ubiquitously at higher levels than ADAR2 and thus may be considered a more therapeutically relevant recruitment target than ADAR2. In addition to optimising chemical modifications for editing activity, an N-acetylgalactosamine (GalNAc) modification was included to improve AIMer delivery to hepatocytes via asialoglycoproteins receptor [REF]. The use of these short and chemically modified AIMers circumvented the need for packaging the gRNA into an ancillary delivery vehicle such as a lipid nanoparticle or AAV. For testing the optimised AIMers in vivo, three different, 30 nt GalNAc AIMers were used to target a UAG sequence in the 3’UTR of the ACTB transcripts in the liver of cynomolgus monkeys. Six animals were dosed with subcutaneous injections once a day for 5 days. Two days post final injection, all AIMers showed substantial editing ranging from 30 to 50%. At day 45 post injection, the editing persisted and ranged from 35 to 39% (Figure 4). While the editing rates of the initially best performing ADARs decreased over time, the editing rate of the AIMer with the lowest initial editing efficiency of 30% showed a small increase in editing efficiency to 34% post injection, highlighting the importance of gathering longitudinal editing data. No bystander editing or hepatotoxicity was observed and off-target editing at predicted sites was <5%. An overview of the different gRNA designs used to recruit native ADAR is given in Figure 5.Overview of the design and development of ADAR recruiting guide RNAs.The feasibility of targeting pre-mRNA with RNA editing has been investigated in several studies. Apart from determining which guide RNA is most effective, the ability to target the pre-mRNA would broaden the RNA editing scope to intronic and splice site mutations, which currently present important therapeutic targets in DNA editing and ASO therapy [REF]. Furthermore, it would expand RNA editing to target exonic mutations adjacent to splice donor and splice acceptor site, which have been shown to cause both missense and splicing defects [REF].Localisation of the components required for editing (endogenous ADAR and gRNA) is an important indication of the feasibility of pre-mRNA targeting. Both Yi et al. and Reautschnig et al. demonstrated gRNA localisation to both the cytoplasm and the nucleus, albeit in different proportions. While Yi et al. found the circ-arRNA to localise to both cellular compartments in equal proportions, Reautschnig et al. found the CLUSTER 16p8 gRNA predominantly localised to the nucleus when transfected. These findings suggest that RNA editing takes place at least partially in the nucleus. The nuclear localisation of ADAR1p110 (and ADAR2) underscores the probability of this assumption. Reautschnig et al. compared the editing yields of a pre-mRNA gRNA to those of a gRNA targeting the mRNA in the coding sequence close to an exon/intron border of three endogenously expressed genes in HEK293FT cell lines. For the GUSB target transcript, there was no difference in editing rates between the two gRNAs. For the other two targets, GPI and NUP43, the pre-mRNA gRNA was found to be one-third and one half as effective as a gRNA targeting mature mRNA. The Wei group investigated pre-mRNA and mRNA gRNA targeting the coding sequence of in the IDUA transcript both in vitro and in vivo [REF]. In a primary fibroblast line of a patient with Hurler syndrome patient, the gRNA targeting the pre-mRNA outperformed the gRNA targeting the mature mRNA. The same held true when the two arRNA were electroporated into another primary fibroblast cell line of a patient with an IDUA mutation, where the arRNA targeting the pre-mRNA showed an editing rate of nearly 30%. When tested in vivo on the Idua W392X mouse model the circ-arRNA/pre-mRNA showed the same target efficiency of 10% as achieved with the circ-arRNA/mRNA. These results were confirmed when looking at IDUA enzyme activity and decrease in GAG accumulation, which both gRNA exhibited in equal measure. In a particularly interesting study, Katrekar et al. used a pre-mRNA targeting gRNA to correct a G > A point mutation located in the last nucleotide of exon 4 of the OTC gene transcript of the spfash mouse. Due to its location, the G > A mutation was shown to lead to the production of a mutant protein as well as missplicing by read-through of the splice donor site [REF]. A 4.6–8.2% correction in the pre-mRNA transcripts as well as a reduction in the incorrectly spliced product was observed. In the correctly spliced OTC mRNA, a high edited fraction (4.6–33.8%) was seen.For RNA therapeutics to be effective, achieving delivery to the target cell type is a critical step. Whereas RPE cells lend themselves to oligonucleotide uptake by having phagocytic and endocytic capabilities [REF], entry into photoreceptor cells is more challenging. As RNA structures are functional in the cytoplasm, direct delivery of RNA may be desirable as it avoids the need to overcome the nuclear membrane barrier. Such delivery of small RNA structures appears to be highly viable in the retina with transfer of antisense oligonucleotides (AON) proving very effective. Intravitreal delivery of an AON targeting the mouse Ush2a gene was detected in photoreceptor cells up to 259 days post-injection, which was associated with desired exon 12 skipping [REF]. Developed by ProQR, clinical trials are now ongoing to deliver an AON for skipping the human equivalent region, exon 13 (NCT05085964, NCT05176717, NCT05158296) following encouraging results from a similar AON therapeutic targeting the CEP290 gene [REF]. These data indicate that small RNA structures can be delivered safely to the vitreous and make their way to the photoreceptor cells where they appear to survive and maintain activity for many weeks. Other chemically modified small RNAs have also been developed and delivered in vivo by intravitreal injection with promising pan-retinal survival up to 9 days post-injection [REF]. ADAR-mediated RNA editing has been achieved with chemically modified short RNAs [REF] and though yet to be tested in the retina, the ProQR AON studies offer encouraging signs that translation to the eye may be viable.However, longer RNA structures may be necessary depending on the treatment strategy, for example, the LEAPER strategy [REF] uses RNA of 71–91 nucleotides to encourage ADAR recruitment. Delivery of longer RNA structures to the retina has been attempted in different ways. Chemically modified Cy5-labelled mRNA (for translation of GFP) was provided by subretinal or intravitreal injection into 6–7-week-old wild-type mice, either as naked RNA or with a lipofectamine-based transfection reagent [REF]. At 24 h post-injection, the mRNA delivered with the transfection reagent by subretinal injection was evident around the injection site in the RPE and photoreceptor cells. By 7 days, the Cy5-labelled mRNA was barely detectable, but GFP expression was apparent in the photoreceptor cells and RPE. No mRNA or GFP expression was evident in eyes that received the naked mRNA. The inclusion of the transfection reagent was also important for mRNA survival following intravitreal injection. At 7 days post-injection, the mRNA was predominantly evident in the ganglion cell layer with some detection in the inner nuclear layer. Sporadic signs of GFP expression were only apparent in the inner nuclear layer. Achieving any delivery of mRNA to the retina and in particular the photoreceptor cells is highly encouraging, but the timeframe of survival and subsequent expression levels are of some concern. RNA editing strategies will likely require sustained presence of the RNA therapeutic and whilst regular repeat treatments of the less invasive intravitreal injection may be viable, it would be preferable to achieve long-term efficacy from a single treatment.Lipid nanoparticles have been used to carry mRNA encoding reporter proteins (eGFP or mCherry) to the retina [REF]. Initial data indicated subretinal injection of these vectors enabled reporter expression in Müller glia and RPE cells at 24- and 72-h post-injection, which was much reduced by 120 h. Nanoparticles may therefore be an option for treatments targeting the RPE, but the timeframe of the expression profile is again of some concern and delivery of RNA using these carriers has yet to show efficacy in other cell types [REF].An alternative strategy may be to use nanoparticles to carry DNA that enables transcription of the required RNA structures. Whilst various nanoparticle forms provide efficient delivery to the RPE, the polyethylene glycol (PEG)-substituted polylysine (CK30PEG) nanoparticles appear to be a promising form for delivery of therapeutic DNA to photoreceptor cells [REF]. However, if a DNA element for long-term expression of the RNA structure is required then adeno-associated virus (AAV) is likely to be the vector of choice. Subretinal injection would provide targeted delivery and sustained transgene expression in the photoreceptor cells. Compared to other therapeutic transgenes, RNA structures for enabling endogenous ADAR-mediated editing are relatively short and therefore easy to package. A U6-expression cassette has been used to deliver ADAR-recruiting RNA for editing in mouse muscle [REF] and more recently, the circular ADAR-recruiting guide RNAs discussed previously in this review were packaged in AAV and delivered for editing in the liver [REF]. If native ADARs prove to be active in the photoreceptor cells, then delivery of ADAR-recruiting RNA structures may be most effectively provided by AAV."
    },
    "2023-37293591_610_en.txt": {
        "title": "The current state of artificial intelligence in endoscopic diagnosis of early esophageal squamous cell carcinoma",
        "prompt": "Abstract: Esophageal squamous cell carcinoma (ESCC) is a common malignant tumor of the digestive tract. The most effective method of reducing the disease burden in areas with a high incidence of esophageal cancer is to prevent the disease from developing into invasive cancer through screening. Endoscopic screening is key for the early diagnosis and treatment of ESCC. However, due to the uneven professional level of endoscopists, there are still many missed cases because of failure to recognize lesions. In recent years, along with remarkable progress in medical imaging and video evaluation technology based on deep machine learning, the development of artificial intelligence (AI) is expected to provide new auxiliary methods of endoscopic diagnosis and the treatment of early ESCC. The convolution neural network (CNN) in the deep learning model extracts the key features of the input image data using continuous convolution layers and then classifies images through full-layer connections. The CNN is widely used in medical image classification, and greatly improves the accuracy of endoscopic image classification. This review focuses on the AI-assisted diagnosis of early ESCC and prediction of early ESCC invasion depth under multiple imaging modalities. The excellent image recognition ability of AI is suitable for the detection and diagnosis of ESCC and can reduce missed diagnoses and help endoscopists better complete endoscopic examinations. However, the selective bias used in the training dataset of the AI system affects its general utility.\n\n1. Introduction\n",
        "text": "Esophageal cancer (EC) is a malignant tumor originating from the esophageal mucosal epithelium and is one of the most common malignant tumors of the digestive tract. The incidence rate and incidence patterns of esophageal cancer vary significantly among different countries and regions. East Asia has the highest incidence rate, which can reach twice that of the world average level (12.2/100,000) [REF]. The pathological type is mainly esophageal squamous cell carcinoma (ESCC), which accounts for more than 90% of cases in China [REF]. In relatively low-incidence areas, such as Europe and the United States, the pathological type is mainly adenocarcinoma [REF]. It was estimated that in 2020 there would be 604,000 new cases of esophageal cancer worldwide (accounting for 3.1% of all cancers), along with its incidence rate ranking tenth among all malignant tumors (the standardized incidence rate is 9.3/100,000 for males and 3.6/100,000 for females), and 544,000 deaths (accounting for 5.5%), and the mortality rate would rank sixth among malignant tumors (the standardized mortality rate is 8.3/100,000 for males and 3.2/100,000 for females) [REF]. China is an important country for esophageal cancer and according to the latest cancer report released by the National Cancer Center in 2019, 246,000 new cases of esophageal cancer and 188,000 deaths were recorded in China in 2015 [REF]. The incidence rate and mortality rate ranked sixth and fourth, respectively, among all malignant tumors, accounting for 53.7% and 55.7% of the global total, respectively [REF]. A total of 70% of patients with esophageal cancer had lost the opportunity for surgery due late detection and a high tumor burden [REF].Most early ESCC and precancerous lesions can be treated using minimally invasive methods of treatment performed under an endoscope, with a 5-year survival rate of patients being as high as 90% [REF]. Patients with advanced ESCC have a low quality of life and a poor prognosis, and the overall 5-year survival rate is less than 20% [REF]. At present, the early diagnosis rate of esophageal cancer is still low [REF]. Most patients are diagnosed after developing progressive dysphagia or metastatic symptoms, and the tumor is often in the middle or late stages by this time. The most effective method of reducing the disease burden in areas with a high incidence of esophageal cancer is to prevent the disease from developing into invasive cancer. Due to the lack of typical clinical symptoms during early esophageal cancer, the key to improving the early diagnostic rate of ESCC is to screen high-risk populations. However, because of the uneven professional level of endoscopists, there are still many missed cases due to failure to recognize lesions. Endoscopic screening program in high-risk areas of ESCC also leads to an increased workload of endoscopists. Studies have showed computer-aided endoscopic monitoring can help detect and classify suspicious lesions, thereby improving the detection rate of ESCC [REF]. Artificial intelligence (AI)-assisted endoscopic diagnosis has shown promising prospects to solve the problems of the sharp increase in the workload and low inspection efficiency [REF]. In this review, we summarize the current status of utilizing AI for endoscopic detecting of early ESCC."
    },
    "2023-37293621_907_en.txt": {
        "title": "Innovative digital technology adapted in nursing education between Eastern and Western countries: a mini-review",
        "prompt": "Abstract: Advanced digital technologies have overcome the limitation of on-site teaching, especially after the COVID-19 epidemic. Various newly-developed digital technologies, such as e-learning, virtual reality, serious games, and podcasts, have gained renewed interest and come into the spotlight. Podcasts are becoming increasingly popular in nursing education as they provide a convenient and cost-effective way for students to access educational content. This mini-review article provides an overview of the development of podcasts in nursing education in Eastern and Western countries. It explores potential future trends in the use of this technology. The literature review demonstrates that nursing education in Western countries has already integrated podcasts into curriculum design, using the podcast to convey nursing education knowledge and skills and to improve students’ learning outcomes. However, few articles address nursing education in Eastern countries. The benefits of integrating podcasts into nursing education appear far greater than the limitations. In the future, the application of podcasts can serve not only as a supplement to instructional methodologies but also as a tool for clinical practicing students in nursing education. In addition, with the aging population increasing in both Eastern and Western countries, podcasts have the potential to serve as an effective delivery modality for health education in the future, particularly for the older adult, whose eyesight declines with age, and those populations with visual impairments.\n\n1. Introduction\n",
        "text": "In recent years, rapid technological change has driven innovation in teaching strategies. Educators have combined digital technology with teaching methods and administrative strategies in ways that suit course goals, settings, and subject matter. With the rise of the COVID-19 pandemic, many schools had to conduct distance education, over long periods and in repeated intermittent waves, in response to the unpredictable occurrence of new outbreaks. Several countries have adopted educational strategies that integrate newly-developed technology devices into courses, e.g., e-learning [REF], augmented reality [REF], virtual reality [REF], mixed reality [REF], educational chatbot [REF] or podcasting [REF]. Newly-developed technology was used, springing up throughout the field of education. These devices have overcome many challenges throughout the education field, especially nursing education. Nursing education courses implement many skills by students’ hands and training how to achieve critical thinking. Of these technologies, podcasts have emerged as a more easily accessible and engaging tool for supplementing educational content. Recently, they have become increasingly important learning tools [REF]. This mini-review article aims to overview the application of podcast technology and its current state in nursing education, which may shed light on possible future trends.Podcasts start back to the 1980s. At that time, digital technology was booming, leading radio to evolve into new, far-reaching forms. The term “podcast” is derived from “iPod” combined with “broadcast.” The term podcast was first coined by the journalist Ben Hammersly from The Guardian in a newspaper article in 2004 [REF]. The definition of a podcast is an audio frequency launching via the internet, a platform homepage, or certain online portals, e. g., iTunes, or YouTube [REF]. With the advent of social media and the cloud, podcasting featured a decentralized open architecture in which audio content is stored on the website and allows users to link and download via RSS, aka. “Rich Site Summary” (or “Real Simple Syndication”). RSS was first developed by Dan Libby and Ramanathan V. Guha at Netscape in 1999, and by adding the open RSS into Apple’s platform, the podcast was transformed into a digital medium for mass consumption with the first business models stemming from the United States since 2012 (“the second age of podcasting”) [REF]. The podcast is soon becoming popular in Western countries because of its convenient properties, such as fast delivery, lower cost, and exceptional user-friendliness [REF]. In addition to independent and amateur users, podcasts are also applied by educators for knowledge exchanges, e.g., the University of Oxford provided 254 free podcasts of entire courses and lessons on iTunes in 2013. Moreover, podcasts played an integral role in the continuing education development in emergency medicine and critical care [REF]. In Eastern countries, such as Taiwan, podcasting did not gain a high profile until 2020 when two local companies– SoundOn and Firstory– provided a high-quality Chinese interface with convenient access technology via smartphones, Bluetooth, and an internet discount package to create a suitable podcast development ecosystem [REF]. In 2000, Taiwan only had 300 podcasts in the market, but it increased to more than 870 new podcasts in the first half of 2020. Furthermore, the frequency of downloads in Taiwan’s podcasts in December 2020 was 446 times and even 579 times in January 2021 compared to Jan. 2020 [REF]. Currently, podcast programs have grown drastically and steadily month-to-month by about 67% in Taiwan since 2020. Taiwanese’s favorite podcast programs are related to entertainment, interests, and professional knowledge, subsequently. Society/culture, news/politics, and gossip topics were the general interests.In general, using podcasts can benefit educational pursuits. For example, podcasts reduced visual fatigue levels, enabled cyclical listening to heighten learning effectiveness, and improved the learning experience. According to the 2020 Edison Research survey, more than half of American audiences over 12 have the habit of listening to podcasts [REF]. Sixty percent of Taiwanese podcast listeners are of the average age of 23–32, among which nearly 95% have a college degree or above, and more than half of them have the habit of listening to podcasts [REF]. In Taiwan, the government promoted distance learning in higher education in the early 1990s [REF]. Taiwan is a global front-runner in digital technology development. Taiwanese nursing educators continually work with new technologies and innovations to apply various high-technology devices in nursing curriculum teaching strategies. However, podcasts were rarely used in healthcare-related education in Taiwan."
    },
    "2023-37296485_2178_en.txt": {
        "title": "Position statement on nutrition therapy for overweight and obesity: nutrition department of the Brazilian association for the study of obesity and metabolic syndrome (ABESO—2022)",
        "prompt": "Abstract: Obesity is a chronic disease resulting from multifactorial causes mainly related to lifestyle (sedentary lifestyle, inadequate eating habits) and to other conditions such as genetic, hereditary, psychological, cultural, and ethnic factors. The weight loss process is slow and complex, and involves lifestyle changes with an emphasis on nutritional therapy, physical activity practice, psychological interventions, and pharmacological or surgical treatment. Because the management of obesity is a long-term process, it is essential that the nutritional treatment contributes to the maintenance of the individual’s global health. The main diet-related causes associated with excess weight are the high consumption of ultraprocessed foods, which are high in fats, sugars, and have high energy density; increased portion sizes; and low intake of fruits, vegetables, and grains. In addition, some situations negatively interfere with the weight loss process, such as fad diets that involve the belief in superfoods, the use of teas and phytotherapics, or even the avoidance of certain food groups, as has currently been the case for foods that are sources of carbohydrates. Individuals with obesity are often exposed to fad diets and, on a recurring basis, adhere to proposals with promises of quick solutions, which are not supported by the scientific literature. The adoption of a dietary pattern combining foods such as grains, lean meats, low-fat dairy, fruits, and vegetables, associated with an energy deficit, is the nutritional treatment recommended by the main international guidelines. Moreover, an emphasis on behavioral aspects including motivational interviewing and the encouragement for the individual to develop skills will contribute to achieve and maintain a healthy weight. Therefore, this Position Statement was prepared based on the analysis of the main randomized controlled studies and meta-analyses that tested different nutrition interventions for weight loss. Topics in the frontier of knowledge such as gut microbiota, inflammation, and nutritional genomics, as well as the processes involved in weight regain, were included in this document. This Position Statement was prepared by the Nutrition Department of the Brazilian Association for the Study of Obesity and Metabolic Syndrome (ABESO), with the collaboration of dietitians from research and clinical fields with an emphasis on strategies for weight loss.\n\n1. Introduction\n",
        "text": "This Position Statement was prepared by the Nutrition Department of the Brazilian Association for the Study of Obesity and Metabolic Syndrome (ABESO), with the collaboration of dietitians from research and clinical fields with an emphasis on strategies for weight loss. The preparation of this document was based on results from the main randomized controlled studies and meta-analyses that tested different nutrition interventions for the treatment of obesity. In addition, we included the analysis and interpretation of studies with interventions that are not scientifically recognized and have a low evidence level, but that are of great popularity. We also addressed the topics of intestinal microbiota, inflammation and nutritional genomics in the context of obesity, as well as the processes involved in weight regain. Aspects related to eating behavior and nutritional strategies recommended for the adoption of adequate eating habits were also included in this Position Statement.Obesity is a chronic, progressive disease resulting from multifactorial causes mainly related to lifestyle (sedentary lifestyle, inadequate eating habits) and to other conditions such as genetic, hereditary, psychological, cultural, and ethnic factors [REF]. The treatment is complex, long-lasting, and involves lifestyle changes with an emphasis on nutritional therapy, physical activity practice, psychological interventions, and pharmacological or surgical treatment. Due to the chronic character of obesity, the diet recommended for it’s treatment must contribute to the individual’s global health. Even a modest reduction in body weight (5%) can already improve the metabolic profile and reduce cardiovascular risk [REF].Figure 1 shows examples of strategies for the long-term follow-up of patients with obesity, seeking global health.Fig. 1Strategies for successful long-term weight lossStrategies for successful long-term weight lossThe main diet-related cause associated with obesity observed in Brazil over the last decades was the increase in calorie intake, mostly associated with a gradual increase in the consumption of ultraprocessed foods. A cross-sectional study conducted in Brazil in 2008–2009, with 55.970 participants, showed that these products represent from 15.5% (lower quartile) to 39.4% (upper quartile) of the total dietary energy intake [REF]. The greater household availability of these foods was positively and independently associated with a higher prevalence of excess weight and obesity in all age groups [REF]. According to NHANES (National Health and Nutrition Examination Survey) data from the past 2 decades, the estimated proportion of energy intake from ultraprocessed foods has increased among youths in the United States and has contributed significantly to their total energy intake (TEI) [REF]. Another important investigation conducted in several countries showed that ultraprocessed foods consumption ranged from 18% of TEI among preschool children in Colombia to 68% among adolescents in the United Kingdom. In almost all countries and age groups, the increase in ultraprocessed foods intake was a potential determinant of obesity in children and adolescents [REF].Low vegetable intake is known to be another relevant factor associated with obesity, as shown in a North American study conducted with 120,877 individuals followed during 4 years [REF]. In this investigation, there was an inverse association between weight gain and the consumption of fruits, vegetables, whole-grains, and nuts [REF]. In the Global Burden of Disease Study 2017 (GBD) [REF], even though there was no specific analysis of the diet-related causes associated with obesity, this important study demonstrated the relationship between low intake of fiber and vegetables (fruits, grains and vegetables) and increased risk of cardiovascular morbidity and mortality.In this scenario, the recommendation of a balanced diet for the prevention and treatment of obesity and its comorbidities is crucial. In the last years, the term “diet” has been associated with food restriction and highly restrictive meal plans that prioritize the exclusion of specific nutrients or food groups. The probable link between the word “diet” and food restriction might be associated with the use of the term “diet foods” in low-fat foods and sugar-free beverages [REF], or even with the beginning of the use of artificial sweeteners in the mid-twentieth century [REF]. Therefore, there was a negative reaction from the general population toward the adoption of an adequate diet, even a non-restrictive diet. Even some health professionals have opted not to prescribe diets for their patients. In this context, it is worth clarifying the real meaning of the word “diet”, which comes from Greek and has a broad meaning [REF]. The Greek term projects its meaning as a synonym for quality of life, comprehending healthy eating from a qualitative and quantitative point of view, among other practices. According to the European Society for Clinical Nutrition and Metabolism (ESPEN), diet is one of the main determinants of an individual’s future health, especially for the prevention of cardiometabolic diseases and cancer [REF]. Thus, ABESO understands that the recovery of the real meaning of the word “diet” can contribute to the adherence to a healthy diet by the population and relieve something that seems difficult to follow.Fad diets cause a significantly negative impact on the treatment of obesity. These approaches are spread by bloggers and digital influencers, as well as by some health professionals. Among the approaches that promise rapid weight loss, there are several fad diets, methods and programs, such as probiotics, teas, thermogenic capsules, phytotherapy, supplements without scientific evidence, gluten-free or lactose-free products, coconut fat, among others. These products are ineffective and have potentially harmful effects on health, and are many times created with the only purpose of promoting the marketing of products with a clear conflict of interest biases. In this regard, ABESO has been giving frequent warnings to the population about approaches that promise a rapid solution for such a complex condition as obesity.Therefore, the elaboration of an individualized healthy meal plan, respecting individual cultural habits, lifestyle, preferences and possibilities, is essential for weight loss and maintenance, facilitating easier adherence to treatment. In this sense, highly restrictive diets are difficult to follow and are associated with increased short and long-term dropouts, potentially leading to weight regain [REF].The proposed treatment involves lifestyle changes, regular practice of physical activity and the adoption of a healthy dietary pattern that promotes an energy deficit to induce weight loss [REF]. Studies that compared different diets for weight loss showed that varied proportions of carbohydrates and fats had similar results in 12 months [REF], demonstrating that the impact of calorie restriction is superior to that of macronutrient composition [REF]. Nevertheless, the adoption of dietary patterns such as the Mediterranean diet [REF] and the Dietary Approaches to Stop Hypertension (DASH) diet [REF], mainly constituted by fruits, vegetables, grains, eggs, low-fat dairy and lean meats, can bring additional benefits by reducing cardiometabolic risk. Figure 2 shows factors of a healthy eating pattern for patients with obesity.Fig. 2Healthy eating pattern for patients with obesityHealthy eating pattern for patients with obesityAn important point to consider is the fact that nutritional guidance in isolation might not be sufficient to promote eating behavior change, since obesity is a cognitive behavioral condition [REF]. Thus, the association of behavior therapy is essential for the development of abilities that will support the attainment of a healthy weight [REF].In this scenario, this Position Statement aims to discuss the main scientific evidence that may contribute to the clinical practice of health professionals regarding the nutritional treatment of obesity and reinforce the relevance of the adoption of an adequate diet that promotes energy deficit, that is sustainable in the long-term and that contributes to the individual’s global health. This document also emphasizes the importance of guiding dietitians on how to manage the nutritional treatment based on scientific evidence in addition to promoting eating behavior changes.Box 1 presents a summary of nutrition recommendations for the treatment of overweight and obesity.Box 1Summary of nutrition recommendations for the treatment of overweight and obesityDiet/StrategyStatementClass of recommendationLevel of evidenceDoes ABESO recommend?Energy densityConsumption of foods and beverages with low energy density associated with energy deficit from diet can be useful in the treatment of obesityIIbBYPortion controlPortion size control can help to reduce the excessive consumption of foods and beveragesIIbBYFoods Fruits and vegetablesEncouraging the intake of fresh fruits and vegetables, associated with a hypocaloric diet, contributes to weight managementIIaAY Fast-foodThe intake of fast-food meals high in sugars, sodium, and saturated and/or trans fat is not recommendedIIbCN Ultraprocessed foodThe consumption of ultraprocessed foods is associated with excessive calorie intake and weight gainIIIBN Sweetened beveragesSweetened beverages or even unsweetened fruit juices contribute to increased calorie intake and body weight, and should be discouraged for the prevention and treatment of obesityIANCalorie-based interventions Low-calorie dietsLow-calorie diets are fundamental for obesity treatment and the management of the meal plan must be associated with lifestyle changesIAY Very low-calorie dietsVLCDs should not be the first option for obesity treatmentIIaASpecific situations Meal replacementsMeal Replacements can help to structure a LCD and increase diet adherenceIIaAY*Based on dietary patterns MediterraneanThe Mediterranean diet has cardiovascular benefits to the individual with obesityIAY Plant-based and vegetarianVegetarian or plant-based diets with reduced intake of ultraprocessed foods can be an option for the prevention and treatment of obesityIIaAY*Based on macronutrients Low-carbLow-carb diets promote weight loss in short and medium duration (3 to 6 months)IIaANLow-carb diets do not promote more weight loss compared to other diet types in long-term studiesIIaA KetogenicThe ketogenic diet should not be recommended for the treatment of obesity as it does not promote a balanced diet or favor adherence to healthy eating habitsIIIAN Low-GIGlycemic Index as a measure of carbohydrate quality appears to be a minor determinant for body weight, weight loss and obesity preventionIIbBNBased on Meal frequency and timingIntermittent calorie restriction presents similar weight loss results as meal plans with continuous calorie restrictionIIbANWeight loss is induced by the energy deficit, and not by the fasting period per se or the number of daily mealsIIbALate consumption of most of the daily calories can negatively impact body weightIIbB Breakfast consumptionThere is no conclusive evidence if breakfast plays a causal role both for weight gain and weight lossIIbAYSweetenersThere is inconclusive evidence about the effect on weight loss of sweeteners when used alone, without healthy food choices accompanied by an energy deficitIIbBN*Supplements PhytotherapicsThere is no recommendation to use phytotherapics for weight lossIIIBN CaffeineThe use of caffeine supplements should not be considered as a strategy for weight loss and obesity treatmentIIIBN Whey protein supplementsThe prescription of whey protein is not indicated as a therapy for weight lossIIIAN Coconut fatThere is no scientific evidence to support the indication of coconut oil as a weight loss strategyIIIBN ProbioticsThere is not enough scientific evidence for the use of probiotic supplementation for weight managementIIIANThere is not enough scientific evidence for performing gut microbiota tests in clinical practice for the treatment of obesityIIIANEating behavior Cognitive behavioral therapyCognitive-Bevahioral Therapy should be used for weight management in patients with overweight and obesityIAY Motivational interviewingMotivational Interviewing can be considered and used for weight management in patients with overweight and obesityIIaAY Mindful eatingThe results related to Mindfulness-Based Interventions in the treatment of obesity are still divergent, and it is yet not possible to guarantee that this is an appropriate approach for all patientsIIaAY*Y Yes, N No*The strategy might be used as part of the hypocaloric diet; the individual’s profile must be always assessedSummary of nutrition recommendations for the treatment of overweight and obesityY Yes, N No*The strategy might be used as part of the hypocaloric diet; the individual’s profile must be always assessed"
    },
    "2023-37301929_1021_en.txt": {
        "title": "Comparing the efficacy in reducing brain injury of different neuroprotective agents following neonatal hypoxia–ischemia in newborn rats: a multi-drug randomized controlled screening trial",
        "prompt": "Abstract: Intrapartum hypoxia–ischemia leading to neonatal encephalopathy (NE) results in significant neonatal mortality and morbidity worldwide, with > 85% of cases occurring in low- and middle-income countries (LMIC). Therapeutic hypothermia (HT) is currently the only available safe and effective treatment of HIE in high-income countries (HIC); however, it has shown limited safety or efficacy in LMIC. Therefore, other therapies are urgently required. We aimed to compare the treatment effects of putative neuroprotective drug candidates following neonatal hypoxic-ischemic (HI) brain injury in an established P7 rat Vannucci model. We conducted the first multi-drug randomized controlled preclinical screening trial, investigating 25 potential therapeutic agents using a standardized experimental setting in which P7 rat pups were exposed to unilateral HI brain injury. The brains were analysed for unilateral hemispheric brain area loss after 7 days survival. Twenty animal experiments were performed. Eight of the 25 therapeutic agents significantly reduced brain area loss with the strongest treatment effect for Caffeine, Sonic Hedgehog Agonist (SAG) and Allopurinol, followed by Melatonin, Clemastine, ß-Hydroxybutyrate, Omegaven, and Iodide. The probability of efficacy was superior to that of HT for Caffeine, SAG, Allopurinol, Melatonin, Clemastine, ß-hydroxybutyrate, and Omegaven. We provide the results of the first systematic preclinical screening of potential neuroprotective treatments and present alternative single therapies that may be promising treatment options for HT in LMIC.\n\n1. Introduction\n",
        "text": "Neonatal encephalopathy (NE) remains one of the leading causes for child mortality and is a major contributor to long term morbidities, including epilepsy, cerebral palsy and neurocognitive delays [REF]. In industrialized countries the incidence of NE is 0.5–1.5 per 1000 live births3; here therapeutic hypothermia (HT) is standard treatment, reducing mortality and long-term morbidities by 20–25%, with a number needed to treat of seven [REF]. However, the burden of NE secondary to intrapartum events is disproportionately higher amongst low- and middle-income countries (LMIC). In particular, sub-Saharan Africa and Southeast Asia account for more than 85% of all NE cases worldwide with regional estimates as high as 14.9 per 1000 live births [REF]. Potential therapies are limited in LMIC as the feasibility, safety and efficacy of HT in all LMIC remains uncertain [REF]. These differences in response to HT may originate from the timing of the HI insult (longer intrapartum insults with early presentation of seizures within 2 h of birth), comorbidities such as poor nutrition and the co-existence of infection/inflammation as seen in studies in sub-Sahara Africa [REF]. Therefore, identifying alternative neuroprotective treatments to HT is of major global interest, as it will have a large impact on the global burden caused by NE.The pathophysiology following neonatal hypoxic-ischemic brain injury is complex and evolves through several different phases. A reduction in placental blood flow and fetal oxygen supply occurs in the first phase [REF]; such vasculature chances lead to a loss of cerebral autoregulation with a reduction of systemic fetal blood pressure [REF]. The decrease of oxygen and depletion of energy causes increased excitotoxicity, an increase of intracellular calcium, oxidative stress and mitochondrial dysfunction [REF], leading to primary energy failure and some apoptotic and necrotic cell death. After reperfusion at resuscitation, the initial hypoxia-induced cytotoxic oedema and accumulation of excitatory amino acids partially resolves, with apparent recovery of cerebral oxidative metabolism. It is thought that the neurotoxic cascade is largely inhibited during the latent phase. Around 6–24 h after HI, a secondary phase occurs despite adequate oxygenation and perfusion. This secondary phase is associated with failure of cerebral oxidative metabolism (secondary energy failure), further glutamate driven excitotoxicity, inflammation, free radical and reactive oxygen species (ROS) production and mitochondrial dysfunction [REF]. The third phase, lasting days to weeks or months, is characterized by chronic inflammation, which may lead to epigenetic changes and impairs neurogenesis, synaptogenesis and axonal growth [REF].Many of the putative therapies for NE target specific pathological injury processes, including excitotoxicity, oxidative stress, overproduction of reactive oxygen species (ROS), mitochondrial dysfunction, neuroinflammation and regeneration [REF]. HT-induced cellular protection, however, is a global process affecting multiple molecular and cellular mechanisms and results in a 6–10% decrease in cerebral metabolism for every 1 °C reduction in body temperature [REF]. However, as there are concerns around the safety and efficacy of HT in LMIC, we focus on economical, simple single therapies as an alternative to HT in this study.There are currently no other safe and effective therapeutic options for NE apart from HT. Some agents are currently under evaluation in randomized-controlled trials (RCTs) in combination with or without HT, for example erythropoietin (clinical trials gov: NCT02811263), allopurinol (clinical trials gov: NCT03162653), topiramate (clinical trials gov: NCT01765218), caffeine (clinical trials gov: NCT03913221), melatonin (clinical trials gov: NCT03806816) or stem cells (clinical trials gov: NCT02612155). Some very potent pre-clinical agents such as inhaled xenon and erythropoietin have failed to show significant neuroprotection in clinical trials when combined with HT [REF]. One of the challenges transferring in-vivo or in-vitro studies into clinical studies is that preclinical studies are often performed in different labs on different continents, using different treatment protocols and models. In addition, they often lack comparison to RCTs regarding experimental design and statistical analysis [REF]. The Vannucci rodent model of unilateral brain injury is an established animal model used by researchers worldwide to screen for neuroprotective potency of agents, understand the underlying pathophysiological mechanisms and study the different mode of actions following neonatal hypoxic-ischemic brain injury [REF]. However, there is considerable variability in the model with groups using different concentrations and duration of hypoxia [REF]. Often, pertinent experimental details such as temperature control during treatment or hypoxia are not reported and the number of animals used not reported [REF]. This challenges researchers to reproduce results although using the “same” animal model and questions the validity of the published results.As previously stated, different treatment options have shown neuroprotection using the Vannucci model. In addition, the Vannucci model has set the basis for hypothermic neuroprotection, which has become standard care. Nevertheless, this model has never been used to prove efficacy and compare the neuroprotective potency of multiple different potential neuroprotective agents in a standardized single lab set-up.The aims of this study were: (i) screen the literature for preclinical and clinical studies of promising putative therapeutic agents for NE; (ii) establish a rank-order for the most promising neuroprotective treatments which could be used in a LMIC setting by using specific scores based on evidence for an efficiency treatment; (iii) Test and directly compare neuroprotective efficacy of 25 therapeutic agents using the standardized Vannucci P7 rat model in an established single lab set-up to identify the most promising agents to progress to more intensive studies in large animal models."
    },
    "2023-37303544_1067_en.txt": {
        "title": "A review on proteomic and genomic biomarkers for gelatin source authentication: Challenges and future outlook",
        "prompt": "Abstract: Biomarkers are compounds that could be detected and used as indicators of normal and/or abnormal functioning of different biological systems, including animal tissues and food matrices. Gelatin products of animal origin, mainly bovine and porcine, are currently under scrutiny mainly due to the specific needs of some sectors of the population related to religious beliefs and their dietary prohibitions, as well as some potential health threats associated with these products. Thus, manufacturers are currently in need of a reliable, convenient, and easy procedure to discern and authenticate the origin of animal-based gelatins (bovine, porcine, chicken, or fish). This work aims to review current advances in the creation of reliable gelatin biomarkers for food authentication purposes based on proteomic and DNA biomarkers that could be applied in the food sector. Overall, the presence of specific proteins and peptides in gelatin can be chemically analysed (i.e., by chromatography, mass spectroscopy, electrophoresis, lateral flow devices, and enzyme-linked immunosorbent assay), and different polymerase chain reaction (PCR) methods have been applied for the detection of nucleic acid substances in gelatin. Altogether, despite the fact that numerous methods are currently being developed for the purpose of detecting gelatin biomarkers, their widespread application is highly dependent on the cost of the equipment and reagents as well as the ease of use of the various methods. Combining different methods and approaches targeting multiple biomarkers may be key for manufacturers to achieve reliable authentication of gelatin's origin.\n\n1. Introduction\n",
        "text": "Biomarkers used for food origin authentication purposes are molecules which presence relates to a specific species. Two prominent categories of these molecules are proteomic (proteins and peptides) and genomic. To be considered a proper biomarker, these molecules should ideally be responsive, specific, and applicable [REF].Collagen, a fibrous protein abundant in different animal tissues like skin, bones, and connective tissue, can be partially hydrolysed to extract gelatin, which is extensively used in food and pharmaceutical applications [REF]. There are two unique types of gelatin, namely types A and B, resulting from the pH of the pre-treatments used to generate these compounds. Type A gelatin is produced by using acid pre-treatments, and it has an isoelectric point at pHs 6–9, whereas type B gelatin, generated by alkaline pre-treatments, has an isoelectric point at pHs 4.8–5.4 [REF]. Porcine and bovine skins and bones are currently the most commonly utilized materials worldwide to supply the demands of gelatin for multiple applications [REF]. However, there are currently public health concerns related to their use as they may be a source of pathogen dissemination, such as prions associated with the consumption of mammalian gelatins [REF]. Moreover, the religious beliefs of some sectors of the population are a limiting factor in the use of gelatins of animal origin by consumers. The dietary laws and requirements followed by Muslims (Halal food) and Jews (Kosher food) need to ensure that food products are free from porcine products and by-products, while Hindu customers demand food products of no bovine origin [REF]. On the other hand, gelatin is also utilized as a supplement or traditional medicine, such as donkey-hide gelatin, a traditional Chinese medicine manufactured from donkey skin [REF].Recognizing the source of gelatin is also important for the consumers of this high-value product in order to avoid product fraud. For instance, although industrial gelatin made from leather waste can have a significant level of chromium, which can lead to kidney damage and, in extreme situations, be linked to cancer [REF], some retailers have substituted industrial gelatin for edible gelatin in food products due the low cost and ease of manufacture of this product. Sausages and pork jelly are food products that are widely appreciated as a delicacy in numerous regions of China, and both items contain gelatin. The marketing expenditure allocated to the promotion of these food products exceeds $15 million [REF]. Furthermore, other cases of food fraud were reported by Demirhan, Ulca [REF] that discovered porcine DNA in some Halal food products, such as marshmallows, gum drops, and Turkish delights, that were declared to be made with Halal gelatin when they were tested. Due to worries about allergies, law, ethics, and the practice of certain religions, using less expensive materials in edible bird's nests, such as porcine gelatin, can cause issues. Using a mix of chemometrics analysis and Fourier transform infrared spectroscopy (FTIR), porcine gelatin was found in edible bird's nests in one instance. Porcine gelatin was discovered through PCA analysis of FTIR data, indicating at least 5% adulteration in edible bird's nests [REF].On the basis of these consumer demands and increased public health concerns, researchers are working on the most reliable and convenient approaches for manufacturers to allow the identification of the original sources of gelatins. Several methods have been established, although there is not an officially authentication method defined yet, as each method has its own advantages and limitations in its implementation. On the other hand, even though some effective methods have been successfully developed in many countries, the excessive prices of equipment/reagents of some of these techniques do not allow their regular and widespread use. One of the highly acceptable strategies is the application of proteomic biomarkers, including proteins and peptides, that could be used to determine the original source of gelatins [REF], while genomic-based methods have also been proven useful for this purpose [REF].The use of novel biomarkers to identify the origin of gelatin represents a significant improvement in the field as these biomarkers offer more precise and reliable methods for gelatin authentication. This data can be utilized to strengthen quality control, guarantee food safety, and shield customers from food fraud [REF]. Hence, this review defines and highlights the methods currently available to establish the origin of gelatin, including the recent developments in proteomic approaches, including chromatography, mass spectroscopy, electrophoresis, lateral flow devices, and enzyme-linked immunosorbent assay methods, as well as genomic approaches based on nucleic acids and PCR detection, including singleplex PCR, PCR southern hybridization, and multiplex PCR methods. The main advantages and challenges of each of these approaches will also be discussed, along with the future outlook on the field and the main research gaps needed to be addressed for an efficient identification of the original biological materials from which different gelatins are generated. Several databases (PubMed, Scopus, the Web of Science, and Google Scholar) were used to identify the sources of information relevant for this review, including sources dating since 1980 onwards until the most recent developments in the field. The criteria used for the selection of the sources of information as tailored to proteomic and genomic biomarkers and their potential application in the field of gelatin authentication."
    },
    "2023-37303599_1056_en.txt": {
        "title": "Bioinspired Additive Manufacturing of Hierarchical Materials: From Biostructures to Functions",
        "prompt": "Abstract: Throughout billions of years, biological systems have evolved sophisticated, multiscale hierarchical structures to adapt to changing environments. Biomaterials are synthesized under mild conditions through a bottom-up self-assembly process, utilizing substances from the surrounding environment, and meanwhile are regulated by genes and proteins. Additive manufacturing, which mimics this natural process, provides a promising approach to developing new materials with advantageous properties similar to natural biological materials. This review presents an overview of natural biomaterials, emphasizing their chemical and structural compositions at various scales, from the nanoscale to the macroscale, and the key mechanisms underlying their properties. Additionally, this review describes the designs, preparations, and applications of bioinspired multifunctional materials produced through additive manufacturing at different scales, including nano, micro, micro-macro, and macro levels. The review highlights the potential of bioinspired additive manufacturing to develop new functional materials and insights into future directions and prospects in this field. By summarizing the characteristics of natural biomaterials and their synthetic counterparts, this review inspires the development of new materials that can be utilized in various applications.\n\n1. Introduction\n",
        "text": "Natural organisms face many challenges, including environmental changes, food acquisition, and predator threats [REF]. Therefore, organisms have evolved biological materials with multifunctional properties beyond soft tissues [REF]. Biomaterials are complex hierarchical structures composed of inorganic hard- and organic soft-ordered phases bridging the macroscale and the nanoscale [REF]. All the multiscale structural models of the biomaterials evolved their innate functional features according to survival necessities [REF]. For example, the lamellar arrangement found in nacre and conch shells endows them with outstanding mechanical strength and toughness (Fig. 1) [REF]. The structure of enamel contains neatly arranged enamel pillars filled with the organic matrix. Such a structural design maintains striking similarities among numerous species (Fig. 1) [REF]. Bone, wood, and bamboo have coaxial layered structures [REF]. This structural arrangement exhibits not only excellent mechanical properties but also exceptional transport capabilities of matters (Fig. 1). Biological materials such as fish scales, lobster claws, and insect shells have a Bouligand-type structure (Fig. 1) [REF]. Fish scales are a natural body armor with remarkable mechanical properties [REF]. Lobster claws are a natural weapon with excellent impact resistance [REF]. The Bouligand-type collagen fibers of insect shells exhibit outstanding mechanical properties and distinct structural color features [REF]. Many species depend on physical characteristics besides mechanical ones to survive [REF]. For example, from an optical perspective, changes in the microstructure of a chameleon’s skin can manipulate the color of the natural light it reflects, thereby achieving concealment [REF]. The array structure of butterfly wings not only endows it with structural color but also exhibits excellent hydrophobicity [REF]. Regarding visual sensory organs, the compound eyes of insects provide them with a full range of visual capabilities (Fig. 1) [REF].Multiscale additive manufacturing technologies. The building blocks of natural materials are primarily composed of an organic matrix and inorganic precursors. Various biomicrostructures result in the formation of different biofunctional materials. For example, (i) the coaxial layered structure leads to the formation of bone, wood, and bamboo; (ii) the columnar microstructure contributes to the formation of enamel; (iii) the Bouligand-type structure yields biomaterials such as fish scales, lobster claws, and insect shells; (iv) an array structure can be found in insect compound eyes; and (v) the lamellar arrangement exists in nacre and conch shells. These natural microstructures inspire the development of additive manufacturing technologies, including self-assembling, layer-by-layer, freeze-casting, 3D printing, and other related techniques.Learning from nature can discover new ways to solve technical bottlenecks in industry and life [REF]. Natural biostructures are an origin of inspiration for designing next-generation structural materials with multifunctional properties [REF]. The formation process of biological materials involves multiple physiological and biochemical processes of cells, fine regulation of genes and proteins, and bottom-up self-assembly using environmental resources [REF]. This process occurs at ambient temperatures and pressures, making it economically and environmentally friendly. By gaining insight from the structure and formation process of biological materials, it is possible to design and develop new materials with high performance under room temperature and pressure [REF].However, the complexity of natural structures far surpasses conventional design and fabrication techniques, which poses a marked challenge for applying biomimetic materials in engineering systems [REF]. Understanding the multilevel structure and formation process of biological materials reveals that organisms’ growth from small to large can be considered a natural additive manufacturing process directed by numerous multiscale biological processes that eventually yield the formation of a macroscopic shape conferring local heterogeneity [REF]. Therefore, to prepare and develop biologically inspired materials, exploration and learning must be conducted from multiple scales, such as nano, micro, and macro [REF]. However, designing and preparing structural materials with cross-scale properties using technical methods remains challenging in materials science and technology. Additive manufacturing, which includes self-assembling, layer-by-layer, freeze-casting, 3D printing, and other technologies, actively constructs structures, and its ability to build ordered microscale structures provides a feasible approach for developing new materials with bioinspired functions [REF]. However, in the process of imitating the formation of natural biological structures and developing new materials with anticipated properties, cross-scale structures, and functions using additive technology, 3 critical scientific bottlenecks must be overcome: (a) understanding the mechanism of the formation process and multiscale structure of natural biological materials, (b) controlling bioinspired additive manufacturing processes in constructing cross-scale structures, and (c) understanding the connection between the structure and functional application of new materials developed by bioinspired additive manufacturing.This review outlines current bioinspired multiscale additive manufacturing advancements for designing new multifunctional materials. The review is divided into 2 parts: (a) classification of typical biomaterial examples based on their microstructural characteristics, including lamellar arrangement, columnar alignment, coaxial layered arrangement, Bouligand structure, and array structure (their multiscale structures are summarized from molecular, nano, and micro to macro levels, alongside examples of the functionality endowed by these special biological structures), and (b) a summary of additive manufacturing technologies used to prepare biomimetic materials with targeted functions based on the classified multiscale structures of biomaterials. Finally, we discuss key mechanisms for developing new functional materials with cross-scale properties in multisystems through bioinspired additive manufacturing, current challenges, and prospects."
    },
    "2023-37303753_631_en.txt": {
        "title": "State of the art review on machine learning and artificial intelligence in the study of neonatal necrotizing enterocolitis",
        "prompt": "Abstract: Necrotizing Enterocolitis (NEC) is one of the leading causes of gastrointestinal emergency in preterm infants. Although NEC was formally described in the 1960's, there is still difficulty in diagnosis and ultimately treatment for NEC due in part to the multifactorial nature of the disease. Artificial intelligence (AI) and machine learning (ML) techniques have been applied by healthcare researchers over the past 30 years to better understand various diseases. Specifically, NEC researchers have used AI and ML to predict NEC diagnosis, NEC prognosis, discover biomarkers, and evaluate treatment strategies. In this review, we discuss AI and ML techniques, the current literature that has applied AI and ML to NEC, and some of the limitations in the field.\n\n1. Introduction\n",
        "text": "Necrotizing enterocolitis (NEC) is a devastating, inflammatory disorder, which impacts mainly preterm infants and remains one of the most common gastrointestinal emergencies in the preterm infant population [REF]. In the United States alone, it is estimated that up to 9% of infants weighing less than 1,500 g at birth will develop NEC [REF]. The mortality rate from NEC is significant and has been reported up to 30%–50% depending on disease severity [REF]. Treatment strategies have remained limited, non-targeted, and have not changed significantly in decades [REF]. Although NEC was formally described in 1965 by Mizrahi et al., the specific causes have yet to be fully elucidated [REF]. To help clinicians with NEC diagnosis, Bell et al. published the first clinical staging system for NEC in 1978 that was designed to help clinicians know when to surgically intervene [REF]. Eight years later, Walsh and Kliegman published a modified version of Bell's staging system [REF]. The Bell and Modified Bell staging systems have consistently been the most widely used clinical definitions and are considered the “gold standard” in the field. However, most researchers and clinicians now focus on Bell ≥2 and believe that Bell stage 1 or Modified Bell stage 1A and 1B are considered largely non-specific [REF]. This has led to the development of six newer definitions for NEC, which all propose to be superior at NEC diagnosis than the Bell and Modified Bell staging definitions [REF].While many discoveries are being made within the NEC field, which may help prevent or treat NEC in the future, there remain fundamental limitations that clinicians and scientists in the field face. First, there is no universal definition of NEC. As discussed in the last paragraph, there now exist multiple definitions of NEC and clinicians and scientists can choose the one that suits their purposes best. This can lead to differences in what clinicians diagnose as NEC at various institutions. An added challenge is that the etiology of NEC has yet to be fully understood. Many in the field believe that NEC is a multifaceted disease and is the common end point of several pathways and pathologies. This multifaceted nature of NEC has made biomarker discovery difficult. Despite the NEC field spending ample time, resources, and research focus attempting to discover better biomarkers to aid in better prevention and mitigation strategies, all biomarkers discovered thus far have been insufficient [REF]. Therefore, NEC as a disease has the potential to benefit greatly from artificial intelligence (AI) and machine learning (ML) [REF]. So far, AI has shown promise in identification and prediction of diseases, biomarker discovery, disease risk evaluation, and development of improved treatment plans for many diseases both for adults and neonates [REF]. While AI and ML studies applied to the healthcare setting have rapidly increased in recent years, most instances have been applied to common and more well-defined diseases such as sepsis or cancer and only a few published studies have applied AI and ML to NEC. This review will summarize basic concepts of AI and ML (Section 2), present and summarize the current published literature on AI and ML in NEC (Section 3), as well as describe some of the limitations and pitfalls of AI and ML (Section 4)."
    },
    "2023-37304013_1366_en.txt": {
        "title": "Tongue acupuncture for the treatment of post-stroke dysphagia: a meta-analysis of randomized controlled trials",
        "prompt": "Abstract: Post-stroke dysphagia is the most common neurological impairment after stroke. The swallowing process is controlled by a network made up of the cerebral cortex, subcortical area, and brainstem structure. The disruption of the swallowing network after stroke leads to dysphagia. The affected swallowing muscles after stroke mainly include the laryngeal muscles (suprahyoid muscle and thyrohyoid muscle) and infrahyoid muscle. These muscles experience kinematic effects and muscle strength weakens, resulting in reduced movement in the swallowing process. Acupuncture can change the excitability of cerebral cortical nerve cells, promote the recovery of neurological function, and enhance neuromuscular excitability, ultimately improving the control of swallowing-related nerves and muscles and promoting swallowing functional recovery. In this meta-analysis, we systematically evaluate the clinical efficacy of acupuncture in the treatment of post-stroke dysphagia.Randomized controlled trials of tongue acupuncture therapy for post-stroke dysphagia were searched and selected from seven electronic databases (PubMed, CBM, Cochrane, Embase, CNKI, VPCS, and Wan fang). The Cochrane Collaboration tool was used to conduct methodological quality assessment. Rev. Man 5.4 software was utilized to perform data analysis.A total of 15 studies with 1,094 patients were included. Meta-analysis Showed that WST score WST score (MD = −0.56, 95% CI (−1.23, 0.12), Z = 1.62, p < 0.00001), SSA score (MD = −1.65, 95% CI (−2.02, −1.28), Z = 8.77, p < 0.00001). These results suggested that the treatment group (tongue acupuncture or tongue acupuncture combined with other therapies) was superior to the control group in reducing WST scores and SSA scores. The clinical efficacy of the tongue acupuncture group was better compared with the control group (MD = 3.83, 95% CI (2.61, 5.62), Z = 6.88, p < 0.00001).The meta-analysis showed that the total effective rate of patients with dysphagia after stroke in the treatment group (acupuncture, tongue acupuncture, and acupuncture combined with other therapy) was higher than that in the control group. These results indicated that acupuncture, tongue acupuncture, and acupuncture combined with other therapy can improve post-stroke dysphagia.\n\n1. Introduction\n",
        "text": "Stroke is an acute cerebrovascular disease defined as ischemia or hemorrhage, of which ischemic stroke accounts for 76% [REF], and leads to a variety of neurological defects. Statistics show that there are 2.4 million new stroke patients in China every year, about 1.1 million deaths, and 11 million stroke patients, most of which are ischemic stroke, and these statistics show an upward and younger trend year by year. China has become the country with the heaviest burden of stroke in the world [REF]. Data show that 29–78% of stroke patients have dysphagia [REF], and the mean incidence of the disease is 50% [REF]. 91% of patients with post-stroke dysphagia are mild [REF]. Although the survival rate of stroke patients has been significantly improved because of the improvement of medical skills such as first aid and thrombolysis, most survivors of stroke are affected by sequelae such as dysphagia and speech, motor, and memory impairments [REF]. Stroke is the most common neurological cause of dysphagia [REF]. Dysphagia is a process in which food cannot be transported safely and efficiently to the stomach because of the damaged structure and function of organs, such as the jaw, the Soft Palate, the lips, the tongue, the throat, the esophagus, etc. Swallowing muscles mainly include the laryngeal muscles (suprahyoid muscle and thyrohyoid muscle) and the subglottis. These muscles experience kinematic effects and muscle strength weakens, resulting in reduced movement in the swallowing process [REF]. Swallowing is one of the most complex somatic reflexes. It is controlled by the cerebral cortex, cortical medulla oblongata pathway, brainstem, the swallowing center, and pairs 3rd, 4th, 5th, 6th, and 7th of the cerebral nerves and C1, C2, and C3 of the spinal nerves [REF]. Stroke disrupts the swallowing network and can lead to dysphagia. According to the location of food passing through, swallowing can be divided into four stages: pre-oral stage, oral stage, pharyngeal stage, and esophageal stage [REF]. The majority of post-stroke dysphagia occurs in the delivery of food and fluid from the oral cavity to the stomach, and their dysfunction occurs primarily in the oral and pharyngeal phases. It often manifests as saliva or food coming out of the mouth, holding food in the mouth for a long time without swallowing, food or water coming out of the nose (nasal reflux), food sticking to the mouth or throat, and bucking when eating or drinking. Moreover, dysphagia can lead to bucking, aspiration pneumonia, malnutrition, etc. Severe cases endanger life due to asphyxia. Thus, post-stroke dysphagia seriously affects patients’ quality of life and increases family and social burdens, and it is necessary to find an effective strategy for promoting the functional recovery of patients with post-stroke dysphagia.The European Stroke Organization and the European Society for Dysphagia have jointly developed the 2021 European guidelines for the diagnosis and treatment of dysphagia after Stroke [REF]. The guidelines recommended that the treatment for post-stroke dysphagia include dietary interventions, nutritional interventions, behavioral interventions (swallowing training), oral health, medication (Capsaicin receptor 1 agonist and dopaminergic drugs), and peripheral or central nervous regulation (repetitive trans cranial magnetic stimulation, trans cranial electrical stimulation, trans cranial direct current stimulation, and pharyngeal electrical stimulation) [REF]. However, so far there is no specific and effective therapeutic schedule for the treatment of post-stroke dysphagia. In China and some East Asian countries, acupuncture has been widely used in the treatment of stroke and achieved a good curative effect. Tongue acupuncture is a special micro-acupuncture therapy. Clinical practice has proved that tongue acupuncture is an effective treatment for dysphagia after a stroke. Tongue acupuncture is a kind of swift pricking blood therapy; the acupuncture therapy has the benefits of being fast, with little pain and no side effects. It is easy to administer by acupuncturists and well-accepted by patients.In traditional Chinese medicine, post-stroke dysphagia can be classified into the “she jian” (which means sluggish tongue impeding speech) and “yin fei (which means the tongue is paralyzed and cannot work well). Their main clinical manifestations are slow rotation of tongue, uncontrolled eating, and loss of speech. Acupuncture is an effective and internationally recognized treatment of stroke that can significantly reduce the disability rate. Acupuncture can stimulate nerve terminal receptors, help nerve sensory input, promote the recovery of the damaged cerebral cortex and subcortical nerve, improve the function of the glossopharyngeal nerves and the reflex arc, and enhance the swallowing reflex [REF].Tongue acupuncture is a special micro-acupuncture therapy created by famous acupuncturist Guan Zhengzhai, based on the theory from Huang Di Nei Jing of the relationship between tongue and Zangfu-meridians theoretic and modern biological holography, combined with decades of clinical experience, and has become an important part of acupuncture together with ear and head acupuncture methods [REF].According to the theory of Chinese medicine, the heart may be reflected on the tongue, which is connected directly or indirectly with the Zangfu-meridians theoretic by the circulation of the meridians and the infusion of qi and blood, closely connected with the heart, spleen, and kidneys. The heart is said to govern blood and vessels as well as the spirit. The tongue is governed by the heart-mind and brain marrow. Stimulation of the tongue may promote brain function repair and improve post-stroke dysfunction through “blood-vessel-heart-spirit.” Acupuncture points on the tongue stimulate the connected meridians or Zangfu-meridians theoretic in order to regulate qi and blood flow, opening and closing the orifices, and at the same time nourishing the blood channels of the tongue, smoothing the tongue meridians, and promoting tongue and pharyngeal recovery. Clinical practice shows that tongue acupuncture therapy is less painful for the patient, is easy to administer, and is more effective when combined with body acupuncture. Tongue acupuncture in the treatment of dysphagia can improve swallowing function by changing the excitability of cortical nerve cells, promoting the recovery of neurological function, enhancing neuron muscular excitability, activating related pathways or the combination of both, and improving the control of swallowing-related nerves and muscles. Thus, tongue acupuncture is an effective, safe, and reliable traditional Chinese medicine therapy with many years of clinical experience and is a potential method for the treatment of dysphagia.This article aimed to evaluate the effectiveness of tongue acupuncture in the treatment of post-stroke dysphagia by Meta-analysis, and we hope to provide a reliable therapy for the treatment of post-stroke dysphagia and to promote its clinical application."
    },
    "2023-37304074_856_en.txt": {
        "title": "Subjective cognitive decline in patients with Parkinson’s disease: an updated review",
        "prompt": "Abstract: Cognitive impairment in patients with Parkinson’s disease (PD) worsens the prognosis of PD and increases caregivers’ burden and economic consequences. Recently, subjective cognitive decline (SCD), which refers to self-reported cognitive decline without detectable objective cognitive dysfunction, has been regarded as an at-risk state of mild cognitive impairment (MCI) and a prodromal stage for dementia in Alzheimer’s disease (AD). However, studies on PD-SCD have thus far been scarce, and at present there is no consensus regarding the definition of SCD nor a gold standard as an evaluation tool. The present review aimed to look for an association between PD-SCD and objective cognitive function and found that PD with SCD occurred with brain metabolic changes, which were consistent with early aberrant pathological changes in PD. Moreover, PD patients with SCD were likely to progress to future cognitive impairment. It is necessary to establish a guideline for the definition and evaluation of SCD in PD. A larger sample size and more longitudinal investigations are needed to verify the predictive effectiveness of PD-SCD and to detect earlier subtle cognitive decline before MCI.\n\n1. Introduction\n",
        "text": "Cognitive dysfunction, one of the most common non-motor symptoms (NMSs) in Parkinson’s disease (PD), is up to six times more common in PD than in the healthy aging population, worsens the prognosis of PD, and increases caregivers’ burden and economic consequences [REF]. It was estimated that 40–50% of PD presents with mild cognitive impairment (MCI) at baseline, and 75–80% of MCI progresses to dementia in a longitudinal study [REF]. Recently, the process of cognitive decline in PD patients have received growing interest.Subjective cognitive decline (SCD) refers to decreases in cognitive capacity without detectable impairment on neuropsychological tests, indicating intact cognitive functions, accompanied by pathological changes, and was believed to be an at-risk state of MCI and a prodromal stage for dementia in Alzheimer’s disease (AD) [REF]. A study showed β-amyloid deposition and atrophy as well as brain activation in people with SCD, suggesting a compensatory mechanism, which might reflect early neuronal dysfunction together with memory performance preserved [REF]. Thus, the state of SCD was recognized as an essential course of AD pathology and a risk factor for cognitive decline [REF]. Similar to AD, SCD may be an intermediate state between cognitive normality and MCI in PD. Thus, a possible three-stage clinical performance related to cognition might be applicable to patients with PD, with SCD as the prodromal phase, followed by MCI, finally leading to dementia [REF]. Neuroimaging study focused on PD patients with SCD (PD-SCD) demonstrated reduced FDG metabolism in the middle frontal, middle temporal, and occipital areas and the angular gyrus of the cortex, which suggested there may be early pathological changes in PD-SCD [REF]. Follow-up studies showed a significantly higher risk of developing PD-MCI and dementia for patients with PD-SCD compared to PD without SCD at baseline [REF]. However, there are inconsistent results, results just shown a correlation between SCD and depression, anxiety and other related mood features rather than cognitive dysfunction [REF]. For example, in Barbosa’s study, he regarded SCD as subjective cognitive complaints (SCC), and found SCD severity was related to depression (p = 0.026) rather than Montreal Cognitive Assessment (MoCA) scores (p = 0.141) in PD with normal cognition (PD-NC), which suggested clinician to alert affective disorder in PD-SCD [REF]. Study led by Baschi defined subjective memory complaints (SMC) as SCD, and results showed PD-SCD was significantly associated with anxiety (OR = 3.93) when compared to PD without SCD [REF].Overall, cognitive impairment in PD is a huge financial burden to society as well as caregivers, and it is time to highlight the importance of identifying cognitive decline as early as possible [REF]. However, studies on PD-SCD have thus far been scarce, and there is no consensus regarding the definition of SCD nor is there a gold standard as an evaluation tool. Little is known about whether there is a clear association between SCD and cognitive dysfunction or later cognitive decline in PD.The review aimed to outline how SCD has been used as a diagnostic criterion in studies on PD as well as to describe the possible correlation related to objective cognitive impairment, help recognize SCD as an at-risk state early indicator and allow clinicians to predict conversion to PD-D more accurately."
    },
    "2023-37304265_1338_en.txt": {
        "title": "HDAC inhibitors enhance the anti-tumor effect of immunotherapies in hepatocellular carcinoma",
        "prompt": "Abstract: Hepatocellular carcinoma (HCC), the most common liver malignancy with a poor prognosis and increasing incidence, remains a serious health problem worldwide. Immunotherapy has been described as one of the ideal ways to treat HCC and is transforming patient management. However, the occurrence of immunotherapy resistance still prevents some patients from benefiting from current immunotherapies. Recent studies have shown that histone deacetylase inhibitors (HDACis) can enhance the efficacy of immunotherapy in a variety of tumors, including HCC. In this review, we present current knowledge and recent advances in immunotherapy-based and HDACi-based therapies for HCC. We highlight the fundamental dynamics of synergies between immunotherapies and HDACis, further detailing current efforts to translate this knowledge into clinical benefits. In addition, we explored the possibility of nano-based drug delivery system (NDDS) as a novel strategy to enhance HCC treatment.\n\n1. Introduction\n",
        "text": "Primary liver cancer is currently the sixth most commonly diagnosed cancer and the third leading cause of cancer-related death worldwide, hepatocellular carcinoma (HCC) accounts for approximately 75%-85% of liver cancer cases [REF]. Due to the tumor heterogeneity, tumor metastasis, and resistance to traditional chemotherapeutic agents, current treatment options such as surgical resection, radiofrequency ablation, neoadjuvant chemoradiotherapy, and liver transplantation for HCC will only benefit a few percentages of patients, novel therapeutic modalities are urgently needed for patients with advanced or unresectable HCC [REF].The crucial role of the immune system in suppressing the growth, proliferation, and progression of tumors is widely accepted [REF]. The immunotherapy of tumors mainly utilizes the host immune system to fight the tumor by regulating the host’s own immune function or enhancing the immunogenicity of the tumors [REF]. HCC is considered to be inflammation-induced cancer, showing good sensitivity to immunotherapies [REF]. Immunotherapy strategies for HCC mainly include immune checkpoint inhibitors (ICIs), cell-based therapies, and tumor immune vaccines. Cytokines such as interferon also show certain anti-HCC effects [REF]. Checkpoint inhibitors are typically monoclonal antibodies that target programmed cell death protein 1 (PD-1), programmed death-ligand 1 (PD-L1) or cytotoxic T lymphocyte-associated antigen 4 (CTLA-4). PD-1 is a surface receptor highly expressed by activated T cells, B cells, dendritic cells (DC), and natural killer cells (NK) which provides inhibitory signals to the immune system to modulate the activity of immune cells in peripheral tissues and keep T-cells from attacking normal cells in the body. The interaction between PD-L1 expressed on cancer cells and PD-1 is a key mediator of cancer immune escape, which leads to the suppression of anticancer immunity and the promotion of tumor progression [REF]. Immune checkpoints blockade with anti-PD-1/PD-L1 antibodies have been successfully utilized in the treatment of various cancers such as melanoma [REF], non–small cell lung cancer [REF], bladder carcinoma [REF], Hodgkin’s lymphoma [REF], and Merkel cell carcinoma [REF]. CTLA-4, another important ICIs target, competitively inhibits the binding of the B7 ligand to the costimulatory receptor CD28, resulting in decreased peripheral T-cell activity. Specific blocking of CTLA-4 can increase the T-cell infiltration of tumors and enhance the killing effect of the immune system on tumors [REF]. In addition, chimeric antigen receptor T cells (CAR-T) and other cell therapies as well as HCC tumor immune vaccines also show good effects and application prospects. However, the unique inhibitory tumor microenvironment (TME) of HCC and the genetic differences of the host make existing immunotherapies challenges. Compared to unprecedented and durable responses in these T cell-inflamed cancers, the objective response rates (ORRs) of PD-1 and PD-L1 blockade in HCC remain relatively low [REF]. It was proved that TME, specific receptors, and signaling pathways can affect the clinical outcome of PD-1/PD-L1 treatment [REF], Combining different immunotherapies or combining immunotherapies with other modalities may provide synergistic effects and facilitate the development of the treatment of HCC [REF].Regulated by related histone-modifying enzymes (HMEs), various post-translational modifications (PTMs) of histone substrates, such as acetylation, methylation, phosphorylation, ubiquitination, and ADP ribosylation, play a crucial role in chromatin dynamics, relative gene regulation and many other biological functions [REF]. Increasing evidence indicates that abnormal epigenetic regulation of gene transcription associated with histone modifications plays a crucial role in cancer initiation, progression, and metastasis [REF]. In contrast to direct mutations or deletions in the main DNA sequence, aberrant epigenetic modifications are potentially reversible by epigenetic therapies [REF]. Several small-molecule inhibitors of HME, such as histone methylation inhibitors, histone demethylation inhibitors, histone deacetylation inhibitors, and DNA methylation inhibitors, can lead to the programmed death of tumor cells by affecting the cell cycle, angiogenesis, proliferation, and migration [REF]. To date, histone deacetylation inhibitors (HDACis) including vorinostat, romidepsin, belinostat, and panobinostat have been approved by FDA for the treatment of hematological malignancies such as cutaneous T-cell lymphoma (CTCL) and multiple myeloma [REF]. Despite promising results in the treatment of blood cancers, the therapeutic efficacy of several HDACis as a single therapeutic agent in solid tumors such as HCC has been unsatisfactory, and the prevalence of drug-induced side effects was relatively high [REF]. Till now, numerous combination therapies involving HDACis in synergy with chemotherapy, radiotherapy, phototherapy, targeted therapy, and immunotherapy have been efficiently developed to enhance therapeutic efficacy [REF].HDACis can regulate gene expression by regulating host epigenetic modification, thereby overcoming the tolerance of HCC patients to immunotherapy and enhancing the therapeutic effect. HDACis have been shown to promote immunotherapies in a variety of tumors [REF]. This effect is mainly achieved by enhancing the immunogenicity of the tumor and regulating the tumor immune microenvironment. Studies have shown that HDACis can increase the expression of PD-1/PD-L1, thereby increasing the sensitivity of tumors to ICIs treatment [REF]. In some tumors, HDACis also increase the expression of MHC molecules that assist the host immune system in recognizing tumor antigens [REF]. The regulation of HDACis on TME can promote the recruitment of T cells and NK cells and exert the function of tumor inhibition by increasing the expression of chemokines, cytokines and NK cell-related receptors. Similar mechanisms were also found in HCC. Moreover, these mechanisms work together to promote the effect of immunotherapies. The effect of HDACis on immunotherapy also allows these drugs to work without high doses. This reduces the possible cytotoxicity and adverse reactions of immune drugs, and also creates chances for wider research and application [REF].In the past few years, the rapid development of nanotechnology and its application in many fields have had a profound impact on the development of biomedicine [REF]. Nano-based drug delivery system (NDDS) constructed on the basis of nanomaterials provides an effective and powerful new strategy for enhancing the efficacy of immunotherapy drugs for HCC [REF]. NDDS specifically targets tumor cells through advanced delivery systems, overcoming inhibitory TME while effectively reducing the damage to normal cells. Currently, a large number of nanomedicine-based therapies are being developed for HCC [REF].Combined multidrug approaches for cancer treatment could overcome the limitations of single therapies, increase antitumor effects, and reduce drug resistance. In this review, we describe immunotherapies and HDACis in detail, explain the mechanism of their therapeutic effects in HCC respectively, and discuss current progress in the combination of novel immunotherapies with HDACis. In addition, concerned that the nano-based drug delivery system (NDDS) exhibits outstanding properties such as targeted delivery, TME response, and site-specific release in the delivery of multi-drug combination, we further discuss the potential clinical applications of NDDS in dual-therapy for HCC briefly."
    },
    "2023-37304269_751_en.txt": {
        "title": "Genetic bases of C7 deficiency: systematic review and report of a novel deletion determining functional hemizygosity",
        "prompt": "Abstract: Primary complement system (C) deficiencies are rare but notably associated with an increased risk of infections, autoimmunity, or immune disorders. Patients with terminal pathway C-deficiency have a 1,000- to 10,000-fold-higher risk of Neisseria meningitidis infections and should be therefore promptly identified to minimize the likelihood of further infections and to favor vaccination. In this paper, we performed a systematic review about clinical and genetic patterns of C7 deficiency starting from the case of a ten-year old boy infected by Neisseria meningitidis B and with clinical presentation suggestive of reduced C activity. Functional assay via Wieslab ELISA Kit confirmed a reduction in total C activity of the classical (0.6% activity), lectin (0.2% activity) and alternative (0.1% activity) pathways. Western blot analysis revealed the absence of C7 in patient serum. Sanger sequencing of genomic DNA extracted from peripheral blood of the patient allowed the identification of two pathogenetic variants in the C7 gene: the already well-characterized missense mutation G379R and a novel heterozygous deletion of three nucleotides located at the 3’UTR (c.*99_*101delTCT). This mutation resulted in an instability of the mRNA; thus, only the allele containing the missense mutation was expressed, making the proband a functional hemizygote for the expression of the mutated C7 allele.\n\n1. Introduction\n",
        "text": "Primary complement (C) deficiencies represent approximately 1-10% of all reported primary immunodeficiencies (PIDs) [REF], even though markedly higher numbers were documented in specific national registries [REF]. The most prevalent C deficiencies affect mannose-binding lectin (MBL), which is estimated to occur in more than 10% of the Caucasian population, and C2 [REF]. Nevertheless, the vast majority of C deficiencies has an estimated prevalence of about 0.03% in the general population [REF]. However, due to the redundancies in the immune system and to the frequent presence of asymptomatic patients, many C disorders remain undiagnosed.C defects are usually inherited in an autosomal recessive pattern (except for C1-inhibitor deficiency, which is autosomal dominant, and properdin deficiency, which is X-linked) [REF] and can be mainly distinguished in: early C deficiencies regarding the classical, lectin or alternative pathway, defects in the terminal C components, and deficiencies affecting C regulatory components and receptors [REF]. Symptoms can broadly vary from almost none to serious and even lethal infections [REF]. Apart from inherited origin, C deficiencies may also be acquired through C overconsumption by immune complexes, reduced hepatic synthesis, increased protein loss via urine, presence of autoantibodies, and somatic mutations [REF].The manifestations of inherited C deficiencies fall broadly into three different clinical scenarios, depending on which factor is missing: i) increased susceptibility to recurrent bacterial infections, due to inadequate opsonization, defective cell lysis, and/or onset of immune complex diseases, is usually associated to abnormalities in alternative, lectin and terminal pathways [REF]; ii) concomitant features of autoimmunity, especially systemic lupus erythematosus, can be observed in classical pathway defects [REF]; iii) dramatically enhanced or uncontrolled C activation leading to other immune-related disorders is mainly due to deficiencies in C regulators [REF]. As a general trend, these deficiencies result into severe sinopulmonary bacterial infections, bacteremia, and/or meningitis, mainly due to encapsulated bacteria (i.e., Streptococcus pneumoniae, Haemophilus influenzae type b, Neisseria meningitidis). In particular, recurrent neisserial infections can indicate possible terminal component deficiencies (C5-C9) [REF] and early alternative pathway deficiency (properdin) [REF].Even in the presence of clinical manifestations, the diagnosis of C deficiencies is quite challenging due to their rarity, the heterogeneity in clinical symptoms and the complexity of diagnostic procedures. Conventionally, the activity of each activation arm of the C is initially tested by functional assays based on hemolytic activity (CH50, LP50, AH50) [REF]. Therefore, methods based on enzyme-linked immunosorbent assay (ELISA) have been developed to test the functionality of all three C pathways [REF], followed by the quantification of specific individual components.Due to the close association between genetic variants and C defects, genetic approaches are increasingly becoming part of the diagnostic path to unveil C deficiencies [REF]. In particular, gene sequencing has become a robust and common tool for confirming single component deficiencies detected through quantitative and functional assays, but also an alternative and immediate diagnostic procedure when functional tests are not easily available. Moreover, functional and protein analysis are time-consuming, less accurate and dependent on C system sensitivity to freeze-thaw cycles [REF]. Despite the analysis may be complicated by copy number variations, point mutations and pseudogenes, the introduction of next generation sequencing and the improvements in genetic tools are gradually changing the approach to the diagnosis of inborn errors of immunity.In the current paper, we aimed at exploiting the promising potentialities offered by genetic workup as a support to the currently available immunological assays used to identify C deficiencies. We analyzed the case of a ten-year-old boy presenting C7 deficiency, reporting a novel mutation responsible for functional hemizygosity, and then performed a systematic review of the literature to unveil the genetic bases of C7 deficiency."
    },
    "2023-37304408_713_en.txt": {
        "title": "Efficacy of LAMP assay for Mycobacterial spp. detection to prevent treatment delays and onset of drug resistance: a systematic review and meta-analysis",
        "prompt": "Abstract: Tuberculosis (TB) remains a deadly disease affecting one-third population globally. Long turnaround time and poor sensitivity of the conventional diagnostics are the major impediments for faster diagnosis of Mycobacterial spp to prevent drug resistance. To overcome these issues, molecular diagnostics have been developed. They offer enhanced sensitivity but require sophisticated infrastructure, skilled manpower and remain expensive.In that context, loop-mediated isothermal amplification (LAMP) assay, recommended by the WHO in 2016 for TB diagnosis, sounds as a promising alternative that facilitates visual read outs. Therefore, the aim of the present study is to conduct a meta-analysis to assess the diagnostic efficiency of LAMP for the detection of a panel of Mycobacterium spp. following PRISMA guidelines using scientific databases. From 1600 studies reported on the diagnosis of Mycobacterium spp., a selection of 30 articles were identified as eligible to meet the criteria of LAMP based diagnosis.It was found that most of the studies were conducted in high disease burden nations such as India, Thailand, and Japan with sputum as the most common specimen to be used for LAMP assay. Furthermore, IS6110 gene and fluorescence-based detections ranked as the most used target and method respectively. The accuracy and precision rates mostly varied between 79.2% to 99.3% and 73.9% to 100%, respectively. Lastly, a quality assessment based on QUADAS-2 of bias and applicability was conducted.LAMP technology could be considered as a feasible alternative to current diagnostics considering high burden for rapid testing in low resource regions.\n\n1. Introduction\n",
        "text": "Tuberculosis (TB) caused by Mycobacterium tuberculosis (MTB) remains a deadly disease affecting millions of people worldwide. It is estimated to affect approximately one-third of the global population and is becoming one of the most fatal infectious diseases. MTB usually attacks the lungs, but TB bacteria can infect any part of the body such as kidney, spine, or brain [REF]. Worldwide, TB is the 13th leading cause of death and the second raging infectious killer after human immunodeficiency virus (HIV)/acquired immunodeficiency syndrome (AIDS) [REF]. In 2020, an estimated 10 million people got ill with TB worldwide, the infection being divided as 5.6 million men, 3.3 million women, and 1.1 million children. TB affects most of the countries among all age groups and can be fatal if not treated properly. Moreover, the emergence of drug-resistant strains has further complicated the problem and has become a rising obstacle against efficient therapeutics [REF]. Therapeutics are available but the effective control of the disease is impeded due to the lack of rapid and accurate diagnostics. Under such significant circumstances, there is an urgent need for rapid, accurate, and cost-effective diagnostic test for TB to identify new cases and reduce the time-to-treatment and prevent its further transmission.The current available methods are primarily based on smear microscopy (acid-fast staining), culture, and nucleic acid amplification. Although methods based on acid-fast staining are sensitive, they pose problems in low-resource places and are time-consuming [REF]. The solid culture method requires around 4-8 weeks, while liquid-based culture methods also require around 10-14 days [REF]. Nucleic acid amplification techniques are based on polymerase chain reaction (PCR) or loop-mediated isothermal amplification (LAMP). Although hemi-nested PCR based on GeneXpert for MTB detection is rapid, sensitive, and specific, it also poses challenges of high cost and high end equipment dependency, which limits its implementation in low-resource regions [REF]. LAMP is an isothermal DNA amplification method that relies on four or six pairs of primers to amplify minute quantities of DNA within a shorter period with simple operation, making it more suitable for low-resource regions [REF]. Thus, research in TB diagnostics aims to find an efficient, reproducible, cost-effective tool with minimal infrastructure requirements. LAMP is a popularly adopted new age technology for rapid nucleic acid amplification which is widely used for pathogen (virus, bacteria, and malaria) detection including severe acute respiratory syndrome coronavirus 2 (SARS CoV-2) [REF]. LAMP-based detection methods have been proved to be more sensitive than GeneXpert assay. In fact, the World Health Organization (WHO) has endorsed LAMP for TB as a replacement for smear microscopy for peripheral settings [REF].In pursuit of developing better diagnostics, which are crucial for achieving global elimination of TB, we performed a systematic review and meta-analysis to access the diagnostic accuracy of LAMP to detect mycobacteria. Even if couple of studies have depicted the efficacy of LAMP during the last decade, an updated version is missing. Moreover, most of these studies were specific to either pulmonary or extrapulmonary TB. Therefore, the present study not only offers an up-to-date diagnostic performance of LAMP for TB detection but also covers other Mycobacterium spp. The pooled sensitivity and specificity of LAMP were analyzed against different references. Further, diagnostic efficiency was determined based on reference methods, target genes, and detection methods of LAMP. Taken together, we aimed to evaluate the diagnostic potency of LAMP as a tool for detection of mycobacteria to address the current TB diagnosis burden in low-resource places."
    },
    "2023-37304435_818_en.txt": {
        "title": "Mental health adverse events with cannabis use diagnosed in the Emergency Department: what are we finding now and are our findings accurate?",
        "prompt": "Abstract: We have previously reviewed the types and numbers of cannabis-associated adverse events that have mental health presentations that are encountered in the Emergency Department. A particular challenge in examining these events is disentangling cannabis use adverse events from adverse events associated with use of multiple recreational substances. Since that review was published, cannabis legalization for recreational use has greatly expanded world-wide and with these changes in the legal climate has come clearer information around the frequency of adverse events seen in the Emergency Department. However, as we examined the current state of the literature, we also examined some of research designs and the biases that may be impacting the validity of the data in this field. The biases both of clinicians and researchers as well as research approaches to studying these events may be impacting our ability to assess the interaction between cannabis and mental health. For example, many of the studies performed examining cannabis-related admissions to the Emergency Department were administrative studies that relied on front line clinicians to identify and attribute that cannabis use was associated with any particular admission. This narrative review provides an overview on what we currently know about mental health adverse events in the Emergency Department with a focus on the mental health impacts both for those with and without a history of mental illness. The evidence that cannabis use can adversely impact genders and sexes differently is also discussed. This review outlines what the most common adverse events related to mental health with cannabis use are; as well as noting the most concerning but much rarer events that have been reported. Additionally, this review suggests a framework for critical evaluation of this field of study going forward.\n\n1. Introduction\n",
        "text": "Cannabis was legalized for recreational use in Canada on 17 October 2018. The reactions to this legislative action appear to be primarily split between two quite divergent viewpoints: positive from the groups who campaigned for cannabis legalization and disappointment from groups involved in treating individuals who experience the negative outcomes of cannabis use. Use of cannabis has quietly increased since legalization in Canada but the enormous business potential expected by the proponents of legalization have also failed to materialize [REF]. Cannabis use overall in Canada has increased each year since legalization from an estimated 15% of all adults over age 15 in 2017 (pre-legalization) to 25.2% of all adults over 15 in 2021 [REF]. This increase is similar to what has been seen in other countries that have legalized (as opposed to decriminalized) cannabis use [REF]. The onset of the COVID-19 pandemic and opioid crisis have stalled what efforts were being made to attempt to inform the general public of the potential harms that cannabis use can pose for some individuals. While it is generally agreed that cannabis adverse events are not common, with increasing tetrahydrocannabinol (THC) concentration coupled with increased frequency of use, this may become a more common issue. For example, the rate of cannabis use disorder, the DSM-5 diagnosis for cannabis dependence or abuse, has increased from an estimated 10% of cannabis users to 22% [REF]. The need to communicate the risks of cannabis use is ever increasing as there are now 38 states in the United States that have legalized medical cannabis use with 19 legalizing recreational use [REF]. South Africa, the Seychelles and Ghana have decriminalized cannabis for personal use [REF]. Other countries such as Canada and Uruguay have fully legalized cannabis use for both recreational and medical use [REF]. One comprehensive study examined the use of cannabis pre-post legalization in 587 4 year colleges in the United States (US) from 2008 to 2018 comparing cannabis use in college students in states with legalized recreational cannabis to those in states with restricted cannabis approaches and found that past 30 day use increased more in colleges where recreational cannabis was legal (OR = 1.23; 95% CI 1.19–1.28) [REF]. Here we aggregate the common and uncommon psychiatric adverse events that can be experienced with cannabis use with the hope that this will serve as a resource for Emergency Department (ED) personnel in discussing cannabis use in relation to ED visits for those who have experienced an adverse event related to mental health.Since publishing our previous paper [REF], further studies have examined adverse events related to cannabis use that can be experienced and result in an Emergency Department visit primarily based on administrative data. Papers examining cannabinoid hyperemesis syndrome (CHS) are probably the most common and the most frequently picked up by the media as this is clearly a dramatic adverse event and the related paradoxical effect of increased nausea that can be associated with use of cannabis during pregnancy. An article on the topic in relation to ED impacts was conducted by Andrews and colleagues but like all the cannabis related side effects, this side effect only affects a small proportion of users. However, from pre-post legalization in Canada, the number of ED visits per 100,000 increased from 15 to 21 to 32 in 2020 [REF]. This still represents a small number of cannabis users. If we frame this as a response to a pharmaceutical that is under consideration for government licensing; this frequency would mean it was considered a rare side effect. If we extend this analysis to examine cannabis adverse events in the same manner as a standard government approved pharmaceutical then the more serious adverse events would be the risk of stroke and some of the lung associated injuries such as hemoptysis which are all more clearly associated with heavy use [REF]. These are what we could consider medical side effects of cannabis use with clear quantitative measures of imaging to show the damage from the event with still more research needing to be done regarding dose relationships and temporal association; but this is outside the scope of this review. What is even more complex to examine and disentangle are the mental health adverse events which we try to address here. Additionally, we examine some of the factors that we believe are potentially complicating analysis of data in this area."
    },
    "2023-37304438_1811_en.txt": {
        "title": "Research status and prospects of acupuncture for autism spectrum disorders",
        "prompt": "Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental disorder and has a predilection for children. Its symptoms, such as lifelong social communication deficits and repetitive sensory-motor behaviors, put a huge burden on the patient’s family and society. Currently, there is no cure for ASD, and some medications that can improve its symptoms are often accompanied by adverse effects. Among many complementary and alternative medicine (CAM) therapies, acupuncture has shown promising application potential, but after years of practice, it has not been recognized as the preferred CAM therapy for ASD. Therefore, we analyzed and discussed the clinical study reports of acupuncture in the treatment of ASD in the past 15 years from the aspects of study subjects, group setting, intervention modalities, acupoint selection, outcome evaluation, and safety. The data accumulated at present are not sufficient to support the clinical effectiveness of acupuncture in ASD and to justify its use in clinical practice. They provide, however, initial evidence of possible effectiveness and encourage further investigation in order to reach firm conclusions. Based on a comprehensive analysis, we believed that following the Standards for Reporting Interventions in Clinical Trials of Acupuncture (STRICTA) and Consolidated Standards of Reporting Trials (CONSORT), screening the optimal combination of acupoints applying a rigorous scientific study design, and performing the related functional experiments may be the effective way to convincingly test the hypothesis that acupuncture may be beneficial in ASD patients. The significance of this review is to provide a reference for researchers to carry out high-quality clinical trials of acupuncture in the treatment of ASD from the perspective of the combination of modern medicine and traditional Chinese medicine.\n\n1. Introduction\n",
        "text": "Autism spectrum disorder (ASD), also known as autism, is a kind of neurodevelopmental disorder characterized by communication deficits, social impairments, repetitive and stereotyped behaviors, and sensory-motor coordination defects [REF]. The latest diagnostic systems, the International Classification of Diseases 11th Revision (ICD-11) and Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition, revised (DSM-5-TR) used the term ASD to summarize autism, Asperger syndrome, and several other diseases [REF]. The World Health Organization estimated that one in 160 children globally has ASD, with a male-to-female ratio of 4:1 [REF]. The number of reported ASD cases has increased approximately 20-fold over the past 30 years. Currently, the prevalence of ASD in the United States and China has exceeded 2%, of which the number of patients in China has exceeded 10 million, increasing at a rate of nearly 200, 000 cases every year [REF]. ASD is child-specific, and its symptoms mostly manifest in early childhood and persists throughout life, which has a serious negative impact on the patients’ daily life, family, and social development. Therefore, active prevention and treatment of ASD has always been an important work committed by medical workers all over the world.At present, the pathogenesis of ASD remains undefined, but some researchers believed that genetic, environmental, neurodevelopmental, and immune factors play an important role in its occurrence and development [REF]. There has been no effective treatment method for ASD due to the lack of sufficient evidence to clarify its mechanism [REF]. In modern medicine, several drugs have been used to treat ASD, but these drugs are only used to target specific symptoms or comorbidities rather than ASD [REF]. Although risperidone and aripiprazole can improve patients’ agitation and irritability, they are often accompanied by adverse effects such as weight gain, nausea, sedation, and hyperglycemia [REF]. In addition, methylphenidate, atomoxetine, and guanfacine have been used to improve attention-deficit and hyperactivity disorder, but can cause adverse effects such as sleep disruption, loss of appetite, nausea, irritability, fatigue, sedation, and hypotension [REF]. In addition to pharmacological therapy, behavioral and educational intervention (BEI) has also been used in the treatment of ASD, but most high-quality BEI need 20–40 h of treatment per week, and it takes a long time to show benefits [REF]. Therefore, both doctors and patients’ families are actively searching for an effective alternative or complementary therapy for ASD [REF].Complementary and alternative medicine (CAM) was first proposed by the National Institutes of Health (NIH) in the United States. It refers to medical care practice outside of modern mainstream medicine. Complementary medicine refers to the diagnosis and treatment methods used simultaneously with alternative conventional medicine. Alternative medicine refers to the method of treating diseases instead of modern mainstream medicine [REF]. The regimens of CAM have been tried in 52–74% of ASD patients, and even some patients received at least seven CAM therapies [REF]. Currently, the CAM therapies for ASD can be divided into biologically based treatments and non-biologically based treatments [REF]. Biologically based treatments include dietary interventions, nutraceuticals, hyperbaric oxygen therapy, and chelation. Although studies have found that eliminating gluten and casein in diet is beneficial to ASD patients, such trials are often difficult to be effectively controlled due to the lack of control group, poor diagnostic characteristics, small sample size, and non-standardized outcome measurement [REF]. Chelation has various adverse effects, especially the loss of essential elements in the body during the treatment process [REF]. The National Center for Complementary and Alternative Medicine (NCCAM) has divided non-biologically based treatments into three groups: mind–body medicine (including yoga, music, and dance), manipulative and body-based practices (such as massage, chiropractic care, and acupuncture), and energy medicine (such as Reiki or homeopathy) [REF]. To date, all these therapies have been observed to be beneficial in treating ASD to some extent, but there is not enough evidence to confirm their clinical effectiveness [REF]. Acupuncture has been shown to be more promising in the treatment of ASD [REF].Acupuncture is a unique treatment method of traditional Chinese medicine (TCM). It consists of inserting the needle into the specific acupoints of the patient’s body at a certain angle, and then the acupoint is stimulated with twirling, lifting, and thrusting of the needle, so as to achieve the purpose of treating diseases [REF]. Acupuncturist usually take the subject’s feeling of “De-qi”(气, 针感) as the standard to judge the treatment response [REF]. In China, acupuncture has been practiced for thousands of years. Many ancient TCM books have recorded its efficacy in treating a variety of diseases, including neurological and mental diseases, such as stroke, depression, and Alzheimer’s disease [REF]. At present, acupuncture has spread to many countries and regions around the world, especially in the United States [REF]. It is reported that 22 states in the United States licensed acupuncturists through the Medical Board, and 48 states have statutes and regulations that determine licensure and medical professional training requirements [REF]. However, despite this development, acupuncture has not been recognized as the authoritative CAM recommendation for the treatment of mental diseases due to the lack of sufficient scientific evidence, which is why we carried out this review [REF].Unlike modern medicine, which only aims at the disease itself or symptoms, the principle of TCM treatment of diseases emphasizes the overall concept and syndrome differentiation [REF]. The advantage of this medical concept is that it can treat patients individually and reduce the probability of adverse events [REF]. In TCM, ASD has no corresponding term, but its symptoms are in line with the category of “Shen zhi” diseases (神志病; mental disorders), such as “Dian zheng” (癫证) (sluggishness, depression, irritability, and decreased appetite), “Wu chi zheng” (五迟证) (delays in standing, walking, hair, teeth, and speech development), and “Jie lu”(解颅) (delayed closure of fontanels, delayed bone development, which is similar to congenital or acquired hydrocephalus in modern medicine) [REF]. The pathogenesis of ASD in TCM is related to the congenital deficiency of the Zang-Fu (viscera) organs such as the heart, liver, spleen, and kidney, as well as their acquired mutual regulation imbalance [REF]. Therefore, TCM divides ASD into four syndrome types, namely, syndrome of exuberant fire of heart and liver (心肝火旺证), syndrome of heart spirit confused by phlegm (痰蒙心窍证), deficiency of heart and spleen syndrome (心脾两虚证), and deficient kidney essence syndrome (肾精不足证) [REF]. In terms of treatment, TCM restores the internal balance of the body by clearing the heart and calming the liver, clearing the heart and purging fire, refreshing the brain and opening the orifices, promoting Yang, and relieving heat [REF]. Correspondingly, according to the need of different syndromes, acupuncturists will select a group of acupoints in various parts of the human body, such as head (scalp acupuncture), ear (auricular acupuncture), tongue (tongue acupuncture), and trunk and limbs (body acupuncture) for a certain session of treatment [REF]. Compared with other CAM therapies of modern medicine, acupuncture has the advantages of low cost, lasting effect, and no serious side effects [REF]. With the development of medical technology, acupuncture has also derived a variety of therapies such as laser acupuncture (LA), electroacupuncture (EA), and transcutaneous electrical acupoint stimulation (TEAS), which have been proved to be suitable for the treatment of a variety of human diseases [REF]. However, because the theory of TCM is complex and difficult to understand, there are differences in the mastery of knowledge among acupuncturists [REF]. In addition, the clinical application of acupuncture also lacks a universal international authoritative standard. Therefore, the analysis and discussion of previous studies will help to formulate unified standards and carry out relevant high-quality research in the future.Several articles have analyzed the clinical studies on acupuncture in the treatment of ASD in recent years, but most authors have not made an in-depth interpretation of the relevant TCM knowledge of ASD and the function of acupoints, especially the potential relationship between TCM and acupuncture and modern medicine [REF]. Therefore, we reviewed the clinical study reports on acupuncture in the treatment of ASD in past 15 years. The reports mainly included randomized, controlled, and double-blinded trial, pilot study, case report, prospective single-blinded controlled study, and retrospective study on manual acupuncture, EA, LA, and TEAS in the treatment of ASD. These reports are analyzed and discussed from the aspects of study subjects, group setting, intervention modalities, acupoints selection, outcome evaluation, and safety. Furthermore, we also discussed the relevant mechanisms of some acupoints selected in these reports by associating TCM with modern medicine in order to provide a more comprehensive reference for the development of acupuncture in the treatment of ASD."
    },
    "2023-37304444_1576_en.txt": {
        "title": "An integrative systematic review of employee silence and voice in healthcare: what are we really measuring?",
        "prompt": "Abstract: The history of inquiries into the failings of medical care have highlighted the critical role of communication and information sharing, meaning that speaking up and employee silence have been extensively researched. However, the accumulated evidence concerning speaking-up interventions in healthcare indicates that they achieve disappointing outcomes because of a professional and organizational culture which is not supportive. Therefore, there is a gap with regard to our understanding of employee voice and silence in healthcare, and the relationship between withholding information and healthcare outcomes (e.g., patient safety, quality of care, worker wellbeing) is complex and differentiated. The following integrative review is aimed at addressing the following questions; (1) How is voice and silence conceptualized and measured in healthcare?; and (2) What is the theoretical background to employee voice and silence?. An integrative systematic literature review of quantitative studies measuring either employee voice or employee silence among healthcare staff published in peer-reviewed journals during 2016–2022 was conducted on the following databases: PubMed, PsycINFO, Scopus, Embase, Cochrane Library, Web of Science, CINAHL and Google Scholar. A narrative synthesis was performed. A review protocol was registered on the PROSPERO register (CRD42022367138). Of the 209 initially identified studies for full-text screening, 76 studies met the inclusion criteria and were selected for the final review (N = 122,009, 69.3% female). The results of the review indicated the following: (1) concepts and measures are heterogenous, (2) there is no unifying theoretical background, and (3) there is a need for further research regarding the distinction between what drives safety voice versus general employee voice, and how both voice and silence can operate in parallel in healthcare. Limitations discussed include high reliance on self-reported data from cross-sectional studies as well as the majority of participants being nurses and female staff. Overall, the reviewed research does not provide sufficient evidence on the links between theory, research and implications for practice, thus limiting how research in the field can better inform practical implications for the healthcare sector. Ultimately, the review highlights a clear need to improve assessment approaches for voice and silence in healthcare, although the best approach to do so cannot yet be established.\n\n1. Introduction\n",
        "text": "Healthcare organizations are unique, in the sense that the services provided involve risks that can range from minor nuisances to life-threatening and/or fatal consequences for patients, which puts a lot of pressure on healthcare professionals, staff, administrators, boards and policymakers. The reality of day-to-day practices in healthcare was brought into sharp focus globally during the recent COVID-19 pandemic, which highlighted the fragility of healthcare systems globally revealing the considerable stress experienced by healthcare staff [REF]. Both past and more recent inquiries into the failings of care have highlighted the critical role of communication and information sharing, indicating that speaking up and voicing concerns is an integral part of safe clinical practice [REF]. The same inquiries, however, have shown that (a) staff’s voiced concerns are frequently not acted upon until a disaster point is reached, (b) professionals with high calling intensity (i.e., professions with psychological contracts that encourage presenteeism even when employees are ill) frequently remain silent on critical issues related to patient safety and/or unprofessional behavior, and (c) whistleblowing is still considered the most “successful” channel to address systemic and organizational problems that have remained unresolved for a long time. Silence in health care has been related to concealing personal errors and covering errors made by others [REF], as well as reduced patient safety [REF].Two influential definitions of employee voice are those of Morrison [REF] which describes “employee voice as informal and discretionary communication of ideas, suggestions, concerns, problems, or opinions about work-related issues, with the intent to bring about improvement or change” (p. 80) and that of LePine and Van Dyne [REF] with voice being a term for “speaking out and challenging the status quo with the intent of improving the situation” (p. 853). More recently, the research on employee voice has been enriched by the increased interest in behaviors of withholding information from colleagues or superiors in the workplace, known as “employee silence”. One of the most influential definitions of employee silence has been provided by Pinder and Harlos [REF]: “an employee’s intentional withholding of genuine expression about behavioral, cognitive, and/or affective assessments of organizational conditions to organizational members who seem capable of changing the situation”. Tangirala and Ramanujam [REF] define employee silence as “employees’ intentional withholding of critical work-related information from other members of their workgroup” (p. 41). However, it remains unclear what is included in the terms “employee voice” and “employee silence” in healthcare, as they can sometimes be discussed in terms of safety voice and safety silence (i.e., speaking about patient safety/patient advocacy or concealing information related to patient safety, respectively); how employee voice/silence fits within the theoretical literature on organizational culture and behavior in healthcare; and/or whether it extends to the professional culture and identity of healthcare staff. This is also related to the fact that although there is a general agreement that employee silence refers to withholding information and employee voice refers to sharing information in the workplace, any attempt to operationalize employee voice/silence reveals difficulties and challenges in identifying what should be considered voice/silence. This can in turn affect the way voice/silence are measured. For example, voice is defined as a discretionary behavior, in that individuals choose whether to engage in verbally expressing themselves or not at any particular moment with this being affected by a variety of factors [REF]. Similarly, the definitions of silence presented previously define silence as the “intentional withholding”. In healthcare though, the notion that voice/silence is a discretionary behavior can be easily misinterpreted, especially if we take into account the type of information that is often likely to be conveyed in healthcare: a concern related to patient safety and/or quality of care. Thus, the content and the context of speaking-up can differentiate the extent to which silence or voice are discretionary, as concealing a medical error for instance has ethical, moral and legal ramifications.Recent literature has suggested that, although on a lexical level employee silence and employee voice seem to be opposite terms, they might also be distinct concepts with different antecedents [REF]. Moreover, silence and voice can occur at the same time, meaning that employees might be speaking up in some situations (or regarding specific issues), but withholding their voices in other situations. For example, the definition by Pinder and Harlos [REF] specifies the withholding of “genuine expression”; this means that even in situations where employees engage in speaking, it cannot be ascertained that they are not engaging in any form of withholding voice (e.g., instead of speaking up about the unprofessional behavior of a colleague they may share a generally neutral comment on workplace behavior). This is particularly relevant to healthcare organizations, where a significant amount of the information shared (or withheld) is frequently related to patient safety and quality of care, which involves the interests not only of the healthcare professionals and the organization itself, but also those of the patients and their families—which has also been discussed as a conflict of interest [REF].The increasing empirical evidence regarding speaking-up in healthcare suggests that silence is the norm while voicing concerns is met with negative consequences for employees [REF]. For example, employee silence has been linked to employee well-being in the literature [REF]. In terms of understanding how silence/voice links with different outcomes for employees, we build upon the example of employee well-being, and more specifically burnout [REF]. The Job Demands-Control Model (JD-C) [REF], the Job Demands-Resources Model (JD-R) [REF] and the Conservation of Resources Model (COR) can help advance our understanding of employee voice behaviors in healthcare organizations [REF] and their links to employee outcomes. Both the JD-C and JD-R models view burnout through the lens of a mismatch between demands and resources; in this context employee silence could be evidence of the mismatch while employee voice could be evidence of a better fit. COR emphasizes the tendency of individuals and groups to always aim to obtaining, retaining, fostering, and protecting the resources they centrally value. One of the main principles of the COR theory suggests that when employees’ resources are (almost) depleted, individuals are more likely to enter a defensive mode to preserve the remaining resources or to seek for alternative survival/adaptation strategies if previous experiences were found to be maladaptive and consuming; these defensive modes can be sometimes aggressive and/or irrational. A common response, for example, might be defensive withdrawal, allowing the individuals to gain time to regroup, wait for help and allow the stressor to pass [REF]. Viewed through this lens, silence could be the result of an employee moving into a defensive mode in response to depleted resources.Greater understanding of how employee voice/silence among healthcare professionals is conceptualized and measured is proposed as a potentially effective way to identify what is considered employee voice/silence in healthcare. It has been argued that withholding concerns is the norm in healthcare [REF] and while healthcare organizations might share some common antecedents of voice behaviors with other industries (e.g., fear of retaliation or losing one’s job; not wanting to risk relationships with colleagues etc.), there are specific characteristics of healthcare education and professional culture that should be taken into account when examining voice among healthcare workers [REF]. Meta-analytic findings suggest that interventions to improve speaking up in healthcare achieve disappointing results [REF]. In order to better understand why this is happening, it is necessary to understand how employee silence and voice are operationalized in healthcare and whether there is a need for a new framework adapted for healthcare organizations specifically. By synthesizing and/or critiquing existing research on employee voice/silence among healthcare professionals, an Integrative Systematic Review can offer new insights and new ways of understanding the phenomenon [REF]. Therefore, the purpose of this integrative review was to explore the following questions;(1) How are employee voice and employee silence conceptualized and measured in healthcare?(2) What is the theoretical background to employee voice and silence in healthcare?"
    },
    "2023-37304848_708_en.txt": {
        "title": "A scoping review of approaches used to develop plant-based diet quality indices",
        "prompt": "Abstract: Plant-based dietary patterns are comprised of a range of foods, and increasingly, diet quality indices are used to assess them and their associations with health outcomes. As the design of these indices varies, a review of existing indices is necessary to identify common features, strengths, and considerations. This scoping review aimed to synthesize the literature on plant-based diet quality indices by examining their 1) basis for development, 2) scoring methodology, and 3) validation approaches. MEDLINE, CINAHL, and Global Health databases were systematically searched from 1980 to 2022. Observational studies were included if they assessed plant-based diets in adults, using an a priori methodology with food-based components. Studies conducted among pregnant/lactating people were excluded. Thirty-five unique plant-based diet quality indices were identified in 137 included articles published between 2007 and 2022. Indices were developed to reflect epidemiological evidence for associations between foods and health outcomes (n = 16 indices), previous diet quality indices (n = 16), country-specific dietary guidelines (n = 9), or foods from traditional dietary patterns (n = 6). Indices included 4 to 33 food groups, with fruits (n = 32), vegetables (n = 32), and grains (n = 30) the most common. Index scoring comprised of population-specific percentile cutoffs (n = 18) and normative cutoffs (n = 13). Twenty indices differentiated between healthy and less healthy plant-based foods when scoring intakes. Validation methods included construct validity (n = 26), reliability (n = 20), and criterion validity (n = 5). This review highlights that most plant-based diet quality indices were derived from epidemiological research, the majority of indices differentially scored healthy and unhealthy plant and animal foods, and indices were most often evaluated for construct validity and reliability. To ensure best practice use and reporting of plant-based dietary patterns, researchers should consider the basis for development, methodology, and validation when identifying appropriate plant-based diet quality indices for use in research contexts.\n\n1. Introduction\n",
        "text": "The global sustainable development goals of optimizing human and environmental health have accelerated interest in plant-based diets recently [REF]. Plant-based diets have been associated with a reduced risk of a wide range of noncommunicable diseases, including cardiovascular diseases, type 2 diabetes, and cancer [REF]. The FAO of the United Nations and the World Health Organisation recommend that a sustainable healthy diet is one that is predominantly plant-based, which “includes whole grains, legumes, nuts and an abundance and variety of fruits and vegetables” and “can include moderate amounts of eggs, dairy, poultry and fish; and small amounts of red meat” [REF]. Thus, plant-based diets can be broadly defined as dietary patterns characterized by high intakes of foods of plant origin, such as fruits and vegetables, and lower intakes of foods of animal origin, such as red meat and dairy products [REF].Research to date has used a variety of ways to assess the continuum of inclusion of plant- and animal-based foods and the different combinations of these foods that constitutes a plant-based diet [REF]. Traditionally many studies have investigated the healthiness of diet type classifications, such as a vegan diet, where there is complete elimination of animal products, or a vegetarian diet, where dairy and eggs may still be consumed [REF]. This results in binary classifications of study populations based on whether participants consume animal foods [REF]. However, the foods that constitute a plant-based dietary pattern may be better described along a continuum with varying degrees of inclusion and exclusion of animal foods, such as in a flexitarian diet [REF]. Hence, more recently, an increasing number of studies have used diet quality indices to characterize and score a plant-based diet [REF].Diet quality indices assess the quality and variety of foods in the diet on the basis of prior knowledge, including dietary guidelines and cultural ways of eating, and create a composite score reflecting compliance with the prespecified criteria [REF]. Several diet quality indices reflect a plant-based diet. For example, the Healthy Nordic Food Index has a strong plant-based focus and scores diets based on 6 components typically consumed as part of a traditional Nordic diet [REF]. However, more recently, specific plant-based diet quality indices have been developed to capture the continuum of plant and animal foods consumed. For example, the provegetarian food pattern by Martínez-González et al. [REF] positively scores 7 plant-based foods and negatively scores 5 animal-based foods. In addition to this, to acknowledge that not all plant-based foods are beneficial for health, Satija et al. [REF] created a series of plant-based indices to differentiate between healthy plant-based foods such as whole grains, and less healthy plant-based foods such as sugar-sweetened beverages. As such, these indices reflect an overall plant-based diet (PDI), a healthful plant-based diet (hPDI), and an unhealthful plant-based diet (uPDI) [REF]. With many new plant-based diet quality indices being developed, and increasing research examining their associations with health outcomes, it is imperative that these indices are reviewed. Therefore, this scoping review aimed to identify and critically evaluate diet quality indices used for assessing plant-based diets among adult populations, examining their basis for development, construction methodology, and validity. This synthesis will enable future studies to select the most suitable index for their research question and to inform this field of research going forward."
    },
    "2023-37304965_618_en.txt": {
        "title": "Extracellular vesicles as biomarkers and modulators of atherosclerosis pathogenesis",
        "prompt": "Abstract: Extracellular vesicles (EVs) are small, lipid bilayer-enclosed structures released by various cell types that play a critical role in intercellular communication. In atherosclerosis, EVs have been implicated in multiple pathophysiological processes, including endothelial dysfunction, inflammation, and thrombosis. This review provides an up-to-date overview of our current understanding of the roles of EVs in atherosclerosis, emphasizing their potential as diagnostic biomarkers and their roles in disease pathogenesis. We discuss the different types of EVs involved in atherosclerosis, the diverse cargoes they carry, their mechanisms of action, and the various methods employed for their isolation and analysis. Moreover, we underscore the importance of using relevant animal models and human samples to elucidate the role of EVs in disease pathogenesis. Overall, this review consolidates our current knowledge of EVs in atherosclerosis and highlights their potential as promising targets for disease diagnosis and therapy.\n\n1. Introduction\n",
        "text": "Atherosclerosis is a significant cause of cardiovascular disease (CVD) that can lead to heart attack, stroke, kidney failure, and major amputation [REF]. Approximately 17.9 million people die from CVD annually [REF]. Atherosclerosis is a chronic inflammatory process characterized by endothelial activation, accumulation of lipoproteins, and recruitment of inflammatory cells that leads to plaques that gradually enlarge and either restrict blood flow or embolize, damaging the heart or peripheral tissues [REF]. The current diagnostic methods for atherosclerosis are associated with rare but significant procedure-related consequences and considerable cost [REF]. The classical biomarkers, such as total cholesterol, low-density lipoprotein (LDL), or serum triglyceride levels, are the gold standard diagnostic tests for atherosclerosis [REF]. C-reactive protein, a non-specific inflammatory marker, has emerged as a clinical marker for residual risk in atherosclerosis patients with good cholesterol control [REF]. Many of these biomarkers can diagnose CVD but cannot definitively predict stroke or myocardial infarction (MI) risk. There is a need for new CVD biomarkers that are cost-effective, improve detection, and identify novel treatment targets. As we enter the era of precision medicine, we need a more granular understanding of biomarkers that can be used as reliable screening tools with metrics to guide personalized intervention to prevent devastating clinical events. The American Heart Association proposed seven metrics in 2010 to define and monitor cardiovascular health [REF]. Managing the disease involves non-pharmacological methods (healthy diet, regular physical activity, and tobacco abstinence) [REF] and pharmacological interventions such as statins to control lipoprotein levels [REF], with newer options such as cholesterol-binding agents (e.g., ezetimibe) [REF] and proprotein convertase subtilisin/kexin type 9 (PCSK9, lowers LDL) inhibitors (e.g., evolocumab) [REF] also available. Notably, several studies have highlighted challenges in achieving therapeutic goals for serum lipids despite high-intensity statin therapy [REF]. In some cases, surgery or stent-based therapies are required to manage more severe atherosclerosis. While current strategies can slow the progression of atherosclerosis and/or prevent clinical events [REF], further research is needed to understand the specific cellular and molecular mechanisms underpinning plaque progression to identify targets for stabilization and/or plaque regression. One area of promise includes delineating cellular communication during atherosclerotic plaque development and progression. In this regard, extracellular vesicles (EVs) have been identified as essential cell-cell communicators that may hold promise in improving our understanding of atherosclerotic disease—from biomarkers to disease pathogenesis [REF] (Figure 1). Exploring the role of extracellular vesicles in atherosclerosis: insights from biomarkers, therapeutics, pathobiology, and translational models. Extracellular vesicles (EVs) as versatile entities for various applications. EVs, small membrane-bound particles, have emerged as promising biomarkers for diagnostic and prognostic purposes in various diseases. They have also shown great potential as therapeutic agents for their ability to carry and deliver bioactive molecules. Moreover, EVs have been implicated in the pathogenesis of many diseases, including cancer, neurodegenerative diseases, and atherosclerosis. Animal studies have contributed significantly to our understanding of the biology and functions of EVs, paving the way for their clinical translation."
    },
    "2023-37304966_730_en.txt": {
        "title": "The role of conduction system pacing in patients with atrial fibrillation",
        "prompt": "Abstract: Conduction system pacing (CSP) has emerged as a promising novel delivery method for Cardiac Resynchronisation Therapy (CRT), providing an alternative to conventional biventricular epicardial (BiV) pacing in indicated patients. Despite increasing popularity and widespread uptake, CSP has rarely been specifically examined in patients with atrial fibrillation (AF), a cohort which forms a significant proportion of the heart failure (HF) population. In this review, we first examine the mechanistic evidence for the importance of sinus rhythm (SR) in CSP by allowing adjustment of atrioventricular delays (AVD) to achieve the optimal electrical response, and thus, whether the efficacy of CSP may be significantly attenuated compared to conventional BiV pacing in the presence of AF. We next evaluate the largest clinical body of evidence in this field, related to patients receiving CSP following atrioventricular nodal ablation (AVNA) for AF. Finally, we discuss how future research may be designed to address the vital question of how effective CSP in AF patients is, and the potential hurdles we may face in delivering such studies.\n\n1. Introduction\n",
        "text": "Cardiac Resynchronisation Therapy (CRT) is a cornerstone in the electrical treatment of heart failure [REF]. Conventional CRT involves biventricular (BiV) pacing from transvenous leads in the right ventricle (RV) and a coronary sinus branch to provide epicardial left ventricular (LV) stimulation. The widespread uptake of CRT has been driven by evidence showing significant benefits in both hospitalisations and mortality for patients with symptomatic dyssynchronous heart failure, that is, those with a LV ejection fraction (EF) of below 35%, and a QRS duration (QRSd) of greater than 130 milliseconds (ms) on a 12 lead electrocardiograph (ECG) [REF]. In recent years, the indications for CRT have expanded, with studies showing benefits in patients with moderate LV dysfunction who have a high burden of RV pacing [REF] and in those requiring an atrioventricular node ablation (AVNA) [REF].Despite its success, there is a significant proportion of patients who either do not derive clinical benefit from conventional CRT [REF], or who cannot be treated due to failure of LV lead implantation or inadequate LV lead performance due to issues such as high thresholds and phrenic nerve stimulation [REF]. Conduction system pacing (CSP), that is, stimulation of His-Purkinje tissue using a transvenous lead-based system [REF], is becoming increasingly popular, not only as a “bail-out” treatment in this population, but also potentially as a first-line option in selected patients [REF]. Initial studies on CSP focused on His-Bundle pacing (HBP), built on the attractive concept of restoring completely physiological ventricular activation. HBP achieves excellent cardiac resynchronisation, but implantation can be difficult, with success rates varying from 56%–95% [REF]. Challenges such as ventricular under-sensing, rising thresholds and requirement for lead revisions have also been observed during long-term follow up [REF]. Left Bundle Branch Pacing (LBBP) is a relatively novel form of CSP which involves screwing a pacing lead deep into the interventricular septum from the RV in order to capture the left bundle system [REF]. This technique has shown encouraging results from observational studies, with reported success rates of 80%–94% and significant improvements in LV function [REF]. Robust data from randomized trials, however, is currently lacking, and despite widespread uptake and investigation of CSP, important questions on its use remain.An area of significant clinical importance is the role of CSP both in the presence of, and in the treatment of atrial fibrillation (AF). AF is the most prevalent arrhythmia worldwide [REF], and is very common in the heart failure population, affecting up to a third of patients receiving CRT [REF]. Several studies have reported an attenuation of clinical benefit achieved with CRT in the presence of AF [REF]. There are likely several mechanisms behind this, including the reduction in cardiac output associated with loss of atrial systole [REF], low BiV pacing percentage due to uncontrolled ventricular rates [REF], and increased risk of inappropriate shocks from implantable cardioverter defibrillators (ICDs) [REF].In view of the significant deleterious consequences that AF has on conventional CRT, and the increasing use of CSP in this patient population, it is crucial to examine whether the presence of AF affects outcomes of HBP and LBBP. This article focuses on two areas. First, we evaluate the possible impact of AF on CSP, specifically LBBP, due to the inability to improve ventricular dyssynchrony by adjusting AVDs to achieve fusion pacing with intrinsic RV conduction [REF]. Next we review the largest clinical body of evidence in the case of CSP use following AVNA for AF. Finally, we discuss how future studies may address the important issue of CSP efficacy in patients with AF."
    },
    "2023-37304970_1025_en.txt": {
        "title": "Digitalization of prevention and treatment and the combination of western and Chinese medicine in management of acute heart failure",
        "prompt": "Abstract: Digitalization has emerged as a new trend in healthcare, with great potential and creating many unique opportunities, as well as many challenges. Cardiovascular disease is one of the major causes of disease-related morbidity and mortality worldwide, and the threat to life posed by acute heart failure is evident. In addition to traditional collegiate therapies, this article reviews the current status and subdisciplinary impact of digital healthcare at the level of combined Chinese and Western medical therapies. It also further discusses the prospects for the development of this approach, with the objective of developing an active role for digitalization in the combination of Western and Chinese medicine for the management of acute heart failure in order to support maintenance of cardiovascular health in the population.\n\n1. Introduction\n",
        "text": "Digital medicine is a new method of medical treatment that involves the application of modern computing technology and information technology to medical treatment. China has gradually begun to apply computing technology in the context of hospitals since the 1960s, and the use of digital medicine has gradually evolved from a low level, involving the use of simple applications, to one of great breadth and depth. The use of digital tools upgrades the practice of medicine to a high-definition and more personalized level. A smartphone-centric approach enables each individual to generate real-world data and pay more attention to their health [REF]. With the development of AI technology, AI is transforming electrocardiograms (ECGs) into screening tools and predictors of cardiac and non-cardiac disease through the discovery of common subclinical patterns in very large data sets, often in asymptomatic individuals, without hard-coded rules [REF]. AI can effectively interpret the information in the ECG, and can thereby provide significant help in rapidly suggesting effective diet, exercise, and maintenance interventions, identifying potential disease hazards, and enhancing the daily protection of patients. On the other hand, the retrieval of past information is a new problem, since patients' activity trajectories are not constant, different regional databases are not connected, and data can be lost due to system updates. The implementation of a consistent system of digital records and medical history-taking will undoubtedly improve the diagnosis and treatment of diseases. At the same time, duplication of tests or delays in the diagnostic or treatment process can be avoided. Ultrasound, endoscopy, tomography, and pathology/histology already offer considerable digital potential, such as AI-supported diagnosis and treatment of visceral diseases. This helps with standardization, improvement of the detection of pathological findings, and acceleration of the diagnosis [REF].Acute heart failure (AHF) is becoming a global epidemic due the high morbidity, mortality, and cost of treatment associated with it [REF]. Hospital admission for acute heart failure is the most common cause of hospitalization in patients over 65 years of age. Now that population aging is a global mega-trend, acute heart failure is bound to become a global public health burden. AHF is associated with significant multi-organ dysfunction, and particularly with worsening renal function and ultimately failure, which leads to a long clinical course for AHF patients and prolonged hospitalization of patients; the management of the acute phase also poses a challenge to hospital systems. A great deal of effort is required to achieve good outcomes. Heart failure patients have nurses who closely monitor changes in their condition during hospitalization, and overall treatment compliance is high. However, after discharge, inadequate disease awareness, along with untimely treatment and index testing, predisposes patients to improper disease management, resulting in repeated admissions and a poor prognosis. Acute heart failure can be triggered again by factors external to the patient or by their own emotions or labor, so it is also necessary to maintain and observe all the patient's basic data indicators. Therefore, certain apps and sports watches used for health monitoring and medication reminders play a very prominent role in the lives of heart failure patients. They can detect and record the patient's heart rate, blood pressure, and sleep quality, which is beneficial for the patient's self-regulation and provides a more comprehensive data record for reference by their doctors. However, although smartphones and watches can measure blood pressure, the reliability of the data is not yet accepted by doctors for analysis of the patient's condition. One way to address these serious shortcomings is to embed multimodal sensors in household appliances to enable the collection of information on physical activity, sleep quality, and vital signs in daily life [REF]. Acute heart failure has an acute onset, and once a patient is encountered with sudden onset of the disease, the time available for effective resuscitation is a matter of seconds. Efficient and timely resuscitation depends on a sound and effective emergency medical system. The modern emergency medical system consists of pre-hospital emergency management, the hospital emergency department, and monitoring rooms. The application of a dedicated network in hospitals helps to quickly transfer the data collected by wireless monitoring equipment to the hospital. The patient's consultation records can be accessed through the database, which is helpful in enabling experts to judge the patient's condition and significantly reduces the time and cost of emergency medical treatment.Currently, conventional Western medical treatment combined with Chinese medicine can effectively improve cardiac insufficiency, clinical symptoms, and quality of life in patients with acute heart failure. There is an urgent need to form clinical guidelines for the treatment of heart failure that can be followed and repeated, especially given the significant advantages of remote monitoring in the management of patients with acute heart failure. There is an urgent need to apply health programs and other digital medical technologies in order to adjust the existing Chinese and Western healthcare models and thus optimize clinical treatment protocols [REF]. Based on this view, this article summarizes the current status of the corresponding research and the main bottlenecks in integrating digital healthcare for acute heart failure with combined Chinese and Western treatments. We also draw out the prospects and potential of the use of digital prevention and treatment with combined Chinese and Western medicine in acute heart failure, in order to provide a basis for the proposal and development of new strategies and protocols for the clinical management of acute heart failure."
    },
    "2023-37305043_620_en.txt": {
        "title": "Association between abnormal lipid metabolism and tumor",
        "prompt": "Abstract: Metabolic Reprogramming is a sign of tumor, and as one of the three major substances metabolism, lipid has an obvious impact. Abnormal lipid metabolism is related to the occurrence of various diseases, and the proportion of people with abnormal lipid metabolism is increasing year by year. Lipid metabolism is involved in the occurrence, development, invasion, and metastasis of tumors by regulating various oncogenic signal pathways. The differences in lipid metabolism among different tumors are related to various factors such as tumor origin, regulation of lipid metabolism pathways, and diet. This article reviews the synthesis and regulatory pathways of lipids, as well as the research progress on cholesterol, triglycerides, sphingolipids, lipid related lipid rafts, adipocytes, lipid droplets, and lipid-lowering drugs in relation to tumors and their drug resistance. It also points out the limitations of current research and potential tumor treatment targets and drugs in the lipid metabolism pathway. Research and intervention on lipid metabolism abnormalities may provide new ideas for the treatment and survival prognosis of tumors.\n\n1. Introduction\n",
        "text": "In the past two decades, due to urbanization, economic growth and population aging, the world has experienced a rapid epidemiological transformation. Infectious diseases have been largely replaced by cardiovascular diseases as the leading cause of death, and dyslipidemia is the second largest risk factor related to cardiovascular disease, which is often ignored. Previous epidemiological studies have stated a positive correlation between abnormal lipid metabolism and increased risk of atherosclerosis, chronic kidney disease, Alzheimer’s disease and osteoporosis. Furthermore, excessive lipid accumulation is also associated with some other diseases, including diabetes [REF], non-alcoholic fatty liver disease/hepatitis, pituitary dysfunction, testosterone deficiency and hypothyroidism [REF]. At the same time, massive studies have also reported the role of lipid metabolism in regulating the biological process of tumors, especially on tumorigenic signal pathway, iron death and tumor microenvironment [REF].Alterations in energy metabolism is deemed to be a denote of cancer and an important target for cancer treatment. Uncontrolled and unlimited cell proliferation of cancer cells requires efficient energy sources, and in order to meet the needs of cancer cells, lipid metabolism is over-activated [REF]. Therefore, metabolic abnormalities, including abnormal lipid metabolism, occurred in the whole process of tumorigenesis and progression. Study on the mechanism of abnormal lipid metabolism in tumor can provide new ideas for tumor treatment.Lipid metabolism pathway plays an important role in many metabolic pathways. Lipids are defined as a diverse group of molecules insoluble in water, including triacylglycerides, phosphoglycerides, sterols and spatholipids. Among them, fatty acid is the principal component of triacylglycerol ester, which is mainly used for energy storage. Phosphoglycerides, along with sterols and sphingolipids, are the main structural components of biofilms. In addition, lipids can also play an important role as second messengers and hormones in signal transmission [REF]. Changes in lipid metabolism can alter the stability of biofilms and are associated with tumor aggressiveness. Many tumor cells exhibit a high rate of de novo lipid synthesis. In 1953, some scholars proposed that tumor tissues could synthesize lipids in a manner similar to embryonic tissues [REF]. Therefore, there is increasing evidence that cancer cells exhibit specific changes in different aspects of lipid metabolism and may be involved in the disease progression of tumors (Figure 1). This review mainly introduces the research of abnormal lipid metabolism and tumor and the potential role of drugs that related to lipid metabolism in tumor from three aspects: cholesterol, triglyceride and Sphingolipid.Abnormal lipid metabolism and tumor progression. In tumor cells, cholesterol metabolism is generally enhanced, thereby supporting the progression of cancer. This can be demonstrated from four aspects: a: enhanced cholesterol biosynthesis, b: increased exogenous cholesterol uptake by LDLR, c: increased cholesterol esterification by ACAT1, and d: increased production of hydroxysterols. In addition, the intrinsic driving factors of its carcinogenesis include: (1) activating carcinogenic genes such as MYC, which leads to the activation of mevalonate pathway genes, further increasing the expression of miR-33b, thus increasing the expression of MYC through positive feedback; (2) In the process of relying on target ABCA1, p53 mediated inhibition of the mevalonate pathway is absent; (3) SQLE activates Akt by inhibiting PTEN expression, leading to CE accumulation."
    },
    "2023-37305056_666_en.txt": {
        "title": "Jinlida granules combined with metformin improved the standard-reaching rate of blood glucose and clinical symptoms of patients with type 2 diabetes: secondary analysis of a randomized controlled trial",
        "prompt": "Abstract: Previous studies found that Jinlida granules could significantly reduce blood glucose levels and enhance the low-glucose action of metformin. However, the role of Jinlida in the standard-reaching rate of blood glucose and improving clinical symptoms has yet to be studied. We aimed to elaborate on the efficacy of Jinlida in type 2 diabetes (T2D) patients who experience clinical symptoms based on secondary analysis of a randomized controlled trial.Data were analyzed from a 12-week, randomized, placebo-controlled study of Jinlida. The standard-reaching rate of blood glucose, the symptom disappearance rate, the symptom improvement rate, the efficacy of single symptoms, and the total symptom score were evaluated. The correlation between HbA1c and the improvement of clinical symptoms was analyzed.For 12 weeks straight, 192 T2D patients were randomly assigned to receive either Jinlida or a placebo. The treatment group showed statistically significant differences in the standard-reaching rate of HbA1c < 6.5% (p = 0.046) and 2hPG (< 10 mmol/L, 11.1 mmol/L) (p < 0.001), compared with the control group. The standard-reaching rate of HbA1c < 7% (p = 0.06) and FBG < 7.0 mmol/L (p = 0.079) were not significantly different between the treatment and control groups. Five symptoms exhibited a statistical difference in symptom disappearance rate (p < 0.05). All the symptoms exhibited a significant difference in symptom improvement rate (p < 0.05). The mean change in total symptom score from baseline to week 12 was −5.45 ± 3.98 in the treatment group and −2.38 ± 3.11 in the control group, with statistically significant differences (p < 0.001). No significant correlations were noted between symptom improvement and HbA1c after 12 weeks of continuous intervention with Jinlida granules or placebo.Jinlida granules can effectively improve the standard-reaching rate of blood glucose and clinical symptoms of T2D patients, including thirst, fatigue, increased eating with rapid hungering, polyuria, dry mouth, spontaneous sweating, night sweat, vexing heat in the chest, palms, and soles, and constipation. Jinlida granules can be used as an effective adjuvant treatment for T2D patients who experience those symptoms.\n\n1. Introduction\n",
        "text": "Diabetes is becoming an increasingly serious global public health issue. The International Diabetes Federation (IDF) shows that patients with diabetes mellitus are on the rapid rise worldwide, and the number of patients with diabetes worldwide is expected to reach 783 million by 2045 [REF]. An epidemiological investigation exhibited that the estimated prevalence of diabetes in China in 2018 was 12.4%, and only 50.1% of treated diabetic patients were adequately controlled [REF]. People living with diabetes are at risk of developing several serious and life-threatening complications, leading to an increased need for medical care, reduced quality of life, and undue stress on families [REF]. Diabetes and its complications, if not well managed, can lead to frequent hospital admissions and premature death [REF]. Despite the emergence of new antidiabetic drugs, the glycemic control achieved is far from perfect, and some drugs may cause adverse events and increase cardiovascular disease as well as ineffectively improve the clinical symptoms [REF].Therefore, there is an urgent need for new drugs that are effective with minimal side effects and favorable hypoglycemic effects, as well as relieving clinical symptoms. Traditional Chinese medicine (TCM) has been applied in the treatment of diabetes for thousands of years. In recent years, TCM has made significant progress in modernization and globalization. Jinlida granule is a Chinese patent medicine composed of 17 Chinese herbs (Ginseng, rhizoma polygonati, rhizoma atractylodis lanceae, sophorae flavescentis, ophiopogon japonicus, rehmanniae, polygoni multiflori, dogwood, poria perrin, eupatorium, coptis chinensis, anemarrhena, epimedium, salvia, puerariae, semen litchi, and cortex lycii radices). It was approved by the China Food and Drug Administration (CFDA) as a treatment drug for type 2 diabetes (T2D) in 2005 (National Drug Approval No. Z20050845) and has been widely used in clinical practice [REF]. In the previous work, our research team conducted a randomized, placebo-controlled clinical trial to evaluate the efficacy and safety of Jinlida in T2D with a sample size of 192 subjects who received the Jinlida or placebo for 12 weeks based on using metformin. Our results showed that glycosylated hemoglobin (HbA1c) was reduced more significantly in the Jinlida group [REF]. Furthermore, meta-analysis and systematic reviews of Jinlida granules showed significant reductions in fasting blood glucose (FBG), 2-h postprandial blood glucose (2hPG), and HbA1c [REF]. Animal experiments show that Jinlida granules could promote the thermogenesis of brown and beige adipocytes by enhancing the mitochondrial function and inhibiting the expression of miR-27a, thereby improving the metabolism of glucose and lipids [REF]. Jinlida granules could also reduce insulin resistance in rats fed with high fat by regulating phosphorylation of c-Jun N-terminal kinase (JNK) and p38 mitogen-activated protein kinase (MAPK) [REF]. However, as previously reported, current studies in Jinlida put more emphasis on the effect of blood glucose levels and their potential mechanisms and largely ignore the role of improving clinical symptoms in T2D.Though the efficacy and safety of Jinlida, as well as its potential action mechanisms, were elucidated, the role of improving clinical symptoms in Jinlida has yet to be studied. Hence, in this work, we conducted a secondary analysis based on the previous randomized controlled trial [REF] to investigate the role of the standard-reaching rate of blood glucose and improving clinical symptoms in Jinlida to bridge the gap between hypoglycemic agents and improvement in diabetic clinical symptoms."
    },
    "2023-37305058_846_en.txt": {
        "title": "Extracellular vesicle-mediated intercellular and interorgan crosstalk of pancreatic islet in health and diabetes",
        "prompt": "Abstract: Diabetes mellitus (DM) is a systemic metabolic disease with high mortality and morbidity. Extracellular vesicles (EVs) have emerged as a novel class of signaling molecules, biomarkers and therapeutic agents. EVs-mediated intercellular and interorgan crosstalk of pancreatic islets plays a crucial role in the regulation of insulin secretion of β-cells and insulin action in peripheral insulin target tissues, maintaining glucose homeostasis under physiological conditions, and it’s also involved in pathological changes including autoimmune response, insulin resistance and β-cell failure associated with DM. In addition, EVs may serve as biomarkers and therapeutic agents that respectively reflect the status and improve function and viability of pancreatic islets. In this review, we provide an overview of EVs, discuss EVs-mediated intercellular and interorgan crosstalk of pancreatic islet under physiological and diabetic conditions, and summarize the emerging applications of EVs in the diagnosis and treatment of DM. A better understanding of EVs-mediated intercellular and interorgan communication of pancreatic islets will broaden and enrich our knowledge of physiological homeostasis maintenance as well as the development, diagnosis and treatment of DM.\n\n1. Introduction\n",
        "text": "Diabetes mellitus (DM) is a systemic metabolic disease characterized by hyperglycemia with high mortality and morbidity [REF]. The International Diabetes Federation estimates that 10.5% of people aged 20 to 79 years old are currently suffering with DM and the incidence will rise to 11.3% by 2030 and to 12.2% by 2045 [REF]. Long-term hyperglycemia brings a higher risk of neurovascular injury, which leads to a series of complications such as cardiovascular disease, diabetic neuropathy, diabetic kidney disease and diabetic retinopathy, in association with negative impacts on patients’ quality of life and heavy economic burdens [REF]. DM can be mainly classified into type 1 diabetes mellitus (T1DM) and type 2 diabetes mellitus (T2DM) based on the pathogenesis [REF]. Additionally, DM can be the secondary disease caused by Coxsackie-viral infection [REF], cystic fibrosis [REF] and pancreatic cancer [REF]. Although the etiologies of different types of DM are distinct, it is generally accepted that the progression of DM is highly associated with abnormal communications between cells, tissues and organs [REF]. Extracellular vesicles (EVs) are a group of heterogenous, multi-functional lipid bilayer membrane vesicles secreted by a wide spectrum of cells into extracellular space [REF]. The characteristics of EVs are highly heterogenous and influenced by the parental cell types [REF]. EVs contain various biologically active molecules such as nucleic acids, proteins and lipids, where the bioactivity of their contents could be preserved by the membrane structure, making EVs as suitable carriers transmitting signals among cells [REF]. Over the last decade, EVs have garnered great interest for their critical roles in mediating intercellular and interorgan communications, which have been demonstrated to be involved in various physiological and pathological processes [REF]. Beside their intrinsic bioactive properties, EVs have also been confirmed with other advantages, such as abundant sources, low immunogenicity, biocompatibility, flexibility to modify, and ability to cross biological barriers [REF], making EVs-based therapies as attractive strategies in the treatment of various diseases [REF].The pancreatic islet discovered by Paul Langerhans in 1869 consist of its own vasculature and five types of hormone-producing cells known as α-cells, β-cells, δ-cells, pp-cells and rare ϵ-cells [REF]. The hormones produced by different endocrine cells play a crucial role in controlling blood glucose level, making pancreatic islet an important mini-organ to maintain glucose homeostasis [REF]. Among different cell types, insulin-secreting β-cells occupy a decisive position given that loss and failure of β-cells are highly associated with the emergence and deterioration of DM [REF]. Present researches indicate that EVs-mediated crosstalk between pancreatic islets and extra-islet tissues plays key roles in maintaining glucose homeostasis under physiological conditions, whereas such crosstalk is strongly involved in the occurrence and development of DM under pathological conditions [REF]. Under physiological conditions, pancreatic β-cells can lower blood glucose by secreting insulin to increase glucose uptake of peripheral tissues, while peripheral tissues may promote insulin secretion and growth of β-cells via EVs-mediated crosstalk [REF]. On the other hand, EVs containing autoantigens derived from pancreatic islets may target autoimmune cells, causing severe autoimmune response in T1DM. Furthermore, peripheral tissues-derived EVs impose negative impacts on pancreatic islets, leading to dysfunction and death of islet cells in T2DM [REF]. EVs-mediated crosstalk between pancreatic islets and other tissues can also lead to the secondary diabetes after other diseases [REF]. Notably, EVs have also been widely applied as biomarkers and therapeutic agents for DM. Lakhter et al. reported that up-regulated miR-21-5p cargo in β-cell derived EVs in response to inflammatory cytokines can serve as a potential biomarker of T1DM [REF]. Sun et al. showed that EVs secreted by mesenchymal stem cells (MSCs) can be used for the treatment of T2DM by promoting survival and insulin secretion of pancreatic β-cells [REF].This review aims to outline the current knowledge of EVs-mediated intercellular and interorgan crosstalk of pancreatic islet, focusing on its roles in regulating systemic metabolism, as well as its potential applications in the diagnosis and treatment of DM. We hope to provide a new and specific perspective for understanding certain physiological and pathological processes and give new thoughts to novel biomarkers discovery and anti-diabetic drug development."
    },
    "2023-37305172_1191_en.txt": {
        "title": "Sex differences in cardiovascular outcomes of SGLT-2 inhibitors in heart failure randomized controlled trials: A systematic review and meta-analysis",
        "prompt": "Abstract: In patients with heart failure (HF), randomized controlled trials (RCTs) of sodium-glucose transporter-2 inhibitors (SGLT-2is) have proven to be effective in decreasing the primary composite outcome of cardiovascular death and hospitalizations for HF. A recently published meta-analysis showed that the use of SGLT-2is among women with diabetes resulted in less reduction in primary composite outcomes compared with men. This study aims to explore potential sex differences in primary composite outcomes among patients with HF treated with SGLT-2is.We systematically searched the medical database from 2017 to 2022 and retrieved all the RCTs using SGLT-2is with specified cardiovascular outcomes. We used the PRISMA (Preferred Reporting Items for a Review and Meta-analysis) method to screen for eligibility. We evaluated the quality of studies using the Cochrane Risk of Bias tool. We pooled the hazard ratio (HR) of the primary composite outcomes in both sexes, performed a meta-analysis, and calculated the odds ratio (OR) of the primary composite outcomes based on sex.We included 5 RCTs with a total number of 21,947 patients. Of these, 7837 (35.7 %) were females. Primary composite outcomes were significantly lower in males and females taking SGLT-2is compared to placebo (males - HR 0.77; 95 % CI 0.72 to 0.84; p = 0.00001; females - HR 0.75; 95 % CI 0.67 to 0.84; p = 0.00001). Pooled data from four of the RCTs (n = 20,725) revealed a greater occurrence of the primary composite outcomes in females compared with males (OR 1.32; 95 % CI 1.17 to 1.48; p = 0.0002).SGLT-2is reduce the risk of primary composite outcomes in patients with HF, regardless of sex; however, the benefits were less pronounced in women. Further research needs to be done to better explain these observed differences in outcomes.\n\n1. Introduction\n",
        "text": "Heart failure (HF) incidence is increasing globally, with almost half of all HF patients being women [REF]. The 2022 Heart Disease and Stroke Statistics report by the American Heart Association reported a projected rise in HF prevalence by 46 % from 2012 to 2030, expecting to increase the total percentage of the population from 2.4 % in 2012 to 3 % in 2030 [REF]. HF causes significant morbidity and mortality among women [REF], and the incidence of HF tripled between ages 65–74 and 75–84 [REF]. In women, HF tends to occur at an older age and usually manifests as HF with preserved ejection fraction (HFpEF). Furthermore, women are more likely to be more symptomatic than men [REF]. Although women are less prone than men to develop coronary artery disease (CAD) at premenopausal age, they reported worse quality of life and increased risk for depression [REF]. Guideline-directed medical therapy for HF does not differ in women; however, women are vastly underrepresented in HF trials [REF].Sodium-glucose transporter-2 inhibitors (SGLT-2is) are hypoglycemic agents which block the sodium-dependent glucose transporter-2 in the early proximal renal tubule of the kidneys. Their primary effect is increasing urinary glucose excretion and decreasing blood glucose levels. Additional effects of SGLT-2is, especially on the cardiovascular system, have been increasingly reported in the literature. In early 2022, the US Food and Drug Administration approved empagliflozin to treat HF. SGLT-2is use is currently one of the four pillars in managing patients with HF with reduced ejection fraction (HFrEF) [REF]. In patients with HF and preserved ejection fraction (HFpEF), empagliflozin showed a 21 % lower relative risk in the composite of cardiovascular death or hospitalization for HF [REF]. In the recently published 2022 American Heart Association/American College of Cardiology/Heart Failure Society of America (AHA/ACC/HSFA) heart failure guideline, SGLT-2is received a class I recommendation for patients with HFrEF, including those at risk for HF (Stage A) [REF]. Similarly, the 2021 European Society of Cardiology guidelines for acute and chronic HF highlighted the importance of SGLT-2is (class IA recommendation) in reducing HF hospitalization and mortality risk in patients with HFrEF [REF]. Possible mechanisms of cardiovascular benefits of SGLT-2 inhibitors are changes in energy metabolism, reduction of blood pressure (BP) and vessel stiffness, increase in hemoglobin and hematocrit, improvement of myocardial remodeling, and weight loss [REF].Men and women may differ in myocardial adaptation after a cardiac event [REF]. HFpEF is more common in women than in men. Furthermore, women with HFpEF have more comorbidities than those with HFrEF, with >60 % having at least four comorbidities [REF]. White women with HFpEF had the highest proportion of hospitalization for HF [REF]. A meta-analysis by Singh et al. showed that the reduction in primary composite outcomes with SGLT-2is appears to be significantly less in women with diabetes than in men [REF]. However, to our knowledge, no study has investigated whether there are sex differences in cardiovascular (CV) outcomes among HF patients on SGLT-2is. We conducted a thorough systematic review and meta-analysis of all the CV outcomes studied with SGLT-2is in patients with HF with or without type 2 diabetes mellitus (DM) to study the primary composite outcomes based on the sex of the patients.This study aimed to determine the effect of SGLT-2is on primary composite outcomes among patients with HF, with or without type 2 DM, stratified by sex, through a systematic review and meta-analysis of relevant randomized controlled trials.A comprehensive search and review of the PubMed, SCOPUS, and Cochrane databases was conducted from 2017 to 2022. Results were limited to randomized clinical trials and clinical trials only. Relevant keywords such as “SGLT-2 inhibitors,” “heart failure,” “chronic heart failure,” and “acute heart failure” and their combinations were used in the search. All subsequent articles cited in these studies were also considered and reviewed.Only dedicated cardiovascular outcome trials conducted with SGLT-2is investigating the primary outcome of the composite of cardiovascular death, hospitalization for heart failure, and urgent visit for heart failure, and published the results of subgroup analysis based on sex were included. Studies with either HFrEF or HFpEF participants were included. Other studies which did not have cardiovascular death and HF hospitalizations as the primary endpoint and did not have a randomized controlled trial design were excluded from the analysis. Studies that have met the specified inclusion and exclusion criteria were compiled. The study was done in line with the statements mentioned in the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) (11). A detailed PRISMA diagram for the process of inclusion and exclusion for selecting key studies that have been used in this meta-analysis has been depicted in Fig. 1.The full text and all supplementary appendices were obtained, screened, and reviewed using the Cochrane Risk of Bias tool. The tool evaluates studies on seven domains, including random sequence generation, allocation concealment, blinding of participants and personnel, blinding of outcome assessment, incomplete outcome data, selective reporting, and other bias. Three authors (FBR, DDL, and VST) independently extracted data using a standardized data collection form. Differences in rating given by the reviewers were discussed by all authors and resolved with a consensus. Statistical analysis was completed using Cochrane Review Manager (RevMan) version 5.4. Inverse variance weighted averages of logarithmic hazard ratio using random effects model were used to calculate for study weight. The pooled hazard ratio [HR], stratified by sex, and the 95 % confidence interval (CI) were based on intention-to-treat analysis. Odds Ratio [OR] was then computed between pooled HR to compare primary composite outcomes between sexes. Heterogeneity was evaluated using the I2 method and Cochrane Q statistics. Results were classified as low (<25 %), moderate (25 to 50 %), and high (>50 %) heterogeneity based on the 12. All reported two-sided p values are considered significant if <0.05.The primary outcome of interest was a composite of the following outcomes: cardiovascular death, hospitalization for heart failure, and urgent visit for heart failure."
    },
    "2023-37305178_1676_en.txt": {
        "title": "Circulating microRNA profiles in Wilms tumour (WT): A systematic review and meta-analysis of diagnostic test accuracy",
        "prompt": "Abstract: Wilms tumour (WT) is caused by aberrant embryonic kidney development and associated with dysregulated expression of short, non-protein-coding RNAs termed microRNAs (miRNAs). At present, there is no reliable circulating biomarker of WT, and this remains an urgent unmet clinical need. Such biomarkers may assist diagnosis, subtyping/prognostication, and disease-monitoring. Here, we established the list of dysregulated circulating miRNAs in WT from the existing published literature.Regardless of publication date, PubMed, Scopus, Web-of-Science, and Wiley online library databases were searched for English/French studies on WT circulating miRNAs. The PRISMA-compliant search was registered in PROSPERO. The QUADAS tool measured retained article quality. The meta-analysis assessed the sensitivity and specificity of miRNAs for WT diagnosis.Qualitative analysis included 280 samples (172 WT patients; 108 healthy controls) from five of 450 published articles. The study uncovered 301 dysregulated miRNAs (144 up-regulated, 143 down-regulated, 14 conflicting). The pooled sensitivity, specificity, and AUC of the 49 significantly dysregulated microRNAs from two studies was 0.67 [0.62; 0.73], 0.95 [0.92; 0.96] and 0.77 [0.73; 0.81] respectively, indicating a stronger diagnostic potential for WT.Circulating miRNAs show promise for WT diagnosis and prognosis. More research is needed to confirm these findings and determine associations with tumour stage/subtype.CRD42022301597.\n\n1. Introduction\n",
        "text": "Wilms tumour (WT), also known as nephroblastoma, is a malignant kidney tumour that accounts for 5% of all paediatric cancers and 95% of childhood malignant renal neoplasms [REF], with an incidence of 1 in 10,000 children [REF]. It is caused by aberrant kidney development at the embryonic stage, during mesenchymal epithelial transition (MET) at the beginning of nephrogenesis [REF]. It typically affects children aged 0–4 years of age [REF], with a median diagnostic age between 3 and 4 years [REF], and an equal distribution between males and females.Whilst the majority of WT cases (90%–95%) are unilateral, 5–10% of cases affect both kidneys (synchronous bilateral or multifocal tumours) [REF]. Approximately 98–99% of WT cases are sporadic, whereas 1–2% are familial [REF]. Metastatic forms are observed in approximately 12% of cases [REF]; they are 80% pulmonary, 10% hepatic [REF], but lymph node or bone metastasis may be observed in rare cases [REF]. Up to 20% of WTs are discovered inadvertently during routine check-up visits as tumours can become large without manifesting any specific symptoms [REF]. However, WT is generally characterised by the development of a palpable hard abdominal mass or swelling that may be associated with abdominal pain, intra-tumoral bleeding, microscopic haematuria (in 25% of cases), hypertension (25%), fever (20%) and vomiting [REF]. Medical imaging, primarily ultrasound, helps establish the diagnosis, which may determine if the mass is intra- or extra-renal, solid or cystic. Three dimensional imaging (CT scan and/or MRI) are then performed, which are required for staging purposes and initial disease-monitoring [REF].WT is an undifferentiated mesodermal tumour comprising variable amounts of embryonic kidney elements (blastocyst, epithelium, and stroma). Based on prognosis, there are two distinct histopathological types - favourable (>90% of cases) and unfavourable (6–10%). Anaplastic and sarcomatous variants are unfavourable and associated with poor clinical outcomes [REF]. WT is treated following either the North American NWTS protocol or the European SIOP protocol [REF]. In Morocco and much of Europe, the SIOP protocol is implemented, with preoperative chemotherapy, then total nephrectomy with primary ligation of the vascular pedicle and lower resection of the ureter, followed by postoperative chemotherapy [REF]; usually with combination chemotherapy [REF]. Sometimes, chemotherapy is combined with post-operative radiation therapy, depending on the stage and histology of the tumour [REF]. Nevertheless, whilst maximising survival, these therapies can have adverse side effects, including short-term toxicity, particularly gastrointestinal and cardiac, and long-term toxicity, which manifests in the slowing of bone growth with a risk of scoliosis as well as an increased risk of secondary malignancies [REF].MicroRNAs (miRNAs) are short, highly conserved, non-protein-coding, single-stranded RNA molecules of approximately 20–22 nucleotides in length [REF]. They were first discovered and identified in 1993 to be post-transcriptional downregulators of protein-coding genes, by binding to the 3′ untranslated region (3′UTR) of messenger RNAs (mRNAs), inhibiting protein translation (translation silencing) via deadenylation (poly-A tail degradation) [REF]. Circulating miRNAs are stable over time, even after repeated freeze-thaw cycles and are typically contained within membrane-bound particles termed exosomes/extracellular vesicles. A minority are bound to serum protein complexes (e.g., Argonaute, lipoproteins). Levels of miRNAs can change in body fluids when cellular damage or tissue injury occurs [REF].As miRNAs cause mRNA degradation, in cancer, miRNA effects are therefore ‘opposite’ depending on whether the protein-coding target gene is an oncogene or a tumour suppressor gene [REF]. Hence, up-regulation of oncomirs (miRNAs with an oncogenic effect) or down-regulation of suppressor-miRs (miRNAs with a tumour suppressor effect) may play a causal role in the generation or maintenance of tumours [REF]. Thus, expression profiles of miRNAs are used to classify cancerous and normal tissues and for disease-monitoring and response to therapy [REF]. The role of miRNAs is context-dependent. For example, a particular miRNA may act as an oncomir in one tissue/cancer and as a suppressor in another, based on the differing protein-coding transcriptomes, and thus mRNA targets, in different tissue types. Taking for example, miR-203 that acts as a tumour suppressor in pancreatic cancer by modulating DUSP5 expression [REF], in renal cancer by targeting FDF2 [REF], while it is an oncogene in breast cancer by targeting fibroblast growth factor 2 [REF] as well as in melanoma cancer by targeting BMI1 gene [REF]. Interestingly, miR-203 in ovarian cancer acts as a suppressor miR when targeting BIRC5 gene [REF], but as an oncogene by regulating PDHB gene [REF].Bioinformatic data indicates that a single miRNA can bind to more than 200 target genes of different functions. Thus, miRNAs potentially control the expression of at least one-third of human mRNAs, highlighting their potential role in most cellular processes. Several studies have shown that miRNAs act as key regulators of cell proliferation, apoptosis, and cell differentiation. Therefore, deregulation or inhibition of miRNA expression has been involved in many diseases [REF].Interest is currently focused on studying alterations in miRNA expression levels in various tissues at different stages of diverse diseases [REF]. For instance, miRNAs are implicated in numerous types of paediatric cancer, including childhood germ cell tumours [REF]. Hence, profiling miRNA expression levels in cancer can become a tool for diagnosis and prognosis or for identification of therapeutic targets [REF].MiRNAs have emerged as promising theranostic agents in the last decade, owing to their small size and ability to regulate multiple cellular pathways by targeting specific genes [REF]. This has spurred interest in developing miRNA-based therapeutics, which involves expressing or depleting specific miRNAs in diseased cells or organs using viral vectors, liposomes, antibodies or nanoparticles. However, delivering miRNAs to specific cells or tissues remains challenging [REF]. Furthermore, some studies suggest that miRNAs could be used in combination therapy, due to the ability of some mimetic or antimimetic miRNAs to modulate the sensitivity of cancer cells to chemotherapeutic drugs [REF].Clinical trials investigating the role of miRNAs in cancer treatment have shown promising results, such as the study of Xie et al. that found that miR-621 enhances chemosensitivity of breast cancer cells to paclitaxel plus carboplatin (PTX/CBP) by suppressing FBXO11-dependent inhibition of p53 [REF]. Another study by Hsu et al. demonstrated that the delivery of miR-122 to a MYC-driven mouse model of hepatocellular carcinoma strongly inhibited tumorogenesis [REF]. However, to date, there have been no clinical trials investigating the use of miRNA delivery for tumour suppression in Wilms tumour, and most studies on miRNAs and WT have been limited to in vitro experiments.WT is primarily managed using two distinct approaches. In Europe, neoadjuvant chemotherapy is often initiated without tissue biopsy; although 90–95% of patients benefit from not needing to undergo biopsy to start treatment, up to 5% of children retrospectively will be observed to have received inappropriate treatment [REF]. In contrast, North American trials have identified pre-treatment prognostic biomarkers to stratify high-risk patients for intensified treatment, and upfront tumour resection or tissue biopsy is preferred. The identification of accurate blood-based biomarkers for WT, such as miRNAs, may help prevent misdiagnosis and mistreatment and may contribute to standardising treatment by providing valuable information on risk stratification. The main focus of research was to profile miRNAs in WT tissue. Among these miRNAs, miR-21, miR-140-5p, and miR-155-5p have been the most extensively studied. MiR-21 is consistently found to be overexpressed and is linked to poor prognosis of WT due to its correlation with down-regulation of PTEN expression [REF]. On the other hand, miR-140-5p and miR-155-5p, which are typically down-regulated in WT, have demonstrated potential in suppressing cellular proliferation and metastasis. MiR-140-5p targets TGFBRI and IGF1R genes [REF], while miR-155-5p targets CREB1 gene [REF]. Despite these findings, no circulating miRNA has been widely studied as a diagnostic biomarker for WT to date. This systematic review and meta-analysis compiles the list of WT-associated circulating miRNAs identified to date, discusses their correlation with tumour histological types, where available, and assesses their diagnostic performance for WT."
    }
}