Die Auffassung vom Tod des Menschen als anthropologisch-kulturelles Phänomen kann je nach sozialer, religiöser, philosophischer oder bioethischer Prägung individuell und gesellschaftlich sehr verschieden sein. Dessen ungeachtet ist eine allgemeingültige Definition von Kriterien des (eingetretenen) Todes des Menschen als biologisches Wesen auf einer naturwissenschaftlich-medizinischen Basis möglich und aus verschiedenen Gründen (juristische, medizinische, alltagspraktische) notwendig. Neben den klassischen Kriterien des Todes (unaufhebbarer Herz-Kreislauf-Stillstand („Herztodkriterium“), äußere Todeszeichen (Totenflecke, Totenstarre, Verwesung etc.)) ist seit mehreren Jahrzehnten international das Kriterium des irreversiblen Hirnfunktionsausfalls („Hirntodkriterium“) anerkannt.Zum Hirntod kommt es auf der Intensivstation, wenn infolge einer schweren Hirnschädigung ein vollständiger, unumkehrbarer Hirnfunktionsausfall eintritt, das Herz-Kreislauf-System aber durch die maschinelle Beatmung für einen gewissen Zeitraum weiter funktioniert. Der Hirntod führt unausweichlich zum Herzstillstand, ohne Beatmung innerhalb von Minuten, unter Beatmung/Intensivtherapie i. d. R. innerhalb von Tagen.Das ärztlich-diagnostische Vorgehen zur Feststellung des irreversiblen Hirnfunktionsausfalls (Hirntoddiagnostik) ist in der seit 1982 regelmäßig aktualisierten Richtlinie der Bundesärztekammer detailliert vorgeschrieben [REF]. Die Hirntodfeststellung ist in Deutschland gemäß Transplantationsgesetz (TPG) zwingende Voraussetzung für die postmortale Organspende. Daneben wird sie auf der Intensivstation auch zur Prognosebeurteilung bei akuter schwerster Hirnschädigung mit anhaltendem Koma durchgeführt. Gegenwärtig erfolgt die Hirntoddiagnostik jährlich bei ca. 2000–3000 Patienten in Deutschland, also vergleichsweise selten gemessen an der jährlichen Sterbezahl von über 900.000 in Deutschland.Bei der Hirntoddiagnostik handelt es sich heute um die am besten dokumentierte ärztliche Todesdiagnostik, die bei richtliniengemäßer Ausführung bislang nicht zu bestätigten Fehldiagnosen geführt hat [REF]. Hingegen kommt es in Deutschland jährlich zu 3–10 fehlerhaften ärztlichen Todesfeststellungen anhand der klassischen Todeszeichen [REF]; der Scheintod fällt dann spätestens bei der Aufbahrung oder der regulären 2. Leichenschau vor Feuerbestattung auf. Dennoch gibt es eine anhaltende Kontroverse nicht in Bezug auf die klassischen Todeskriterien, sondern nur in Bezug auf das Hirntodkriterium. Diese Kontroverse nährt sich aus verschiedenen Quellen. Zum einen gibt es teilweise Sorgen über die Zuverlässigkeit der ärztlichen Diagnostik, die durch unzutreffende, skandalisierende Medienberichte bei Einzelfällen mit bekannt gewordenen Protokollfehlern geschürt wurden (z. B. referiert in [REF]).Damit im Zusammenhang steht die gelegentlich publizierte falsche Aussage, dass bei dem künstlich beatmeten und intensivtherapierten Hirntoten eine Restempfindung, z. B. von Schmerzen, verblieben sein könnte. Tatsächlich werden nach jedem erhobenen Vorwurf der fehlerhaften Hirntodfeststellung diese Fälle von speziellen Kommissionen der Bundesärztekammer detailliert untersucht; bei allen abschließend überprüften 43 Fällen der Jahre 2010–2014 konnten die Kommissionen bestätigen, dass diese Patienten hirntot waren [REF].Zum anderen gibt es eine teils philosophisch, teils religiös, teils biologisch-ethisch geprägte akademische Debatte, deren kontroverse Positionen sich im geteilten Votum des Deutschen Ethikrates 2015 pro/kontra das Hirntodkriterium mit 18 zu 7 Stimmen widerspiegeln (Position A/Position B), auch wenn der Ethikrat einstimmig der Meinung war, dass der irreversible Hirnfunktionsausfall ausreichende Voraussetzung für die (postmortale) Organspende ist [REF]. Auf Unstimmigkeiten und resultierende Dilemmata dieses Votums für das ärztliche Handeln ist hingewiesen worden [REF]. Dazu gehört die Unvereinbarkeit der Position B mit dem ärztlichen Tötungsverbot und der Dead-Donor-Regel bei der Organspende. Nachfolgend ist eine Bestätigung des Hirntodkriteriums durch die Bundesärztekammer publiziert worden [REF], unter Verzicht auf eine direkte Auseinandersetzung mit den Argumenten der Position B [REF], die teilweise bereits in den 1990er-Jahren Gegenstand wissenschaftlicher und medizinethischer Auseinandersetzung in Deutschland gewesen sind [REF].Im vorliegenden Artikel werden 6 prototypische, immer wieder vorgebrachte Thesen gegen das Hirntodkriterium aus einer aktuellen neuromedizinisch-neurowissenschaftlichen Perspektive betrachtet.
Ärzt*innen müssen in ihrer Berufstätigkeit mit einer Vielzahl von Stressoren umgehen, was mit einer erhöhten Gesundheitsgefährdung einhergehen kann. In einer US-amerikanischen Übersichtsarbeit konnte gezeigt werden, dass eine solche Gefährdung nicht nur relevant für Fragen des beruflichen Gesundheitsschutzes ist, sondern auch einen wichtigen Einflussfaktor für Fragen der Sicherheit und der Qualität der Patientenversorgung darstellt [REF]. Zudem wurde im Rahmen einer Studie in Deutschland bereits vor Ausbruch der COVID-19-Pandemie eine besondere Gesundheitsgefährdung von Ärzt*innen am Anfang ihrer Berufstätigkeit durch arbeitsbedingte Belastungsfaktoren festgestellt [REF]. Die Autor*innen schlussfolgerten, dass eine Veränderung der Rahmenbedingungen notwendig sei, um ein nachhaltig gesundes und effektives Arbeiten zu ermöglichen [REF]. Die weltweite COVID-19-Pandemie stellt alle Berufstätigen in der medizinischen Versorgung vor besondere Herausforderungen. Auch in Deutschland bringt sie eine Reihe pandemiebedingter Stressoren für Gesundheitsfachkräfte mit sich [REF]. Hierbei sind mit Blick auf berufsethische Anforderungen an Gesundheitspersonal in der Patientenversorgung Aspekte zu nennen, die einen Einfluss auf das professionelle Handlungsvermögen haben können. Fachkräfte in der Gesundheitsversorgung haben die professionelle Verpflichtung übernommen, ihr Handeln an medizinethischen Prinzipien auszurichten. Diese Verantwortungsübernahme beginnt schon in Praxisphasen der Ausbildung. Dazu gehört vor allem: Patient*innen durch ihr Handeln wohlzutun (Beneficence/Wohltun), ihnen nicht zu schaden (Nonmaleficence/Nichtschaden), die Autonomie von Patient*innen zu respektieren (Respekt der Autonomie) und die Versorgungressourcen auf eine gerechte Art und Weise zwischen den Anspruchsgruppen aufzuteilen (Gerechtigkeit; [REF]). Dabei sind die Wirkungen der Pandemie und der pandemiebedingten Veränderungen in den medizinischen Versorgungsabläufen nicht für jede Region, jedes Krankenhaus, jedes Behandlungsteam oder Individuum gleich. Sie können mancherorts Veränderungen beinhalten, die es erschweren, die ethischen Werte und Prinzipien zu realisieren. Diese und weitere pandemiebedingte Herausforderungen können selbst für erfahrenes Gesundheitspersonal belastend sein [REF]. Wenig Aufmerksamkeit wird jedoch bisher den zahlreichen Berufseinsteigenden und Studierenden in klinischen Semestern geschenkt, die in den ersten Wochen der COVID-19-Pandemie in Deutschland zur Unterstützung durch ihre Arbeitskraft aufgerufen wurden [REF]. Letztere sind noch keine Ärzt*innen, aber übernehmen Aufgaben, die durchaus anspruchsvoll sind und vermutlich vielerorts aufgrund ihres Ad-hoc-Eintritts in die berufliche Tätigkeit zunächst nicht klar definiert wurden. Zusammengefasst werden in diesem Beitrag Medizinstudierende in praktischen Tätigkeiten (bei einer Famulatur, einer Hilfskraftanstellung oder im praktischen Jahr) und approbierte Ärzt*innen am Anfang ihrer Berufstätigkeit (Berufseinsteigende) in den Blick genommen. Die Motivation, zu helfen und die im Studium erworbenen Kompetenzen zur Anwendung zu bringen, ist bei beiden Gruppen vielerorts sehr stark ausgeprägt [REF]. Sie können sich jedoch selbst in einer vulnerablen Situation befinden, besonders dann, wenn sie noch wenige Möglichkeiten hatten, Bewältigungsstrategien im Umgang mit moralisch herausfordernden Situationen zu entwickeln. Für eine gemeinsame, aber doch differenzierte Betrachtung dieser beiden Zielgruppen sprechen Bestrebungen der letzten Jahre, Aus- und Weiterbildung mehr miteinander zu verbinden [REF].Es ist davon auszugehen, dass ein erheblicher Teil von Berufseinsteigenden in der Versorgung während der Coronapandemie mit moralischen Konflikten oder pandemiebedingten Einschränkungen der Handlungsmöglichkeiten konfrontiert ist, wobei der Umgang damit zurzeit zu wenig Aufmerksamkeit erfährt. Moralische Konflikte sowie innere oder äußere Einschränkungen der Möglichkeiten, gemäß eigenen moralischen Überzeugungen zu handeln, können mit psychischer Belastung in Form von „moralischem Stress“ (MoS; Englisch: „moral distress“) einhergehen [REF]. Moralischer Stress kann eine Herausforderung, Bedrohung oder sogar eine Verletzung der beruflichen Integrität bedeuten [REF].MoS ist in der englischsprachigen Literatur ein wissenschaftlich vielbeachtetes Konzept, das in Deutschland bisher kaum bekannt ist. Es gibt einen großen Korpus von englischsprachiger Forschungsliteratur im Kontext der Pflegewissenschaften und -ethik, der konzeptuelle, qualitative und quantitative Forschung umfasst (für einen Überblick: [REF]). Auch im deutschsprachigen Raum gibt es erste Arbeiten in diesem Bereich [REF]. Wir gehen allerdings davon aus, dass die Zugehörigkeit zu einer Berufsgruppe und deren Position in der Hierarchie der Krankenhausversorgung einen wichtigen Aspekt in der Betrachtung des Phänomens darstellen, weshalb die Forschung im Bereich der Pflege nicht ohne Weiteres auf die Forschung zu anderen Berufs- bzw. Akteursgruppen übertragbar erscheint [REF].MoS ist für Berufstätige im Gesundheitswesen in Bezug auf die Entwicklung ihrer Kompetenz im Umgang mit berufsethischen Anforderungen von großer Bedeutung. Eine wissenschaftliche Beschäftigung mit MoS findet vor allem innerhalb der klinischen Ethik statt, innerhalb derer sich Wissenschaftler*innen und Praktiker*innen mit der Umsetzung solcher berufsethischen Anforderungen in der Patientenversorgung im Krankenhaus befassen. Aber das Phänomen kann auch im ambulanten Sektor eine Rolle spielen [REF]. Das Konzept MoS wird im Folgenden näher vorgestellt, in Bezug auf die Gruppe der Medizinstudierenden und Berufseinsteigenden in der aktuellen Pandemiesituation analysiert und um eine Beschreibung seitens bestehender Konzepte für einen individuellen und systemischen Umgang mit MoS ergänzt.
Bewegung ist eine zentrale Gesundheitsdeterminante, deren positive Wirkungen u. a. auf Typ-2-Diabetes, bestimmte Krebsarten, kardiorespiratorische Erkrankungen, psychische Beschwerden und andere Beeinträchtigungen gut dokumentiert sind [REF]. Dennoch bewegen sich große Teile der Bevölkerung zu wenig – so erreicht mehr als die Hälfte der erwachsenen Menschen in Deutschland nicht das national [REF] wie international (durch die Weltgesundheitsorganisation WHO) empfohlene Ausmaß an zumindest mäßig anstrengender, aerober körperlicher Aktivität von ca. 2,5 h pro Woche [REF]. Globale Trends deuten darauf hin, dass sich dieser Wert in allen Bevölkerungsgruppen künftig noch verschlechtern könnte [REF]. Zu wenig Bewegung wird allein in der europäischen Region der WHO jährlich für etwa eine Million Todesfälle verantwortlich gemacht [REF]. Dadurch wird Bewegungsmangel zur gesellschaftlichen Herausforderung.Die Politik hat in den vergangenen Jahren auf verschiedenen Ebenen auf diese Problematik reagiert, u. a. durch Bewegungsempfehlungen [REF] und politische Leitlinien auf internationaler Ebene [REF]. In Deutschland wurde 2016 für die Nationalen Empfehlungen für Bewegung und Bewegungsförderung die international verfügbare Evidenz systematisch gesichtet und an den deutschen Kontext angepasst [REF].Es bleibt jedoch eine Herausforderung, hieraus konkrete Interventionen abzuleiten, die tatsächlich zu nennenswerten Verhaltensänderungen auf Bevölkerungsebene, dauerhaften Strukturveränderungen und zu einer nachhaltigen Verbreitung aktiver Lebensstile führen. Die Wirksamkeit linearer Top-down-Ansätze mit Blick auf diese Zielgrößen wird zunehmend angezweifelt [REF]. Stattdessen rücken theoretische und methodische Konzepte in den Fokus, welche das komplexe Zusammenspiel von individuellen und umweltbezogenen Faktoren berücksichtigen. Hierzu zählen z. B. ökologische und biopsychosoziale Modelle [REF], Theorien zu Struktur und Handeln [REF] oder der Ansatz der Handlungsmöglichkeiten [REF]. Zudem mehren sich die Hinweise, dass Ansätze zur Gesundheits- und Bewegungsförderung vor allem dann erfolgreich und nachhaltig sind, wenn die Settingmitglieder, Multiplikator*innen und politische Entscheidungsträger*innen aktiv in die Programmplanung und -implementierung einbezogen werden [REF]. Basierend auf theoretischen Ansätzen wie interaktivem Wissenstransfer [REF], der Co-Produktion von Wissen [REF] oder transdisziplinärer Forschung [REF] rücken dabei sowohl in der Gesundheitsförderung allgemein [REF] als auch in der Bewegungsförderung im Besonderen [REF] zunehmend partizipative Interventionsformate in den Fokus. Dabei vernetzen sich Fachleute mit Setting-Vertreter*innen und treffen gemeinsam mit ihnen Entscheidungen zu Inhalten und Strategien.Vor diesem Hintergrund berichtet dieser Artikel über die Erfahrungen, die der Forschungsverbund Capital4Health in den letzten Jahren mit der kooperativen Planung, einem konkreten partizipativen Ansatz zur Entwicklung struktureller Bedingungen und damit einhergehenden neuen Handlungsmöglichkeiten (im Sinne von Amartya Sen, siehe unten) im Bereich Bewegungsförderung gemacht hat [REF]. Zudem trägt der Beitrag eine Reihe übergreifender Erkenntnisse zu den Erfolgen, Problemen und Forschungsherausforderungen zusammen, die sich aus Sicht der Projektverantwortlichen aus den bisherigen Projektergebnissen in den einzelnen Settings ergeben. Aus unserer Sicht sind diese Erkenntnisse für all jene von Relevanz, die sich mit der Frage beschäftigen, wie es gelingen kann, mit partizipativen Methoden der Bewegungsförderung auf verschiedenen Ebenen Wirkungen zu erzeugen, d. h. (a) konkrete Maßnahmen, die in verschiedenen Settings passgenau umgesetzt werden können (Outputs), (b) Veränderungen im Bewegungsverhalten bzw. in den Determinanten für Bewegung der Zielgruppe (Outcomes) und (c) systembezogene Veränderungen, die die Handlungsmöglichkeiten für Bewegung erweitern (Impact). Dazu zählen sowohl Forschende als auch politische Entscheidungsträger*innen sowie relevante Gruppen und Organisationen aus verschiedenen Settings der Bewegungsförderung.
Vor einigen Jahrzehnten begannen Wissenschaftler*innen sich über die eigene gesellschaftliche Verantwortung als Forschende Gedanken zu machen. Dies warf Fragen nach dem Wert von Forschung für die Allgemeinheit – ihrem Impact – auf. Um den Nutzen von Forschung für die Gesellschaft besser zu verstehen, wird sich seitdem zunehmend mit Forschungsimpact beschäftigt [REF]. Der Begriff Impact kann mit verschiedenen Worten ins Deutsche übersetzt werden, beispielsweise mit Wirkung, Auswirkung, Bedeutung, Nutzen, Effekt, Einfluss oder Folge. Greenhalgh et al. definieren Forschungsimpact als Nutzen, der über die Erzeugung wissenschaftlicher Erkenntnisse und Theoriebildung hinausgeht [REF]. Gemeint sind in diesem Zusammenhang die ökonomischen, umweltbezogenen, kulturellen, sozialen und gesundheitlichen Effekte von Forschung auf gesamtgesellschaftlicher Ebene, wozu in den Gesundheitswissenschaften ebenfalls die Verringerung gesundheitlicher Ungleichheit gehört.In der partizipativen Gesundheitsforschung (PGF) werden Personen, die von der Thematik der Forschung betroffen sind (z. B. Fachkräfte, Patient*innen, Adressat*innen von gesundheitsfördernden Maßnahmen) in den Forschungsprozess eingebunden. Sie gestalten das Forschungsvorhaben aktiv mit, indem sie sich an den Entscheidungen über die Forschungsfragen, die Art der Datenerhebung, die Interpretation der Ergebnisse und die Verbreitung der Ergebnisse beteiligen. Die PGF möchte nicht nur Gesundheitsprobleme und ihre Ursachen beschreiben und erklären, sondern auch den notwendigen sozialen Wandel zur Verbesserung der Situation herbeiführen [REF]. Die Ziele der PGF sind es, einerseits neue Erkenntnisse zu generieren und andererseits Veränderungen zur Förderung von Gesundheit und Wohlbefinden anzustoßen sowie gesundheitliche Chancengleichheit zu stärken [REF].Innerhalb der PGF hat sich ein Verständnis von Impact entwickelt, das sehr breit gefasst ist. So werden unter dem Begriff Forschungsimpact nicht nur beabsichtigte und unbeabsichtigte Veränderungen auf Makroebene (z. B. Gesellschaft, nationale Politik), sondern auch auf Meso- und Mikroebene (z. B. Kommunen, Institutionen und Individuen) subsumiert. Im Gegensatz zur Definition von Impact in der Evaluations- und Interventionsforschung steht Impact im Rahmen der PGF sowohl für kurzfristige als auch langfristige Wirkungen. Die International Collaboration for Participatory Health Research (ICPHR) definiert Forschungsimpact wie folgt ([REF], Übers. d. Verf.):   	Unter dem Begriff „Impact“ sind die zahlreichen Veränderungen vereint, welche die an der Forschung beteiligten Personen betreffen, sowie jene, die sich innerhalb des komplexen sozioökologischen Systems oder systemübergreifend in den Bereichen ereignen, in denen partizipative Gesundheitsforschung durchgeführt wird. Impact ereignet sich während des gesamten Forschungsprozesses und setzt sich nach dessen Abschluss weiter fort.  Die Anerkennung dessen, was die Entstehung von Wirkungen beeinflusst und wie verschiedene Faktoren dazu beitragen, ermöglicht es zu verstehen, wie sich Forschungsprozesse und -ergebnisse auf gesundheitliche Chancengleichheit auswirken können. Für die PGF, der das Streben nach Veränderungen innewohnt, ist es folgerichtig, Veränderungsprozesse zu analysieren und zu reflektieren. Zudem ist die Beantwortung der Frage, wie Partizipation als Katalysator für Veränderung wirkt, für sie von besonderer Bedeutung.„PartKommPlus – Forschungsverbund für gesunde Kommunen“ ist ein partizipativ arbeitender Verbund, der sich das Ziel setzte, den eigenen Forschungsimpact samt seiner Entstehungswege zu erfassen, zu beschreiben und online1 zu veröffentlichen [REF]. Um eine passende Vorgehensweise für dieses Ziel zu finden, wurde die britische Forscherin Tina Cook, welche Expertise in PGF und Forschungsimpact besitzt [REF], als Prozessberaterin hinzugezogen und ein internes Grundlagenpapier verfasst. Der vorliegende Artikel basiert sowohl auf diesem Grundlagenpapier als auch auf dem binationalen Austausch über Wirkungen. Der Beitrag bildet Diskussionen ab, wie sie im Vereinigten Königreich und in Deutschland über Impact geführt werden, und erörtert, wie Forschungsimpact innerhalb und außerhalb der PGF verstanden, erhoben und bewertet wird. Es werden zudem Wirkfaktoren beschrieben, die zu einer Stärkung gesundheitlicher Chancengleichheit führen können.
Die partnerschaftliche Forschung von Wissenschaftler*innen gemeinsam mit verschiedenen Co-Forschenden aus Lebenswelt, Berufspraxis, Administration und Politik hat eine lange Tradition und wird in vielen Ländern und in unterschiedlichen Konzeptionen vertreten. In den Gesundheitswissenschaften etabliert sich diesbezüglich der Ansatz der Partizipativen Gesundheitsforschung (PGF; [REF])/Participatory Health Research (PHR; [REF]). PGF vertritt den Anspruch, unterschiedliche Perspektiven aller Akteur*innen für und in der Forschung sichtbar zu machen und Wissens- und Ergebnisbestände mittels Koproduktion zu erarbeiten. Aufgrund der derzeitigen hierarchischen Ordnung der verschiedenen Wissenstypen von Wissenschaft, Praxis, Lebenswelt o. Ä. wird dabei auch immer deren Machtgefüge und -gefälle zueinander thematisiert. Diese Art zu forschen zeigt in der Anwendung des Ansatzes Konsequenzen in allen Phasen eines Forschungsprozesses und unterscheidet sich von konventionellen Ansätzen der Gesundheitsforschung. Daher brauchen partizipative Forschungsprojekte andere Organisationsformen und nicht zuletzt angepasste finanzielle Förderstrukturen, um ihrem Anspruch von Partizipation in der Wissensgenerierung gerecht werden zu können.Der vorliegende Text bezieht sich auf den Artikel „Partizipative Gesundheitsforschung in Deutschland – quo vadis?“ („Quo vadis?“), der der Frage nachgeht, wie sich PGF in Deutschland in geeigneter Weise fördern lässt. „Quo vadis?“ basiert auf Erfahrungen, Erkenntnissen und Analysen aus dem Netzwerk Partizipative Gesundheitsforschung (PartNet1; [REF]) und stellt Ansätze zur sinnvollen Fortentwicklung der Förderung von PGF vor. So sollten partizipative Prozesse strukturell gefördert werden, indem etwa die Mitarbeit von Bürger*innen und Patient*innen angemessen vergütet und zusätzliche Aufwände von partizipativen Prozessen in der Forschung finanziell gefördert werden. Partizipation sollte als konkret zu realisierendes Kriterium in Förderprogrammen und Ausschreibungen verankert werden, wie dies zum Teil auch schon erfolgt. Dies beinhaltet die Aufforderung, in Anträgen zu Partizipation als Kriterium Stellung zu nehmen (ohne jedoch verpflichtend zu sein!), flexible Ausschreibungsformate zu schaffen, in denen partizipative Projekte gezielt gefördert werden, Gutachter*innen bei der Bewertung von Partizipation zu unterstützen sowie formative Evaluationen von partizipativen Projekten und generell eine partizipative Methodenentwicklung sicherzustellen. Zudem sollte die Vernetzung von partizipativ Forschenden gefördert werden.Der vorliegende Artikel ist eine Ergänzung zu „Quo vadis?“. Er macht in der Vorstellung verschiedener Projekte Erfahrungen sichtbar, die zu den o. a. Schlussfolgerungen geführt haben. Ein Beispiel für eine gelungene Förderung partizipativer Forschung durch einen gemeinsamen Lernprozess zwischen Antragstellenden und dem Projektträger ist der Forschungsverbund PartKommPlus. Daran anschließend beschreiben Erfahrungen mit der Förderlinie „Klinische Studien mit hoher Relevanz für die Patientenversorgung“, wie Förderbedingungen Partizipation behindern können, obwohl diese gleichzeitig gefordert ist. Wie schwierig es für Selbstvertretungsorganisationen der Zivilgesellschaft ist, öffentliche Förderung für ihre Forschungsinteressen und -zielsetzungen zu erhalten, verdeutlicht das Projekt „Alt werden von Menschen mit Kleinwuchs“. Alle Beispiele illustrieren, wie Förderbedingungen im Wortsinn förderlich, aber eben auch ein großes Hemmnis für die Durchführung partizipativ angelegter Forschung sein können. Auf diese Weise werden verschiedene Perspektiven unterschiedlicher Akteur*innen in einen gemeinsamen Dialog gesetzt, der sich als Beitrag zur Diskussion um die Weiterentwicklung der Förderung von PGF versteht.
Unzählige Einzelvorhaben und dauerhaft betriebene (Surveillance‑)Systeme sammeln weltweit Gesundheitsdaten für Public-Health-Akteure, als Grundlage evidenzgestützter Entscheidungen. Der Zeitraum zwischen Beobachtung (Datenerfassung) und Anwendung der Erkenntnisse kann dabei Wochen, Monate oder Jahre umfassen – für viele Fragstellungen der öffentlichen Gesundheit ist diese Verzögerung akzeptabel. Während der COVID-19-Pandemie wurde jedoch besonders deutlich, wie wichtig eine möglichst aktuelle und umfassende Datengrundlage zur zielgerichteten Handlungssteuerung ist. Die dafür notwendige Automatisierung der Verarbeitungs- und Kommunikationsprozesse ist dabei die Voraussetzung für Aktualität sowie ressourcenschonende Skalierbarkeit und Kontinuität. Bisher standen der Entwicklung digital automatisierter Echtzeitsysteme mit wissenschaftlichem Qualitätsanspruch vielerorts technische, fachliche und organisatorische Herausforderungen im Weg. Die COVID-19-Pandemie dient seit ihrem Beginn Anfang 2020 als Motor für die Entwicklung zukunftsfähiger Systeme.Akute Gesundheitskrisen werden in Institutionen der primärmedizinischen Versorgung direkt erlebbar. Sie können wertvolle versorgungsnahe Daten liefern [REF]. Gesellschaftliche Aspekte z. B. auf sozialer, individueller und wirtschaftlicher Ebene sind ebenfalls Teil der Evidenzgrundlage zur Entscheidungsfindung. Auch in diesen Bereichen gibt es Bestrebungen, möglichst aktuelle und umfassende Informationen zur Verfügung zu stellen, sie sind jedoch nicht Teil dieses Beitrags, der sich auf Datenquellen aus der medizinischen Versorgung beschränkt. Viele der im Folgenden besprochenen Aspekte können auf die Nutzung von Echtzeitdaten anderer Akteure des Gesundheitsbereichs übertragen werden, wie z. B. Gesundheitsämter, Diagnostiklabore und Einrichtungen mit Aufgaben in den Bereichen Vorsorge oder Rehabilitation.Ein datenbasierter Einblick in das, was in der medizinischen Versorgung aktuell passiert, kann für sämtliche übergeordneten Ziele einer umfassenden Public Health Surveillance genutzt werden [REF], wie etwa das frühe Erkennen neuer Gesundheitsgefahren. Die Daten ermöglichen ein kontinuierliches Monitoring der räumlich-zeitlichen Verteilung relevanter Gesundheitsphänomene und dienen der Analyse von Community-Interventionen oder bevölkerungsweiten Maßnahmen. Es sind immer mehr elektronische Versorgungsdaten verfügbar, die Informationen über die Gesundheit und das Inanspruchnahmeverhalten der Bevölkerung enthalten.Dabei können 2 überlappende Informationsbereiche voneinander abgegrenzt werden: aktuelle klinische Daten und Daten zur aktuellen Versorgungssituation. Aus den in der Versorgungsroutine ohnehin erfassten Daten können Informationen zu Erkrankungen oder weiter gefassten individuellen patientenbezogenen Gesundheitsphänomenen abgeleitet werden, ggf. komplementiert durch Kenntnisse und Einschätzungen von Pfleger*innen und Ärzt*innen. Im Sinne einer syndromischen Surveillance können so Daten zu Häufigkeit, Schweregrad und Ausprägung relevanter Entitäten zusammengeführt werden und als Grundlage zur Public-Health-Handlungssteuerung genutzt werden [REF]. Hierbei bezieht sich die Bezeichnung „syndromisch“ auf die vornehmliche Nutzung (i. d. R. wenig qualitätsgesicherter) klinischer Angaben, kann aber nach dem hier verwendeten umfassenderen Verständnis sämtliche Daten mit einbeziehen, die im jeweiligen Setting zur eindeutigen Definition abgrenzbarer (ggf. auch gewichteter) Fallkategorien verwendet werden (z. B. Angaben zur Diagnose, klinische Labordiagnostik). Im Rahmen der COVID-19-Pandemie konnten so z. B. Notaufnahmevorstellungen von Personen mit respiratorischen Leitsymptomen und intensivmedizinisch behandelte beatmete Patient*innen mit einer nachgewiesenen COVID-19-Erkrankung erfasst werden [REF].Daten aus dem primärmedizinischen Kontext können auch verwendet werden, um Einblick in die aktuelle Versorgungssituation, Prozessabläufe sowie lokal verwendete und vorhandene Ressourcen zu erhalten. Üblicherweise arbeitet die Versorgungsforschung nicht mit Echtzeitdaten und zielt in ihrer Arbeitsweise auf langfristig zur Gestaltung der Versorgung nutzbaren Erkenntnisgewinn. Das schnelle Tempo der Veränderungen während der ersten Monate der COVID-19-Pandemie und die wiederkehrend beobachteten Versorgungsengpässe (z. B. in Norditalien, New York, in der zweiten Welle auch in Deutschland) zeigten jedoch, welche zentrale Rolle ein datenbasiertes Abbild der akuten Versorgungssituation für lokale und überregionale Public-Health-Entscheidungen einnimmt. Im Rahmen der COVID-19-Pandemie konnten so das Inanspruchnahmeverhalten von Notaufnahmen und die intensivmedizinischen Versorgungskapazitäten im zeitlichen Verlauf beschrieben werden – eine unmittelbare Entscheidungsgrundlage [REF] z. B. für strategische Patient*innen-Verlegung regional und überregional.Es sind vornehmlich staatliche Institutionen mit Public-Health-Auftrag, die als empfehlende bzw. entscheidende Gesundheitsakteure wahrgenommen werden: Gesundheitsämter, Landes- und Bundesbehörden, Gesundheitspolitik. Es gibt aber viele weitere Institutionen, die in ihrer Tätigkeit mit Empfehlungen und Entscheidungen zu Fragen der öffentlichen Gesundheit gefordert sind. Der Begriff „Public-Health-Handlungssteuerung“ bezieht hier alle strategischen und ggf. individuellen Fragen im Gesundheitsbereich mit ein, auch solche, die nicht direkt auf die individuelle Patientenversorgung abzielen. Aus einem datenbasierten Echtzeitabbild der aktuellen Versorgungslage profitieren so z. B. auch die für Präklinik zuständigen Steuerungsorgane, Management und Controlling von Krankenhäusern sowie nicht originär dem Gesundheitsbereich zugeschriebene, aber betroffene Akteure z. B. aus den Bereichen Bildung und Wirtschaft.Der vorliegende Artikel beschreibt zunächst, wie ein Echtzeitsystem aufgebaut sein muss, damit eine automatisierte Datenverarbeitung aus dem jeweiligen Versorgungssetting bis hin zur Kommunikation und Anwendung als qualitätsgesicherte Grundlage für strategische Entscheidungen stattfinden kann. Exemplarisch wird ein bereits im ersten halben Jahr der COVID-19-Pandemie angepasstes System vorgestellt, das Routinedaten aus Notaufnahmen in Echtzeit verarbeitet und diese Public-Health-Akteuren bereitstellt (AKTIN-Notaufnahmeregister, SUMO-System). Es erfolgen Vergleiche mit einem Echtzeitsystem, dessen Aufbau vom Robert Koch-Institut (RKI) verantwortetet wird (DIVI-Intensivregister; [REF]).
Das Bundesministerium für Bildung und Forschung fördert von 2014 bis 2022 durchgängig 5 Forschungsverbünde, die prioritäre Themen der Primärprävention und Gesundheitsförderung erforschen. Zu den Zielen der Verbünde gehört, die Evidenzgrundlagen für Präventions- und Gesundheitsförderungsmaßnahmen in den jeweiligen Themenschwerpunkten zu verbessern sowie die Umsetzung von Maßnahmen systematisch zu unterstützen und zu evaluieren (Tab. 1). Eine kompakte Darstellung der einzelnen Verbünde sowie Verweise auf detailliertere Informationen sind auf der Webseite des Forschungsnetzwerkes Primärprävention und Gesundheitsförderung www.fp2g.net zu finden. Mit den genannten – und weiteren verbundspezifischen – Zielen lässt sich die wissenschaftliche Arbeit der Präventionsforschungsverbünde einem Verständnis von evidenzbasierter Public Health (EBPH) zuordnen, das EBPH als Prozess der Integration wissenschaftsbasierter Interventionen und Präferenzen der jeweiligen Community mit dem Ziel der Verbesserung der Gesundheit der Bevölkerung [REF] definiert. Offensichtlich ist, dass anders als in der evidenzbasierten Medizin nicht die individuellen Patient*innen im Mittelpunkt stehen, sondern die Gesundheit auf Bevölkerungsebene. Die Berücksichtigung interdisziplinärer Erkenntnisse bei Begründung, Auswahl und Entwicklung von Maßnahmen ist ein weiteres Charakteristikum von EBPH. Dies ist verbunden mit einem systematischen und transparenten Vorgehen im Prozess der Entscheidungsfindung und -umsetzung [REF], das sich in seinen Grundprinzipien an die evidenzbasierte Medizin anlehnt [REF].Evidenzbasiertes Vorgehen umfasst nach dem Modell von Gerhardus (2010) das Durchlaufen der Bereiche „Entscheiden und Umsetzen“, „Austauschen und Handeln“, und „Evidenz entwickeln“ (Abb. 1; [REF]). Der Bereich Entscheiden und Umsetzen steht dabei am Anfang und am Ende des Prozesses und umfasst sowohl die Entscheidung, welches Public-Health-Problem untersucht werden soll, als auch die Entscheidung, ob auf Basis des Forschungsprozesses z. B. die zuvor entwickelte und evaluierte Public-Health-Intervention durchgeführt wird. Im mittleren Bereich, Austauschen und Handeln, wird das Public-Health-Problem in wissenschaftlich zu untersuchende Fragestellungen überführt bzw. es werden anhand der im Forschungsprozess gewonnenen Erkenntnisse konkrete Handlungsempfehlungen abgeleitet. Im Bereich Evidenz entwickeln erfolgen die Auswahl der am besten geeigneten Methoden zur Beantwortung der Fragestellungen sowie die Durchführung des Forschungsprozesses mit den ausgewählten Methoden. Beim Durchlaufen der genannten Bereiche ist zu betonen, dass evidenzbasiertes Vorgehen in Prävention und Gesundheitsförderung nicht nur im Verantwortungsbereich von Wissenschaftler*innen liegt, sondern die intensive Beteiligung von Stakeholder*innen einschließt. Die Einbindung von Stakeholder*innen ist in allen 3 Bereichen in unterschiedlicher Form umsetzbar. Im Bereich „Entscheiden und Umsetzen“ bietet sich partizipatives Vorgehen besonders an, um relevante Public-Health-Probleme zu identifizieren bzw. Public-Health-Maßnahmen angemessen und adressatenspezifisch umzusetzen.Bei der Auswahl der Methoden zur Evidenzentwicklung ist zu beachten, dass sowohl die Adressatengruppen und Settings als auch die Maßnahmen in der Primärprävention und Gesundheitsförderung sehr vielfältig sind. Das Herzstück der evidenzbasierten Medizin, randomisierte kontrollierte Studien (RCT), bildet daher nur einen kleineren Teil des verfügbaren und benötigten Methodenspektrums der EBPH [REF]. Die 5 Forschungsverbünde spiegeln die Diversität und Breite sowohl in Hinsicht auf den Zugang als auch die Methoden und die Umsetzung eines evidenzbasierten Vorgehens in der Forschung zu Prävention und Gesundheitsförderung wider.Wir beschreiben und diskutieren im Folgenden die Methodenvielfalt und die Vorgehensweisen der Forschungsverbünde orientiert am Prozess des evidenzbasierten Vorgehens nach dem EBPH-Modell von Gerhardus (Abb. 1; [REF]). Dabei erläutern wir jeweils kurz die inhaltlichen Kernthematiken der von den Autor*innen koordinierten Verbünde und gehen dann – aufgrund der Diversität der Verbünde unterschiedlich akzentuiert – auf Aspekte der Evidenzbasierung ein.
Der Begriff der Evidenzbasierung hat seine Ursprünge in der Medizin, wo das Konzept bereits in den 1990er-Jahren geprägt wurde und sich seitdem als internationale Bewegung etabliert hat. Evidenzbasierte Medizin (EBM) beschreibt die Integration von bestverfügbaren wissenschaftlichen Erkenntnissen, klinischer Expertise, Werten und Präferenzen von PatientInnen [REF]. Das Herzstück der EBM ist die Betonung einer verlässlichen wissenschaftlichen Basis von klinischen Entscheidungen. Inhaltlich gab es aber während der letzten 30 Jahre vielfältige Weiterentwicklungen, vor allem wird zunehmend die Bedeutung der Werte und Präferenzen von PatientInnen in Richtung partizipative Entscheidungsfindung („shared decision-making“) betont und umgesetzt [REF].Evidenzbasierung stellt auch in der öffentlichen Gesundheit oder Public Health mit ihren vielfältigen Aktivitäten hinsichtlich Prävention, Gesundheitsförderung und Gesundheitsschutz eine zentrale Anforderung dar. Insbesondere muss die Wirksamkeit von Public-Health-Maßnahmen auf Bevölkerungsebene und unter Alltagsbedingungen sorgfältig beleuchtet werden. Außerdem sollten potenziell negative Auswirkungen möglichst gering ausfallen, denn auch Public-Health-Maßnahmen können eine Vielzahl von nichtintendierten Folgen, die positiv oder negativ ausfallen können, haben. Analog zu EBM sind also Wirksamkeit und Sicherheit von Bedeutung, der Nachweis eines Nettonutzens (d. h. Nutzen > Schaden) einer Maßnahme ist somit eine wichtige Voraussetzung für eine evidenzbasierte Public Health (EBPH). Darüber hinaus betreffen Public-Health-Maßnahmen große Bevölkerungsgruppen und diverse Lebensbereiche und können unter Umständen individuelle Freiheiten beschneiden, weshalb die Schaden-Nutzen-Abwägung noch umfassender verstanden werden muss als in der EBM [REF].Die Bedeutung von Qualitätssicherung und damit auch von Evidenzbasierung hinsichtlich der Kernhandlungsfelder von Public Health ist im deutschen Gesundheitssystem an vielen Stellen verankert. So schreibt zum Beispiel das deutsche Sozialgesetzbuch vor, dass alle bei der gesetzlichen Krankenversicherung erstattungsfähigen Gesundheitsleistungen nach den Prinzipien der EBM bewertet werden müssen, u. a. auf Grundlage der wissenschaftlichen Expertisen des Instituts für Qualität und Wirtschaftlichkeit im Gesundheitswesen (IQWiG; [REF]). Auch bei der Umsetzung des im Jahr 2015 verabschiedeten Präventionsgesetzes wird auf Evidenz und Qualitätsorientierung gesetzt, hier sind insbesondere die Bundesrahmenempfehlungen der Nationalen Präventionskonferenz von Bedeutung [REF]. Eine einheitliche Grundlage und Operationalisierung, auf die sich alle relevanten Akteure aus Praxis, Politik und Wissenschaft in Deutschland beziehen können, sind deshalb dringend notwendig. Hier setzt ein vor Kurzem veröffentlichtes Memorandum zu evidenzbasierter Prävention und Gesundheitsförderung der Bundeszentrale für gesundheitliche Aufklärung (BZgA; [REF]) an mit dem Ziel, einen Standard für das Verständnis und die Umsetzung von Evidenzbasierung in Deutschland zu setzen.Aus der EBM lassen sich allgemeine Prinzipien von Evidenzbasierung ableiten; diese gelten auch für die EBPH. Zudem lassen sich diese Prinzipien in weiteren Gesundheitsberufen, wie Pflege und Physiotherapie [REF], sowie in Sektoren und Wissenschaftsfeldern wie Bildung [REF], Psychologie [REF], Management [REF] oder Politik [REF] anwenden. Diese Prinzipien werden je nach Quelle unterschiedlich aufgeführt und kategorisiert [REF]. In dem bereits erwähnten Memorandum der BZgA werden diese allgemeinen Prinzipien als 5 STIIP-Prinzipien beschrieben (Tab. 1). Diese beinhalten Systematik (S), Transparenz und Umgang mit Unsicherheit (T), Integration und Partizipation (I), Umgang mit Interessenkonflikten (I) und strukturierter, reflektierter Prozess (P). Das Prinzip von Transparenz im Umgang mit Unsicherheit kann auf mehreren Ebenen umgesetzt werden:– Durch einen explizit gestalteten Prozess für die Entscheidungsfindung, wie zum Beispiel bei Leitlinien– Durch eine vorab festgelegte und klar beschriebene Methodik für die Zusammenführung von wissenschaftlichen Erkenntnissen und der Bewertung der Qualität verwendeter Studien, insbesondere im Rahmen von systematischen Übersichtsarbeiten und– Durch die systematische und transparente Bewertung von Unsicherheit in der verwendeten Evidenz, wie zum Beispiel durch Evidenzgrade dargestelltDie Operationalisierung dieser Prinzipien ist in Tab. 1 kurz dargestellt. Eine ausführlichere Beschreibung der methodischen Vorgehensweisen für die Erhebung und Bewertung von Evidenz sowie der etablierten Verfahren zur Entscheidungsfindung findet sich im Memorandum der BZgA [REF].Dieser Artikel hat das Ziel, ein gemeinsames Verständnis von evidenzbasierter Public Health zu fördern – basierend auf dem Memorandum der BZgA, das auf Prävention und Gesundheitsförderung fokussiert, und weiteren internationalen Quellen, die sich mit allen Bereichen von Public Health befassen. Der Artikel sichtet systematisch internationale Definitionen, Konzepte und Modelle von Evidenzbasierung in Public Health und diskutiert auf dieser Basis die Notwendigkeit einer Weiterentwicklung der Herangehensweisen der EBM für eine EBPH durch die Entwicklung von public-health-spezifischen Umsetzungsfaktoren.
Kinder- und Jugendärztliche Dienste (KJGD, synonym: Fachdienste für Kinder- und Jugendgesundheit) sind neben den Amtsärztlichen und den Sozialpsychiatrischen Diensten sowie dem Infektionsschutz ein Kernbereich des kommunalen Öffentlichen Gesundheitsdienstes (ÖGD) in Deutschland (Abb. 1). Aufgaben des KJGD sind es, die Gesundheit von Kindern und Jugendlichen zu schützen und zu fördern sowie allgemeine und individuelle Gesundheitsgefährdungen zu erkennen, zu mildern oder zu beseitigen. Konkreter umfassen diese Aufgaben [REF]: die Gesundheitsförderung in Gemeinschaftseinrichtungen für Kinder,das Hinwirken auf eine gesunde, altersgerechte Entwicklung durch die Feststellung der individuellen Förderbedarfe mittels zielgerichteter Vorsorge- und Früherkennungsuntersuchungen bei Kindern und Jugendlichen in Kindergärten und Schulen, die Reduzierung der Folgeschäden bei Kindern und Jugendlichen mit Entwicklungsstörungen und Behinderungen durch sozialpädiatrische Hilfen (auch aufsuchend) sowie die Beratung der öffentlichen Entscheidungsträger unter anderem in Form der Gesundheits-(und Sozial‑)Berichterstattung. Das Anliegen, die Gesundheit von Kindern und Jugendlichen zu erhalten, soll durch die Gesundheitserziehung, das Impfwesen sowie Aufklärungs- und Beratungsangebote bei Reihenuntersuchungen (Schuleingang) oder in Sprechstunden für Schüler, Eltern, pädagogische Fachkräfte und gegebenenfalls weitere Bezugspersonen erreicht werden. Bei Entwicklungsverzögerungen beziehungsweise Behinderungen bietet der KJGD verschiedene Hilfen nach den Sozialgesetzbüchern (SGB V, SGB VIII, SGB IX, SGB XII ff.) an, die sowohl in den Fachdiensten selbst als auch auf deren Vermittlung hin von anderen Einrichtungen erbracht werden. Hierzu gehören zum Beispiel Frühförderung, Eingliederungshilfe oder Sonderpädagogik [REF].In den ÖGD-Gesetzen vieler Bundesländer wird diesem explizit eine betriebsmedizinische Verantwortung zugeschrieben, die sich auf Kinder bzw. Schüler in Gemeinschaftseinrichtungen bezieht. Für den „Arbeitsplatz Schule“ wird sie von einem Schularzt und seinem Team umgesetzt. Hier geht es zum einen um Individualuntersuchungen und die Klärung von Fragen zu Unterstützungsbedarfen im Kita- und Schulalltag (zu denen auch viele Aspekte der Schuleingangsuntersuchungen zählen), um die gutachterliche Tätigkeit im Kontext chronischer Erkrankungen, um Fragen der Inklusion, um die gemeinwesenbezogene Zusammenarbeit mit Jugend- und Sozialhilfe und vieles mehr. Zum anderen geht es aber auch um die Gestaltung gesundheitsförderlicher Rahmenbedingungen, um ein kleinräumiges Monitoring des Gesundheitszustands, die Kommunikation darüber und das Ableiten von Handlungsempfehlungen und Maßnahmen für die Verantwortungsträger sowie um Politikberatung. Diese Aufgaben sind von ihrer Natur in der kommunalen Daseinsvor- und -fürsorge verankert. Sie werden in der Regel von einem multiprofessionell aufgestellten KJGD erfüllt und haben einige natürliche Schnittmengen mit den Aufgaben, die im Ressort für Öffentliche Hygiene und Infektionsschutz des Gesundheitsamts zu erfüllen sind.Die regelhaften Leistungen der KJGDs, zumeist in den Gesundheitsdienstgesetzen der Länder oder kommunalen Verordnungen festgelegt, haben nun durch den Beginn der COVID-19-Pandemie einen jähen Stopp erfahren. Die wichtigen Einrichtungen von Betreuung und Bildung für Kinder sind besonders betroffen, treffen doch hier verschiedenste institutionelle Verpflichtungen und Aufträge auf unterschiedliche Ansprüche, Erwartungen und Interessen. Es ist für einen Großteil der Bevölkerung – alle Familien mit Kindern – wichtig, dass der Betrieb von Kita, Schule oder Hort verantwortungsbewusst und vertretbar gestaltet wird. An der Basis, in der Kommune vor Ort, musste und muss sich der Öffentliche Gesundheitsdienst auch hier bewähren – nicht nur in der Erfüllung von Aufgaben, die in direktem Bezug zu Fall- und Verdachtsfällen einer SARS-CoV-2-Infektion, zur Kontaktpersonennachverfolgung oder gar zum Ausbruchsmanagement stehen. Es soll deshalb der Frage nachgegangen werden, welche Aufgaben zur Pandemiebewältigung der KJGD inner- oder außerhalb von Bildungseinrichtungen konkret übernommen hat, welche Aufgaben stattdessen teilweise oder gänzlich entfallen sind und welche mittel- und langfristigen Folgen dadurch erwartet werden können.
Prävention und Gesundheitsförderung (PGF) als Kernhandlungsfeld der Public Health hat sich in den letzten Jahren zunehmend organisiert. Angesichts der Herausforderungen des demografischen, technologischen und gesellschaftlichen Wandels und der Verschiebung der Krankheitslast hin zu chronisch degenerativen und psychischen Erkrankungen haben eine Vielzahl von AkteurInnen sowie Institutionen in Deutschland unterschiedliche Aktivitäten entwickelt. Das im Jahr 2015 verabschiedete Präventionsgesetz (PrävG) hat vonseiten des Bundes neue Impulse für die Entwicklung von PGF in Deutschland gesetzt. Das Geschehen voranbringen sollen Strukturen wie die Nationale Präventionskonferenz, die dort zu entwickelnde nationale Präventionsstrategie, der zugehörige Präventionsbericht, die dadurch geforderten kassenübergreifenden Leistungen sowie damit zusammenhängende Koordinierungsprozesse.So hat sich ein innerhalb der zivilgesellschaftlichen, staatlich-föderalen, privaten und sozialversicherungsrechtlichen Strukturen organisiertes, sehr komplexes System der PGF herausgebildet. Zusammengenommen besteht es aus Institutionen, die teilweise in einer gewissen Hierarchie verbunden sind, aus AkteurInnen mit bestimmten Kompetenzen und Disziplinen sowie aus Regeln und Prozessen der Zusammenarbeit zwischen AkteurInnen und zwischen Institutionen. Geregelt ist diese Zusammenarbeit innerhalb des föderalen Systems mittels einer gesetzlichen Grundlage und/oder gemäß von Selbstverpflichtungen verschiedener Partner (z. B. innerhalb des Kooperationsverbundes gesundheitliche Chancengleichheit).Die beschriebenen Charakteristika sind Kerncharakteristika eines sich organisierenden „Systems“: Systeme sind definiert als im Allgemeinen abgrenzbare, natürliche oder künstliche „Gebilde“, die aus verschiedenen Komponenten bestehen und die aufgrund bestimmter geordneter Beziehungen untereinander als gemeinsames Ganzes betrachtet werden (können; [REF]). Die Organisation innerhalb eines Systems zeichnet sich aus durch Regelwerke und Instrumente zur Erreichung der Ziele, die über Organisationsmitglieder, Spielregeln, Hierarchien, Grenzen und Zwecke definiert werden [REF]. Das Handeln in einem sich organisierenden System PGF in Deutschland ist wiederum in einen größeren Kontext eingebettet. Neben politischen und gesellschaftlichen Rahmenbedingungen sind dies Anforderungen, mit denen definierte „Leistungen“ im Gesundheitsbereich begründet werden müssen. Die Orientierung an der Evidenzbasierung hat beispielsweise für medizinische Leistungen der gesetzlichen Krankenversicherung Eingang in das sozialrechtliche Regelwerk und damit in die Verfahrensordnung des Gemeinsamen Bundesausschusses genommen [REF]. Auch in den Bundesrahmenempfehlungen der Nationalen Präventionskonferenz sind Evidenz und Qualitätsorientierung eine wiederkehrende Terminologie [REF], allerdings in abgeschwächter Form.Somit ist Evidenzbasierung eine wichtige Anforderung an das sich entwickelnde System PGF. Evidenzbasierung in der Public Health ist definiert als: „Entscheidungen auf Grundlage einer systematischen und bewussten Integration der für die Frage relevanten besten verfügbaren wissenschaftlichen Erkenntnisse, der praktischen Erfahrungen und der Expertise relevanter Fachleute sowie der Werte und Präferenzen der betroffenen Personen“ (siehe auch Beitrag von Rehfuess et al. in diesem Themenheft und [REF]).Obwohl die Anforderung der Evidenzbasierung formuliert ist, gibt es viele Kontroversen um die Umsetzungsmöglichkeiten der Evidenzbasierung in PGF [REF]. Die Kontroversen werden zunehmend weniger dogmatisch und die Erfahrung aus angelsächsischen Ländern zeigt, dass mehr Evidenzbasierung möglich ist und auf welche Weise sie umgesetzt werden kann. Dieser Beitrag möchte auf dieser internationalen Wissens- und Erfahrungsbasis erläutern, was die wesentlichen Elemente, Schritte, Prozesse und Vorgehensweisen für eine Evidenzbasierung und die Stärkung/den Ausbau eines Systems PGF sind.Aufbauend auf internationalen wissenschaftlichen Erkenntnissen zu den oben genannten Themen und den Konzepten von Evidenzbasierung aus unterschiedlichen Wissenschaftsfeldern (z. B. Management, Pädagogik, Psychologie) sollen für das System PGF in Deutschland – angelehnt an das Memorandum Evidenzbasierte Prävention und Gesundheitsförderung der Bundeszentrale für gesundheitliche Aufklärung (BZgA; [REF]) – folgende spezifischen Fragen beantwortet werden:              		1. Wie können evidenzbasierte Interventionen der PGF operationalisiert werden als Basis für ein evidenzbasiertes Handeln von AkteurInnen in der Praxis?                           		2. Was sind wichtige Elemente für die Entwicklung von (organisationalen) Kapazitäten („capacity building“) für evidenzbasiertes Handeln im System PGF?                                 3. Wie können evidenzbasierte Interventionen der PGF operationalisiert werden als Basis für ein evidenzbasiertes Handeln von AkteurInnen in der Praxis?Was sind wichtige Elemente für die Entwicklung von (organisationalen) Kapazitäten („capacity building“) für evidenzbasiertes Handeln im System PGF?
Die Digitalisierung im Gesundheitswesen kann für die Prävention von Krankheiten und die Versorgung der Bevölkerung große Chancen bieten. Dieser positive Effekt der Digitalisierung ist allerdings an einige Voraussetzungen gebunden. Digitale Versorgungsformen oder Präventionsangebote dürfen keine Scheininnovationen sein, sondern müssen ihren Nutzen für die Versorgung nachweisen. Sie müssen sowohl medizinisch sicher sein als auch konform zum Datenschutz. Darüber hinaus sollten digitale Gesundheitsangebote für alle Anwender verständlich sein, sodass möglichst alle Menschen an diesem medizinischen Fortschritt teilhaben [REF].Soziale Determinanten wie Bildung, Berufsstatus und Einkommen haben beträchtliche Auswirkungen auf die Gesundheit. Ein niedriger sozialer Status geht mit einer niedrigeren Lebenserwartung, höheren Risiken für chronische Erkrankungen und einem höheren Risiko für Berufsunfähigkeit einher [REF]. Da sich bei sozial Benachteiligten von Geburt an größere Gesundheitsbelastungen summieren, unter anderem infolge von ungünstigeren Lebensbedingungen und riskanterem Gesundheitsverhalten, spricht man in diesem Zusammenhang auch von schlechteren Gesundheitschancen bzw. gesundheitlicher Chancenungleichheit [REF].Digitalen Gesundheitsangeboten wurde schon früh das Potenzial zugesprochen, die gesundheitliche Chancengleichheit zu verbessern [REF]. Niedrigschwellige Barrieren, schnelle Implementierung, Personalisierbarkeit und einfache Skalierungseffekte sprechen für dieses Potenzial. Von einer Digitalisierung von Versorgungsformen und digitalen Präventionsangeboten kann jedoch nur profitieren, wer auch tatsächlich Zugang zum Internet hat und über entsprechende Hardware verfügt. Zugang und Hardware sind jedoch ungleich verteilt, man spricht in diesem Kontext auch vom „Digital Divide“ [REF]. Mit der steigenden Verfügbarkeit von Internetzugang zeigt sich der Digital Divide zunehmend in Ungleichheiten von digitalen Kompetenzen und Unterschieden im Nutzungsverhalten [REF]. Im Kontext der digitalen Gesundheitsangebote ist dies von hoher Relevanz, da diese, je nach Ausgestaltung, eine größere Einbindung der Nutzerinnen und Nutzer sowie Eigenverantwortung voraussetzen [REF]. Die individuelle Gesundheitskompetenz sowie die digitale Kompetenz des Einzelnen kann also einen starken Einfluss darauf haben, wie gut digitale Versorgungs- und Präventionsangebote in Anspruch genommen werden und wie effektiv diese Angebote genutzt werden.Aufgrund des Digital Divide ist anzunehmen, dass jene Bevölkerungsgruppen, die heute schon die größte Krankheitslast tragen, von digitalen Versorgungsformen und Präventionsangeboten am wenigsten profitieren und sich dadurch die gesundheitliche Chancenungleichheit weiter vergrößert [REF]. Um diese These zu prüfen, werden im Folgenden die Ergebnisse von Studien berichtet, die sich mit dieser Frage befasst haben. Da für Deutschland bislang nur wenige Studien vorliegen, werden zum Teil auch Ergebnisse aus Studien anderer Länder dargestellt. Insgesamt wurden in diese explorative Analyse 16 Studien eingeschlossen, die sich bezüglich des Designs, der Methodik und der Studienpopulation stark voneinander unterscheiden. Dies ist bei der Interpretation und Einordnung der Ergebnisse zu berücksichtigen. Der Schwerpunkt liegt im Folgenden auf Studien, die soziale Unterschiede in der Nutzung von digitalen Angeboten der Prävention und Gesundheitsversorgung untersucht haben. Ergänzend dazu werden Ergebnisse von Studien einbezogen, die soziale Unterschiede in Bezug auf die Kompetenz zur Nutzung digitaler Gesundheitsangebote betrachtet haben (E-Health Literacy). Vorab wird kurz auf das Konzept und die Definition von Digital Divide eingegangen.
Rahmenbedingungen und Strukturen in den Lebenswelten spielen eine zentrale Rolle für die Gesundheit und das Gesundheitsverhalten der Bevölkerung. Fußgängerfreundliche Infrastrukturen in Stadtbezirken können bei deren Bewohnerschaft beispielsweise das Risiko für Adipositas, Diabetes mellitus Typ II und Bluthochdruck reduzieren [REF]. Auch die Verfügbarkeit gesunder Nahrungsmittel in Schulen kann ein gesünderes Ernährungsverhalten bei Schulkindern fördern [REF]. Präventionsmaßnahmen haben daher ein höheres Erfolgspotenzial, wenn sie Maßnahmen der Verhaltensprävention mit einer gesundheitsförderlichen Gestaltung von Lebenswelten verknüpfen. Seit der Ottawa-Charta der Weltgesundheitsorganisation von 1986 hat dieser auch als Lebenswelt- oder Settingansatz bezeichnete Ansatz international an Bedeutung gewonnen [REF]. In Deutschland wurde der Lebensweltansatz 1989 mit der Einführung des §20 im SGB V, dem Sozialgesetzbuch für die Gesetzliche Krankenversicherung (GKV), erstmals für die Krankenkassen gesetzlich verankert und von ihnen insbesondere in der betrieblichen Gesundheitsförderung (BGF) entwickelt. Mit dem Gesetz zur Stärkung der Gesundheitsförderung und der Prävention (Präventionsgesetz – PrävG) im Jahr 2015 wurde der Lebensweltansatz im SGB V auf andere Lebenswelten übertragen und deutlich gestärkt. Im Zuge der Umsetzung des PrävG hat sich das GKV-Bündnis für Gesundheit als gemeinsame Initiative aller gesetzlichen Krankenkassen zur Weiterentwicklung und Umsetzung von Gesundheitsförderung und Prävention in Lebenswelten gebildet. Das Bündnis fördert u. a. Strukturaufbau und Vernetzungsprozesse, die Entwicklung und Erprobung gesundheitsfördernder Konzepte für sozial und gesundheitlich benachteiligte Zielgruppen sowie Maßnahmen zur Qualitätssicherung und wissenschaftlichen Evaluation. Die Aufgaben des GKV-Bündnisses für Gesundheit setzt die Bundeszentrale für gesundheitliche Aufklärung (BZgA) mit Mitteln der gesetzlichen Krankenkassen im Auftrag des GKV-Spitzenverbandes gemäß §20a Abs. 3 und 4 SGB V um.Das GKV-Bündnis für Gesundheit leistet auch Beiträge zu einer stärkeren Evidenzbasierung lebensweltbezogener Maßnahmen der Gesundheitsförderung und Prävention. Im Rahmen des Bündnisses werden Synthesen bestehender Evidenzquellen erstellt, zum Beispiel durch systematische Übersichtsarbeiten zu nationalen und internationalen Studien. Zudem werden Bestandsaufnahmen bestehender gesundheitsfördernder Angebote durchgeführt sowie Handlungsempfehlungen für Fachkräfte und Beteiligte in der Praxis entwickelt. Die Zahl der methodisch überzeugenden Studien im Bereich der lebensweltbezogenen Gesundheitsförderung und Prävention aus Deutschland ist im internationalen Vergleich noch gering und die Evidenz uneinheitlich [REF]. Vor diesem Hintergrund fördert das GKV-Bündnis für Gesundheit gezielt anwendungsbezogene Forschungsvorhaben und unterstützt die Krankenkassen bei Evaluationsvorhaben, um Evidenz zu Konzepten der lebensweltbezogenen Gesundheitsförderung zu schaffen. Damit evidenzbasierte Ansätze häufiger in die praktische Umsetzung kommen, ist das GKV-Bündnis für Gesundheit bestrebt, die vorhandene Evidenz für Fachkräfte und Beteiligte in der Praxis leicht zugänglich zu machen. Dazu werden insbesondere Übersichtsarbeiten auf der Informationsplattform www.gkv-buendnis.de veröffentlicht. In Workshops mit Expertinnen und Experten der Krankenkassen sowie weiteren Akteurinnen und Akteuren werden Ergebnisse ferner hinsichtlich ihrer Anwendungsmöglichkeiten reflektiert. Um wissenschaftliche Erkenntnisse, insbesondere aus systematischen Übersichtsarbeiten, auch einem breiteren Publikum besser zugänglich zu machen, wurde die Datenbank „Wissen für gesunde Lebenswelten“ entwickelt, die seit dem 17.11.2020 online zugänglich ist [REF]. Die Konzeption sowie die weitere Entwicklung der Datenbank erfolgen durch die BZgA im Auftrag des GKV-Spitzenverbandes gemäß §20a Abs. 3 und 4 SGB V.Ziel dieses Beitrags ist es, diese neue Datenbank vorzustellen. Zunächst werden Ziele der Datenbank und das methodische Vorgehen bei der Erstellung der Datenbankeinträge beschrieben. Anschließend folgt eine Darstellung aktueller Datenbankinhalte sowie der Nutzungsmöglichkeiten für Krankenkassen und weitere Fachkräfte und Beteiligte in der Praxis. Wir erläutern zudem die Entwicklung eines innovativen Instruments zur Bewertung der methodischen Qualität von Übersichtsarbeiten. Die Diskussion gibt schließlich einen Ausblick auf geplante Weiterentwicklungsschritte sowie auf die Einbindung der Datenbank in andere Maßnahmen zum Wissenschafts-Praxis-Transfer.
Im Januar 2021 verabschiedete der Exekutivrat (EB148) der Weltgesundheitsorganisation (WHO) mit breiter Unterstützung eine Resolution zur Mundgesundheit – laut WHO-Generalsekretär Dr. Tedros in der Geschichte der Mundgesundheit ein wegweisender Meilenstein. Nachdem die Mundgesundheit über Jahrzehnte hinweg nur eine geringe gesundheitspolitische Priorität genoss, spiegelt die aktuelle Resolution ein Umdenken auf höchster Ebene wider, das die Mundgesundheit als einen wesentlichen und untrennbaren Bestandteil der Allgemeingesundheit anerkennt [REF]. Die Zahlen sprechen für sich: Unbehandelte Karies der bleibenden Zähne ist die weltweit häufigste aller chronischen, nichtübertragbaren Erkrankungen. Im Jahr 2015 wurde die Zahl der weltweit Betroffenen auf 2,5 Mrd. und die altersstandardisierte Prävalenz auf etwa 34 % geschätzt [REF]. Zudem gibt es bei der Kariesverteilung in der Bevölkerung in allen Altersgruppen ein starkes soziales Gefälle [REF]. Auch in Deutschland ist Karies weitverbreitet. Während laut Fünfter Deutscher Mundgesundheitsstudie (DMS V) die Kariesprävalenz bei Kindern und Jugendlichen über die vergangenen Jahrzehnte erfreulicherweise stark zurückgegangen ist und 81 % der 12-Jährigen 2015 kariesfrei waren, wurde bei knapp 25 % der jungen Erwachsenen sanierungsbedürftige Karies festgestellt und weniger als 3 % hatten keine Karieserfahrung [REF].Karies ist keine harmlose Erkrankung – sie beeinträchtigt Gebissfunktion und Ästhetik und kann die Lebensqualität erheblich mindern. Karies verursacht Schmerzen und kann vor allem bei Kindern zu Appetitverlust, Schlaflosigkeit, Konzentrationsstörungen sowie Schulabwesenheit und sogar Gewichtsverlust führen [REF]. Karies ist neben der Parodontitis eine der Hauptursachen für Zahnverlust und ihre Behandlung ist zeitaufwendig und teuer.Karies ist jedoch überwiegend vermeidbar. Die entscheidende Ursache für die Entstehung von Karies ist übermäßiger Zuckerkonsum [REF]. Es ist wichtig, diese Tatsache klar hervorzuheben. Faktoren wie Speichelfluss und Aufnahme von Fluoriden wirken dem Kariesprozess entgegen und spielen somit eine wichtige Rolle, werden aber als Effektmodifikatoren angesehen [REF]. Der Zuckerkonsum selbst wird wiederum durch umweltbedingte und psychosoziale Faktoren bestimmt [REF].Ein hoher Zuckerkonsum ist auch mit einem erhöhten Risiko für Parodontalerkrankungen assoziiert. Es wird angenommen, dass durch die Nahrung aufgenommener Zucker chronisch entzündliche Erkrankungen wie Parodontitis begünstigt [REF].Die wirtschaftlichen Kosten für die Behandlung von Karies, Parodontitis und Zahnverlust sind immens. Laut einer Modellrechnung beliefen sich die direkten und indirekten Kosten von durch übermäßigen Zuckerkonsum verursachten Zahnerkrankungen im Jahr 2010 auf global 172 Mrd. US-Dollar. Für Deutschland wurden dabei pro Jahr und Person 210 € an Zahnbehandlungskosten errechnet [REF]. Angesichts der erheblichen Beeinträchtigungen der Lebensqualität für den Einzelnen sowie der hohen Kosten für Gesundheitssystem und Gesellschaft stellen diese zuckerbedingten Erkrankungen also ein signifikantes Gesundheitsproblem dar.Zucker spielt aber nicht nur für die Mundgesundheit eine Schlüsselrolle. Ein hoher Zuckerkonsum, vor allem durch zuckrige Getränke, geht mit einem erhöhten Risiko für die Entstehung von Übergewicht und Adipositas sowie deren Folgeerkrankungen einher [REF]. Wissenschaftlich belegt sind außerdem Zusammenhänge zwischen Zuckerkonsum und Diabetes Typ 2 [REF] sowie Herz-Kreislauf-Erkrankungen [REF]. Als Risikofaktor lange vernachlässigt, ist Zucker infolge dieser Erkenntnisse mehr und mehr ins Blickfeld der Gesundheitspolitik gerückt.In diesem Beitrag beschreiben wir die derzeit geltenden Empfehlungen der WHO zum Zuckerkonsum sowie Daten zum Verzehr in Deutschland. Unser Ziel ist es, wichtige Faktoren, die den Zuckerkonsum beeinflussen, zu beleuchten und Konzepte und Strategien zur Zuckerreduzierung darzulegen, um die Mundgesundheit zu verbessern.
Die Alkohol- und Suchtprävention ist ein bedeutendes Handlungsfeld im betrieblichen Personal- und Gesundheitsmanagement. Information und Aufklärung der Beschäftigten sowie frühzeitige Interventionen bei Auffälligkeiten am Arbeitsplatz tragen zum Erhalt und zur Wiederherstellung ihrer physischen und psychischen Gesundheit bei. Da sich auf diese Weise dramatische Verläufe bei Suchtproblemen genauso wie lange krankheitsbedingte Abwesenheiten vermeiden lassen, profitieren Beschäftigte und Betriebe gleichermaßen davon.In diesem Übersichtsartikel werden zunächst die Entwicklung und der aktuelle Stand betrieblicher Suchtpräventionskonzepte aufgezeigt. Ziel ist es, ihre Wirkung, Verbreitung und Relevanz anhand der vorliegenden Datenbasis zu belegen. Betriebliche Suchtprävention, das kann verdeutlicht werden, ist nicht allein auf der Verhaltensebene und den Problemen einzelner Beschäftigter zu denken. Sie zielt in hohem Maße auch auf die Verhältnisse, wie z. B. suchtfördernde Arbeitsbedingungen, und die Prävention von Gefährdungen. Zudem nimmt sie die strukturelle Verankerung des betrieblichen Suchtpräventionsprogramms in den Blick. Die Qualitätsstandards für die betriebliche Suchtprävention und Suchthilfe der Deutschen Hauptstelle für Suchtfragen (DHS; [REF]) liefern ein fachlich und rechtlich abgestimmtes Konzept für die Umsetzung in der Praxis und unterstreichen seine Relevanz im Sinne der gesicherten arbeitswissenschaftlichen Erkenntnisse [REF]. Forschungen zur erwiesenen Wirksamkeit im Sinne der evidenzbasierten Medizin [REF] fehlen allerdings bisher.Im darauffolgenden Kapitel wird dargelegt, inwiefern der betriebliche Kontext ein wichtiges Setting bietet, um zum einen erwachsene Menschen im Berufsalter regelmäßig auf die Wirkung und die Funktion von Alkohol aufmerksam zu machen und zum anderen Regelungen zur Konsumreduzierung zu vereinbaren. Die Datenlage, die sich mit dem riskanten Alkoholkonsum befasst, spricht dafür, diese Möglichkeiten systematisch zu nutzen [REF]. Im Anschluss werden die Verfahren vorgestellt, die als wichtige Instrumente des Personal- und Gesundheitsmanagements die rechtlich und fachlich angemessenen Interventionen bei riskantem Konsum und Suchtgefährdung erleichtern. Eine wichtige Rolle nehmen dabei die Führungskräfte mit ihren Fürsorge- und Präventionspflichten ein. Von ihrer Qualifikation und Handlungsbereitschaft hängt wesentlich der Erfolg der Interventionen ab [REF].Verbindliche Angebote der betrieblichen Suchtprävention zur Information und Aufklärung sowie zur Beratung und Unterstützung werden in einem weiteren Kapitel beschrieben. Sie tragen dazu bei, die dafür erforderlichen strukturellen Voraussetzungen mitzugestalten. Die dafür erforderlichen strukturellen Voraussetzungen werden in einer Vereinbarung festgelegt [REF].Abschließend werden Maßnahmen zur übergreifenden Prävention angesprochen. Sie gewinnen angesichts sich verändernder Anforderungsstrukturen im Betrieb an Bedeutung [REF]. Im Rahmen des Gesundheitsmanagements ist die betriebliche Suchtprävention dafür ein wichtiger Kooperationspartner. Für die Bearbeitung des Themas wird im Beitrag im Wesentlichen auf Literatur zurückgegriffen, in die Ergebnisse grundlegender Literaturrecherchen eingeflossen sind, v. a. die Monografie Vom Alkoholverbot zum Gesundheitsmanagement – Entwicklung der betrieblichen Suchtprävention von 1800 bis 2000 [REF], für die in den 1990er-Jahren Literaturdatenbanken herangezogen wurden wie PsycLIT, Psyndex, APA PsycInfo sowie die Bestände der Staatsbibliothek Hamburg, der Deutschen Bibliothek (Frankfurt am Main bis 1990), der Amerika-Gedenkbibliothek, des Archivs für Soziale Fragen (beide in Berlin), der Niedersächsischen Landesbibliothek und des Archivs der Deutschen Hauptstelle für Suchtfragen (DHS). Der Rechercheprozess wurde für neuere Veröffentlichungen unter Einbeziehung von PsychData, PubMed und der Gesundheitsberichterstattung des Bundes jeweils auf den neuesten Stand gebracht.
Versorgung und Versorgungsforschung stehen auch in der Urologie immer wieder im Fokus der wissenschaftlichen und allgemeinen Diskussionen. Es hat in der Vergangenheit hierzu einige Positionspapiere und viele interessante Beiträge auf den verschiedensten Plattformen gegeben und es gibt zahlreiche spannende Projekte aus der Urologie, die entweder retrospektiv oder prospektiv beachtliche Datensammlungen generiert haben und weitere, die Daten generieren werden [REF]. (Fast) allen diesen Projekten ist gemein, dass sie einen festen vordefinierten Rahmen der zu erfassenden verschiedenen Datenpunkte zugrunde gelegt haben. Das hat den Vorteil einer hohen Homogenität und „Analysefähigkeit“ der Daten, was die Verlässlichkeit der generierten Aussagen zu erhöhen scheint. Auch benötigt man eine kleinere Stichprobe, da Unterschiede klarer erkennbar werden, wenn sie nicht durch ein „Grundrauschen“ ungefilterter Daten überdeckt werden.Diese Form der Analytik hat aber auch – im Vergleich zu „Big Data“ – einige Limitationen und Nachteile: Der Blick auf die Grundgesamtheit erfolgt immer durch den vorher definierten Filter, der durch die Festlegung der Datenpunkte entstanden ist, die Eingabe der Daten ist mit einem nicht unerheblichen Zeit‑, Personal- und Kostenaufwand verbunden und es können nur die Korrelationen aufgedeckt werden, die zwischen den (vor)definierten Datenpunkten bestehen.Ein „ungefilterter“ Blick auf „alle“ Daten wäre ideal – ist aber unrealistisch. Eine Näherung könnte aber darin bestehen, dass man ohne Vorauswahl und möglichst vollständig alle vorhandenen umfangreichen Daten in einen Pool einschleust. Da Zusammenhänge oder Beziehungen von Datenpunkten im Gesamtkollektiv nicht zugrunde gelegt werden, kann eine solche heterogene Grundgesamtheit besser Analyseverfahren wie z. B. Clusteranalysen unterzogen werden, als das bei prädefinierten Datensammlungen der Fall ist. Bei Analyseverfahren wie der Clusteranalyse geht es letztlich darum, mit einem Segmentierungsverfahren aus einer Grundgesamtheit homogene Teilmengen zu bilden, die nach gleichzeitiger Heranziehung „aller“ Eigenschaften zur Gruppenbildung gefunden werden können [REF]. Hierbei können auch bisher unbekannte Beziehungen von Eigenschaften aufgedeckt werden um so zumindest hypothesengenerierend wertvolle Grundlagen für weitere Überprüfungen darzustellen. Auch andere Analyseverfahren (z. B. Netzwerkanalysen [REF]), die v. a. aus der Soziologie entlehnt werden, zielen darauf ab, die Beziehungen von Datenpunkten zu charakterisieren und in einen „neuen“ Gesamtzusammenhang zu stellen. Auch hierbei können die verschiedenen Algorithmen lernend alternierend angepasst werden, um weitere Korrelationen aufdecken zu können. Diese „neuen“ Zusammenhänge können dann wiederum an definierten Datensets geprüft und an weiteren Kohorten prospektiv validiert werden. Erfolgreich ist die Anwendung und Transferierung von Netzwerkanalysen beispielsweise für die Erforschung verschiedener Markersignaturen bei Tumorpatienten bereits eingesetzt worden [REF].Ein weiterer extremer Vorteil des hier besprochenen Modells von UROscience besteht darin, Daten automatisiert aus dem Arzt‑/Praxisinformationssystem (AIS) zu extrahieren, was die Bereitstellung erleichtert und v. a. keinen zusätzlichen Arbeitsaufwand bedeutet. Ein Nachteil mag darin bestehen, dass eine Verbesserung der dann anonymisierten Daten nur über Umwege erreichbar ist, da eine „source-data verification“ nicht wie bei einer klassischen Datensammlung stattfinden kann.Als Pioniere innerhalb des Spitzenverbands Fachärzte Deutschlands e. V. (SpiFa) haben die urologischen Fachärzte aus dem Berufsverband der Deutschen Urologen (BvDU) gemeinsam mit dem DIFA (Deutsches Institut für Fachärztliche Versorgungsforschung) das Projekt UROgister/UROscience initiiert [REF], dem zwei unterschiedliche Ansätze zugrunde liegen – der Aufbau einer urologischen Versorgungsforschungsdatenbank (UROscience) und die Entwicklung eines Tools zur Unterstützung der verpflichtenden Krebsregistermeldung (UROgister; https://difa-vf.de/urogister-krebsregistermeldung). Derzeit stellen 105 Urologen Daten für die Bewertung und Beforschung insbesondere berufspolitisch relevanter Fragestellungen aller Art zur Verfügung, die über ganz Deutschland verteilt sind (Abb. 1). Der dafür aus dem Hauptausschuss des BvDU ins Leben gerufene UROscience-Beirat unter dem Vorsitz von Prof. Peter J. Goebell begleitet den Umgang mit den urologischen Daten, bewertet und prüft Anfragen an die DIFA zu Analysen und kann Fragen aus der Urologie an die Datenbank aktiv platzieren, die besonders im berufspolitischen Kontext relevant erscheinen.
Alkoholkonsum ist einer der weltweit führenden Risikofaktoren für Morbidität und Mortalität [REF]. Langfristig ging der Alkoholkonsum in Deutschland in den vergangenen Jahren zwar kontinuierlich zurück: 2019 konsumierten 32,9 % der 18- bis 25-Jährigen mindestens wöchentlich Alkohol, 2004 waren es noch 43,6 % [REF] (zu weiteren Prävalenzen s. Beiträge von Orth und Merkel sowie Kraus et al. in diesem Themenheft). Aber mit einem jährlichen Pro-Kopf-Konsum (der Bevölkerung ab dem Alter von 15 Jahren) von Reinalkohol in Höhe von 10,9 l liegt Deutschland immer noch deutlich über dem Durchschnitt der Mitgliedstaaten der Organisation für wirtschaftliche Zusammenarbeit und Entwicklung (OECD) von 8,9 l [REF].Aufgrund des Zusammenhangs zwischen der Konsummenge und dem Ausmaß alkoholbedingter Erkrankungen kann die langfristige bevölkerungsweite Senkung des Pro-Kopf-Konsums eine wirkungsvolle Strategie sein, um einen Rückgang der alkoholbezogenen Schäden, d. h. körperliche und psychische Erkrankungen, Unfälle, Arbeitsausfälle usw., zu erreichen, wie dies z. B. auch im Nationalen Gesundheitsziel „Alkohol reduzieren“ 2015 formuliert wurde [REF]. Als besonders wirkungsvoll zur Verringerung des Alkoholkonsums und damit zur Prävention von alkoholbedingter Morbidität und Mortalität gilt die Kombination von Maßnahmen der Verhaltens- und Verhältnisprävention, also etwa die Beschränkung der Verfügbarkeit oder die Erhöhung der Preise für alkoholische Getränke in Kombination mit Aufklärung über die Gesundheitsrisiken durch deren Konsum.Die vorrangige Aufgabe der Bundeszentrale für gesundheitliche Aufklärung (BZgA) ist dabei die dauerhafte Aufklärungs- und Informationsarbeit. Seit dem Jahr 2009 liegt daher ein Schwerpunkt auf der Prävention des Alkoholmissbrauchs (im Folgenden „Alkoholprävention“). Aktuell werden 3 Kampagnen zur Alkoholprävention durchgeführt („Null Alkohol – Voll Power“ für 12- bis 16-Jährige, „Alkohol? Kenn dein Limit.“ für 16- bis 20-Jährige bzw. die gleichnamige Kampagne für Erwachsene) sowie das Programm „Alkoholfrei Sport Genießen“ in Kooperation mit Breitensportverbänden (Abb. 1). Ab dem Alter von 12 Jahren werden alle Altersjahrgänge sowie beide Geschlechter gleichermaßen adressiert, wobei ein deutlicher Schwerpunkt auf den Jugendlichen und jungen Erwachsenen im Alter zwischen 16 und 20 Jahren mit der größten bundesweiten Alkoholpräventionskampagne „Alkohol? Kenn dein Limit.“ liegt. Innerhalb der erwachsenen Allgemeinbevölkerung ab 21 Jahren liegen die Schwerpunkte auf Eltern, Schwangeren und ihrem Umfeld, suchtbelasteten Familien sowie auf älteren Menschen. Damit wird eine Präventionskette geschaffen, die alle Altersgruppen und bestimmte Teilpopulationen differenziert in den Blick nimmt, eine entsprechend abgestufte Zielsetzung aufweist und deren Elemente sich hinsichtlich Kommunikation und Themenwahl entsprechend unterscheiden und ergänzen. Wichtige Multiplikatoren/innen, die in die Kampagnen einbezogen werden, sind u. a. Eltern, Lehrkräfte sowie Ärzte/innen. Ziel dieses Beitrags ist es, die wesentlichen Bestandteile der BZgA-Alkoholprävention überblicksweise vollständig darzustellen, um zu zeigen, dass grundsätzlich unterschiedliche Interventionsformen auf verschiedenen Ebenen innerhalb eines Gesamtkonzepts zusammenwirken. Es soll deutlich werden, dass dabei adressatengerecht ausdifferenzierte Maßnahmen aufeinander aufbauen und sich ergänzen. Nach der Darstellung des für alle 3 Kampagnen einheitlichen konzeptionellen Rahmens sowie der diese flankierenden Aktivitäten werden die Kampagnen mit ihren jeweiligen Zielen und Adressaten/innengruppen genauer beschrieben. Auf die konkreten Maßnahmen der Internetkommunikation, der Personalkommunikation sowie der Printmedien und Massenkommunikation gehen die dann folgenden 3 Kapitel ein. Dort wird gezeigt, wie sich die Kampagnenangebote jeweils adressatenspezifisch unterscheiden bzw. ergänzen.
Alkohol ist die psychoaktive Substanz, mit der junge Menschen als Erstes, am häufigsten und leicht in Kontakt kommen [REF]. Sie verspricht sowohl Gemeinschaftsgefühl und Spaß als auch Unterstützung bei der Bewältigung der Anforderungen und Herausforderungen des Jugendalters [REF]. Gleichzeitig sind mit Alkohol die schwerwiegendsten Schädigungen und Probleme des Jugend- und Erwachsenenalters verbunden [REF]. Trotz der zu beobachtenden Abnahme des Alkoholkonsums über die letzten Jahrzehnte, konsumiert eine substanzielle Gruppe von Jugendlichen und jungen Erwachsenen Alkohol in einer riskanten Art und Weise [REF]. Die sozioökologische Sichtweise weist auf die multikausal bedingte Entwicklung von Alkoholkonsum und -missbrauch hin [REF]. Einflussreiche Risiko- und Schutzfaktoren sind in allen Mikrosystemen jugendlicher Entwicklung sowie auf Makroebene empirisch identifiziert worden [REF].Einen verantwortungsvollen Konsum in einem bezüglich Alkohol relativ liberal regulierten Land wie Deutschland [REF] zu erlernen ist herausfordernd. Seine Entwicklung kann mit zielgerichteten verhaltensbezogenen Präventionsprogrammen und mit der verhältnisbezogenen, gesundheitsförderlichen Gestaltung der Lebensbedingungen der jungen Menschen unterstützt werden [REF]. Eine evidenzbasierte Entscheidung, wie suchtpräventiv zu handeln ist, verknüpft das bestmögliche wissenschaftliche Wissen mit der Expertise der Praxis und den Besonderheiten der jeweiligen Zielgruppe und des Kontextes [REF]. Die Expertisen zur Suchtprävention der Bundeszentrale für gesundheitliche Aufklärung (BZgA) liefern das aktuelle wissenschaftliche Wissen zur Wirksamkeit von Angeboten in unterschiedlichen Handlungsfeldern der Suchtprävention [REF]. Ziel der Expertise ist es, die Wirksamkeit existierender suchtpräventiver Ansätze anhand von aktuellen, hochwertigen wissenschaftlichen Studien zu beurteilen. Wirksamkeit wird dabei definiert als Verhinderung, Verzögerung oder Reduktion des Konsums von Tabak, Alkohol, Cannabis und anderen illegalen psychoaktiven Substanzen. Ebenso wird die derzeitige Prävention des problematischen Glücksspielverhaltens bewertet. In Augenschein genommen werden verhaltens- und verhältnisbezogene Präventionsformen, darunter universelle und selektive Strategien. Universelle Programme richten sich an Personen, die als Gesamtgruppe ein durchschnittliches Risiko für einen späteren Substanzmissbrauch aufweisen (z. B. Gesamtbevölkerung, Klassenverbände). Selektive Programme richten sich an Personen, die als Gruppe ein überdurchschnittliches Risiko für einen späteren Substanzmissbrauch aufweisen (z. B. Kinder aus suchtkranken Familien, Kinder mit Verhaltensauffälligkeiten). Die aus der Forschung abgeleiteten Aussagen werden jeweils für die unterschiedlichen Handlungsfelder gruppiert: Familie, Schule, Hochschule, Medien, Gesundheitsversorgung, Kommune und gesetzliche Rahmenbedingungen. Innerhalb dieser Handlungsfelder werden die Schlussfolgerungen nach Substanzen getrennt. Ihre Aussagekraft (Evidenzstärke) wird ausgewiesen.Der Adressatenkreis der Expertise sind Verantwortliche für Suchtprävention (Entscheidungsträger*innen) auf allen handlungspolitischen Ebenen sowie Personen, die mit der Entwicklung und/oder Durchführung präventiver Maßnahmen betraut sind.Für den folgenden Beitrag wurde der Fokus auf die Wirksamkeit alkoholpräventiver Ansätze begrenzt und die alkoholbezogene Literatur ausgewertet. Die konkrete Fragestellung lautet: Welche Ansätze haben in den jeweiligen Handlungsfeldern der Suchtprävention alkoholpräventive Effekte? Zuerst wird die Methode vorgestellt, die in [REF]1 ausführlich beschrieben ist. Die darauffolgenden Ergebnisse sind nach Handlungsfeld gruppiert. In der abschließenden Diskussion wird ein Schwerpunkt auf den Vergleich zwischen internationaler und nationaler Evidenz sowie auf die Evidenz für alkoholpolitische Strategien gelegt.
Die Behandlung von Pathologien der Aorta stellt auch in der Schweiz ein Schwerpunkt vieler gefäßchirurgischer Kliniken dar. Die Gefäßchirurgie ist in der Schweiz erst seit der Schaffung eines spezifischen Weiterbildungsprogramms mit eigenständigem Facharzt für Gefäßchirurgie im Jahr 2015 als eigenständige Spezialisierung etabliert. Dementsprechend findet in diesem noch jungen Fachgebiet an vielen Krankenhäusern ein struktureller Wandel statt. Es gibt eine Großzahl an Krankenhäusern, an denen die Gefäßchirurgie historisch bedingt an die chirurgische Klinik angegliedert ist. Hier werden gefäßchirurgische Eingriffe auch durch Kollegen/-innen mit allgemeinchirurgischer Weiterbildung durchgeführt, einige davon führen den früheren Schwerpunkttitel für Gefäßchirurgie. Daneben gibt es 5 universitäre Zentrumsspitäler, an denen traditionell jeweils eine gemeinsame Klinik für Herz- und Gefäßchirurgie geführt wurde. Hier fanden strukturelle Neuerungen mit der Schaffung von eigenständigen Kliniken für Gefäßchirurgie bereits statt, respektive sind im Gang. Zusätzlich gibt es nicht universitäre kantonale Krankenhäuser, an denen eigenständige Kliniken für Gefäßchirurgie ein breites Spektrum an Eingriffen durchführen. Zudem bieten auch Privatspitäler gefäßchirurgische Eingriffe an.Die Schweiz hat ein föderalistisches Gesundheitssystem und ist damit kantonal geregelt. Die lückenlose Erfassung von Eingriffen an der Aorta ist deshalb eine große Herausforderung. Dies zum einen aufgrund der Tatsache, dass viele zum Teil auch sehr kleine Krankenhäuser einen kantonalen Leistungsauftrag erhalten und solche Eingriffe durchführen. Zum andern erschwert die Tatsache, dass je nach Kanton verschiedene Disziplinen diese Eingriffe durchführen dürfen (d. h. Gefäßchirurgie, Herzchirurgie, Chirurgie, Angiologie, Radiologie und Kardiologie), eine flächendeckende Erfassung.Aortenaneurysmen werden in Krankenhäusern jeglicher Grösse behandeltMit Swissvasc wurde bereits 1999 ein gesamtschweizerisches Gefäßregister gegründet, welches das Ziel hatte, möglichst alle in der Schweiz durchgeführten gefäßchirurgischen Eingriffe und deren Behandlungsergebnisse systematisch zu erfassen [REF]. Die Teilnahme war seit der Gründung des Registers jedoch freiwillig. Die starke Zunahme an eingegebenen Eingriffen in den ersten Jahren von Swissvasc wird auf eine initial höhere Eingabedisziplin zurückgeführt. Ab 2016 wurde Swissvasc nach einer grundlegenden Überarbeitung neu gestartet. Dank einer Matrixstruktur und einem neuen IT-Partner (Adjumed Services AG) konnte ein komplett neu designtes Register präsentiert werden. Fortan konnten sämtliche Eingriffe am vaskulären System erfasst werden. Damit ist nicht nur eine quantitative Erfassung von Operationen möglich, vielmehr können auch Teilschritte innerhalb einer Operation abgebildet werden. So können bei Swissvasc zum Beispiel Abfragen hinsichtlich der klinischen Präsentation, der lokalen Voroperationen, der Sterilität des Operationssitus, der zugrunde liegenden Pathologie, aber auch nach Zusatzeingriffen zur Inflow- respektive Outflow-Optimierung durchgeführt werden. Zusätzlich können Follow-up-Informationen mit einer Vielzahl von Ergebnisparametern erfasst werden, womit systematische Auswertungen erleichtert wurden. Dadurch wurde die Eingabedisziplin ins Register deutlich verbessert. Seit 2018 fordern einige Kantone, darunter der einwohnerstärkste Kanton Zürich (1,54 von 8,57 Mio., 18 %), die systematische Erfassung sämtlicher Eingriffe an der Aorta und die entsprechenden 30-Tage-Behandlungsergebnisse. Die Eingabe in Swissvasc ist hier für Listenspitäler obligatorisch. Für die übrige Schweiz ist die Partizipation weiterhin freiwillig. Seit 2019 erfassen alle größeren Krankenhäuser, die in der Schweiz Eingriffe an der Aorta durchführen, ihre Eingriffe in Swissvasc (Ausnahme Herzchirurgie).Ziel dieser Studie ist es, die Versorgungsrealität von Aortenerkrankungen in der Schweiz anhand der in Swissvasc erfassten Aorteneingriffe darzustellen. Da in verschiedenen Studien eine Mengen-Ergebnis-Beziehung beschrieben wird, soll insbesondere die Anzahl Spitäler und deren Fallzahlen bei der Versorgung von Aortenpathologien aufgezeigt werden [REF].
Die Tatsache, dass entsprechend den Angaben des Statistischen Bundesamts seit vielen Jahren die Zahl der Personen mit amtlich anerkannter Schwerbehinderung in Deutschland fast 8 Mio. beträgt [REF], bedeutet nicht, dass alle Behinderungen mit einem Problem für die Mundgesundheit einhergehen müssen. Gemäß der UN-Behindertenrechtskonvention werden grundsätzlich 4 Arten von Behinderungen unterschieden: a) geistige Behinderung, b) körperliche Behinderung, c) psychische Behinderung und d) sensorische Behinderung [REF]. Alle diese Behinderungen können nicht nur einzeln, sondern auch in unterschiedlichen Kombinationen vorliegen. In diesem Zusammenhang muss darauf hingewiesen werden, dass nicht alle Behinderungen von Geburt an bestehen oder in der frühen Kindheit auftreten, sondern zum größten Teil im Laufe des Lebens erworben werden.In einem systematischen Review aus dem Jahr 2012 wurde gezeigt, dass in Deutschland die Mundgesundheit bei Menschen mit geistiger Behinderung schlechter ist als die der Allgemeinbevölkerung [REF]. Hierfür konnten jedoch nur 3 Feldstudien ausgewertet werden. Bisher wurde noch von keiner zahnmedizinischen Fachgruppe ein Konsensus publiziert, welche Behinderung eine besondere Herausforderung für die Erhaltung der Mundgesundheit darstellt. Aus der klinischen Erfahrung heraus kann man versuchen, sich der Beantwortung dieser Frage zu nähern, indem in Bezug auf Personen mit Behinderung die folgenden 6 Kardinalfragen gestellt werden. Die Beantwortung dieser Fragen führt aus Sicht der Autoren der vorliegenden Publikation zu einer Liste an Beeinträchtigungen oder Behinderungen, die eine negative Auswirkung auf die Mundgesundheit haben können. Diese in Infobox 1 aufgeführte Liste erhebt nicht den Anspruch auf Vollständigkeit.Da Karies weltweit die chronische Erkrankung mit der höchsten Prävalenz ist [REF], beschäftigt sich die große Mehrheit der Studien zur Mundgesundheit mit der Prävalenz und dem Schweregrad von Karies. Aus nationalen Studien zur Mundgesundheit in Deutschland geht hervor, dass die Karieserfahrung bei 12-jährigen Kindern stark rückläufig ist und dass diese bei jüngeren Erwachsenen ebenfalls deutlich abgenommen hat, wenn auch nicht so ausgeprägt wie bei den Kindern [REF]. In diesen Studien wird jedoch nicht auf die Kariesprävalenz von Personen mit Behinderung eingegangen. Ziel dieser Übersichtsarbeit ist es, einen Überblick zum aktuellen Stand der Daten zur Mundgesundheit bei Menschen mit Behinderung in Deutschland zu geben und auf die Konsequenzen aufmerksam zu machen, die sich daraus ergeben.
Der Zahnhalteapparat (Parodont) besteht aus verschiedenen Gewebetypen (Epithel, Bindegewebe, Zement und Knochen), die nicht nur für die Verankerung des Zahnes im Kieferknochen sorgen, sondern auch einen dichten Verschluss um den Zahn ausbilden, um dort das Eindringen von oralen Mikroorganismen in den Körper zu verhindern (Abb. 1). Parodontalerkrankungen zählen zu den häufigsten Erkrankungen weltweit [REF]. Neben Parodontitis und Gingivitis (Zahnfleischentzündung) gibt es noch weitere Erkrankungen und Veränderungen, die das Parodont betreffen können [REF]. Von ihnen hat Parodontitis allerdings die stärksten Auswirkungen auf die (orale) Gesundheit von Erwachsenen und das Gesundheitswesen (gesundheitsökonomische Aspekte). Die Prävalenz der Erkrankung nimmt mit steigendem Alter allmählich zu. Dabei haben die meisten Betroffenen einen leichten bis moderaten Krankheitsverlauf. Schwere Formen von Parodontitis treten vor allem im höheren Erwachsenenalter und bei Senioren auf. In der Global Burden of Disease Study aus dem Jahr 2015 wurde die Prävalenz für schwere Parodontitis weltweit auf 7,4 % geschätzt [REF]. Schätzungen auf Basis der Daten der 5. Deutschen Mundgesundheitsstudie (DMS V) legen nahe, dass in Deutschland ca. 10 Mio. Menschen an einer schweren Parodontitis erkrankt sind [REF]. Jährlich werden dagegen nur etwa 1 Mio. systematische Parodontalbehandlungen mit den gesetzlichen Krankenkassen (GKV) abgerechnet [REF]. In diesem Artikel soll ein Überblick über etablierte diagnostische Verfahren und die Therapie von Parodontitis gegeben und mögliche Gründe für die Diskrepanz zwischen hohem Versorgungsbedarf und unzureichender Inanspruchnahme bzw. Behandlungszahlen dargestellt werden.Parodontitis ist eine chronisch entzündliche nichtübertragbare Erkrankung, die alle Anteile des Zahnhalteapparates betrifft und weitgehend irreversible Schäden des Parodonts verursacht (Abb. 1). Bei einer Gingivitis dagegen bleibt die Entzündung auf das Zahnfleisch (Gingiva) beschränkt, die dabei auftretenden klinischen und histologischen Veränderungen sind im Gegensatz zur Parodontitis reversibel. Gingivitis und Parodontitis geht immer die Akkumulation von Biofilm (Schicht aus einer Mischpopulation von Mikroorganismen auf Oberflächen) im Grenzbereich von Zahn und Gingiva voraus. In der Mundhöhle gibt es eine Reihe ganz unterschiedlicher Habitate für die oralen Mikroorganismen, dabei bieten Zähne mit ihrer nicht abschilfernden Oberfläche sehr günstige Bedingungen für die Anlagerung und langfristige Kolonisation von Bakterien. Zudem enthalten der Speichel und die gingivale Sulkusflüssigkeit (Flüssigkeit in der Zahnfleischtasche) Nährstoffe für das bakterielle Wachstum, aber auch antibakteriell wirksame Komponenten [REF].Durch die Unterschiede in den ökologischen Bedingungen auf den supra- und subgingivalen Zahnoberflächen bilden sich in den Biofilmen dieser ökologischen Nischen spezifisch angepasste, strukturell und funktionell organisierte mikrobielle Gemeinschaften aus [REF]. Das komplexe Gleichgewicht zwischen den verschiedenen bakteriellen Spezies beeinflusst dabei maßgeblich die Entstehung von oralen Erkrankungen wie Karies, Gingivitis und Parodontitis. Bei parodontaler Gesundheit besteht eine Symbiose zwischen dem Biofilm und einer angemessenen immuninflammatorischen Wirtsantwort. Selbst ein klinisch gesundes Parodont zeigt histologisch immer eine begrenzte Infiltration von Entzündungszellen im Bereich des epithelialen Attachments der Gingiva.Die bakterielle Kolonisation der Zahnoberflächen induziert zunächst nur eine Entzündungsreaktion in der Gingiva, die durch das Immunsystem moduliert wird und über lange Zeit persistieren kann. Verschiedene Einflüsse können aber eine Störung des Ökosystems im Mund bewirken und durch das Überwachsen spezifischer, meist gramnegativer Pathobionten (Symbionten, die unter bestimmten Bedingungen pathologisch werden) eine Veränderung des subgingivalen Biofilms in Richtung einer proinflammatorisch wirkenden Dysbiose (Ungleichgewicht der Mikroflora) induzieren [REF]. Durch diese Dysbiose kann es bei dafür anfälligen Individuen zu Parodontitis kommen, die mit einer Fehlsteuerung der inflammatorischen Antwort einhergeht und bei der es zu einem Abbau von Bindegewebe und Alveolarknochen kommt (Abb. 2). Man geht davon aus, dass Parodontitis immer eine Gingivitis vorausgeht [REF]. Allerdings entwickelt sich nicht aus jeder Gingivitis eine Parodontitis und es ist bisher nicht möglich, diese Fälle zu identifizieren, bevor die Schäden am Parodont röntgenologisch und klinisch messbar werden [REF]. Parodontitis ist eine multifaktorielle Erkrankung. Der Verlauf und die Schwere der Erkrankung werden neben der bakteriellen Ätiologie durch eine Reihe weiterer Faktoren bestimmt, die das dynamische Gleichgewicht der Interaktionen zwischen Mikroorganismen und Wirt beeinflussen können, unter anderem metabolisch schlecht eingestellter Diabetes mellitus, Nikotinkonsum, Stress, genetische und lokale Faktoren, wie z. B. Zahnengstand [REF]. Wie bei anderen chronischen Erkrankungen besteht ein deutlicher Zusammenhang zwischen Parodontitis und sozioökonomischen Variablen sowie dem Verhalten einschließlich der Exposition von Risikofaktoren und den individuellen Mundhygienemaßnahmen [REF].Der Verlauf von Parodontitis ist zumeist langsam und schmerzlos, leichte und moderate Formen von Parodontitis zeigen daher über viele Jahre zumeist wenige oder nur milde Symptome. Veränderungen der Gingiva (Zahnfleischbluten, Rötung und Schwellung) sind oft die ersten und einzigen Anzeichen, die von den Patienten oft nicht wahrgenommen oder richtig eingeordnet werden, bis es dann zu einer Lockerung der Zähne kommt. Gingivitis und frühe Stadien der Parodontitis sind für Betroffene nicht zu unterscheiden. Fortgeschrittene Formen von Parodontitis sind durch eine deutliche Zerstörung des parodontalen Ligaments und des Knochens gekennzeichnet, die zu einer merklichen Lockerung der Zähne, Zahnwanderung, einem Rückgang der Gingiva mit „schwarzen Dreiecken“ in den Zahnzwischenräumen und freiliegenden Zahnhälsen führen. Trotzdem wird Parodontitis oft als „stille“ Erkrankung beschrieben [REF].Bei einer Untersuchung wurde die Selbsteinschätzung der parodontalen Situation mit dem klinischen Befund verglichen. Dabei zeigte sich, dass etwa 75 % der Studienteilnehmer von einer moderaten oder schweren Form von Parodontitis betroffen waren, aber 62–86 % dieser beiden Gruppen nicht der Meinung waren, an Parodontitis erkrankt zu sein [REF]. Bei einer telefonischen Befragung in Deutschland wurde deutlich, dass in der Bevölkerung erhebliche Defizite beim Wissen darüber, was Parodontitis ist und welche Konsequenzen die Erkrankung haben kann, bestehen, aber auch welche Risikofaktoren mit der Erkrankung assoziiert sind und welche präventiven Maßnahmen effektiv sein können [REF]. Das fehlende Bewusstsein für die eigene Erkrankung kann dazu führen, dass zahnärztliche Behandlung erst dann in Anspruch genommen wird, wenn es bereits zu massiven Gewebeverlusten gekommen ist. Dadurch werden umfangreiche Therapiemaßnahmen notwendig und die Prognose für den Erhalt der Zähne verschlechtert sich meist deutlich.Wenn in Deutschland ca. 10 Mio. Menschen an schwerer Parodontitis erkrankt sind [REF], aber jährlich nur etwa 1 Mio. systematische Parodontalbehandlungen mit den gesetzlichen Krankenkassen abgerechnet werden [REF], würde es theoretisch 10 Jahre benötigen, um diese Erkrankungslast zu therapieren. Darüber hinaus liegt bei 1 Neuerkrankung an schwerer Parodontitis unter 142 Personen pro Jahr die jährliche Neuerkrankungsrate bei einer Bevölkerung von 80 Mio. bei etwa 500.000 [REF], was die Lage verschärft. Diese Situation kann durchaus als Unterversorgung bewertet werden, für die es unterschiedliche Gründe geben kann.Wie sieht es mit der Ausbildung der Zahnärzte hinsichtlich parodontaler Erkrankungen aus? Die noch gültige Approbationsordnung für Zahnärzte sieht im Rahmen der Zahnärztlichen Prüfung in der Fächergruppe Zahnerhaltungskunde eine eigene Prüfung in Parodontologie vor. Das alleine stellt aber noch nicht sicher, dass in der zahnmedizinischen Ausbildung genug Wert auf parodontale Aspekte gelegt wird. An den deutschen Universitätseinrichtungen, die Zahnmedizin ausbilden, sind regelhaft Professoren für Zahnerhaltungskunde, zahnärztliche Prothetik, Kieferorthopädie sowie Mund‑, Kiefer‑, Gesichtschirurgie berufen. Professuren für Parodontologie sind eher die Ausnahme [REF]. Dies ist ein Indiz dafür, welch geringe Bedeutung dem Fach beigemessen wird. Ohne Professur hat es eine Fachdisziplin schwer, nachhaltige Bedeutung in der Ausbildung zu erlangen.Deutschland weist im Vergleich zum Ausland mit rund 87 Zahnärzten pro 100.000 Einwohner eine besonders hohe Dichte an Zahnärzten auf (Quelle: statista, www.de.statista.com). Jedoch ist im Gegensatz zu anderen Industrienationen die Weiterbildung von Fachärzten hierzulande noch nicht weit entwickelt. Anders als in den Fächern Kieferorthopädie und zahnärztliche Chirurgie gibt es nicht in allen Zahnärztekammern eine Möglichkeit zur Weiterbildung zum/zur Fachzahnarzt*ärztin für Parodontologie. Lediglich die Zahnärztekammer Westfalen-Lippe sieht eine solche Weiterbildung vor. Seit Jahrzehnten wehren sich andere Zahnärztekammern gegen eine fachzahnärztliche Weiterbildung im Bereich Parodontologie und begründen dies mit dem Argument, Parodontitis könnte auch durch nicht weitergebildete Zahnärzte behandelt werden. Dies trifft allerdings auch für kieferorthopädische und oralchirurgische Leistungen zu, die jeder Zahnarzt auch ohne Zusatzqualifikation erbringen kann. Ein wichtiger Grund für die fehlende Ausdifferenzierung durch die Zahnärztekammern ist vermutlich auch, dass die Verhandlungsposition der Kassenzahnärztlichen Vereinigung gegenüber den Kostenträgern durch die Aufspaltung der Zahnärzteschaft in verschiedene Fachgruppen nicht geschwächt werden soll.Möglicherweise können nicht weitergebildete Zahnärzte leichte und moderate Formen von Parodontitis tatsächlich gut behandeln, aber offenbar erfolgt bisher keine ausreichende Umsetzung. Bei der bestehenden Prävalenz der Parodontitis muss parodontale Prävention und Therapie notwendigerweise Teil des Therapiekonzeptes einer jeden Zahnarztpraxis sein. Nur mit Fachzahnärzten sind die hohen Zahlen nicht zu bewältigen, aber Fachzahnärzte könnten die allgemeinzahnärztlichen Kollegen wesentlich unterstützen.Die Deutsche Gesellschaft für Parodontologie (DG PARO) hat deshalb vor knapp 30 Jahren eine der Fachzahnarztausbildung analoge Ausbildung zu Spezialisten für Parodontologie ins Leben gerufen. Die Ausbildung zum DG PARO-Spezialisten für Parodontologie® ist mit der Ausbildung zum Fachzahnarzt für Parodontologie identisch und umfasst eine Ausbildung von ca. 5000 h. In Deutschland gibt es aktuell 244 solche DG PARO-Spezialisten für Parodontologie (Quelle: Deutsche Gesellschaft für Parodontologie e. V., Stand Ende 2020). Deutlich stärker steigen dagegen die Zahlen von Absolventen postgradualer Masterprogramme in diesem Fachgebiet. Die Ausbildung mit insgesamt ca. 3200 h erfolgt zumeist berufsbegleitend und hat eine stärkere theoretische Ausrichtung als die des Spezialisten und des Fachzahnarztes.Weitere mögliche Erklärungen für die geringen Behandlungszahlen sind die bisher zumindest zum Teil inkonsistenten Richtlinien, die die systematische Therapie von Parodontopathien im Rahmen der GKV regeln, sowie die nicht ausreichende Honorierung dieser Therapie. Am 17.12.2020 hat der Gemeinsame Bundesausschuss (G-BA) nach einem fast 10 Jahre währenden Prozess eine neue Behandlungsrichtlinie beschlossen, die parodontale Therapie nach modernen evidenzbasierten Standards erlaubt. Unbehandelt oder unzureichend therapiert führt Parodontitis zu einer Zerstörung der zahntragenden Gewebe und letztendlich dem Verlust von Zähnen. Die Erkrankung ist eine der Hauptursachen für Zahnverlust bei Erwachsenen weltweit [REF] und hat damit einen negativen Einfluss auf Kaufunktion, orale Ästhetik und Lebensqualität der Betroffenen. Eine systematische Therapie von Parodontitis kann dagegen die Lebensqualität der Betroffenen signifikant
Die häufigste zu behandelnde Stoffwechselerkrankung im Kindes- und Jugendalter ist der Diabetes mellitus Typ 1. Aufgrund der kontinuierlich steigenden jährlichen Rate an Neuerkrankungen sind immer mehr betroffene Kinder und Jugendliche zu verzeichnen [REF].Immer niedrigere HbA1c-Werte als Ausdruck der mittleren Stoffwechsellage sind erreichbar, ohne dass die Gefahr der Unterzuckerungen steigt [REF]. Die Leitlinie der International Society for Pediatric and Adolescent Diabetes (ISPAD) hat daher einen HbA1c-Therapiezielwert < 7 % festgelegt, wenn den Patienten Möglichkeiten wie eine Insulinpumpe und ein Glucosesensor zur Verfügung stehen. Ein niedrigerer HbA1c-Wert ist auf lange Sicht gleichbedeutend mit einem geringeren Risiko für diabetesbedingte Folgeerkrankungen. Unverändert in den letzten Jahren bleibt das Risiko für eine diabetische Ketoacidose bei Manifestation; im Zuge des ersten Lockdowns der durch die „coronavirus disease 2019“ (COVID-19) ausgelösten Pandemie hat sich dieses Risiko bei Kleinkindern sogar verdoppelt [REF]. Hier sind präventive Ansätze gefordert, um eine solche Entwicklung zu verhindern. Positive Beispiele gibt es z. B. aus dem Raum Stuttgart; die dortige Aufklärung bei der Einschulung hat zur Reduktion der Ketoacidoserate geführt [REF]. Die Arbeitsgemeinschaft für Pädiatrische Diabetologie hat diese Maßnahme nun auf ganz Deutschland ausgeweitet (diabetes-kinder.de).Pumpe und Sensor sind in der heutigen Zeit Standards in der Diabetestherapie bei Kindern und JugendlichenDer wesentliche Fortschritt in der Behandlung von Menschen mit einem Typ-1-Diabetes besteht neben der Verabreichung neuer Insuline [REF] in der stetigen Weiterentwicklung der technischen Möglichkeiten, bei der sich die Pädiatrie mit einem hohen Nutzungsgrad zeigt. Insulinpumpen und Glucosesensoren werden in allen Altersgruppen, die von Kinderärzten betreut werden, deutlich häufiger angewendet als von Erwachsenen; bei Kleinkindern beträgt die Nutzungsrate für die Insulinpumpe > 95 % [REF]. Die exponentielle Zunahme der Nutzung von Glucosesensoren mit aktuell > 70 % aller pädiatrischen Patienten ist eine direkte Folge der Kostenübernahme durch die gesetzliche Krankenversicherung (bei Erfüllung der Indikationskriterien) im Jahr 2016. Mehr als 60 % aller Minderjährigen mit einem Typ-1-Diabetes nutzen eine Insulinpumpe: Pumpe und Sensor sind Therapiestandards.Die Vorteile der Insulinpumpentherapie (20-fach feinere Dosierbarkeit als bei Pens, kontinuierliche Abgabe einer bis zu halbstündlich einstellbaren Basalrate, Rechenunterstützung durch integrierten Computer) mit denen eines Glucosesensors (kein Stechen zur erforderlichen Einzelmessung, sondern „immer“ aktuelle Messwerte, Alarmfunktionen, Trendanzeige) zu kombinieren, ist also naheliegend. Außerdem entwickeln sich mit der Verwendung einer Insulinpumpe weniger akute Komplikationen [REF], und es besteht ein Vorteil in ihrer frühen Anwendung [REF].Ein Gerät mit der Funktion der vorausschauenden Abschaltung der Insulinzufuhr durch eine Pumpen-Sensor-Kombination ist in Deutschland seit einigen Jahren verfügbar, seit dem vergangenen Jahr auch im zweiten, wissenschaftlich getesteten System.Ein geschlossener Regelkreislauf, in der Forschung lange als „closed loop“ bezeichnet, ist inzwischen als sog. automatische Insulinabgabe erhältlich („automated insulin delivery“, AID, [REF]).
In der modernen Schilddrüsenchirurgie ist der Chirurg bereits in die OP-Indikation eingebunden, sofern es sich nach erster Diagnostik durch niedergelassene Ärzte, Nuklearmediziner, Endokrinologen oder Internisten nicht eindeutig um ein konservativ zu behandelndes Krankheitsbild handelt. Die gemeinsame interdisziplinäre Einschätzung ermöglicht es dem Chirurgen, frühzeitig seine Stellungnahme einzubringen und im Bedarfsfall eine Operationsplanung und ein befundadaptiertes Resektionsausmaß vorzuschlagen. Dies bedeutet sowohl mit dem Patienten als auch mit dem behandelnden Diagnostiker die operativen Möglichkeiten bei dem jeweils betroffenen Krankheitsbild in Zusammenschau mit einer Risikoevaluierung bezüglich Morbidität des Eingriffs darzustellen. So steht beispielsweise außer Frage, dass eine fortgeschrittene Basedow-Erkrankung einer (möglichst) totalen Thyreoidektomie unterzogen werden muss. Weniger bekannt ist vielen zuweisenden Nuklearmedizinern und Endokrinologen, dass solitäre Knoten in der Schilddrüse bei entsprechend günstiger Lokalisation (und definitiv gutartiger Histologie) auch einer sparsamen funktionskritischen und gewebeerhaltenden Knotenresektion unterzogen werden können, ohne dass dabei ein erhöhtes Risiko für einen potenziellen späteren Re-Eingriff gegeben ist. Aktuelle Leitlinien geben einen Entscheidungskorridor vor, gemäß der betreffenden Leitlinie der operativen Therapie benigner Schilddrüsenerkrankungen kann mit dem Patienten in solchen Fällen ein individuelles Therapieziel vereinbart werden [REF]. Über die Vor- und Nachteile einer gewebeerhaltenden parenchymsparenden Operation (Notwendigkeit der Komplettierungsoperation bei Schilddrüsenkarzinom, hohes Rezidivrisiko bis zu 50 % innerhalb 10 Jahren bei gutartiger Struma multinodosa) versus einer totalen Thyreoidektomie (Funktionsverlust, Organverlust, Notwendigkeit der lebenslangen Substitutionstherapie, erhöhtes Risiko eines Hypoparathyreoidismus) sollte der Patient aufgeklärt werden. Dies ist im Operationsbericht bei der Indikationsbeschreibung dann explizit zu erwähnen [REF]. Unter dem Aspekt einer hohen Rate an postoperativem Hypoparathyreoidismus propagieren wir, die prinzipielle Thyreoidektomie oder Hemithyreoidektomie, wie in den Leitlinien noch empfohlen, für manche Indikationen zu überdenken.Auch bei der Behandlung des Schilddrüsenkarzinoms haben sich in den letzten Jahren völlig neue Aspekte ergeben. Durch ausgedehnte Qualitätsstudien des Outcomes nach Schilddrüsenoperation hat sich beispielsweise gezeigt, dass die totale Thyreoidektomie, insbesondere mit begleitender zentraler Kompartmentlymphadenektomie zu einer beträchtlichen Morbidität, speziell durch das Risiko des Hypoparathyreoidismus, führt [REF]. Für jeden arrivierten und routinierten Schilddrüsenchirurgen mit Verantwortungsbewusstsein ist dieses Problem nicht zu leugnen, wenngleich in der Literatur viele Autoren durch Selbstdarstellung optimierter Daten dieses Problem konsequent negieren. Der ehrliche Chirurg muss jedoch zugeben, dass das nicht der Realität entspricht, und Bergenfelz et al. hat mit seiner skandinavischen Registerstudie gezeigt, dass nach totaler Thyreoidektomie nahezu 5 % der Patienten an einer permanenten Nebenschilddrüsenunterfunktion leiden [REF]. Aufgrund der Drop-Out Rate in der Langzeitbeobachtung dürfte die Dunkelziffer noch deutlich höher liegen. Dies sind wohl alarmierende Daten, die einerseits ein streng befundorientiertes Operationsverfahren einfordern und andererseits unangebrachte Radikalität verbieten. Die gekonnte Balance zwischen zurückhaltendem Resektionsausmaß und maximaler Radikalität zur Vermeidung von Rezidiven und folgenden Re-Eingriffen mit größerer kumulativer Komplikationsrate muss auf der Grundlage evidenzbasierter Daten erfolgen.Ein entscheidender Fortschritt in der modernen Schilddrüsenchirurgie ist zweifelslos die zunehmende Minimierung eingriffstypischer Komplikationen. Die passagere Recurrenspareserate ist durch subtile mikrochirurgische und atraumatische Operationstechnik auf ein niedriges Maß reduziert worden. Einen wesentlichen Beitrag hat hier das intraoperative Neuromonitoring mit seinem ausgezeichneten negativen Vorhersagewert gebracht. Das bedeutet, dass bei regelrechtem Ableitungssignal bzw. einer adäquaten Amplitude von Nervus recurrens und Nervus vagus nach Entfernung eines Schilddrüsenlappens in über 95 % der Fälle von einer regulären Stimmbandfunktion auszugehen ist. Permanente Paresen stellen mit unter 0,5 % eine Seltenheit dar [REF]. Der Hypoparathyreoidismus ist nach wie vor ein Sorgenkind der Schilddrüsenchirurgen, speziell bei großen Strumen, bei der Autoimmunhyperthyreose und bei der zentralen Halsdissektion. Die Lokalisation der Nebenschilddrüsen kann äußerst exponiert sein. Sowohl das Erkennen als auch das vaskularisierte Erhalten der Epithelkörperchen kann selbst für den routinierten Schilddrüsenchirurgen eine Herausforderung darstellen [REF]. Die dritte eingriffstypische Komplikation, die Nachblutung, ist zu einem seltenen Ereignis geworden und wird deshalb im Zuge der modernen Aufklärung häufig vernachlässigt. Die Rate ist innerhalb der letzten Jahrzehnte durch subtile mikrochirurgische Operationstechniken und neue Operationsinstrumente zur Gewebeversiegelung deutlich zurückgegangen [REF]. Im Einzelfall aber bleibt das akute Blutungsereignis durchaus dramatisch und erfordert rasches Handeln, nötigenfalls die sofortige Entlastung durch Wundöffnung am Krankenbett. „Standard operative procedures“, Schulung von Pflegepersonal und Ärzten im fachübergreifenden Bereitschaftsdienst mit regelmäßiger Bewusstmachung des Problems der Nachblutung sind daher unverzichtbar. Jede Abteilung, die Schilddrüsenoperationen durchführt, sollte eine entsprechende SOP zum Management der Nachblutung bereitstellen. Zu bemerken ist, dass zur Optimierung der Ergebnisqualität einer endokrin-chirurgischen Abteilung stets eine lückenlose Erfassung der postoperativen aber auch permanenten Komplikationen in einer Datenbank notwendig ist. Qualitätsanalysen erlauben die Orientierung nach externen Daten und vorgegebenen Benchmarks. Die Auswertung der individuellen Einzelleistung der Chirurgen zeigt mitunter abweichende Komplikationsraten, selbst wenn diese als erfahren und spezialisiert gelten [REF]. Bei der Optimierung des operateursspezifischen Outcome gilt es auch anzusetzen, um das Gesamtergebnis einer chirurgischen Abteilung zu optimieren und somit für eine Zertifizierung gewappnet zu sein.
COVID-19 stellt Individuen und Gesellschaften weltweit vor eine der größten Herausforderungen der letzten Jahrzehnte. Public-Health-Forschung, epidemiologische und klinische Studien sind daher unabdingbar, um die Ausbreitung des für die Pandemie verantwortlichen SARS-CoV‑2 und dessen Varianten nachzuverfolgen, die Folgen für die Gesundheit und das soziale Leben besser zu verstehen sowie wirksame Therapie- und Impfmethoden zu identifizieren. Dadurch entsteht für Politik, Wirtschaft, Gesundheitsversorgung und Gesellschaft eine empirische Grundlage zur Eindämmung und zum Umgang mit der Pandemie. Diese bedarf jedoch einer fortlaufenden Aktualisierung.In sehr kurzer Zeit entstanden zahlreiche Projekte, Studien und Netzwerke zur Erforschung von SARS-CoV‑2 und COVID-19. Aus der Perspektive von Forschenden sind hiermit erhebliche Herausforderungen verbunden. Es fällt zunehmend schwer, einen Überblick zu behalten. Dieser Überblick ist jedoch unabdingbar, um Forschungsaktivitäten besser zu koordinieren, ungeplante Doppelforschung zu vermeiden und Studien harmonisiert zu implementieren. Derzeit wird die Zusammenführung von Wissensquellen durch die unzureichende Berücksichtigung von Standards und einen Mangel an harmonisierten Methoden auf allen Ebenen des Forschungsprozesses erschwert.Aufgrund der bestehenden Pflicht zur Registrierung klinischer Studien in Registern sind deren Metadaten gut strukturiert verfügbar, z. B. in der International Clinical Trials Registry Platform (ICTRP) der Weltgesundheitsorganisation (WHO; [REF]) oder dem Deutschen Register Klinischer Studien (DRKS; [REF]). Dagegen ist die Situation für epidemiologische und Public-Health-Studien wesentlich unübersichtlicher. Zwar gibt es national und international mehrere Übersichten im Internet, z. B. zu seroepidemiologischen Studien am Robert Koch-Institut (RKI; [REF]), ein COVID-19-Forschungsregister der American Society for Microbiology [REF], die COVID-19-Forschungsübersicht der Medizininformatik-Initiative [REF] oder des Rats für Sozial- und Wirtschaftsdaten [REF], aber diese Übersichten sind in Umfang, Aktualität und Informationstiefe uneinheitlich.Noch schwieriger wird es, wenn studienübergreifend ein detaillierter Einblick in die Protokolle, Erhebungsinstrumente, Itembanken und weitere Studiendokumente gewonnen werden soll, obwohl Technologien zur übersichtlichen Aufbereitung und Darstellung solcher Informationen grundsätzlich verfügbar sind [REF]. Dies wäre zum Beispiel wichtig, um eigene Erhebungen mit bestehenden vergleichbar zu planen. Nur vereinzelt bieten Projekte Zugang zu relevanten Informationen. So wurde im Netzwerk Universitätsmedizin (NUM; [REF]), das vor allem krankenhausbezogene Forschung koordiniert, mit dem German Corona Consensus Dataset (GECCO) ein positives Beispiel für harmonisierte Datenerhebungen auf Basis von internationalen medizinischen IT-Standards anhand eines abgestimmten Kerndatensatzes geschaffen [REF]. Auch das GESIS – Leibniz-Institut für Sozialwissenschaften stellt eigene Erhebungsinstrumente und weitere Studiendokumente auf ihren Webseiten bereit [REF].Ein weiteres Problem betrifft den Zugriff auf neueste Forschungsergebnisse und Daten, die inzwischen häufig in Preprints publiziert werden, bevor sie mit oft mehrmonatigen Verzögerungen in etablierten Literaturdatenbanken wie PubMed oder Web of Science auffindbar sind. Ein zentraler Zugriff auf diese verteilt vorliegenden Preprint-Archive ist daher wichtig, um aktuelle Ergebnisse besser zu finden und die Idee des breiten Communityreviews von Preprints im Sinne einer Qualitätssicherung praktisch zu unterstützen.Darüber hinaus wurden im Laufe der Pandemie weitere Hürden offensichtlich, die eine effiziente Forschung erschweren: Obwohl teilweise dieselben Personen in verschiedene Studien eingeschlossen wurden und weitere Gesundheitsdaten dieser Personen z. B. bei den Krankenkassen gespeichert sind, fehlen ausreichende Optionen, diese Daten auf individueller Ebene zu verknüpfen. Dies beschränkt die Möglichkeiten, ein ausreichend umfassendes Bild des Krankheitsgeschehens zu erhalten, um verlässlichere Aussagen zur Verlaufsprognose oder zu Impffolgen schnell zu erhalten. Dies ist der Fall, obwohl es außerhalb von reinen Forschungsprojekten positive Beispiele gibt, die durch eine entsprechende Gesetzgebung abgedeckt sind, wie etwa die Zusammenführung von Melde- und Sequenzierungsdaten am RKI, um verbesserte Aussagen zu Virusvarianten zu erhalten.Zusammenfassend erfüllen die deutschen klinischen Studien zu COVID-19 sowie entsprechende Datenbestände in Epidemiologie und Public Health trotz positiver Beispiele die Ansprüche der sogenannten FAIR-Prinzipien [REF] noch nicht im vollen Umfang. Dabei steht FAIR für die Auffindbarkeit (Findable), Zugänglichkeit (Accessible), Interoperabilität (Interoperable) und Wiederverwendbarkeit (Reusable) von Forschungsdaten LINK zu [REF]. Um diesem Defizit zu begegnen, wurde als Teil des deutschen interdisziplinären Netzwerkprojekts „Nationale Forschungsdateninfrastruktur für personenbezogene Gesundheitsdaten“ (NFDI4Health; [REF]) die Task Force COVID-19 etabliert [REF]. Ihr Ziel ist es, eine bundesweite Informationsinfrastruktur zu entwickeln, um medizinische, epidemiologische und Public-Health-Forschung nach FAIR-Kriterien leichter zugänglich zu machen und Forschungsergebnisse besser zu kommunizieren. Dabei betrachtet die NFDI4Health Task Force COVID-19 neben Forschung in Bezug auf PatientInnen mit COVID-19 auch die Public-Health-Folgen des Pandemieausbruchs auf die Allgemeinbevölkerung. Wesentliche Arbeitsinhalte der NFDI4Health Task Force COVID-19 umfassen:                            1. die Erstellung eines Studienportals zum Auffinden deutscher COVID-19-Forschungsinitiativen mit strukturierten Gesundheitsdaten aus epidemiologischen und klinischen Studien inkl. Impfstudien, administrativen Datenbanken, der Primärversorgung und der Gesundheitsberichterstattung, das auch die semantisch aufbereitete vergleichende Darstellung von Items aus Erhebungsinstrumenten umfasst;                                         2. die auf Text-Mining (Extraktion aus großen Textmengen) basierende Aufbereitung und Darstellung von SARS-CoV-2- und COVID-19-bezogener Forschung unter Nutzung der Inhalte verschiedener Preprint-Server in einer semantischen Suchmaschine;                                         3. ein Konzept zur Verknüpfung von Forschungs- und Routinedaten;                                         4. Services zum verbesserten Umgang mit Bilddaten;                                         5. die Anwendung standardisierter Analyseroutinen für harmonisierte Qualitätsbewertungen. 			   Die primäre Zielgruppe der Infrastruktur sind Forschende, die Studien zu SARS-CoV‑2 oder COVID-19 durchführen oder planen. Die intendierte Anwendung der ersten beiden Arbeitsinhalte betrifft insbesondere das leichtere Auffinden deutscher COVID-19-Ressourcen zur besseren Planung eigener Studien sowie zur Bewertung von Studienergebnissen. Mit den Arbeitsinhalten 3–5 werden Hilfsmittel an die Hand gegeben, um die Qualität spezifischer Aspekte von COVID-19-bezogener Forschung wie das Verknüpfen von Daten aus verschiedenen Quellen (Record-Linkage) oder Bildanalyse zu unterstützen. Eine Übersicht zu den verschiedenen Arbeitsinhalten findet sich in Abb. 1. Der Schwerpunkt dieser Publikation liegt auf der Darstellung des Studienportals und der Suchmaschine für Preprint-Publikationen (Arbeitsinhalte 1–2).
Sicherheit in den Kliniken ist nicht erst seit den Anschlägen zu Beginn des jungen Jahrtausends [REF] zu einem ernst zu nehmenden Aspekt in der Krankenhausplanung geworden [REF]. Ein wesentlicher Anteil der Literatur, die Veränderungen in der Vorbereitung der Klinik auf Großschadenslagen thematisiert, sind Publikationen nach dem 11. September 2001 [REF] mit den an diesem Tag stattgefundenen Anschlägen auf das World Trade Center in New York. Während der Schwerpunkt in diesen Arbeiten auf konzeptionellen Ideen wie „emergency plans“ liegt, ist die Thematik „Sicherheit an und in der Klinik“ sowohl qualitativ als auch quantitativ kaum abgebildet.Lange Zeit hat man eine mögliche und unmittelbare Bedrohung des Gesundheitssystems als solche nicht wahrgenommen, zudem scheint die dokumentierte Fallzahl an Anschlägen auf Krankenhäuser in Europa im Vergleich zu anderen Kontinenten eher gering [REF]. Die Analyse der stattgehabten terroristischen Anschläge in europäischen Großstädten mit einem möglichen „second hit“ – Szenario fernab des primären Anschlagsortes – hat hier in der strategischen Planung zu einem Umdenken geführt. Dazu gehört auch die unvermeidliche Wahrnehmung, dass terroristische Vereinigungen und Gruppierungen keinen „humanitären Maßstab“ und „keine humanitäre Hemmschwelle“ mehr an ihre Anschlagsziele legen, sondern mit ihren Mitteln einer Gesellschaft möglichst großen Schaden zufügen wollen [REF]. Dabei muss neben mit Sprengstoff bewaffneten Terroristen auch eine mögliche Bedrohung durch biologische, chemisch und nukleare Kampfmittel einkalkuliert werden [REF].Für die präklinische Situation bei einem Terroranschlag besteht offensichtlich eine hohe Gefährdung für die Personen vor Ort, insbesondere auch für die Rettungs- und Einsatzkräfte. Hier besteht teilweise eine unmittelbare Bedrohung durch z. B. den Einsatz von Schusswaffen und explosiven Stoffen sowie eine in der Regel sehr hohe Dynamik vor Ort. Hossfeld et al. konnten diese Aspekte bereits sehr schön darstellen und gerade auch das Konzept der sogenannten unsicheren, teilsicheren und sicheren Zone um einen Anschlagsort und der Relevanz für die Rettungskräfte präklinisch beschreiben [REF].Angesichts der Neubewertung dieser möglichen Bedrohungslagen mit Auswirkung auf das gesellschaftliche Leben und deren Funktionstüchtigkeit werden, gerade z. B. auch nach Ankündigungen und Aufrufen terroristischer Gruppierungen zur Durchführung von Anschlägen eben auch an Kliniken, Krankenhäuser zur kritischen Infrastruktur gezählt und sind damit hinsichtlich ihrer gesellschaftlichen Bedeutung ein potenziell attraktives Ziel für „asymmetrische“ Kräfte, wie eben z. B. terroristische Organisationen. Das Bundesamt für Bevölkerungsschutz und Katastrophenhilfe (BBK) hat hierfür einen Leitfaden zur Identifikation und zur Reduzierung von Ausfallrisiken in kritischen Infrastrukturen des Gesundheitswesens formuliert [REF].Daraus resultierten das Konzept und die Notwendigkeit der Erstellung einer Krankenhausalarm- und Einsatzplanung (KAEP) als Fundament der Sicherstellung der Patientenversorgung und zum Schutz des Krankenhauses [REF] bei u. a. terrorassoziierten MANV-Situationen. In dieser Planung sind mitunter die unterschiedlichen Möglichkeiten von Schadenslagen, die Zuweisung von Führungs- und Leitungsverantwortlichkeiten sowie das Zusammenspiel zwischen verschiedenen Behörden beim Auftreten von unterschiedlichen Szenarien zu regeln und stellen somit entsprechende Handlungsanweisungen dar.Ziel dieser Publikation ist, neben einer entsprechenden Literatursichtung v. a. eine Bestandsaufnahme anhand einer Fragebogenauswertung zu diesem Themenkomplex vorzustellen. Befragt wurden Teilnehmer*innen der 3. Notfallkonferenz der DGU. Inhalte des Fragebogens zielten v. a. auf die Bewertung von Sicherheitsfragen in der eigenen Einrichtung bzw. klinischen Infrastruktur ab. Dabei wurden als Indikatoren sowohl das Vorhandensein einer KAEP als auch Absprachen mit örtlichen Behörden und die Kooperation mit einem Sicherheits- und Wachdienst thematisiert.
Der früher postulierte Schwellenwert einer signifikanten Bakteriurie von 105 Erregern pro ml im Mittelstrahlurin (Kass-Zahl) rückt zunehmend in den Hintergrund. Wichtiger ist die klinische Symptomatik, was unterschiedliche Leitlinien aufgreifen [REF]. Ein möglicher Grund für negative Urinkulturen bei symptomatischen Patienten ist das Vorliegen von atypischen oder anaeroben Bakterien, oder Bakterien unterschiedlicher Taxa (polymikrobielle Infektion; [REF]). Bei rezidivierenden Harnwegsinfektionen (rHWI) sollten komplizierende Faktoren wie beispielsweise funktionelle und/oder anatomische Veränderungen unbedingt ausgeschlossen werden. Die aktuelle S3-Leitlinie der Arbeitsgemeinschaft der Wissenschaftlichen Medizinischen Fachgesellschaften e. V. (AWMF) zu unkomplizierten Harnwegsinfektionen (HWI) bei erwachsenen Patienten empfiehlt bei prämenopausalen, nicht schwangeren Frauen zunächst die Durchführung einer Urinkultur sowie Sonographie des Harntraktes inklusive Restharnbestimmung [REF].Zur exakten Identifizierung und Differenzierung von Uropathogenen können heute moderne Methoden wie die 16S-rRNA-Sequenzierung und „expanded quantitative urine culture“ (EQUC) eingesetzt werden [REF]. Die EQUC nutzt dabei optimierte erweiterte Protokolle mit verschiedenen Urinmengen, Kombinationen von Nährböden, Agarmedien und Inkubationszeiten [REF]. Price et al. empfehlen ein Stufensystem, um bei symptomatischen Patientinnen ohne Erregernachweis in der Kultur weitere Pathogene mittels erweiterter EQUC zu detektieren [REF]. Die 16S-rRNA-Squenzierung verwendet Bruchstücke der ribosomalen Ribonukleinsäure als Marker für bekannte und bisher unbekannte Organismen im Sequenzierungsprozess [REF]. Die einzelne Sequenz wird mit den 16S-rRNA-Sequenzen aller bekannten Uropathogene und -kommensalen verglichen. Die resultierenden Daten werden häufig als Histogramm dargestellt, wobei jede Probe als Balken und jedes Bakterium durch eine Farbe repräsentiert wird (s. Abb. 1 unten). Proben können anhand ihrer bakteriellen Zusammensetzung sortiert werden, die sich daraus ergebenden Beziehungen werden oft durch ein Dendrogramm dargestellt (s. Abb. 1, oben). Bioinformatische und biostatistische Ansätze werden dann verwendet, um Assoziationen mit demografischen Merkmalen, Symptomen und Ergebnissen zu bestimmen [REF]. Bemerkenswert ist, dass die Kosten für die 16S-rRNA-Gen-Sequenzierung bei deutlich unter 100 € liegen, wobei aktuell aufgrund der fehlenden klinischen Konsequenz der Ergebnisse keine Kostenübernahme durch die Krankenversicherung erfolgt. Aufgrund der Einschränkungen dieser Technologie, einschließlich der Interpretierbarkeit und der fehlenden Möglichkeit, Informationen über bakterielle Antibiotikaresistenzen abzurufen, ist es wichtig, auch die zusätzlichen Kosten für wachstumsbasierte Empfindlichkeitstests zu berücksichtigen. Der Preis einer Urinkultur beträgt etwa 40 €, und erhöht sich auf etwa 330 € bei zusätzlichen PCR („polymerase chain reaction“) und Tests auf Pilz- oder anaerobe Spezies [REF]. Die Existenz eines Harnblasenmikrobioms ist ein momentan kontrovers diskutiertes, hoch aktuelles Thema und Gegenstand der Forschung. Ein entscheidender Einfluss auf Genese und Verlauf chronischer Erkrankungen wie beispielsweise die interstitielle Zystitis wird angenommen [REF]. Die Vorstellung von sterilem Urin scheint daher überworfen [REF]. Abb. 2 zeigt eine Übersicht über das weibliche Kernmikrobiom des Urogenitalsystems und dessen Abweichungen bei HWI. Die möglichen Funktionen eines intakten Blasenmikrobioms bestehen in einer protektiven Barriere gegenüber pathogenen Erreger, indem beispielsweise deren Adhäsion verhindert wird, der Produktion antimikrobieller Wirkstoffe und Aktivierung der körpereigenen Immunabwehr [REF]. Mehrere aktuelle Studien konnten das Vorhandensein einer natürlichen mikrobiellen Besiedelung der Harnblase nachweisen. Alternativen zur antibiotischen Therapie rücken zukünftig weiter in den Vordergrund [REF].Gemäß der AWMF-S3-Leitlinie zu unkomplizierten HWI folgen die Präventionsmaßnahmen bei Frauen mit rHWI einem Stufenkonzept. Vor Einleiten einer antimikrobiellen Langzeitbehandlung sollte ausführlich über die Vermeidung von Risiken (z. B. geringe Trinkmenge, Unterkühlung, übertriebene Intimhygiene) beraten werden (Evidenzgrad Ib, Empfehlungsgrad A). Eine Immunprophylaxe durch orale Gabe eines Escherichia-coli-Lysats (OM-89) über 3 Monate (Evidenzgrad Ia, Empfehlungsgrad B) oder durch drei parenterale Injektionen von inaktivierten spezifizierten Enterobakterien im Abstand von einer Woche kann erwogen werden (Evidenzgrad Ib, Empfehlungsgrad C). Auch Mannose kann in Betracht gezogen werden (Evidenzgrad Ib, Empfehlungsgrad C; [REF]). Die Leitlinie der „European Association of Urology” (EAU) begrenzt den Einsatz von D‑Mannose derzeit auf klinische Studien [REF]. Eine Alternative zu den oben genannten Maßnahmen könnten auch Probiotika wie z. B. Lactobacilli darstellen [REF]. Probiotika sind Mikroorganismen, die in therapeutischer Dosierung in verschiedenen Applikationsformen verabreicht werden können. Hierdurch soll die natürliche Barrierefunktion der Vaginalschleimhaut aufrechterhalten oder wiederaufgebaut werden, um die Migration pathogener Erreger zu verhindern [REF]. Ein protektiver Effekt von Lactobacilli auf die Entwicklung von symptomatischen HWI konnte bisher jedoch nicht eindeutig nachgewiesen werden [REF]. Gegenwärtig gibt es keine Evidenz zum Einsatz von Probiotika in dieser Indikation [REF]. Bei hohem Leidensdruck der Patientin sollte erst nach Versagen der nicht antimikrobiellen Prävention eine kontinuierliche Langzeitantibiotikaprophylaxe über 3–6 Monate (Evidenzgrad IV, Empfehlungsgrad B) oder ggf. eine postkoitale Prophylaxe mit Verabreichung einer Einzeldosis eines Antibiotikums erfolgen (Evidenzlevel Ib, Empfehlungsgrad B). Nach der EAU-Leitlinie sind sowohl eine kontinuierliche niedrig dosierte als auch eine postkoitale antimikrobielle Prophylaxe geeignet, die Rate rHWI zu reduzieren (Evidenzgrad 1b, starke Empfehlung; [REF]). Weitere Forschungsvorhaben sind wünschenswert, um die Rolle der nicht-antibiotischen Therapie und Prävention von rHWI zu klären.Ziel dieser Übersichtsarbeit ist es, aktuell verfügbare Evidenz über die Bedeutung der Blasenmikrobiome gesunder Frauen und solcher mit rHWI systematisch zu präsentieren. Außerdem werden relevante Studien zur Wirksamkeit von Probiotika bei rHWI strukturiert dargestellt. Hierdurch soll der gegenwärtige Stand der Forschung und ein Ausblick auf eine Therapie abseits der üblichen antimikrobiellen Behandlungsoptionen gegeben werden.
Wie sieht die Zukunft im Gesundheitsbereich aus? Soviel lässt sich an dieser Stelle schon sagen: Sie wird auf jeden Fall sehr viel digitaler. Mit der Digitalisierung werden viele Chancen für eine verbesserte, effizientere Versorgung, vor allem auch in infrastrukturschwachen Regionen, sowie für die Entwicklung neuer Therapieoptionen verbunden. Die sinnvolle Verknüpfung von Daten ermöglicht die Verbesserung präventiver, diagnostischer und medizinisch-therapeutischer Maßnahmen. Immer mehr digitale Angebote, die z. B. therapiebegleitend Daten erfassen, sammeln, zusammenführen und auswerten, durchdringen daher den Gesundheitsmarkt und erfahren in Form von Apps, Wearables, Plattformtechnologien oder telemedizinischen Ansätzen zunehmenden Zuspruch [REF]. Die nationale E‑Health-Strategie der Bundesregierung sowie geänderte gesetzliche Rahmenbedingungen haben in den letzten Jahren den Weg für eine stärkere Digitalisierung im Gesundheitsbereich geebnet, um Weichen für eine bessere Gesundheitsversorgung zu stellen: So haben, um einige aktuelle Beispiele zu nennen, durch das Digitale-Versorgung-Gesetz (DVG) digitale Gesundheitsanwendungen (DiGA), den Einzug in die Regelversorgung der gesetzlichen Krankenversicherung (GKV) gefunden und mit dem Terminservice- und Versorgungsgesetz sind gesetzliche Krankenkassen dazu verpflichtet, ihren Versicherten die elektronische Patientenakte (ePA) anzubieten. Das elektronische Rezept wird mit dem Gesetz für mehr Sicherheit in der Arzneimittelversorgung (GSAV) sukzessive eingeführt. Auf Basis des Patientendaten-Schutz-Gesetzes (PDSG) werden mit der Einführung der Terminologie SNOMED CT in Deutschland und somit durch eine Stärkung der semantischen Interoperabilität wichtige Voraussetzungen für eine einheitliche und sinnvolle Nutzbarmachung der – exponentiell zunehmenden – Gesundheitsdaten, vor allem auch aus dem „realen Versorgungsalltag“ (Real World Data) in Deutschland und Europa geschaffen.Mit Projekten wie der Medizininformatikinitiative [REF], „GAIA X“ [REF] oder dem „Data Analytics and Real World Interrogation Network (DARWIN EU)“ [REF] werden wichtige Dateninfrastrukturen als Wegbereiter für den geplanten Europäischen Gesundheitsdatenraum (EHDS; [REF]) geschaffen. Damit wird gleichzeitig eine Brücke zwischen Forschungsfragen und praktischer, alltäglicher medizinischer Gesundheitsversorgung gebaut. Standardisierte medizinische Inhalte und Daten sind wichtige Grundlagen für diese Bereiche sowie für regulatorische Fragestellungen zu Nutzen und Risiken der auf dem Markt befindlichen Arzneimittel und Medizinprodukte gleichermaßen und stellen eine wichtige Voraussetzung für die interoperable Vernetzung einer digitalen Infrastruktur im Zusammenspiel aus DiGA, ePA, Telematikinfrastruktur (TI) etc. dar.Neben dem wichtigen Aspekt der Standardisierung stellt auch der Ausbau der bisherigen Datentransparenzstelle durch das DVG und die Änderung der Datentransparenzverordnung zu einem Forschungsdatenzentrum beim Bundesinstitut für Arzneimittel und Medizinprodukte (BfArM) eine wichtige Grundlage für eine zeitnahe und zielführende Bereitstellung und Analyse der Gesundheitsdaten von ca. 70 Mio. gesetzlich Versicherten dar.Unter Nutzung von Methoden der künstlichen Intelligenz (KI) erschließen sich ganz neue Möglichkeiten für diagnostische und therapeutische Fragestellungen sowie für eine schnellere und effizientere Risikosignalerkennung und -bewertung.Das BfArM nimmt also im Zusammenwachsen und Zusammenspiel der einzelnen digitalen Komponenten (Abb. 1) einer modernen Gesundheitsversorgung schon jetzt Aufgaben an zentralen Schnittstellen für die (digitalgestützte) Patientenversorgung der Zukunft wahr und stellt sich dabei innovativen Ansätzen – unter Wahrung der für die Nutzer wichtigen Aspekte wie Datenschutz und Informationssicherheit, die sich u. a. aus der Datenschutz-Grundverordnung oder dem DVG ergeben. Gleichzeitig stellen die zunehmend zu beobachtende Vielfalt, Komplexität und Dynamik digitaler Produkte, immer kürzere Entwicklungszyklen, das stärkere Zusammenwachsen von Arzneimitteln und Medizinprodukten im Versorgungsalltag, dezentralisierte Studienansätze und die zunehmenden Datensätze nicht nur Chancen für eine patientenzentrierte, bessere und zunehmend diesbezüglich integrierte Gesundheitsversorgung dar, sondern bringen auch neue Herausforderungen und Risiken – wie zu Datenqualität, Datenschutz, Cybersicherheit, Interoperabilität, ethischen Fragestellungen u. a. – mit sich.Mit seinen zentralen Aufgaben, aber auch mit einer engen Zusammenarbeit auf nationaler und europäischer Ebene setzt sich das BfArM proaktiv dafür ein, die Chancen einer digitalisierten Gesundheitsversorgung in patientenrelevante Möglichkeiten mit Mehrwert für die Versorgung der Patientinnen und Patienten zu verwandeln. Dazu gehört auch, die damit verbundenen Herausforderungen gezielt anzugehen, mit den dynamischen Entwicklungen auf einer Höhe zu sein und bestehende Prozesse und Verfahren, wie zum Beispiel den DiGA-Fast-Track oder die Risikobewertung bei Arzneimitteln und Medizinprodukten, kontinuierlich weiterzuentwickeln, digitale Innovationen auf ihrem Weg zu wirksamen und sicheren Produkten durch Beratungs- und Informationsangebote zu begleiten und zu fördern oder beispielsweise Rahmenbedingungen für die Nutzung großer Datensätze (Big Data) bzw. Real World Data zu schaffen.Der vorliegende Beitrag gibt einen Überblick über die Aufgaben des BfArM im Zusammenhang mit der Digitalisierung des Gesundheitswesens und beleuchtet die jeweiligen Hintergründe.Die wissenschaftliche Bewertung von Daten bildet die Grundlage für die Nutzen-Risiko-Entscheidungen der Zulassungs- und Risikobewertungsbehörden. Diese Daten stammten im Rahmen von z. B. Zulassungsanträgen bei Arzneimitteln bisher überwiegend aus randomisierten kontrollierten klinischen Studien. Aufgrund ihrer hohen internen Validität gelten diese auch immer noch als Goldstandard, die Ergebnisse lassen sich im Versorgungsalltag aber aufgrund der niedrigeren externen Validität nicht immer so bestätigen. Die Weiterentwicklung und Nutzung zusätzlicher Datenquellen, z. B. aus digitalen Anwendungen im Versorgungsalltag, aus Berichten von Ärzten und Anwendern zu unerwünschten Arzneimittelwirkungen und Vorkommnismeldungen bei Medizinprodukten oder auch von Registerdaten, liefern hier wichtige komplementäre Informationen. Auch wenn diese Real-World-Evidenzdaten dadurch zunehmend vielfältiger und heterogener werden, ermöglichen solche Ansätze eine bessere Berücksichtigung der Versorgungsrealität. Daten aus Registern, DiGA, Wearables etc. können gerade bei seltenen Erkrankungen oder bei Fragen zum kontinuierlichen Therapie‑/Krankheitsverlauf wichtige zusätzliche Informationen liefern.Um diese Daten sinnvoll und strukturiert zu nutzen und in die Evaluation von Nutzen und Risiken vor der Zulassung von Arzneimitteln, aber auch im gesamten Lebenszyklus im Rahmen der Vigilanzaufgaben zu integrieren, setzt sich das BfArM mit wichtigen Fragen in diesem Kontext auseinander: Welche Datenquellen haben einen Mehrwert und (wie) können andere (zum Teil neue) Datenformen bei der Nutzen-Risiko-Bewertung einbezogen werden? Welcher Auswertungsstrategien bzw. -methoden bedarf es, um diesen umfangreichen Datensatz, der über die Daten aus den „klassischen Studien“ hinausgeht, zu bewerten und welche Anforderungen stellt das an unsere Infrastruktur und die erforderliche Expertise? Welcher Standards – semantischer und technischer Interoperabilität – bedarf es, damit alle im Kontext dieser Daten „die gleiche Sprache“ sprechen?So beforscht das BfArM zum Beispiel innovative Ansätze zur datengestützten Risikosignalerkennung und -bewertung bei Medizinprodukten und überführt diese in die regulatorische Praxis. Im Rahmen seiner Risikobewertung zu Vorkommnismeldungen steht das BfArM dabei vor der Herausforderung, schnell und zuverlässig übergreifende Ursachenzusammenhänge und Risikomuster bei zunehmend komplexen Medizinprodukten zu erkennen und zu bewerten sowie ggf. erforderliche Maßnahmen zur Risikominimierung abzuleiten. Hinzu kommt die praktische Herausforderung, dass die Anzahl der gemeldeten Vorkommnisse seit Jahren exponentiell ansteigt und inzwischen bei über 20.000 Meldungen pro Jahr liegt, die entsprechend bewertet werden müssen. Das BfArM entwickelt daher in seiner Forschungsgruppe zur Medizinproduktesicherheit wissenschaftliche Ansätze und praktische Analysetools z. B. auf Basis von KI. Diese können die Assessorinnen und Assessoren in der regulatorischen Praxis bei der schnellen Bewertung eingehender Meldungen unterstützen, indem sie z. B. wichtige Hinweise zu vergleichbaren Fällen geben oder einen erweiterten, auch produkt- und herstellerübergreifenden Überblick zu Risikomustern und Ursachenverläufen ermöglichen. Diese Ansätze werden in Zukunft weiter ausgebaut und z. B. hinsichtlich neuer, stärker trend- und signalbezogener Risikobewertungsverfahren ergänzt.Daten stammen aber nicht nur aus nationalen Quellen, sondern werden heutzutage global generiert; das BfArM bringt daher seine Expertise in die europäischen Gremien und Arbeitsgruppen ein, wie zum Beispiel in die Big Data Steering Group, um gemeinsam Lösungsansätze zu erarbeiten, Standards zu definieren und so das Potenzial der großen Datenmengen zu erschließen.Ansätze künstlicher Intelligenz (KI) entwickeln sich mit hoher Geschwindigkeit und Dynamik in vielen Lebensbereichen und werden z. B. auch zunehmend zum Bestandteil von Medizinprodukten. Neben vielen Chancen ergeben sich daraus auch neue regulatorische Herausforderungen z. B. zur Bewertung von Risiken „lernender Produkte“, die sich während der klinischen Anwendung ohne direkte Einflussnahme eines Menschen fortentwickeln. Dies betrifft z. B. Fragen der Prüf- und Validierungsmöglichkeiten (initial sowie im Anwendungsverlauf), Fragen der Verantwortlichkeit des Herstellers und der Anwender (die durch Anwendung den Lernprozess des Systems beeinflussen) und schließlich Cybersicherheitsrisiken, die sich z. B. aus Angriffen auf die Datenintegrität der Systeme ergeben können.Das BfArM beschäftigt sich neben der Forschung zur eigenen Anwendung von KI im Rahmen seiner Risikobewertung auch intensiv mit Fragen zur Bewertung entsprechender Ansätze als Bestandteil von Medizinprodukten. Es beteiligt sich dazu aktiv an Forschungsprojekten, wie z. B. dem aktuellen Verbundprojekt „KIMEDS: KI-assistierte Zertifizierung medizinischer Software“ in der Ausschreibung des Bundesministeriums für Bildung und Forschung (BMBF) „Medizintechnische Lösungen für eine digitale Gesundheitsversorgung“, sowie an entsprechenden Arbeitsgruppen und europäischen wie internationalen Initiativen, wie z. B. der ITU/WHO1-Fokusgruppe AI4Health [REF].Auch im Arzneimittelbereich kommt im BfArM bereits KI zum Einsatz: So entwickelt das BfArM ein Modell des maschinellen Lernens, welches Meldungen zu Nebenwirkungen automatisch bewerten sowie kausale Zusammenhänge zwischen Arzneimittel und Nebenwirkung erkennen und klassifizieren soll. Und in pharmakologischen/genetischen Studien werden umfangreiche Datensätze (mit Einwilligung der Probanden) anschließend analysiert und statistisch ausgewertet, indem diese nach definierten Mustern durchsucht bzw. auf Assoziationen zwischen interessierenden Merkmalsausprägungen getestet werden.Das BfArM arbeitet kontinuierlich an Möglichkeiten zur Verbesserung und Erweiterung seiner Risikobewertung bei Arzneimitteln und Medizinprodukten. So untersucht es z. B. derzeit im Rahmen eines durch das Bundesministerium für Gesundheit (BMG) geförderten Forschungsprojektes mögliche Hürden und Lösungsansätze zur Unterstützung des Risikomeldens bei der Anwendung von Medizinprodukten, da diese Meldungen die zentrale Säule der schnellen und zuverlässigen Risikosignalerkennung und -bewertung bilden. Darüber hinaus beteiligt sich das BfArM z. B. aktiv an dem aktuellen Forschungsprojekt „GIGA FOR HEALTH“ im Rahmen des Förderrahmens „5G.NRW“, in dem gemeinsam mit der Universitätsklinik Düsseldorf und vielen weiteren Partnern aus Forschung und Industrie neue Möglichkeiten zur Nutzung des 5G-Standards für die Gesundheitsversorgung erforscht werden. Das BfArM wird in diesem Zusammenhang insbesondere eine App zur schnellen, mobilen Erfassung und Meldung von Risiken bei Medizinprodukten entwickeln und gemeinsam mit den klinischen Partnern in der Praxis evaluieren. Durch die Nutzung des 5G-Standards können entsprechende Risikomeldungen einschließlich ggf. umfangreicher multimedialer Zusatzinformationen, wie z. B. Fotos und Filme, schneller und sicherer übertragen werden.Auf Basis des DVG und
Der Öffentliche Gesundheitsdienst (ÖGD) in Deutschland wurde durch die SARS-CoV-2-Pandemie bezüglich seiner grundlegenden Aufgaben an seine Leistungsgrenze gebracht. Dies betrifft die Infektionskettenaufklärung, Infektionsquellenidentifizierung sowie die Infektionsprävention, hier speziell u. a. die Indexfallermittlung, die Kontaktpersonennachverfolgung und das Quarantänemanagement von Infizierten und Verdachtsfällen. In der Folge kam es zu Kapazitätsengpässen bei vielen Gesundheitsämtern [REF]. Durch die Pandemie wurde offenbar, dass die etablierten Verwaltungsstrukturen der Gesundheitsämter, aber auch der anderen Einrichtungen des ÖGD in der Regel nicht vorbereitet sind, die Herausforderungen einer solchen infektiologischen Gefahrengroßlage bzw. dauerhaften Krisensituation unverzüglich, fehlerfrei, durchhaltefähig und in einer einheitlichen, kooperativen Führung zu bewältigen. Es zeigte sich eine sehr unterschiedliche Effektivität der Ämter gerade in der Kontaktnachverfolgung, die jeweils vom Ausmaß des lokalen Infektionsgeschehens, der personellen und digitalen Ausstattung sowie des bisher etablierten Ausbruchsmanagements abhing.Viele Gesundheitsämter mussten verstärkt eigenverantwortlich handeln und passten ihre Strategien an. Primär wurde der Fokus auf eine effizientere Kontaktnachverfolgung gelegt, um eine Eindämmung der Infektionen zu erreichen. Nicht immer gelang es aber, die infektionsverursachenden Kontakte der Indexfälle zu ermitteln (Quellenidentifikation). Besonders problematisch war dabei der sehr unterschiedliche Grad der Digitalisierung der Meldesysteme in den Gesundheitsbehörden, der sich auf die Effizienz der Abläufe stark auswirkte. Ergänzende neue Herausforderungen der COVID-19-Pandemie waren und sind u. a. der zeitweise Mangel an diagnostischen Nachweissystemen, intensivmedizinischen und sonstigen Behandlungskapazitäten, Schutzausstattungen und Präventionsmöglichkeiten, wie Impfungen. In vielen Fällen wurde auf Verfahren und Protokolle aus dem Bereich der Katastrophenbekämpfung zurückgegriffen, da viele Elemente von dort auch für ein effektives und wirksames Infektionsausbruchsmanagement notwendig sind.Je nach Altersgruppe, Grunderkrankungen und anderen Co-Faktoren treten asymptomatische bzw. mild bis moderate COVID-19-Infektionsverläufe unterschiedlich häufig auf. Es kommt zu einer hohen Anzahl an nicht erkannten Infektionen (sog. Dunkelziffer), die aber für den Verlauf des pandemischen Infektionsgeschehens von entscheidender Bedeutung ist. Neben dem Versuch, diese Fälle durch eine Strategie mit flächendeckenden, z. T. anlasslosen, jederzeit verfügbaren Testangeboten zu erkennen, hat sich eine andere Methodik als sinnvoll und nützlich herausgestellt, die für die betroffenen, infizierten Personen weniger invasiv ist – das infektiologische Abwassermonitoring als Teilgebiet der Abwasserepidemiologie.Die Abwasserepidemiologie (engl.: „wastewater-based epidemiology“, WBE) bekommt in den Umweltwissenschaften zunehmend Zuspruch als eine diagnostische Methode, um den Konsum von Drogen und Medikamenten für gesamte Siedlungsgebiete abzuschätzen [REF]. Die Methodik eignet sich aber auch dazu, die Verbreitung von Infektionserregern und deren genotypische Eigenschaften (z. B. Resistenzgene, sonstige Mutationen) zu erfassen. So scheiden Infizierte SARS-CoV‑2 nicht nur über den Speichel, sondern auch über den Magen-Darm-Trakt mit dem Stuhl aus. Im Urin ist das Virus in der Regel nicht nachweisbar ([REF]; Tab. 1). Die Ergebnisse des Monitorings wurden gemeinsam mit den lokal diagnostizierten Infektionen und digitalen Geoinformationen zu kritischen Infrastrukturen (KRITIS) des Landkreises, wie zum Beispiel Abwasserentwässerungssysteme, Pflegeheime, Kitas und Schulen, in einem Dashboard zur Lageführung fusioniert. Die Visualisierung von Informationen mittels solcher, häufig durch Geoinformationssysteme (GIS) gestützter Dashboards hat sich seit Beginn der COVID-19-Krise zunehmend als weitverbreitetes Hilfsmittel etabliert [REF]. Ziel war es, den Entscheidern des Krisenstabs vor Ort eine objektive Grundlage zur Beurteilung der weiteren Maßnahmen in der Pandemiebekämpfung im Landkreis zur Verfügung zu stellen. Gemäß der Empfehlung der EU-Kommission, das Abwassermonitoring flächendeckend einzusetzen [REF], dient der Landkreis Berchtesgadener Land als bundesweites Musterbeispiel für eine effiziente Integration von Innovationen in das Krisenmanagement der Ämter.Die vorliegende Arbeit dokumentiert zunächst die Einbindung der Abwasserbefunde für die Lagebeurteilung sowie die angewandte Messmethode und berichtet dann über die Anwendung des Abwassermonitorings im Landkreis Berchtesgadener Land und ausgewählte Ergebnisse.
Die GeSiD-Studie „Gesundheit und Sexualität in Deutschland“ ist die erste bundesweite repräsentative Untersuchung mit durch Peer-Review-Verfahren unabhängig beurteilten, national und international publizierten Ergebnissen [REF]. Anliegen dieses kurzen Beitrags ist es, eine Einführung zur Studie und zu ihren verschiedenen Untersuchungsbereichen zu geben. Nach Schilderung der Ausgangslage wird ein Einblick in das methodische Vorgehen gegeben, um anschließend auf einige Ergebnisse einzugehen und einen Ausblick für die Zukunft zu geben.Anfang der 2010er-Jahre existierten für die meisten europäischen sowie für viele andere Länder der westlichen Welt Survey-Untersuchungen zur sexuellen Gesundheit der erwachsenen Bevölkerung [REF]. Für die Frage, warum es für Deutschland bisher keine wiederkehrenden Untersuchungen gab, wie beispielsweise in den USA oder im Vereinigten Königreich, lassen sich nur unsichere Hypothesen bilden. Im Deutschland der Nachkriegszeit wurde das erste Institut für Sexualforschung 1959 an der Medizinischen Fakultät der Universität Hamburg gegründet. Der erste Institutsleiter, Hans Giese, hatte bereits in seiner Frankfurter Zeit Fragebogenuntersuchungen mit homosexuellen Männern durchgeführt. Gegenüber den die Sexualwissenschaft grundlegend verändernden ersten großen Sex-Surveys des US-amerikanischen Zoologen Alfred F. Kinsey [REF] zeigte sich Giese zwar einerseits skeptisch, andererseits war er aber wohl auch interessiert an der Durchführung empirischer Untersuchungen zur Sexualität in der deutschsprachigen Bevölkerung. Ab Mitte der 1960er-Jahre führte sein Mitarbeiter Gunter Schmidt eine Untersuchung zur Sexualität Studierender in Deutschland durch [REF]. In der Folge wurde diese Studie in 15-jährigem Abstand mit Förderung durch die Deutsche Forschungsgemeinschaft bisher noch dreimal bis ins Jahr 2012 wiederholt. Zuletzt wurde die 4. Welle der Hamburger Studenten-Sexstudien von Arne Dekker und Silja Matthiesen verantwortet [REF].Dass Deutschland sich ab Ende der 1980er-Jahre im Zuge des Aufkommens von HIV/Aids weniger stark als andere Länder auf die Rolle des Sexualverhaltens bei der Verbreitung der Krankheit konzentrierte und sich kein wiederkehrender Sex-Survey etablierte, könnte unterschiedliche Gründe haben. Wir wissen, dass es einige Skepsis bei den Sexualforschenden selbst gab [REF]. Im Jahr 1983 war die damals geplante Volkszählung mit der Anerkennung des informationellen Selbstbestimmungsrechts als vom Grundgesetz geschütztes Gut zunächst gekippt und modifiziert auf 1987 verschoben worden. Dieser Vorgang hatte erheblichen Einfluss auf spätere Entwicklungen des Datenschutzes. Es ist naheliegend, dass in diesem gesellschaftlichen Klima der Versuch, Daten zur sexuellen Gesundheit und zum sexuellen Verhalten zu erheben, besonders kritisch gesehen wurde. Hinzu kommt, dass eine solche Studie organisatorisch für sexualwissenschaftliche Einrichtungen jener Zeit eine Herausforderung gewesen wäre – das Institut für Sexualwissenschaft in Frankfurt und die Abteilung für Sexualforschung in Hamburg waren damals relativ klein.Schließlich erfolgte durch die Bundeszentrale für gesundheitliche Aufklärung (BZgA) ab 1987 zunächst jährlich eine Untersuchung zur Entwicklung von Wissen, Einstellungen und Verhaltensweisen sowie zu Veränderungen des Informations- und Kommunikationsverhaltens im Zusammenhang mit HIV und Aids in der Allgemeinbevölkerung (die Studie „AIDS im öffentlichen Bewusstsein“; [REF]).Die zunehmende Bedeutung des Themenfeldes sexuelle Gesundheit in der Weltgesundheitsorganisation [REF] bahnte ab 2013 den Weg für einen Sex-Survey in Deutschland. Zunächst hatte die BZgA eine Expertise zu Sex-Surveys in Europa in Auftrag gegeben [REF]. Anschließend erfolgte eine Erstellung von Indikatoren und eines ersten Fragebogenentwurfs. In der Folge wurde eine Pilotstudie zur Machbarkeit und zum Vergleich verschiedener Erhebungsmethoden an einer Stichprobe von 1155 Frauen und Männern in Zusammenarbeit mit dem Sozialforschungsinstitut Kantar Emnid durchgeführt [REF]. Diese Pilotstudie zeigte erstens, dass es möglich ist, in Deutschland detaillierte und intime Fragen zur Sexualität zu stellen – es gab keine ernsthaften Beschwerden oder Kritik von Adressat:innen. Außerdem zeigte sich, dass eine Interviewstudie hinsichtlich verschiedener Parameter wie Datenqualität und Anteil der Nichtteilnehmenden einer postalischen Versendung von Fragebögen überlegen ist. Schließlich erbrachte die Vorstudie erste interessante Ergebnisse zum Beispiel zum Thema sexuelle Orientierungen oder sexuelle Praktiken [REF]. Der Durchführung der Interviews im Rahmen der GeSiD-Studie in den Jahren 2018 und 2019 stand damit nichts mehr im Wege.
Mit sexuellen Gesundheitsinformationen sind Informationen gemeint, welche die sexuelle und reproduktive Gesundheit betreffen. Dazu gehören beispielsweise Informationen über die Prävention von und die Intervention bei sexuell übertragbaren Infektionen (STI), sexuellen Übergriffen, ungeplanten Schwangerschaften und sexuellen oder reproduktiven Störungen, aber auch Informationen über sexuelle Identitäten, sexuelle Techniken und Lebensstile sowie sexuelles Vergnügen und Wohlbefinden [REF].Entsprechende Informationen werden im Rahmen formaler sowie informeller Sexualaufklärung vermittelt, etwa durch pädagogische und medizinische Fachkräfte sowie durch Eltern, Peers und Medienpersonen. Entscheidend für das gängige Verständnis von sexuellen und reproduktiven Gesundheitsinformationen ist die Intention der Wissensvermittlung. Daher werden Materialien mit der Hauptintention von Entertainment, Erregung oder Marketing (z. B. erotische Geschichten, Pornovideos, Werbung für Penisverlängerungen) nicht zu den Informationsmaterialien gezählt. Sexuelle Gesundheitsinformationen basieren teils auf Faktenwissen (z. B. wie sicher sind verschiedene Verhütungsmethoden gemäß dem Bewertungsmaßstab Pearl-Index), teils aber auch auf Erfahrungswissen (z. B. wie fühlt man sich als bisexuelles Mädchen und wie kann man sich in der Familie am besten outen). Der Zugang zu umfassenden und evidenzbasierten sexuellen Gesundheitsinformationen wird in Forschung und Praxis als Voraussetzung für sexuelle Gesundheit und als sexuelles Menschenrecht betrachtet [REF].In den letzten Jahren ist die Bedeutung digitaler Medien für die Verbreitung sexueller Gesundheitsinformationen stark gestiegen. Denn online kann man jederzeit diskret und schamfrei nach sexuellen Informationen suchen, was sowohl jüngere als auch ältere Menschen weltweit immer häufiger tun [REF]. Gleichzeitig wächst das Angebot an Online-Sexualaufklärung beständig [REF]. Denn sowohl professionelle Sexualaufklärung als auch sexualbezogene Peer Education werden zunehmend über digitale Medien bereitgestellt, etwa über Websites, Apps, Bots oder Games und natürlich über Social-Media-Plattformen [REF]. Mit sozialen Medien sind Online-Plattformen gemeint, auf denen die Nutzenden sich in eigenen Profilen darstellen, mit anderen vernetzen, Inhalte rezipieren, bewerten und kommentieren sowie eigene Inhalte erstellen und veröffentlichen können [REF]. Social-Media-Plattformen erfreuen sich gerade bei den jüngeren Generationen großer Beliebtheit und ermöglichen eine niedrigschwellige Teilnahme an der öffentlichen Online-Kommunikation, das schließt den Austausch sexueller Gesundheitsinformationen ein [REF].So veröffentlichen auf der weltweit meistgenutzten Social-Media-Plattform YouTube sowohl Gesundheitsprofis als auch Gesundheitslaien in großer Menge und Vielfalt sexuelle Gesundheitsinformationen. Millionenfach angeschaut und tausendfach kommentiert werden beispielsweise deutschsprachige YouTube-Videos mit Titeln wie: „Daran merkst du, dass du asexuell bist“ (0,8 Mio. Abrufe), „Warum ich die Pille nicht mehr nehme“ (1,5 Mio. Abrufe), „Wie ist das vergewaltigt zu werden“ (2,7 Mio. Abrufe) oder „10 Fakten über Selbstbefriedigung“ (1,8 Mio. Abrufe). Die Bedeutung von Internet und sozialen Medien für die sexuelle Informationsversorgung und damit auch für die sexuelle Gesundheit der Bevölkerung wird weithin anerkannt. Die Bewertung dieser Situation ist jedoch in Forschung und Praxis ambivalent: Denn der Chance auf verbesserte sexuelle Informationsversorgung, und damit auch auf verbesserte sexuelle Gesundheit, steht das Risiko gegenüber, dass Menschen online unkontrolliert auf verzerrte, lücken- und fehlerhafte Informationen (engl. „misinformation“) sowie auch auf gezielte Falschinformationen (engl. „disinformation“) stoßen und sich ihre sexuelle Gesundheit dadurch verschlechtert [REF].Für eine datenbasierte Einschätzung der sexuellen Gesundheit in Deutschland ist es im Digitalzeitalter somit von großer Bedeutung, nicht nur die selbst berichteten sexualbezogenen Verhaltensweisen, Wissensbestände und Einstellungen der Bevölkerung in Surveys zu ermitteln, sondern auch die sexualbezogenen Online-Informationsangebote und Online-Diskurse systematisch durch Medieninhalts- und Medienqualitätsanalysen zu erfassen und einem regelmäßigen Monitoring zu unterziehen. Dies ist bislang nicht der Fall. Zumindest liegen kaum Studien vor, die untersuchen, welche deutschsprachigen Informationen über sexuelle und reproduktive Gesundheit im Internet öffentlich bereitstehen und welche Qualität diese Angebote der formalen und informellen Online-Sexualaufklärung haben [REF]. Der vorliegende Beitrag ruft dazu auf, diese Forschungslücke zu schließen. Dafür wird der internationale Forschungsstand zu Inhalten und Qualität von sexuellen Gesundheitsinformationen in sozialen Medien erstmals im Rahmen eines Scoping Reviews systematisch aufgearbeitet.
Sauerstoff (O2) wird in den Körperzellen benötigt, um aus den Nährstoffen Energie zu gewinnen. Im Blut wird O2 überwiegend an Hämoglobin (Hb) des Erythrozyten gebunden. Die Menge von O2 im Blut kann als Messung der Sauerstoffsättigung des Hämoglobins oder durch Messung des O2-Partialdrucks (paO2) ausgedrückt werden.Die Sauerstoffsättigung als zentraler Zielparameter hat den Vorteil eines gemeinsamen Zielparameters in Pulsoxymetrie (SpO2) und Blutgasanalysen (SaO2). In einer großen britischen Studie an 37.000 Patienten lag die pulsoxymetrisch gemessene O2-Sättigung (SpO2) im Median bei 98 % für Erwachsene im Alter von 18 bis 64 Jahre, für Ältere bei 96 % [REF].Die Pulsoxymetrie hat eine hohe Sensitivität, aber nur eine geringe Spezifität zur Erfassung von Hypoxämien. Bei kritisch kranken Patienten liegt das 95 %-Konfidenzintervall der Abweichung von pulsoxymetrischer zu arterieller Sättigung bei ±4 % [REF].Bei einer Hypoxämie ist der Sauerstoffpartialdruck oder der Sauerstoffgehalt im arteriellen Blut erniedrigt. Eine Hypoxie bezeichnet dagegen die Unterversorgung von Organen und Gewebe mit Sauerstoff.Eine hypoxämische Hypoxie liegt vor, wenn der Sauerstoffpartialdruck im Blut vermindert ist.Es gibt derzeit keine genaue wissenschaftliche Evidenz, wann und wie viel Sauerstoff in der Behandlung der Hypoxämie notwendig ist. An großen Kollektiven von Krankenhauspatienten und Patienten im Rettungseinsatz wurde die Assoziation einer Hypoxämie mit erhöhter Sterblichkeit beschrieben [REF].Permissive Hypoxämie setzt ausreichende Hämoglobinwerte (üblicherweise >10 g/dl) und einen supranormalen Herzindex (größer 4,5 l/min/m2) voraus, um eine adäquate Sauerstoffversorgung (DO2) aufrechtzuerhalten. Permissive Hypoxämie wurde bisher nicht in randomisierten Studien bei Erwachsenen gegenüber Normoxämie untersucht.Es gibt Patienten mit chronischer Hypoxämie (z. B. der Fetus, Patienten mit Mischzyanose, Bevölkerungsgruppen, die in großer Höhe leben, oder solche mit chronischer Hypoventilation), die trotz Hypoxämie nicht akut gefährdet sind. Aus historischen Veröffentlichungen der Höhenmedizin und Flugmedizin [REF] ist bekannt, dass Sättigungswerte von unter 70 % innerhalb kurzer Zeit zu Bewusstseinsverlust führen. Selbst gesunde Probanden haben bei Hypoxämien unter 80 % kognitive Einschränkungen [REF].Es ist unklar, welchen Einfluss die Sauerstofftherapie auf das Überleben und andere patientenrelevante Endpunkte hat.Es gibt zahlreiche Argumente gegen Hyperoxie und Hyperoxämie als Therapieziel: Bei Hyperoxämie unter O2 sind eine Reihe von Nebenwirkungen beschrieben [REF]. Eine Metaanalyse von 25 randomisierten, kontrollierten Studien an 16.037 Patienten mit verschiedenen akuten Erkrankungen wie Sepsis, Schlaganfall, Trauma, Herzinfarkt und Herzstillstand zeigte mit hoher Evidenz ein erhöhtes relatives Risiko der Sterblichkeit im Krankenhaus unter Hyperoxämie [REF]. Hohe O2-Konzentrationen verursachen bei Gesunden direkte Lungentoxizität und Resorptionsatelektasen [REF]. Hyperoxämie kann zu fälschlich beruhigenden SpO2-Werten führen und die Erkennung der Verschlechterung von Patienten mit Hypoxämie verzögern [REF]. Bei COPD-Patienten war eine prästationäre Hyperoxämie mit erhöhter Krankenhaussterblichkeit verbunden [REF]. In 21 Studien an 7597 Patienten verbesserte Hyperoxie intra- und postoperativ die Wundheilung nicht [REF].
Bevölkerungsweite Repräsentativerhebungen sind ein wichtiges Instrument empirischer Sozialforschung. Sie werden weltweit zur Bearbeitung von Fragestellungen in unterschiedlichen Gesellschafts- und Lebensbereichen genutzt, z. B. im Kontext von Arbeitsmarktsituationen (z. B. [REF]), Bildungschancen (z. B. [REF]) oder Religion (z. B. [REF]). Auch im Gesundheitsbereich sind epidemiologische Studien sowie Surveys zur Gesundheitsberichterstattung und zum Gesundheits- und Sexualverhalten eine wichtige Informationsquelle. Aktuelle Beispiele aus dem deutschsprachigen Raum sind die Gesundheitsberichterstattung des Robert Koch-Instituts (RKI) sowie repräsentative Surveys zu Themenschwerpunkten körperlicher und psychischer Gesundheit von Kindern, Jugendlichen und Erwachsenen [REF], Wiederholungsbefragungen der Bundeszentrale für gesundheitliche Aufklärung (BZgA) etwa zu den Themenkomplexen Rauchen, Alkoholkonsum und Drogenaffinität [REF], zum Infektionsschutz [REF], zur Organ- und Gewebespende [REF], zu HIV und anderen sexuell übertragbaren Infektionen [REF] sowie Erhebungen der Bundesanstalt für Arbeitsschutz und Arbeitsmedizin (BAuA) zu Zusammenhängen von Arbeit und mentaler Gesundheit (z. B. [REF]). Ergebnisse solcher Untersuchungen können Aufschluss geben zu Prävalenzen von Gesundheitsproblemen und Krankheitsbildern, Gesundheitsverhalten, gesundheitsassoziierten Risikofaktoren, Gesundheitschancen und gesundheitsbezogener Versorgung. Diese Aufschlüsse sind notwendig, um gezielte Maßnahmen für vorherrschende Gesundheitsprobleme angepasst an die spezifischen Zielgruppen gestalten und bereitstellen zu können. Es hat sich gezeigt, dass dieser zielgruppengerechte Ansatz (eng.: „tailored approach“) notwendig ist, um gewünschte positive Veränderungen, beispielsweise im Gesundheitsverhalten, herbeizuführen [REF]. Gesundheitsforschung ohne Berücksichtigung dieser Diversität bei der Erhebung, Auswertung und Interpretation von Gesundheitsdaten würde eine verzerrte Informationsgrundlage liefern. Auf dieser Basis wäre die Konzeption zielgruppengerechter und nachhaltiger Präventions- und Versorgungsangebote nicht nur deutlich erschwert, sondern auch der Erfolg dieser Angebote gefährdet, da diese nicht auf die Einzigartigkeit und jeweiligen Bedürfnisse der Zielgruppen abgestimmt wären [REF].Ein zentrales Differenzierungskriterium sowohl bei der Erhebung als auch bei der Auswertung gesundheitsbezogener Surveys ist das Geschlecht. Häufig gibt es auch geschlechtsspezifisch unterschiedliche Fragesets oder Fragebögen, eben weil Menschen aufgrund körperlicher Unterschiede unterschiedliche Krankheiten entwickeln [REF], unterschiedliche Symptomatiken für dieselben Krankheitsbilder berichten [REF] oder sich im gesundheitsbezogenen Verhalten deutlich unterscheiden. Regelmäßig wird das Merkmal Geschlecht dabei binär als weiblich/männlich oder Mann/Frau erhoben. Dies impliziert eine biologische Definition von Geschlecht und zielt auf eine Unterscheidung nach biologischen Geschlechtsmerkmalen ab. Dieses Geburtsgeschlecht oder juristische Geschlecht wurde bis 2018 amtlich in der Geburtsurkunde und im Personalausweis dokumentiert und spiegelt sich zumeist in einem geschlechtsspezifischen Vornamen wider. Durch eine Abfrage des Merkmals Geschlecht, die als Antwortmöglichkeiten ausschließlich „weiblich“ und „männlich“ vorsieht, reproduziert sich jedoch ein statisches Modell der Zweigeschlechtlichkeit, welches als veraltet oder zumindest als unvollständig kritisiert werden muss: Geschlecht ist nicht länger statisch nur als binär zu klassifizieren. Inter*/Intersexualität und Intergeschlechtlichkeit bezeichnen etwa Varianten der Geschlechtsentwicklung auf der Ebene des biologischen Körpers, während mit Trans*/Transsexualität und Transgender, aber etwa auch mit Genderfluidität (immer wieder ein anderes Geschlecht) und agender (ungeschlechtlich/geschlechtslos) auf der Ebene der Geschlechtsidentität Varianten präsent sind, die über ein statisches und binäres Geschlechtermodell hinausgehen. Diese Vielfalt und ihre politische sowie juristische, aber auch medizinische Anerkennung sind äußerst wichtig.Dass das Bewusstsein für die Geschlechtervielfalt in der westlichen Welt in den letzten Jahren gestiegen ist, zeigt sich u. a. daran, dass Länder wie Dänemark, Niederlande, Kanada oder Australien es ihren Bürger:innen seit einigen Jahren ermöglichen, neben den Kategorien „weiblich“ oder „männlich“ ein „X“ als Geschlechterkennzeichnung in ihrem Pass zu wählen. Auch in Deutschland gibt es seit dem 01.11.2013 mit der Änderung des Personenstandsgesetzes diese Option. Seit dem Jahr 2018, mit dem Gesetz zur Änderung der in das Geburtenregister einzutragenden Angaben, gibt es eine erweiterte Option, Geschlecht im eigenen Ausweis kenntlich zu machen, nämlich den Eintrag „männlich“, „weiblich“, „divers“ oder „kein Eintrag“. Spätestens seit dieser Novelle des Personenstandsgesetzes und der Einführung des dritten Geschlechtseintrags „divers“, muss sich auch die Survey-Forschung in Deutschland die Frage stellen, wie Geschlecht in Surveys am besten operationalisiert und wie es analysiert und repräsentiert werden kann.Zunächst lässt sich feststellen, dass die Problematik in der empirischen Forschung noch weitgehend ignoriert wird. So verdeutlichte ein Review über 106 veröffentlichte psychologische Studien aus den Jahren 2016–2018, dass in 76 % der durchgeführten Studien das Geschlecht der Teilnehmenden weiterhin als binäres Konstrukt (weiblich/männlich) erhoben wurde [REF]. Diese Praxis folgt zum einen nicht den ethischen Prinzipien der Wissenschaft, der Integrität und des Respekts [REF]; zum anderen kann für nichtbinäre Personen die binäre Geschlechtsfrage als Identitätsverleugnung empfunden werden [REF]. Die daraus resultierenden, wenn auch unbeabsichtigten Verletzungen und Diskriminierungen, zu denen auch „Misgendering“ (der falsche Gebrauch von Pronomen oder anderen geschlechtsspezifischen Wörtern) und „Trans-erasure“ (die Verneinung von transgeschlechtlichen Personen) gehören, sind gut dokumentiert [REF].Der Zusatz einer dritten Geschlechtskategorie („divers“) in Befragungen mag zunächst als ein einfacher und pragmatischer Ansatz erscheinen. Jedoch stößt der Zusatz einer dritten Kategorie „divers“ auf neue vielfältige Probleme. „Divers“ ist zwar als juristische Kategorie eingeführt, umfasst aber verschiedenste Varianten der Geschlechtsentwicklung und wird der Geschlechtervielfalt nicht gerecht. Handelt es sich bei nichtbinären Personen um inter*/intersexuelle Personen, die einen dritten, juristischen Geschlechtseintrag haben, oder um trans*/transsexuelle Personen, die sich vor, nach oder in der Transition befinden? Darüber hinaus würden Personen, die sich als genderqueer (weder ganz/immer weiblich, noch ganz/immer männlich), genderfluid, bigender (doppelgeschlechtlich), trigender (sich mit 3 Geschlechtern identifizierend) und agender identifizieren, in eine Art „Sammelbecken“ fallen, aus dem man keine Rückschlüsse darüber ziehen kann, ob und, wenn Ja, welche Unterschiede es zwischen den verschiedenen diversen Gruppen hinsichtlich spezifischer Fragestellungen gibt. Dies ist allerdings wichtig und notwendig, um zielgruppenspezifische Informationen und Maßnahmen entwickeln und bereitstellen zu können.Einige wenige Studien haben bereits angedeutet, dass es sich bei der Gruppe „divers“ keineswegs um eine homogene Gruppe handelt, sondern dass diese heterogen ist und Subgruppen sich in bestimmten Merkmalen deutlich voneinander unterscheiden, wie zum Beispiel in der subjektiven Lebensqualität [REF], dem Ausüben riskanter Verhaltensweisen [REF], in Gesundheitsproblemen und in ihrem Bedürfnis nach adäquater, diskriminierungsfreier medizinischer Gesundheitsversorgung [REF].Ohne weitere Antwortoption und wenn die befragte Person die Frage nicht überspringen kann, ist davon auszugehen, dass die genannte Personengruppe gezwungenermaßen eine der beiden binären Antwortoptionen auswählt, was zu methodologischen Problemen und zu geschlechtsspezifischen Fehlklassifizierungen führen kann [REF]. Auch ein Abbruch der Befragung ist als Konsequenz denkbar. Selbst wenn trans* Personen eine binäre Geschlechtsabfrage eindeutig beantworten können, würden diese sich aufgrund ihrer Angaben nicht als transgeschlechtlich identifizieren lassen. Aufgrund dieser Schwierigkeiten fordert die American Psychological Association explizit dazu auf, in der Wissenschaft: (a) Geschlecht als nichtbinäre Kategorie anzuerkennen und (b) die binäre Geschlechtserfassung zugunsten von genaueren und inklusiven Messungen zu verwerfen [REF].Es stellt sich die Frage, wie das Vorhaben einer inklusiven Erfassung von Geschlecht wissenschaftlich umgesetzt werden kann: Wie können empirische Studien oder spezifisch repräsentative Surveys so gestaltet werden, dass dem ethischen Aspekt der Geschlechtervielfalt methodisch Rechnung getragen wird? Welche Herausforderungen ergeben sich in diesem Zusammenhang?Am Beispiel der GeSiD-Studie zu „Gesundheit und Sexualität in Deutschland“, der ersten umfassenden Erhebung zur Erwachsenensexualität in Deutschland aus dem Jahr 2019, wollen wir aufzeigen, welche Facetten von Geschlecht in Surveys erhoben werden könnten, welche Herausforderungen sich bei der Auswertung und Interpretation der Daten stellen und welche Implikationen sich dadurch für die Entwicklung von Repräsentativerhebungen ergeben.
Tumor Budding beschreibt den Effekt von diskohäsivem Wachstum einzelner Tumorzellen bis zu Verbänden < 5 Tumorzellen im tumorassoziierten Stroma der Infiltrationszone verschiedener Karzinome (s. Infobox für die Definition eines Tumor Buds [REF]). Der Effekt wurde erstmal von Imai et al. [REF] in den 1950er-Jahren als „Aussprossung“ bezeichnet [REF]. Tumor Budding kann im Übergangsbereich von Normalgewebe zum Karzinom als Zeichen der Infiltration (peritumorales Budding) und innerhalb eines Karzinoms (intratumorales Budding) beobachtet werden.Im CRC konnte Tumor Budding als unabhängiger prognostischer Faktor durch zahlreiche Studien bestätigt werden [REF]. Es zeigte sich ebenfalls eine Korrelation mit höherem Tumorgrad, lymphovaskulärer Infiltration, Lymphknotenmetastasen und Fernmetastasen.Als Ursache für diesen migratorischen Effekt von Tumorzellen wird die epithelial-mesenchymale Transition (EMT) postuliert, bei welcher epitheliale Zellen eine geringere Zell-Zell-Adhäsion durch Verlust von E‑Cadherin zeigen und vermehrt Marker von mesenchymalen Zellen wie Vimentin, SMA („smooth muscle actin“) oder Fibronektin exprimieren [REF]. Es handelt sich hierbei um einen mehrstufigen Prozess, bei welchem die Zellen von epithelial zu mesenchymal transformieren und welcher durch die Aktivierung der Wnt/β-Catenin-Pathways bei Verlust von membranärer E‑Cadherin-Expression gekennzeichnet ist. In der aktuellen Forschung wird angenommen, dass Tumor Buds eine partielle epithelial-mesenchymale Transition (pEMT) darstellen, da etwa bei Tumor Buds des duktalen Adenokarzinoms des Pankreas keine Korrelation zwischen der membranären Reduktion von E‑Cadherin und der Expression von Vimentin besteht [REF].Die EMT kann außerhalb der Karzinogenese auch bei physiologischen Prozessen, wie z. B. der Embryogenese oder der Wundheilung, beobachtet werden. Die aberrante Expression von EMT-assoziierten Markern bei Karzinomen wird als Kennzeichen für ein erhöhtes Metastasierungsrisiko betrachtet [REF].Auch wenn Tumor Budding schon länger insbesondere beim CRC im Fokus zahlreicher Studien stand, war die Anwendung in der Praxis bis vor wenigen Jahren wegen der nicht standardisierten Auswertung limitiert. Im Rahmen der „International Tumor Budding Consensus Conference“ (ITBCC) wurde ein 2016 ein Konsens zur Auswertung des Tumor Budding mit einheitlichen Empfehlungen definiert, welche die Standardisierung der Auswertung und somit die Aufnahme in die Leitlinien ermöglichte (zusammengefasst in Tab. 1 und 2; [REF]). Seither wurde Tumor Budding als zusätzlicher Risikofaktor für die Progression des CRC in die Guidelines der UICC [REF], den „College of American Pathologists“- (CAP-)Richtlinien [REF], den „National Comprehensive Cancer Network“- (NCCN-)Richtlinien [REF], der deutschen S3-Leitlinie [REF] und bei den „European Society for Medical Oncology consensus“- (ESMO-)Richtlinien [REF] aufgenommen.
Während in der ICD-10 [REF] schädlicher Gebrauch und Abhängigkeit von Alkohol unterschieden werden, folgte das DSM‑5 ([REF], Übersicht in [REF]) einem dimensionalen Ansatz und definiert Alkoholkonsumstörungen anhand von 11 Kriterien, von denen mindestens 2 bis 3 für eine leichte Störung definiert sein müssen. Die 2022 in Kraft tretende ICD-11 wird an der Unterscheidung schädlicher Gebrauch und Abhängigkeit festhalten. Alkoholgebrauchsstörungen sind häufig. In vielen epidemiologischen Untersuchungen werden Prävalenzraten von 6 bis 7 % mitgeteilt [REF]. Etwa 2 bis 3 % der Erwachsenenbevölkerung sind alkoholabhängig. Zu den zahlreichen körperlichen und neurologischen Folgeschäden gehören Lebererkrankungen, eine erhöhte Krebsrate, ein signifikantes Unfall- und Suizidrisiko, aber auch das Risiko für Gewalttaten, zahlreiche psychiatrische Folgestörungen sowie soziale Probleme [REF]. Die Prognose ist bei hoher Mortalität/Morbidität in vielen Fällen immer noch ungünstig [REF]. Die verfügbaren Ansätze zur Therapie von Alkoholkonsumstörungen umfassen ein breites und umfangreiches Spektrum [REF]. Trotz erwiesener Effizienz etablierter Therapien sind Konsum- und Rückfallereignisse häufig.Die 2016 publizierte S3-Leitlinie zu Screening, Diagnose und Behandlung alkoholbezogener Störungen [REF] befindet sich derzeit in Revision. Als übergeordnete Therapieziele wurden in der S3-Leitlinie aus 2016 neben abstinenzorientierten Therapien auch Harm-reduction-Strategien (Verminderung der Trinkmenge) als mögliche Behandlungsziele definiert.Als wirksame psychosoziale und psychotherapeutische Behandlungsansätze in der Entwöhnungsbehandlung der Alkoholabhängigkeit sind Interventionskomponenten wie z. B. motivationale Interventionsformen, Verhaltenstherapie und kognitive Verhaltenstherapie wie Kontingenzmanagement, Angehörigenarbeit und Paartherapie mit hohem Empfehlungsgrad genannt. Mitunter wird bei insgesamt moderaten Effekten eine leichte Überlegenheit der kognitiven Verhaltenstherapien gegenüber anderen spezifischen Therapien vermutet, die jedoch in einer aktuellen Metaanalyse [REF] nicht bestätigt wurde. Darüber hinaus finden in der S3-Leitlinie die psychotherapeutische Kurzzeittherapie, kognitives Training sowie andere Psychotherapieformen Erwähnung.Als Medikamente sind Acamprosat, Naltrexon und Disulfiram empfohlen. Für Acamprosat und Naltrexon war bei einer sehr guten Evidenzbasierung (Level 1a) allerdings nur der Empfehlungsgrad B („sollte gegeben werden“) ausgesprochen worden. Disulfiram erhielt bei Evidenzbasierung 1b den Empfehlungsgrad 0. Die Datenlage zur Empfehlung von Nalmefen war bei Erstellung der Leitlinien noch nicht ausreichend. Tab. 1 gibt einen Überblick über die zur Pharmakotherapie der Alkoholabhängigkeit eingesetzten Substanzen. Die neurobiologischen und neurochemischen Grundlagen der Alkoholabhängigkeit sind komplex, werden aber mittlerweile gut verstanden (Übersicht in [REF]). Zentrale Strukturen bei der Wirkung von Rauschdrogen sind dopaminerge Neurone im mesolimbischen Bereich (ventrales Tegmentum, Nucleus accumbens) und ihre Projektion in den präfrontalen Kortex, der für Kontrollfunktionen und die Inhibition dysfunktionalen Verhaltens verantwortlich ist. Für Belohnung und Belohnungsantizipation spielt die Ausschüttung von Dopamin im mesolimbischen Bereich eine entscheidende Rolle. Ein abhängiger Konsum von Alkohol entwickelt sich aus einem Zusammenspiel von positiven (z. B. alkoholinduzierte Entspannung, Euphorie) und negativen Konsequenzen (z. B. Entzugssyndrome). Kurz zusammengefasst werden neurochemisch für die positiv verstärkenden Wirkungen vor allem Effekte auf Dopamin, das endogene Opioidsystem, das serotonerge und GABAerge sowie das endogene Cannabinoidsystem verantwortlich gemacht, für die negative Verstärkung vor allem die vermehrte Freisetzung von Kortikotropin-Releasing-Faktor, die GABAerge Down-Regulierung sowie Veränderungen im glutamatergen System (Übersicht in [REF]). Aber auch appetitregulierende Hormone wie Ghrelin scheinen von Bedeutung zu sein [REF]. Chronischer Alkoholkonsum führt zu erheblichen adaptiven Veränderungen und Anpassungen, vor allem der Rezeptorfunktionen im Gehirn, wobei es beim Alkoholentzug zu einer vermehrten Freisetzung exzitatorischer Neurotransmitter bei erhöhter Rezeptorempfindlichkeit und einer Übererregbarkeit des Gehirns kommt.Bislang sind nur wenige Medikamente zur Alkoholentwöhnung zugelassen, obwohl mittlerweile eine ganze Reihe von Substanzen untersucht wurde (Übersicht in [REF]). Es gibt etablierte methodische Standards für die Durchführung von Pharmakotherapiestudien bei Alkoholabhängigkeit, wobei primäre Outcome- oder Responderkriterien entweder eine Verbesserung der Abstinenzrate, eine Erhöhung der Zeit bis zum ersten Konsum, eine Verminderung der Rückfallraten oder eine Trinkmengenreduktion sind (siehe [REF]). In verschiedenen Therapiestudien werden dabei sehr unterschiedliche Outcomekriterien verwendet [REF]. Sekundäre Outcomekriterien sind biologische Marker (Transaminasen, Carbohydrate-defizientes Transferrin [CDT], Ethylglukuronid), sozioökonomische Faktoren („health care utilization“) oder Wiederaufnahmen in Kliniken (Übersicht in [REF]). Zuletzt wurde von einer Expertengruppe die WHO-Klassifikation unterschiedlicher Risikoniveaus („drinking risk levels“) als Outcomekriterium für Pharmakotherapiestudien empfohlen [REF].Voraussetzung für einen positiven Wirksamkeitsnachweis in klinischen Studien ist unter anderem, dass die pharmakologischen Interventionen auch ausreichend implementiert werden. Hohe Missing- und Drop-out-Raten, wie sie für den Bereich der Alkoholentwöhnung typisch sind, verhindern eine ausreichende Treatment-Implementierung und damit auch die Chancen, in Studien einen tatsächlich vorhandenen Effekt nachweisen zu können [REF]. Wichtig ist zudem, dass die Auswahl der Outcomekriterien dem Wirkmechanismus der Substanz angepasst ist.
Zur Eindämmung der COVID-19-Pandemie wurden in den Jahren 2020 und 2021 kontaktbeschränkende Maßnahmen behördlich angeordnet (Abb. 1). Diese Maßnahmen waren aus epidemiologischer Sicht notwendig, aber mit stark erhöhten Belastungen für Familien verbunden. Insbesondere Eltern von jüngeren Kindern empfanden während des ersten Lockdowns ihre Situation als belastend [REF]. Die allgemeine Lebenszufriedenheit nahm ab [REF]. Zudem wurden Partnerschaftskonflikte während des ersten Lockdowns von Eltern jüngerer Kinder deutlich häufiger berichtet als von Müttern und Vätern älterer Kinder oder kinderlosen Paaren [REF]. Diese belasteten Familiensituationen wirkten sich nachteilig auf die psychische Gesundheit von Kindern aus [REF].Noch deutlicher als im repräsentativen Familienquerschnitt hat sich die Situation für psychosozial belastete Familien verändert: Familien, die ohnehin mit besonderen Herausforderungen leben, wie bspw. Armut oder psychischer Erkrankung, mussten – zusätzlich zur Bewältigung dieser grundsätzlichen Herausforderungen – die pandemiebedingten Belastungen kompensieren [REF]. Gleichzeitig konnten aufgrund der Maßnahmen zum Infektionsschutz spezifische Unterstützungsleistungen wie die Frühen Hilfen (Infobox 1), die häufig auf direkten persönlichen Kontakten beruhen, nicht oder nicht wie gewohnt eingeleitet und weitergeführt werden.Das Nationale Zentrum Frühe Hilfen (NZFH) hat, gefördert aus Mitteln der Bundesstiftung Frühe Hilfen des Bundesministeriums für Familie, Senioren, Frauen und Jugend, frühzeitig und ad hoc pandemiebegleitende, empirische Untersuchungen durchgeführt (Infobox 2; Abb. 1). Dies war notwendig, um der Situation angepasste Entwicklungen anzustoßen [REF].Da erste Erkenntnisse sehr zeitnah generiert werden mussten und psychosozial belastete junge Eltern in Bevölkerungsstichproben stark unterrepräsentiert waren, wurden alternative forschungspragmatische Zugänge gewählt: zum einen Interviews und Fokusgruppen mit kleinen Stichproben psychosozial belasteter Mütter, zum anderen Onlineerhebungen bei Gelegenheitsstichproben mit Fachkräften in den Frühen Hilfen. So konnten erste Eindrücke aus verschiedenen Blickwinkeln gewonnen werden.Im Folgenden beleuchten wir anhand dieser Quoten- bzw. Gelegenheitsstichproben zunächst die Situation und Versorgung der Familien im Frühjahr 2020, dann skizzieren wir Entwicklungen bis zum Frühjahr 2021. Da sich die Digitalisierung bzw. die „Hilfe auf Distanz“ als zentrales Entwicklungsthema der Frühen Hilfen herauskristallisiert, fragen wir anschließend, inwieweit die Erfahrungen aus der Coronapandemie Optionen für einen Qualitätsentwicklungsschub in den Frühen Hilfen bieten.Frühe Hilfen sind Angebote für Eltern ab der Schwangerschaft und Familien mit Kindern bis 3 Jahre. Sie bieten Eltern Unterstützung, Beratung und Begleitung. Sie sind freiwillig und kostenfrei. Ziel ist es, jedem Kind eine gesunde Entwicklung und ein gewaltfreies Aufwachsen zu ermöglichen.Gesundheitsfachkräfte in den Frühen Hilfen sind Familienhebammen und Familien‑, Gesundheits- und Kinderkrankenpflegerinnen, die Familien niedrigschwellig und aufsuchend betreuen und begleiten. Ihre Aufgaben umfassen beispielsweise die alltagspraktische Unterstützung von Familien und die Förderung von Kompetenzen der Eltern in der Versorgung und Erziehung ihrer Kinder. Evaluation der Qualitätsdialoge Frühe Hilfen (QDFH).Die „Qualitätsdialoge Frühe Hilfen“ sollen die Frühen Hilfen qualitätsgesichert weiterentwickeln. Von 2018 bis 2021 nehmen 23 Kommunen an diesem Projekt teil. Dieser Praxisprozess wird wissenschaftlich begleitet. Die hier vorgestellten Ergebnisse basieren auf Daten der 2. Erhebungswelle der quantitativen Onlinebefragung und auf qualitativen, leitfadengestützten Befragungen zwischen Januar und März 2021. An der Onlinebefragung nahmen 302 Akteure der beteiligten Kommunen teil. Leitfadeninterviews wurden mit 12 Steuerungsverantwortlichen der strategischen und operativen Ebene geführt. Bezogen auf die COVID-19-Pandemie wurden 9 Items in den quantitativen Onlinefragebogen integriert und der Interviewleitfaden um das Thema „Arbeiten unter Coronabedingungen“ ergänzt.              Gesundheitsfachkräftebefragung I (Frühjahr 2020): Bei der Gesundheitsfachkräftebefragung I [REF] handelt es sich um eine qualitative Ad-hoc-Befragung von Familienhebammen und Familien‑, Gesundheits- und Kinderkrankenpflegenden. Die Rekrutierung der Teilnehmenden erfolgte im Schneeballverfahren, der Befragungslink wurde initial an n = 5 Fachkräfte versandt, die diesen weiterleiteten. Der Onlinefragebogen enthielt überwiegend offene Fragen zur beruflichen Situation der Fachkraft und zur Situation in den betreuten Familien. Die Feldzeit dauerte vom 30.03. bis zum 14.04.2020. Der Datensatz umfasst Antworten von n = 58 Gesundheitsfachkräften, die in der längerfristigen aufsuchenden Betreuung tätig sind.              Gesundheitsfachkräftebefragung II (Frühjahr 2021): Im Mai 2021 (Feldzeit 11.–26.05.) wurden erneut Fachkräfte befragt, um ein aktuelles Bild der Lage in den Frühen Hilfen zu erhalten. Die Rekrutierung erfolgte wieder im Schneeballsystem mit einem initialen Versand an n = 43 Fachkräfte. Der Fragebogen enthielt hauptsächlich geschlossene Fragen, entwickelt auf Basis der Ergebnisse der ersten Fachkräftebefragung. Insgesamt nahmen n = 82 Fachkräfte an der Befragung teil.  Befragung von Müttern zu ihrer Situation im ersten Lockdown: Um auch die Sichtweise der Familien selber berücksichtigen zu können, wurden im November 2020 3 Minifokusgruppen mit je 3 Müttern und 6 Einzelinterviews mit 5 weiteren Müttern durchgeführt. Die Teilnehmenden wurden so ausgewählt, dass neben dem Merkmal „niedrig gebildet“ mindestens ein weiteres Belastungsmerkmal vorlag. Retrospektiv wurde das Alltagsleben der teilnehmenden Mütter während der Zeit der kontaktbeschränkenden Maßnahmen im Frühjahr 2020 exploriert.
Dieser Artikel stellt die aktuelle Problematik der Übertragung hochinfektiöser Viren (SARS-CoV-2) über Aerosole in der Innenraumluft vor. Verschiedene, für Schulen geeignete infektionsmindernde Maßnahmen, insbesondere bezüglich der Lüftung von Räumlichkeiten, werden beschrieben und verglichen. Der Artikel schließt mit geeigneten Empfehlungen und einem Ausblick für Lüftungsanforderungen in der Zukunft.Seit der weltweiten Ausbreitung des SARS-CoV‑2 nimmt die Frage nach geeigneten und angemessenen Maßnahmen zu einer wirksamen Eindämmung der Pandemie eine zentrale Rolle ein. Die begrenzte Fähigkeit der Gesundheitsbehörden, gemeldete Infektionsfälle zurückzuverfolgen macht deutlich, dass die Mechanismen und Orte, an denen das Virus bevorzugt übertragen wird, vielfältig und in ihrer jeweiligen Relevanz nicht gut bekannt sind. Insbesondere zu Beginn der Pandemie gab es viele Unsicherheiten und Kontroversen bei der Planung und Priorisierung infektionsmindernder Maßnahmen im gesamtgesellschaftlichen Kontext.Zahlreichen Hinweisen zufolge spielen menschliche Zusammenkünfte in Innenräumen eine zentrale Rolle bei der Verbreitung von SARS-CoV‑2 [REF]. Mechanismen der Infektion sind die Übertragung virushaltiger Aerosolpartikel bzw. Tröpfchen im Nahfeld einer infizierten Person sowie die Anreicherung und Übertragung virushaltiger Aerosolpartikel in Innenräumen [REF].An Bildungseinrichtungen wie Schulen kommen Menschen verschiedener Altersgruppen über viele Stunden auf engem Raum zusammen. Während Infektionen bei Kindern meistens einen milden Verlauf haben [REF], erkranken Erwachsene, wie zum Beispiel das Lehrpersonal und auch die Eltern und Großeltern der Kinder, abhängig von Alter und bestehenden Vorerkrankungen mit deutlich schwerwiegenderen Verläufen [REF]. Als Präventionsmaßnahme wurde in Deutschland schon bald die verstärkte Lüftung von Innenräumen diskutiert [REF]. Ab März 2020 wurde jedoch in vielen Bundesländern begonnen, Schulen zu schließen bzw. den Unterricht online zu verlagern. Mit Beginn der zweiten Infektionswelle im Oktober 2020 wurden Schulschließungen als schnelle und geeignete Maßnahme zur Verringerung der Infektionsfälle betrachtet.Die Beurteilung von Infektionsrisiken in Innenräumen entwickelte sich mit dem Wissen über die aerogene Verbreitung der Infektion, der Inzidenz der Erkrankung in der Bevölkerung, dem Auftreten von Virusmutationen [REF] und zusätzlich mit ersten Erkenntnissen über die Langzeitfolgen von COVID-19-Erkrankungen bei Erwachsenen [REF] und Kindern [REF]. Zum Zeitpunkt des Schreibens (Oktober 2021) war die Kenntnis über die Entwicklung der Pandemie und ihre Einflussfaktoren noch immer im Fluss und es war kaum abzusehen, wann aufgrund von Impfungen und steigender Immunität in der Bevölkerung jegliche Präventionsmaßnahmen aufgehoben werden können.Die respiratorische Aufnahme von Aerosolpartikeln1 gilt als Hauptübertragungsweg für SARS-CoV‑2 [REF]. Mit der ausgeatmeten Luft verbreitet jeder Mensch auch Aerosolpartikel in seiner unmittelbaren Umgebung. Wie bei nahezu allen Atemwegserkrankungen, die mit typischen Symptomen einhergehen, scheiden infizierte Personen Partikel aus, welche die Krankheitserreger enthalten. Eine Besonderheit von SARS-CoV‑2 besteht darin, dass auch infizierte Menschen ohne Krankheitssymptome über einen Zeitraum von mehreren Tagen virushaltige Partikel ausscheiden können. Dies hat unter anderem dazu geführt, dass sich in der Anfangsphase der Pandemie viele Personen auf diesem Weg infiziert haben.SARS-CoV-2-Einzelviren haben Durchmesser im Bereich 0,06–0,14 μm [REF]. Sie werden in der Regel als Bestandteil größerer wässriger Partikel ausgeatmet, welche sich in Abhängigkeit der Umgebungsbedingungen (relative Luftfeuchte und Temperatur) bezüglich ihres Durchmessers und Wasseranteils ändern können [REF]. Im medizinischen Sprachgebrauch werden größere, teilweise gerade noch sichtbare, Aerosolpartikel häufig als „Tröpfchen“ beschrieben und solche kleiner als 5 µm als „Aerosol(partikel)“. Physikalisch handelt es sich bei beiden jedoch um Aerosolpartikel, bei denen vor allem die Partikelgrößenverteilung ausschlaggebend ist für deren Verhalten im Innenraum, die Möglichkeit der Inhalation und die Eindringtiefe in die Atemwege [REF]. Anzahl und Durchmesser der ausgeatmeten Partikel hängen stark von der Art der menschlichen Aktivität ab: Bei ruhiger Atmung entstehen vorwiegend kleine Partikel (< 5 µm), beim Sprechen, Rufen, Singen oder unter körperlicher Anstrengung insgesamt vermehrt Partikel und beim Niesen und Husten zusätzlich größere Partikel bis zu einer Größe von 100 µm [REF]. „Feuchte Aussprache“ erzeugt noch größere, mit dem Auge sichtbare Speicheltropfen. Diese Informationen sind von hoher Bedeutung für die Bewertung der Situation an Schulen, wo Schülerinnen, Schüler und Lehrerpersonal eine große Bandbreite an Aktivitäten verfolgen.In dicht belegten Innenräumen können virushaltige Aerosolpartikel zum Risiko werden, wenn sie sich bei begrenztem Luftaustausch im Raum anreichern. Während größere Partikel im Bereich von 100 µm innerhalb von Sekunden zu Boden sinken, können Partikel kleiner als 10 µm viele Minuten und Stunden in der Luft verbleiben [REF]. Größere Partikel bzw. Tröpfchen kommen daher nur für eine luftgetragene Infektion im Nahbereich einer infizierten Person infrage, wogegen kleinere Partikel, die sich mit der Luftströmung im gesamten Raum verteilen können, für Infektionen sowohl im Nah- wie im Fernfeld sorgen können. In allen Fällen ist davon auszugehen, dass im Nahfeld (< 1,5 m) einer infizierten Person höhere Konzentrationen an möglicherweise infektiösen Partikeln anzutreffen sind. Die genauen Prozesse, die zur Emission, zur Ausbreitung und zur Veränderung der ausgeschiedenen Aerosolpartikel führen, sind von einer Vielzahl unterschiedlicher Faktoren abhängig und bislang im Einzelfall kaum vorherzusehen.Hinsichtlich der Infektiosität bei Kindern bestehen noch zahlreiche Unklarheiten. Beispielsweise ist nicht bekannt, welche Altersgruppen die höchste Infektiosität aufweisen, wobei angenommen werden kann, dass Kinder weniger infektiös sind als Erwachsene [REF]. Problematisch im schulischen Kontext ist, dass die Mehrzahl der Kinder nach bisheriger Studienlage einen asymptomatischen oder milden Krankheitsverlauf zeigt [REF], wodurch unerkannte Infektionen wahrscheinlicher werden. Allerdings legen Erkenntnisse nahe, dass Schulkinder deutlich weniger zu Infektionen in Schulräumen beitragen als das Lehrpersonal [REF] und dass Infektionen beim Aufenthalt in Schulräumen – bei Einhaltung der AHA + L-Regeln (Abstand einhalten, Hygieneregeln beachten, im Alltag eine Maske tragen und Lüften) – insgesamt selten sind [REF].Grundsätzlich müssen folgende Bedingungen erfüllt sein, damit eine Person über SARS-CoV-2-haltige Aerosolpartikel infiziert werden kann:
Sind Kinder und Jugendliche relevante Vektoren für die Übertragung von SARS-CoV‑2 („severe acute respiratory syndrome coronavirus type 2“)? Und wie wirkt sich dies aus, wenn Kinder eine Schule oder Kindertagesstätte besuchen? Das sind Fragen, die sowohl die Wissenschaft als auch die Medien und Politik seit Beginn der Pandemie im März 2020 beschäftigen.Da hier anfangs und insbesondere auch zu Beginn der dritten Pandemiewelle im Winter und Frühjahr 2021 angesichts von Virusmutationen große Unklarheit herrschte, wurden weltweit Schulen geschlossen, um so das Infektionsgeschehen einzudämmen. Inzwischen sind die Übertragungswege von SARS-CoV‑2 und damit auch die infektionsepidemiologische Rolle von Kindern klarer geworden [REF]. Kinder unter 10 Jahren schienen im Jahr 2020 insgesamt seltener mit SARS-CoV‑2 infiziert zu sein als Erwachsene [REF]. Auf ein geringeres Risiko von Kindern, sich anzustecken, deuten Studien zur Übertragung in privaten Haushalten zu Beginn der Pandemie hin [REF].Eine Analyse zu Ausbrüchen an Schulen und Kinderbetreuungseinrichtungen in Deutschland kam zu dem Ergebnis, dass die Weitergabe des Virus in den Einrichtungen zwar stattfindet, jedoch das Infektionsgeschehen insgesamt hierdurch nicht maßgeblich beeinflusst wird [REF]. Diese Einschätzung wird auch in einer Stellungnahme der Deutschen Gesellschaft für Pädiatrische Infektiologie e. V. (DGPI) zur Rolle von Schulen und Kindertagesstätten in der COVID-19-Pandemie vertreten [REF]. Trotz all dieser Erkenntnisse hielt (und hält) die deutsche Politik bei hohen Zahlen von positiven SARS-CoV-2-PCR-Tests in der Bevölkerung (im Weiteren als „Inzidenz“ bezeichnet) am Konzept des Distanzunterrichts fest und öffnet Bildungseinrichtungen bisher nur zögerlich für den Regelbetrieb. Dies scheint insbesondere auch bedauerlich angesichts der Tatsache, dass eine evidenzbasierte und auf interdisziplinärem Expertenkonsens aufgebaute S3-Leitlinie bereits Anfang Februar 2021 klare Empfehlungen für Zeiten hoher Inzidenzen in der Gesamtbevölkerung ausgesprochen hat, die Schulschließungen nur noch in Ausnahmefällen notwendig machen [REF]. In diesem Artikel möchten wir deshalb diskutieren, welche Rolle Kinder und Jugendliche bei der Übertragung von SARS-CoV‑2 spielen, wenn sie eine Schule oder Kindertagesstätte besuchen. Hierzu soll die aktuelle Datenlage mit Stand Juni 2021 beleuchtet werden. Zudem möchten wir auf mögliche Schwierigkeiten bei der Interpretation dieser Daten hinweisen und aufzeigen, wie zukünftige Studien gegebenenfalls angelegt sein sollten, um noch aussagekräftigere Ergebnisse zu liefern. Nicht zuletzt möchten wir darauf eingehen, unter welchen Umständen ein regulärer Präsenzunterricht gelingen kann, ohne das Risiko einer unkontrollierten Ausbreitung von SARS-CoV‑2 in Kauf nehmen zu müssen.Einschränkend zur Bewertung der Rolle von Kindern und Jugendlichen in der Pandemie muss an dieser Stelle erwähnt werden, dass sich die infektionsepidemiologische Lage aufgrund des Auftretens neuer Varianten stetig ändert. Bei Erstellung des Artikels war in Deutschland die Alphavariante vorherrschend. Zu der dann aufkommenden Deltavariante kann der Artikel aufgrund der bis dato unzureichenden Datenlage nur allgemeine Aussagen treffen. Es ist jedoch zu erwarten, dass sich die Möglichkeiten der Prävention auch mit dem Auftreten neuer Varianten nicht grundlegend ändern werden; sowohl die Weltgesundheitsorganisation (WHO) als auch das Europäische Zentrum für die Prävention und die Kontrolle von Krankheiten (ECDC) haben darauf hingewiesen, dass neue Varianten (die auch weiterhin zu erwarten sind) die Notwendigkeit von Schulunterricht in Präsenz nicht infrage stellen dürfen.
Der Begriff „atopisch“ klassifiziert die atopische Dermatitis (AD) als eine der Atopie zugeordneten Erkrankung. Der Begriff „Atopie“ wurde erstmals 1923 von den Allergologen Arthur F. Coca und Robert A. Cooke beschrieben [REF]. Neben der AD besteht die konventionelle Triade der atopischen Erkrankungen aus der allergischen Rhinokonjunktivitis und dem allergischen Asthma bronchiale, wobei gelegentlich auch Ig(Immunglobulin)E-vermittelte Nahrungsmittelallergien hinzugezählt werden [REF]. Gehäuft geht die AD den anderen atopischen Erkrankungen voraus, worauf der Begriff des „atopischen Marsches“ basiert [REF]. Die Pathophysiologie der AD beschreibt eine komplexe Orchestrierung aus genetischen Faktoren, Barrieredefekten, einer Typ-2-dominierten Immunantwort sowie Umwelteinflüssen [REF]. Durch moderne Systemtherapien wie Biologika und „small molecules“ stehen zunehmend sehr wirksame Medikamente zur Behandlung der moderat bis schweren AD zur Verfügung. Das stetig wachsende Armamentarium und die vereinfachte Zugänglichkeit bergen jedoch die Gefahr, dass Einflussfaktoren wie Allergien aufgrund einer zum Teil aufwendigen Diagnostik und Therapie weniger beachtet werden, obwohl hierfür nebenwirkungsarme und gut wirksame Ansätze zur Verfügung stehen.Atopieatopischen MarschesDie atopische Dermatitis geht gehäuft mit anderen atopischen Erkrankungen wie der allergischen Rhinokonjunktivitis und dem allergischen Asthma einher.Bei ca. 80 % der AD-Patienten lassen sich allergenspezifische IgE-Antikörper gegen Aero- oder Nahrungsmittelallergene nachweisen, zumeist wird hier auch von einer extrinsischen AD gesprochen. Im Gegensatz dazu lassen sich beim intrinsischen Typ keine IgE-vermittelten Sensibilisierungen nachweisen. Im Jahr 1950 wurde erstmals eine kontrollierte Exposition von AD-Patienten mit inhalativen Allergenen durchgeführt, die nach 24 h eine Verschlechterung der rhinitischen Beschwerden sowie der AD hervorrief [REF]. Im Jahr 1996 folgte die inhalative Exposition mit Hausstaubmilbenallergen, die bei AD-Patienten zu einer Aggravation führte [REF]. Unterstützt wurde die Annahme des Einflusses von Allergenen auf die Haut durch den Atopie-Patch-Test (APT), bei dem u. a. Aeroallergene in Form einer Epikutantestung angewendet werden [REF]. Hierbei lassen sich ekzematöse Reaktionen durch die entsprechenden Allergene triggern. Im Jahr 2015, 65 Jahre nach den ersten Hinweisen, erfolgte erstmals die placebokontrollierte Exposition der gesamten Haut [REF]. Insgesamt 17 AD-Patienten mit Sensibilisierung gegenüber Gräserpollen wurden in einer Expositionskammer für jeweils 4 h an 2 aufeinanderfolgenden Tagen mit einem Gräserextrakt aerogen exponiert, und es wurde die Schwere der AD betrachtet [REF]. Bereits nach 24 h zeigte sich eine erste Verschlechterung der AD, die am zweiten und dritten Tag der Studie ihren Höhepunkt erreichte. Die Aggravation zeigt sich insbesondere an den Luft- und damit Allergen-exponierten Arealen. Bei Ekzemmorphen mit primärer Manifestationslokalisation in luftexponierten Arealen sollte folglich an mögliche allergene Trigger gedacht werden (Abb. 1). Um das Jahr 2000 wurde die Umhüllung von Matratzen, Kopfkissen und in manchen Studien auch von Bettdecken mit milbendichten Überzügen, als „Encasing“ bezeichnet, untersucht. In einer ersten placebokontrollierten Studie mit 48 AD-Patienten zeigte sich eine signifikante Reduktion der Schwere der AD [REF]. Eine Folgestudie mit 86 Patienten konnte zwar eine Reduktion der Allergenkonzentration zeigen, jedoch keine Besserung der Hautsymptome [REF]. In einer Cochrane-Übersichtsarbeit wurden 7 Studien zu dieser Thematik inkludiert [REF]. Es wurden moderate Verbesserungen des Ekzems bei Patienten mit extrinsischer AD konstatiert. Allerdings sprachen die Autoren, basierend auf der mangelnden Evidenz aus ausreichend guten klinischen Studien, keine allgemeine Empfehlung für die Anwendung aus. Für das Asthma gibt es eine weitere, etablierte Methode zur Allergenreduktion, bei der über ein Gerät ein kontinuierlicher Luftstrom („temperature controlled laminar airflow“ [TLA]) induziert wird, der zu einer Reduktion der Allergenkonzentration während der Nacht führt. In einer kleineren Proof-of-concept-Studie mit 10 AD-Patienten konnte kürzlich eine Reduktion der Schwere der AD, insbesondere im Kopf‑/Halsbereich, nach 3 Monaten erzielt werden [REF].AllergenexpositionEncasingAllergenreduktionVerschiedene Studien konnten den Zusammenhang zwischen Aeroallergenen und der atopischen Dermatitis zeigen. Allergenreduzierende Maßnahmen zeigen bisher nur eingeschränkte Effekte.Die einzige langfristige, kausale Therapie von Allergien ist die Allergenimmuntherapie (AIT), auch als Hyposensibilisierung bezeichnet. Die Ursprünge nimmt die AIT vor über 2000 Jahren, als Mithridates VI. von Pontos sich mit der regelmäßigen Applizierung von Schlangengift in aufsteigender Dosierung vor möglichen Vergiftungen schützen wollte [REF]. Das eigentliche Geburtsjahr der AIT ist jedoch 1911, als Leonhard Noon im Journal Lancet eine Studie zur subkutanen Anwendung eines verdünnten Pollenextraktes zur Therapie der allergischen Rhinokonjunktivitis vorstellte [REF]. Beachtenswert ist, dass er das Verfahren an sich selber austestete [REF].HyposensibilisierungDas allgemeine Konzept der AIT umfasst das Ziel, dass die regelhafte Applikation des Allergens zu einer Toleranzentwicklung führt. Auf immunologischer Ebene wurden in den letzten 2 Dekaden verschiedene Effekte der AIT aufgedeckt [REF]. Die Anzahl an toleranzinduzierenden Zellen, sog. regulatorische T‑Zellen (Treg-Zellen), nimmt zu. Tregs setzen u. a. die inhibierenden Mediatoren IL(Interleukin)-10 [REF] und TGF(„transforming growth factor“)-β [REF] frei. Diese tragen dazu bei, dass B‑Zellen vermehrt IgG2- und IgG4-Antikörper bilden [REF], wodurch die Bindung von IgE an das Allergen verhindert wird. Ferner werden im Verlauf Mastzellen und eosinophile Granulozyten in der Haut reduziert. Neuere Studien zeigen auch eine Beeinflussung der zum angeborenen Immunsystem gehörenden „innate lymphoid cells“ (ILCs) [REF]. Es finden also auf verschiedenen Ebenen des Immunsystems Anpassungen hin zu einer Toleranzreaktion statt, sodass auch nach Beendigung der AIT ein anhaltender Effekt zu erwarten ist.ToleranzentwicklungTreg-ZellenAktuell werden zumeist 2 Applikationswege einer AIT unterschieden: 1. sublingual (sublinguale Immuntherapie [SLIT]) oder 2. subkutan (subkutane Immuntherapie [SCIT]). Die erste placebokontrollierte Studie zur SCIT erfolgte bereits 1954 [REF], gefolgt von der ersten SLIT-Studie 1986 [REF]. In beiden Studien lag der Fokus auf Patienten mit allergischen Atemwegserkrankungen. Für die SCIT und SLIT gibt es gute Evidenz für die allergische Rhinokonjunktivitis bei verschiedenen Allergenen. Auch für das Asthma gibt es für einzelne Präparate ausreichend Evidenz. Ein weiterer Applikationsweg wurde ebenfalls etabliert: Für die Behandlung der Erdnussallergie bei Kindern wurde im letzten Jahr die erste orale Immuntherapie zugelassen.SCITSLITorale ImmuntherapieDie einzelnen Präparate zeigen eine deutliche Heterogenität bezüglich der Zusammensetzung und Konzentration einzelner Allergene, sodass ein Vergleich zwischen 2 Präparaten auf Allergenebene erschwert wird. Eine Prüfung der Wirksamkeit einer SCIT oder SLIT bedarf folglich einer differenzierten Betrachtung (Tab. 1).
Lehrpersonen haben die anspruchsvolle Aufgabe, die heterogenen Lernbedürfnisse ihrer Schüler*innen im Unterricht adäquat zu adressieren. Eine formativ ausgerichtete individuelle Lernunterstützung von Schüler*innen ist eine zentrale Möglichkeit, dies gezielt umzusetzen [REF]. Schlüsselkomponenten einer solchen Lernunterstützung sind die lernbegleitende Diagnose des Lernstands sowie informatives Feedback, das an diese Diagnose anknüpft [REF]. Diesem Ansatz wird ein hohes Potenzial für die Förderung des Lernfortschritts zugeschrieben [REF]. Jedoch muss gemäß dem Angebots-Nutzungs-Modell angenommen werden, dass ihr Erfolg wesentlich von der Wahrnehmung und Nutzung durch die Schüler*innen abhängt [REF].Tatsächlich zeigen Ergebnisse der Unterrichtsqualitätsforschung, dass Schüler*innen derselben Klassen die Lernunterstützung interindividuell deutlich unterschiedlich wahrnehmen [REF]. Diesen Divergenzen genauer auf den Grund zu gehen, kann dazu beitragen, Unterstützungsbedürfnisse von Lernenden besser zu verstehen und Schlüsse für eine adaptive Gestaltung von Lernunterstützung zu ziehen. Insbesondere in der Primarstufe erscheint dies relevant, da hier nicht nur die Heterogenität zwischen Schüler*innen besonders hoch ausgeprägt ist, sondern darüber hinaus jüngere Schüler*innen stärker als ältere auf die Lernunterstützung der Lehrperson angewiesen sind [REF].Erste empirische Befunde zur Erklärung solcher Divergenzen in der Unterrichtswahrnehmung deuten darauf hin, dass diese in Verbindung mit kognitiven (z. B. Vorwissen) und affektiv-motivationalen (z. B. Selbstwirksamkeit) Lernvoraussetzungen von Schüler*innen stehen [REF]. Zugleich wird theoretisch angenommen, dass das reziproke Wechselspiel kognitiver und affektiv-motivationaler Lernvoraussetzungen von Schüler*innen für die Wahrnehmung und Nutzung von Unterrichtsangeboten entscheidend ist [REF]. Bislang ist jedoch weitgehend ungeklärt, in welchem Zusammenhang konkret eine solche Kombination kognitiver und affektiv-motivationaler Voraussetzungen mit der Wahrnehmung von Lernunterstützung steht, insbesondere dann, wenn kognitive und affektiv-motivationale Merkmale unterschiedliche Ausprägungen aufweisen (z. B. hoch ausgeprägtes Vorwissen gepaart mit geringer Selbstwirksamkeit).An dieser Stelle setzt die vorliegende Studie an. Im Zentrum steht die Frage, ob zwischen Primarschüler*innen Unterschiede in der Wahrnehmung der Lernunterstützung durch ihre Lehrperson bestehen und wie sich etwaige Unterschiede erklären lassen. Mittels eines personenzentrierten Vorgehens werden anhand des Vorwissens der intrinsischen Motivation und Selbstwirksamkeit in Mathematik kognitiv-motivationale Schüler*innenprofile identifiziert. Personenzentrierte Ansätze eigenen sich sehr gut, um Schüler*innengruppen mit unterschiedlichen Merkmalskonfigurationen abzubilden, da mehrere Lernvoraussetzungen simultan untersucht und Wahrnehmungsdivergenzen zwischen den Gruppen differenziell beleuchtet werden können.Zur theoretisch-konzeptuellen Grundlegung wird im Folgenden zuerst das Konstrukt der individuellen Lernunterstützung erläutert (Abschn. 2.1). Danach werden interindividuelle Wahrnehmungsunterschiede thematisiert (Abschn. 2.2), und es wird auf die diesbezügliche Bedeutung individueller Voraussetzungen eingegangen (Abschn. 2.3). Des Weiteren werden Vorteile eines personenzentrierten Untersuchungsansatzes aufgezeigt (Abschn. 2.4), woraus schließlich die Fragestellungen und die Hypothesen der vorliegenden Studie abgeleitet werden (Abschn. 2.5).
Seit Beginn der durch SARS-CoV‑2 („severe acute respiratory syndrome coronavirus 2“) verursachten COVID-19-Pandemie haben sich weltweit ca. 262 Mio. Menschen infiziert; ca. 5,2 Mio. sind verstorben (Worldometer Corona 30.11.2021; [REF]). Die schnelle pandemische Ausbreitung des Virus führte zur Frage nach geeigneten Schutzmaßnahmen im medizinischen und im öffentlichen Bereich. Damit rückte auch die Desinfektion in den Fokus des allgemeinen Interesses. In der Folge stieg die Nachfrage nach Desinfektionsmitteln nicht nur im medizinischen Bereich, sondern auch im öffentlichen und im privaten Umfeld stark an.Gleichzeitig traten bestehende Wissenslücken und Fehleinschätzungen in Bezug auf adäquate und nichtadäquate Desinfektionsmaßnahmen hervor: Aktionen wie das Besprühen bzw. Benebeln ganzer Straßenzüge mit Desinfektionsmitteln prägten in mehreren Ländern das Bild der Coronavirusbekämpfung. In Deutschland konnte großflächiges Besprühen von Straßen verhindert werden, trotzdem stieg auch hierzulande die Nachfrage nach Desinfektionsmitteln auch in Bereichen, in denen diese bisher nicht oder kaum eingesetzt worden waren, stark an. Dies betraf zum einen Verbraucher aus Sorge, sich im öffentlichen Umfeld mit SARS-CoV‑2 zu infizieren – obwohl in verschiedenen Stellungnahmen darauf hingewiesen wurde, dass Händewaschen in der Regel ausreicht und eine Flächendesinfektion im Haushalt nicht erforderlich ist, so in den Stellungnahmen der Weltgesundheitsorganisation (WHO), des Robert Koch-Instituts (RKI), der Bundeszentrale für gesundheitliche Aufklärung (BZgA), der Desinfektionsmittelkommission der Deutschen Vereinigung zur Bekämpfung der Viruskrankheiten (DVV) und der Gesellschaft für Virologie (GfV; [REF]).Auch in vielen öffentlichen Bereichen, wie z. B. Supermärkten, Restaurants und Bahnhöfen, hoffte man durch Flächen- und Händedesinfektion die Pandemie eindämmen zu können. Vor allem nach dem ersten Lockdown setzten die örtlichen Behörden in Deutschland auf Hände- und Flächendesinfektion im öffentlichen Bereich. Allein für die Schulträger in Nordrhein-Westfalen wurden 20.000 l Desinfektionsmittel zum Abruf gestellt [REF]. Durch diese zusätzliche Nachfrage nach Desinfektionsmitteln im Inland und bereits seit Anfang Januar durch das Ausland (z. B. China) kam es in Deutschland zur starken Verknappung etablierter Produkte.Eine Steigerung der Produktion war nur eingeschränkt möglich, da auch Verpackungen und Rohstoffe in der erforderlichen Qualität extrem knapp wurden. Die bisher verfügbaren Desinfektionsmittel, vor allem für die Händedesinfektion, waren zu diesem Zeitpunkt kaum noch erhältlich. Deshalb wurde intensiv nach Lösungen gesucht, um den Mangel möglichst schnell zu beseitigen und die Desinfektion vor allem im medizinischen Bereich im gewohnten Qualitäts- und Sicherheitsstandard durchführen zu können. Dazu war eine Kommunikationsstrategie notwendig, die den Anwender informieren, aber nicht verunsichern sollte. Währenddessen tauchten im Handel immer mehr neue und ungewöhnliche Produkte auf (Abb. 1). In Deutschland wurden in der Folge die Empfehlungen auf anerkannte Verfahren zur Desinfektion beschränkt. Inwieweit die ergriffenen Maßnahmen zur Desinfektion ihrem Ziel gerecht wurden bzw. welche Bedeutung sie speziell bei SARS-CoV‑2 haben, lässt sich nach fast 2 Jahren Pandemie auf der Basis von Erfahrungen und wissenschaftlichen Erkenntnissen genauer einschätzen. Der vorliegende Beitrag beschreibt zunächst die Anforderungen an chemische Hände- und Flächendesinfektionsmittel und die rechtlichen Grundlagen für ihre Verkehrsfähigkeit. Im Folgenden werden die auf dieser Basis ergriffenen Maßnahmen und die speziellen Fragestellungen bei der Organisation der Desinfektionsmittelversorgung dargestellt. Dabei wird auch auf die Besonderheiten der Übergangsregelungen im Biozidrecht eingegangen. Des Weiteren wird die Bedeutung der Flächendesinfektion bei SARS-CoV‑2 erörtert. Abschließend werden die während der Pandemie gewonnenen Erkenntnisse bei der Organisation der Desinfektionsmittelversorgung dargestellt und Lehren gezogen, die dazu beitragen sollen, bei zukünftigen Pandemien bezüglich der Desinfektion besser vorbereitet zu sein.
In Deutschland geschieht ca. jede Stunde ein Suizid, jeden Tag, an 365 Tagen im Jahr. Mit rund 1 % aller Todesfälle stellt der Suizid damit keine unbedeutende Todesursache dar, trotzdem wurde er in der Vergangenheit oft tabuisiert, zum Teil auch ignoriert. Informationen zu Suiziden und anderen Todesursachen entstammen vor allem der Todesursachenstatistik. Die Auswertung dieser Daten liefert wichtige Hinweise auf relevante Gesundheitsindikatoren wie Sterbeziffer, verlorene Lebensjahre und vermeidbare Sterbefälle. Sie ist Basis für die Todesursachenforschung, die die Zusammenhänge zwischen verschiedenen Einflussfaktoren wie Alter, Geschlecht, regionale Besonderheiten in der Sterblichkeit und diesbezügliche Veränderungen im Laufe der Zeit untersucht. Aus diesen Ergebnissen heraus werden Empfehlungen für kommende Handlungsfelder und Strategien für die epidemiologische Forschung, die Prävention und die Gesundheitspolitik abgeleitet. Ziel ist es, die Lebenserwartung und Lebensqualität der Bevölkerung durch präventive und medizinisch-kurative Maßnahmen zu erhöhen.Dabei bietet die Todesursachenstatistik folgende Vorteile: Zum einen stellt die Kontinuität der Erhebung sicher, dass dem Nutzer Daten aus weit zurückreichenden Zeiträumen zur Verfügung stehen. Zum anderen werden die Daten entsprechend der Internationalen Statistischen Klassifikation der Krankheiten und verwandter Gesundheitsprobleme (ICD-10) der Weltgesundheitsorganisation (WHO) verarbeitet, was eine internationale Vergleichbarkeit ermöglicht. Letzter großer Pluspunkt ist die jährliche Vollerhebung aller Todesfälle.Im Text wird ausschließlich der wertfreie Begriff „Suizid“ verwendet, grundsätzlich stehen für den Begriff des Suizids verschiedene andere Synonyme zur Verfügung: Die ICD-10 der WHO sieht als Bezeichnung „vorsätzliche Selbstbeschädigung“ vor, oft werden aber auch je nach Perspektive Begriffe wie Selbstmord, Selbsttötung oder Freitod gebraucht. In seinem Urteil vom 26.02.2020 [REF] hat das Bundesverfassungsgericht im Zusammenhang mit dem sogenannten assistierten Suizid verdeutlicht, dass das allgemeine Persönlichkeitsrecht (Art. 2 Abs. 1 in Verbindung mit Art. 1 Abs. 1 Grundgesetz) ein Recht auf selbstbestimmtes Sterben umfasst. „Dieses Recht schließt die Freiheit ein, sich das Leben zu nehmen und hierbei auf die freiwillige Hilfe Dritter zurückzugreifen. Die in Wahrnehmung dieses Rechts getroffene Entscheidung des Einzelnen, seinem Leben entsprechend seinem Verständnis von Lebensqualität und Sinnhaftigkeit der eigenen Existenz ein Ende zu setzen, ist im Ausgangspunkt als Akt autonomer Selbstbestimmung von Staat und Gesellschaft zu respektieren.“ Damit wird auch unterstrichen, dass der Suizid strafrechtlich nicht mehr verfolgt werden kann.In diesem Beitrag wird ein Überblick über die Ergebnisse der Todesursachenstatistik in Bezug auf Suizide in Deutschland gegeben. Dies beinhaltet nicht nur einen Überblick über die absoluten Zahlen, sondern auch eine nähere Betrachtung verschiedener Aspekte wie Suizid in Abhängigkeit vom Alter und Geschlecht. Neben der Darstellung der aktuell vorliegenden Ergebnisse des Berichtsjahres 2019 werden auch Zeitvergleiche abgebildet. Ergebnisse für das Berichtsjahr 2020 lagen beim Verfassen dieses Aufsatzes noch nicht vollständig und vollplausibilisiert vor. Auf die bis dato verfügbaren, vorläufigen Daten zum Berichtsjahr 2020, die im Zuge der Monatsberichte seit Juli 2021 veröffentlicht werden, wird daher nur kurz eingegangen.
In Deutschland nehmen sich jährlich mehr als 9000 Menschen das Leben (2019: 9041 Suizide – davon 6842 Männer und 2199 Frauen [REF] – entsprechend einer Suizidrate von 11,6/100.000 Einwohner). In Mitteleuropa steigt mit zunehmendem Lebensalter die Suizidrate an, insbesondere bei den Männern. Es wird davon ausgegangen, dass jeder Suizid im Mittel 8–10 weitere Menschen unmittelbar in Mitleidenschaft zieht [REF], also pro Jahr weitere ca. 70.000–90.000. Bei jüngeren Menschen ist Suizid eine der häufigsten Todesursachen. Es gibt bereits in der 4. Überarbeitung eine deutsche Leitlinie der Arbeitsgemeinschaft der Wissenschaftlichen Medizinischen Fachgesellschaften (AWMF) zum Thema Suizidalität im Kindes- und Jugendalter [REF], die unter Federführung der Deutschen Gesellschaft für Kinder- und Jugendpsychiatrie, Psychosomatik und Psychotherapie e. V. (DGKJP) entstanden ist. Derzeit steht die fünfte Aktualisierung dieser Leitlinie an. Eine Leitlinie zum Umgang mit Suizidalität im Erwachsenenalter gibt es bisher nicht. Im August 2021 wurde jedoch die Finanzierung einer S3-Leitlinie „Umgang mit Suizidalität“ vom Innovationsfonds des Gemeinsamen Bundesausschusses bewilligt.In Deutschland unterstützt und fördert die AWMF die Entwicklung von Leitlinien u. a. zur Behandlung von psychischen und somatischen Erkrankungen. Im Jahr 1995 hat der Sachverständigenrat für die Konzertierte Aktion im Gesundheitswesen die AWMF erstmalig beauftragt, die Entwicklung von Standards, Richtlinien, Leitlinien und Empfehlungen der wissenschaftlichen medizinischen Fachgesellschaften zu koordinieren. Inzwischen sind in Deutschland insgesamt mehr als 800 Leitlinien erschienen. Es besteht jedoch bisher keine dezidierte deutschsprachige Leitlinie für den Umgang mit Suizidalität bei Erwachsenen, obwohl suizidales Verhalten ein wesentlicher Faktor von Mortalität und Morbidität ist und auch ein großes gesellschaftliches Problem darstellt.Im Vereinigten Königreich existiert die englischsprachige (evidenzbasierte) Leitlinie des National Institute for Health and Care Excellence (NICE-Guideline) „Suicide prevention“ (letzte Überarbeitung 2018 [REF]). Aufgrund der unterschiedlichen Versorgungssysteme lässt sie sich jedoch nur schwer auf die deutsche Situation anwenden und übertragen. Darüber hinaus unterscheidet sich die zugrunde liegende NICE-Methodologie grundsätzlich von der AWMF-S3-Methodik, die sowohl evidenzbasiert als auch konsensorientiert ist [REF]. Auch außerhalb der psychiatrisch-psychotherapeutischen Versorgung hat suizidales Verhalten eine hohe Relevanz, in der hausärztlichen Versorgung, aber vor allem in der Neurologie, der Chirurgie, der Allgemeinmedizin/Inneren Medizin einschließlich der Geriatrie. Insbesondere muss im Zusammenhang mit onkologischen und palliativmedizinischen Fragestellungen an Suizidalität gedacht werden. Suizidversuche führen häufig zur Inanspruchnahme des Rettungsdienstes, der Notaufnahmen und zu intensivmedizinischer Versorgung. Die entsprechenden (Weiter‑)Behandlungsketten, z. B. Vermittlung von fachpsychiatrischer Evaluation einer zugrunde liegenden psychischen Erkrankung und spezifische Behandlung des suizidalen Syndroms, sind jedoch nicht standardisiert und es kommt sowohl (häufig) zu Unter- als auch (seltener) zu Überversorgung. Die Qualifikation im Hinblick auf den Umgang mit suizidalem Verhalten ist sehr heterogen; da bislang keine Leitlinie existiert, besteht hier häufig Unklarheit im Hinblick auf das weitere Vorgehen und somit eine erhebliche Versorgungslücke.
Infektionen mit dem Hepatitis-E-Virus (HEV) können beim Menschen zu einer akuten Leberentzündung führen. In den meisten Fällen verläuft die Erkrankung moderat und selbstlimitierend; die Mortalitätsrate wird mit 0,5–4 % angegeben. Schwere Verläufe mit Todesfällen kommen vor allem bei Risikogruppen vor. Bei Infektionen mit dem HEV-Genotyp 1 zeigen vor allem schwangere Frauen schwere Erkrankungsverläufe, während bei Personen mit Lebervorschädigungen häufig schwere Verläufe bei HEV-Genotyp 3‑Infektionen vorkommen. Dieser Genotyp löst bei immunsupprimierten Personen, vor allem bei Transplantationspatienten, auch chronische HEV-Infektionen aus, die oft zu einer fatalen Leberzirrhose führen. Neben der Manifestation in der Leber wurden auch extrahepatische Krankheitsverläufe beschrieben, die vor allem durch neurologische Symptome gekennzeichnet sind, wie sie beim Guillain-Barré-Syndrom zu finden sind oder durch Meningoenzephalitis oder Neuritis [REF].Infektionen mit HEV treten weltweit auf. In Deutschland ist die Zahl der gemeldeten Hepatitis-E-Fälle in den vergangenen Jahren deutlich gestiegen. Waren es im Jahr 2009 noch 109 registrierte Fälle, wurden in 2019 insgesamt 3728 Fälle gemeldet [REF]. Der Anstieg wird vor allem auf eine höhere Aufmerksamkeit der Ärzte für diese Erkrankung und das Vorhandensein besserer diagnostischer Tests zurückgeführt. Neben diesen klinisch auffälligen Erkrankungsverläufen scheint eine HEV-Infektion jedoch auch häufig mit nur milden Symptomen einherzugehen oder sogar asymptomatisch zu bleiben. Serologische Untersuchungen zeigen, dass im Jahr 2010 etwa 15,3 % der deutschen Bevölkerung HEV-spezifische IgG-Antikörper besaßen, die auf eine vergangene Infektion hinweisen [REF].Das HEV ist ein etwa 30–40 nm großes Viruspartikel, das ein Genom aus Einzelstrang-RNA mit positiver Polarität besitzt. Während es über den Darmtrakt als unbehülltes Virus ausgeschieden wird, findet man im Serum von Patienten und im Überstand infizierter Zellkulturen vor allem Partikel, die eine zusätzliche Lipidhülle besitzen. HEV wird in die Familie Hepeviridae, Genus Orthohepevirus eingeordnet, in dem Stämme der Spezies Orthohepevirus A für den Menschen die wichtigste Rolle spielen (Tab. 1). Innerhalb dieser Spezies wurden bisher 8 unterschiedliche Genotypen beschrieben, die sich bezüglich ihres Wirtsspektrums und ihrer Übertragungswege teilweise deutlich unterscheiden [REF]. Die Genotypen 1 und 2 infizieren ausschließlich den Menschen und werden vor allem über kontaminiertes Trinkwasser übertragen. Dies führt in Entwicklungsländern häufig zu großen Krankheitsausbrüchen. Bei der Infektion von Schwangeren sind auch vertikale Übertragungen dieser Genotypen möglich [REF]. Demgegenüber sind die Genotypen 3 und 4 zoonotisch und haben ihr Hauptreservoir in Haus- und Wildschwein, von denen sie vor allem über den Verzehr von Fleischprodukten auf den Menschen übertragen werden können. Diese Genotypen sind vorrangig in Industrieländern weitverbreitet und führen hier zu sporadischen Hepatitis-E-Fällen. Neben dem zoonotischen Übertragungsweg können diese Genotypen auch parenteral über Blut und Blutprodukte übertragen werden [REF]. Aus der Virusspezies Orthohepevirus A wurde weiterhin der Genotyp 7 beim Menschen gefunden, der sein Reservoir vor allem in Dromedaren hat und im Mittleren Osten vorkommt. Als weiteres humanpathogenes Virus wurde kürzlich das Ratten-HEV identifiziert, das besonders in Ratten verbreitet ist und zur Spezies Orthohepevirus C gehört [REF]. Tab. 1 zeigt einen Überblick über die Hepevirusspezies, Genotypen und deren wichtigste Charakteristika.In Deutschland und in vielen anderen europäischen Ländern werden hauptsächlich Infektionen mit dem HEV-Genotyp 3 mit den Subtypen 3c, 3e und 3f gefunden [REF]. Diese Subtypen werden hier gleichermaßen auch in Schweinen, Wildschweinen und anderen Tierarten nachgewiesen, weshalb ein zoonotischer Übertragungsweg auf den Menschen über den Kontakt mit den Tieren oder über den Verzehr von Fleischprodukten wahrscheinlich ist. In diesem Artikel sollen die Übertragungswege des HEV in Deutschland und die daraus resultierenden Konsequenzen für die Lebensmittelsicherheit genauer betrachtet werden. Hierzu wird zunächst die aktuelle Literatur zur Verbreitung des Virus – im Speziellen des Genotyps 3 – in Tieren, zu prinzipiell möglichen Übertragungswegen und zu Untersuchungen an Lebensmitteln vorgestellt, um anschließend sowohl Schlussfolgerungen für die Verhinderung einer Virusübertragung durch Lebensmittel zu ziehen als auch Forschungsbedarf zur Klärung offener Fragen aufzuzeigen.
Anhaltende Schmerzen sind für den Betroffenen sehr beeinträchtigend und haben häufig negative Auswirkungen auf soziale, familiäre Kontakte und den beruflichen Alltag. Trotz einer sich stetig verbessernden Qualität der OP-Techniken konnte gleichzeitig keine eindeutige physische und mentale postoperative Gesundheitsverbesserung der Patienten erzielt werden [REF], was hohe Chronifizierungsraten postoperativer Schmerzen impliziert [REF]. Eine Vielzahl von „randomised controlled trials“ (RCT) zeigte bereits einen positiven Effekt durch eine neurobiologische Schmerzedukation („pain neuroscience education“ [PNE]) bei Patienten mit chronischen lumbalen Schmerzen [REF]. Diese Überlegungen lassen die Fragestellung zu, ob eine präoperative PNE einen positiven Einfluss auf das postoperative Outcome und damit auf geringere postoperative Schmerzchronifizierungsraten hat.Nicht selten entwickelt sich eine Schmerzchronifizierung nach einer Operation.Neuere Untersuchungen von verschiedenen Autoren zeigen, dass bei etwa 40 % aller Patienten, die in den USA aufgrund von degenerativen Veränderungen der Wirbelsäule operiert werden, der Schmerz weit über den normalen Heilungsverlauf einer verletzten Struktur hinaus andauert – sie chronifizieren. Zudem weisen Patienten nach einer OP deutliche funktionelle Defizite auf [REF].Die Entwicklung einer (postoperativen) Chronifizierung ist nicht von objektivierbaren Daten (Geschlecht, Familienstand, Ausbildungsstatus) abhängig [REF], sondern wird neben einer insuffizienten Stellung der OP-Indikation und einem bereits präoperativ bestehenden chronischen Schmerz insbesondere durch neuroplastische Veränderungen (periphere Sensibilisierung, zentrale Sensibilisierung, deszendierende Modulation) bedingt. Infolgedessen treten häufig maladaptive Copingstrategien, ein Angst-Vermeidungs-Verhalten, Katastrophisierungsgedanken und negative Emotionen, insbesondere Angst, auf. Schmerzen, welche als bedrohlich empfunden werden, verhindern die Rückkehr zu einer normalen Aktivität [REF]. Menschen mit einem ausgeprägten Angst-Vermeidungs-Verhalten (AVV) und Neigungen zur Schmerzkatastrophisierung chronifizieren häufiger [REF]. Emotionen und die daraus resultierenden physiologischen Reaktionen eines Menschen können durch Gedanken und Überzeugungen positiv oder negativ beeinflusst werden [REF].Zu den allgemeinen Hauptrisikofaktoren für eine Chronifizierung von Rückenschmerzen gehören der subjektive Umgang mit Arbeitsplatzkonflikten, soziale Isolation, Depressionen, negative Emotionen und Katastrophisierungsgedanken, welche ggf. auch auf eine mangelhafte Aufklärung zurückzuführen sind [REF]. Neuere Ansätze in der Behandlung chronischer Schmerzsyndrome haben eine neurobiologische Edukation als Haupttherapiebaustein integriert. Ziel dieser Edukation ist es, dem Patienten ein umfassendes Wissen über die neurobiologischen und neurophysiologischen Zusammenhänge des subjektiven Schmerzempfindens zu vermitteln und somit die bedrohende Wirkung des Schmerzes zu reduzieren.Eine Übersicht der „key points“ der biomedizinischen sowie der neurobiologischen Edukation ist in Tab. 1 beschrieben, generelle Eigenschaften bezüglich der Inhalte einer PNE finden sich in Tab. 2. In einem RCT von Louw et al. (2014) konnte bereits ein positiver Effekt in Bezug auf den Schmerz, das Katastrophisieren und die Funktion/Beweglichkeit durch eine PNE bei Patienten mit chronischen lumbalen Schmerzen im 1‑Jahres-Follow-up zeigen [REF]. Schmerz als solcher hat primär die Funktion der „Warnung vor einer Bedrohung“ und stellt somit eine Schutzfunktion vor einer potenziellen Gefahr dar. Allein das Verstehen des Schmerzes führt zu einer Reduktion der subjektiv empfundenen Bedrohung durch den Schmerz [REF]. Diese Überlegungen lassen die Fragestellung zu, welchen Effekt eine präoperative PNE auf das postoperative Outcome hätte.Das Ziel dieses systematischen Reviews ist, den Effekt einer präoperativen neurophysiologischen Schmerzedukation auf das postoperative Outcome sowie die unterschiedlichen Herangehensweisen (bzgl. Dauer und Setting) dieser Intervention zu analysieren.Die primäre Forschungsfrage hierzu lautet: Welchen Effekt zeigt eine präoperative neurophysiologische Schmerzedukation auf das postoperative Outcome?Die sekundäre Forschungsfrage lautet: Welche Herangehensweisen wählen die Autoren zur präoperativen neurophysiologischen Schmerzedukation?
Bereits im Jahr 2000 erschien ein Artikel zur regionalen und kommunalen Gesundheitsberichterstattung (GBE), in dem die Autoren Jacob und Michels zu einer besseren Datenbasis für regionale GBE aufriefen und die Relevanz der Identifizierung regionaler Ungleichheiten unterstrichen [REF]. Seitdem sind 20 Jahre vergangen, in denen sich nicht nur die Datenlage, sondern auch die Softwaremöglichkeiten stark verbessert haben. Die verschiedenen Aspekte der GBE werden in dem vorliegenden Themenheft dargestellt und diskutiert. Die GBE ist sehr breit aufgestellt, sowohl in Bezug auf die thematische Bandbreite wie auch auf die Zielgruppen. Sie informiert mithilfe von Berichten und Informationssystemen über Gesundheitszustand, Gesundheitsversorgung oder Gesundheitsdeterminanten der Bevölkerung und soll gesundheitsrelevante Programme unterstützen [REF]. Dieser Beitrag soll zeigen, wie eine räumliche Betrachtung die Ziele der GBE („Daten für Taten“) unterstützen kann.Geografische, das heißt raumbezogene Analysen sind mittlerweile ein fester Bestandteil von Public Health, Epidemiologie, Versorgungsforschung und GBE. Diese Entwicklung ist vor allem darauf zurückzuführen, dass in den letzten Jahren der Zugang zu Gesundheitsdaten mit regionalem Bezug einfacher wurde und dass es inzwischen diverse anwenderfreundliche Softwarelösungen gibt, mittels derer geografische Analysen durchgeführt und deren Ergebnisse in Form von Karten visualisiert werden können. Die Erforschung medizin- bzw. gesundheitsgeografischer Themen blickt auf eine viel längere Geschichte zurück.Im deutschsprachigen Raum wurde der Begriff der medizinischen Geografie vor allem von Leonhard Ludwig Finke geprägt: Der deutsche Arzt beschrieb Ende des 18. Jahrhunderts bereits die erste medizinische Landkarte; August Hirsch, ein deutscher Medizinhistoriker, publizierte ca. 70 Jahre später ein dreibändiges Handbuch zur historisch-geografischen Pathologie [REF]. In den vergangenen Jahrzehnten stand in der Forschungsrichtung vor allem der Einsatz von Geoinformationssystemen (GIS) im Vordergrund. Die so entstandenen Karten stellen einerseits „klassische“ krankheitsökologische Informationen dar (also die regionale Verbreitung einer Erkrankung auf Basis regionaler Einheiten, wie beispielsweise Bundesländer oder Kreise/kreisfreie Städte), analysieren weitergehend aber auch zeitliche Zusammenhänge zwischen Raum und Gesundheit, Zugangsmöglichkeiten zu Versorgungseinrichtungen oder die Präsenz von Kontaktnetzwerken, um nur einige Beispiele zu nennen [REF].Einhergehend mit dieser Entwicklung ist die stetige Zunahme von Gesundheitsatlanten. Dabei handelt es sich um eine Sammlung von Karten, in denen gesundheitsspezifische Sachverhalte mit räumlichem Bezug gebündelt dargestellt werden. Das vermutlich bekannteste Werk sind die Publikationen im Rahmen des Dartmouth Atlas of Health Care [REF], in dem mittels Karten und begleitender Texte Gesundheitsdaten (z. B. zur Häufigkeit von Erkrankungen, zur Gesundheitsversorgung) für die Vereinigten Staaten von Amerika dargestellt werden.Gesundheitsatlanten können primär nach dem Ausgabemedium unterschieden werden: Sie erscheinen online oder in gedruckter Form. Mittlerweile nimmt die Zahl der Onlineatlanten im Vergleich zu den gedruckten Atlanten deutlich zu. Der Vorteil der Onlineatlanten liegt vor allem in der einfacheren Aktualisierbarkeit. Neben dem Ausgabeformat lassen sich diese Werke auch darin unterscheiden, ob es sich eher um einen Atlas im eigentlichen Sinne handelt, das heißt, die Karte mit ihrer Aussage im Vordergrund steht, oder ob es sich eher um einen Bericht zu einem spezifischen Gesundheitsthema handelt, der mit Karten illustriert wird. Darüber hinaus kann auch nach Zielgruppe, Datenquellen, Ergebnisaufbereitung oder auch Themen (Epidemiologie, Prävention etc.) unterschieden werden.In Deutschland gibt es rund 50 Werke [REF], die als „Gesundheitsatlas“ bezeichnet werden können oder den Charakter eines Berichts mit Karten haben. Zu ersteren Werken gehören beispielsweise der Versorgungsaltas des Zentralinstituts für die kassenärztliche Versorgung (Zi) als webbasierte Plattform [REF] oder der Krebsatlas Schleswig-Holstein [REF] als gedrucktes Exemplar. Der BKK Gesundheitsatlas [REF] oder das Gesundheitsmonitoring des Robert Koch-Instituts (RKI; [REF]) haben beispielsweise eher den Charakter eines Berichtes, in dem zur Veranschaulichung Karten hinzugefügt wurden. Grundsätzlich ist zu erwähnen, dass sich die Werke hinsichtlich ihrer Qualität unterscheiden und dahin gehend, ob bei ihrer Erstellung etwa Leitlinien wie die „Gute Epidemiologische Praxis“ [REF] oder die „Gute Kartographische Praxis im Gesundheitswesen“ [REF] berücksichtigt werden.Dieser Beitrag will einen Blick auf die aktuell vorhandenen Instrumente geben, die speziell von der GBE für regionale Analysen genutzt werden. In den folgenden 3 Abschnitten werden die 3 administrativen regionalen Ebenen der GBE, also die Bundes‑, Landes- und Kommunalebene betrachtet. Ein spezieller Fokus wird auf die angewandten Methoden der geografischen Visualisierung gelegt und diese an jeweils 2 Beispielen dargestellt.
Hepatitis-C-Virus(HCV)-Infektionen lassen sich durch antivirale Medikamente sehr effizient behandeln. Trotzdem ist davon auszugehen, dass erst die Entwicklung eines HCV-Impfstoffs die von der Weltgesundheitsorganisation (WHO) angestrebte Elimination des Virus sicherstellen kann. Die Notwendigkeit eines präventiven HCV-Impfstoffs wird bei der Betrachtung epidemiologischer Kennzahlen deutlich. Weltweit leben 71 Mio. Menschen mit einer HCV-Infektion [REF], hiervon leben 5,9 Mio. in der Europäischen Union, wobei es hier zwischen den einzelnen Ländern große Unterschiede in den Prävalenzen gibt [REF]. So liegt die HCV-Seroprävalenz in der Allgemeinbevölkerung in Italien bei 5,9 %, in Deutschland hingegen nur bei 0,4 % [REF].Etwa 70 % der HCV-Infektionen heilen nicht spontan aus, sondern persistieren. Unterbleiben die Diagnose und eine Therapie, können schwere Spätfolgen auftreten und viele dieser Patienten entwickeln nach Jahrzehnten eine Leberzirrhose oder Leberkrebs. In Industrieländern werden Schätzungen zufolge 60 % der Leberzellkarzinome durch eine chronische Hepatitis C verursacht. In Europa wird bei 63 % der Lebertransplantationen als Indikation eine HCV-Infektion angegeben [REF]. Im Jahr 2019 starben weltweit etwa 290.000 Menschen an Hepatitis-C-bedingten Spätfolgen [REF].Für das Erreichen der von der WHO angestrebten HCV-Elimination gibt es die Zielvorgabe, die HCV-Inzidenz bis 2030 um 80 % gegenüber der Inzidenz des Jahres 2015 zu senken (1,75 Mio. HCV-Neuinfektionen im Jahr 2015 [REF]). Dieses Ziel ist ohne einen wirksamen HCV-Impfstoff schwer erreichbar [REF]. Ein wichtiger Grund hierfür ist die ungebrochen hohe Rate an HCV-Neuinfektionen. Gründe hierfür sind beispielsweise die steigende Anzahl injizierender Drogenkonsumenten bedingt durch die anhaltende Opioidkrise in Nordamerika sowie durch nosokomiale Infektionen in Entwicklungs- und Schwellenländern. Im Jahr 2017 wurden 1,5 Mio. HCV-Patienten mit direkt wirkenden antiviralen Substanzen behandelt, im gleichen Jahr gab es jedoch 1,6 Mio. Neuinfektionen [REF]. Im Jahr 2019 haben sich weltweit 1,5 Mio. Menschen neu mit HCV infiziert, es ist somit kein wesentlicher Rückgang bei den Neuinfektionen zu verzeichnen [REF]. Nach Schätzungen der WHO waren im Jahr 2019 nur 21 % aller HCV-positiven Patienten diagnostiziert und in dieser Gruppe haben 63 % eine Behandlung mit antiviralen Substanzen erhalten [REF]. Während der COVID-19-Krise war der Zugang zur HCV-Diagnostik und -Behandlung für viele Patienten weltweit stark eingeschränkt. Aufgrund dessen ist davon auszugehen, dass es zu einem Anstieg der HCV-Prävalenz kommen wird [REF]. Die HCV-Infektionen sind in der Bevölkerung ungleich verteilt. HCV ist hyperendemisch in der Gruppe der injizierenden Drogenkonsumenten; hier liegt die HCV-Seroprävalenz in Deutschland bei 68 % und in 25 weiteren Ländern zwischen 60 % und 80 % [REF]. Bei Männern, die Sex mit Männern haben, liegt die Prävalenz einer HCV-Infektion bei 3,4 % [REF]. Einer HCV-Mikroelimination in diesen Bevölkerungsgruppen steht erschwerend gegenüber, dass sich erfolgreich therapierte Patienten erneut mit HCV infizieren können [REF]. Wenn Personen, die intravenösen Drogenkonsum praktizieren, eine erfolgreiche HCV-Therapie durchlaufen haben und weiterhin Drogen injizieren, beträgt die HCV-Reinfektionsrate nach 6 Monaten 12,6 % und nach 18 Monaten 17,1 % [REF]. Für die Sicherstellung eines Therapieerfolgs wäre der Einsatz eines präventiven Impfstoffs direkt im Anschluss an die Behandlung erforderlich. Für seronegative Personen mit intravenösem Drogenkonsum zeigen Modellierungsstudien einen signifikanten Impfeffekt, wenn bei hoher Impfquote ein Impfstoff mit 30 % Wirksamkeit verabreicht würde [REF].Direkt wirkende antivirale HCV-Medikamente sind hocheffektiv, jedoch kann trotz Heilung einer HCV-Infektion die Lebererkrankung fortschreiten und Leberkrebs entstehen [REF]. In der akuten Infektionsphase verläuft die HCV-Infektion in aller Regel mild und bleibt häufig unbemerkt. Deswegen wird eine Infektion vielfach erst erkannt, wenn bereits ein erheblicher Organschaden entstanden ist. Somit wäre die Prävention einer chronischen HCV-Infektion deren Behandlung deutlich vorzuziehen, da die Spätfolgen der chronischen Infektion vermieden würden. Ein HCV-Impfstoff, der zwar die akute Ansteckung nicht verhindert, aber wirksam chronische Infektionsverläufe abwendet, wäre bereits von hohem medizinischen und gesellschaftlichen Wert. Da schwere Leberschäden fast immer erst nach langjähriger HCV-Infektion auftreten, würde ein solcher Impfstoff die HCV-Morbidität und -Mortalität wesentlich senken.Aufgrund seiner außerordentlichen Diversität und Wandlungsfähigkeit sowie verschiedener viraler Immunevasionsmechanismen ist HCV ein schwieriges Ziel für die Entwicklung eines Impfstoffes. Darüber hinaus bestehen Limitationen bei der Prüfung von Impfstoffkandidaten: Derzeit sind keine robusten Tiermodelle verfügbar, um den Impfschutz vor einer HCV-Ansteckung direkt zu überprüfen. Präzise Immunkorrelate für eine breit schützende Immunität sind nicht vollends etabliert. Darüber hinaus gestalten sich klinische Wirksamkeitsstudien schwierig, da die Virusübertragung vorwiegend bei Drogenkonsumenten auftritt und der Endpunkt für den Impferfolg (die Abwendung einer chronischen Infektion) einen langen Studienzyklus erfordert.In diesem Übersichtsartikel werden die besonderen Herausforderungen bei der HCV-Impfstoffentwicklung aufgezeigt und die aktuellen Ansätze für die Entwicklung eines Impfstoffs dargestellt.
Hepatitis-B-Virus-(HBV-)Infektionen und Koinfektionen mit dem Hepatitis-D-Virus (HDV) verursachen chronische Lebererkrankungen, die zu einem deutlich erhöhten Risiko einer Leberzirrhose und eines hepatozellulären Karzinoms (HCC) führen [REF]. Die Folgen der Infektion manifestieren sich häufig erst nach Jahrzehnten und die Krankheitslast der derzeit > 250 Mio. chronisch HBV-infizierten und mindestens 12 Mio. HDV/HBV-koinfizierten Personen ist hoch [REF]. Die antivirale Behandlung von HBV mit Nukleosid‑/Nukleotidanaloga (NUC) hemmt die vom Virus codierte reverse Transkriptase, verlangsamt die virale Replikation und das Fortschreiten der Krankheit, beseitigt aber nicht das persistierende Virusgenom in Hepatozyten, die cccDNA („covalently closed circular DNA“).Die Koinfektion HBV-infizierter Personen mit HDV verschlechtert die klinische Prognose [REF]. HDV ist ein viroidähnliches Satellitenvirus, das die HBV-Hüllproteine zur Verbreitung nutzt. Es unterdrückt die Replikation von HBV durch bisher unbekannte Mechanismen [REF]. Das beschleunigte Versagen der Leberfunktion ist unabhängig von HBV und wird auf HDV selbst zurückgeführt. Dazu passt die Beobachtung, dass die Unterdrückung von HBV durch Nucleos(t)id Analoga (NUC) bei HDV-Koinfizierten unwirksam ist [REF]. Interferon-Alpha (IFNα) wird als begrenzt wirksame Therapie gegen chronische HDV-Infektionen eingesetzt, jedoch mit potenziell erheblichen unerwünschten Wirkungen. Der Wirkstoff Bulevirtide (BLV) hemmt spezifisch den Eintritt von HBV und HDV in die Wirtszelle. Er ermöglicht die erste gezielte Therapie mit ausgeprägter antiviraler Wirkung gegen HDV bei sehr guter Verträglichkeit.Dieser Artikel beinhaltet eine kurze Übersicht über bisher primär eingesetzte Therapien bei chronischer HBV/HDV-Infektion, konzentriert sich auf Grundlagen und Wirkmechanismus von BLV sowie Ergebnisse klinischer Studien mit diesem Wirkstoff, der seit Juli 2020 von der Europäischen Arzneimittelagentur (EMA) zur HBV/HDV-Therapie begrenzt zugelassen ist.Etwa 2 Fünftel der Weltbevölkerung sind im Laufe ihres Lebens mit HBV in Kontakt gekommen. Davon haben mehr als 250 Mio. Menschen eine chronische Hepatitis B (CHB) entwickelt [REF], die als ein anhaltender Nachweis von HBV-DNA und Hepatitis-B-Oberflächenantigen (HBsAg) im Serum für mehr als 6 Monate definiert ist. Die weltweite Prävalenz von HBsAg im Jahr 2016 wurde auf 3,9 % geschätzt [REF]. CHB verursacht Leberzirrhose und Das hepatozälluläre Carcinom (HCC), was jährlich zu etwa 887.000 Todesfällen führt. Trotz der erfolgreichen Umsetzung von Impfprogrammen in vielen Ländern steigt die Zahl der Todesfälle durch CHB weiterhin an [REF].Eine akute HBV-Infektion heilt bei den meisten immunkompetenten Erwachsenen (90–95 %) aus. Im Gegensatz dazu kommt es bei Neugeborenen und Kleinkindern in > 90 % der Fälle zu einer Chronifizierung als Folge der perinatalen Übertragung des Virus [REF]. Eine natürliche funktionelle Heilung der HBV-Infektion ist definiert als dauerhafte Negativierung der HBV-Serum-DNA und des HBsAg ohne therapeutische Intervention, begleitet von einer Serokonversion zu einem anti-HBsAg-positiven Immunstatus. Bemerkenswerterweise schließt dieser Status ein Wiederauftreten von HBV nicht aus (z. B. während immunsuppressiver Therapien nach einer Transplantation oder während einer Chemotherapie), was darauf hinweist, dass HBV-Genome (cccDNA) nicht vollständig eliminiert werden und reaktiviert werden können [REF]. Eine funktionelle Heilung („functional cure“) als Therapieziel ist dementsprechend definiert als HBV-DNA- und HBsAg-Negativität mit oder ohne Serokonversion zu Anti-HBsAg [REF] nach antiviraler Behandlung. Bei chronisch infizierten Personen wird dies durch die derzeitigen Therapien mit NUC [REF] fast nie und nach IFNα nur selten erreicht. Das Ziel aktueller klinischer Studien ist es, nicht nur die Virusreplikation zu unterdrücken, sondern eine funktionelle Heilung der CHB herbeizuführen.Zur Behandlung der CHB sind derzeit nur NUC (z. B. Tenofovir und Entecavir) und IFNα/pegyliertes (Peg‑)IFNα zugelassen. Da NUC gut verträglich sind, aber nur selten zu einer HBsAg-Clearance und Serokonversion führen, werden sie häufig als Langzeit- oder lebenslange Therapie verabreicht. Durch die NUC-vermittelte Hemmung der HBV-Replikation werden i. d. R. die HBV-DNA im Serum verringert oder negativiert, die Alanin-Aminotransferase-(ALT-)Werte normalisiert und das Fortschreiten der Lebererkrankung verzögert oder verhindert [REF]. Im Vergleich zu NUC ist IFNα weniger wirksam in Hinblick auf den Rückgang der HBV-DNA, jedoch sind die Raten der HBsAg-Negativierung nach 48 Wochen Therapie höher, insbesondere bei Patienten mit niedrigen Ausgangswerten von HBsAg. Peg-IFNα wird i. d. R. bis zu 12 Monate lang unter strenger Überwachung angewendet, da es nur begrenzt verträglich ist und verschiedene potenziell ausgeprägte unerwünschte Arzneimittelwirkungen hat [REF].HDV ist ein RNA-Virus, welches die HBV-Hüllproteine zur Verbreitung seines viroidähnlichen Genoms benötigt. Eine HDV-Infektion manifestiert sich als Ko- oder Superinfektion von HBV-Trägern und verschlechtert deren Krankheitsprognose erheblich [REF]. Etwa 4,5–15 % der HBV-Patienten weltweit sind mit HDV koinfiziert. Dies könnte jedoch unterschätzt sein, da in vielen Ländern (einschließlich Industrieländern wie den USA oder China) bei chronisch HBV-infizierten Patienten nicht routinemäßig auf die HDV-Marker Anti-HDAg oder HDV-RNA getestet wird, obwohl dies in den meisten Leitlinien empfohlen wird. In einigen Ländern (z. B. in der Mongolei) weisen die Ergebnisse systematischer diagnostischer Screenings auf eine HDV-Prävalenz von > 60 % bei HBV-infizierten Patienten hin [REF].Die Entwicklung von Medikamenten zur Behandlung chronischer Hepatitis D (CHD) ist (u. a. durch Limitationen geeigneter HDV/HBV-Infektionssysteme, Schwierigkeiten, die wirtszellabhängige Replikation zu adressieren, aber auch mangelndes Interesse von Pharmaunternehmen) erschwert. Da NUC weder direkt in die HDV-RNA-Replikation eingreifen noch HBsAg unterdrücken, welches für die Bildung und die Verbreitung von HDV-Partikeln erforderlich ist, werden sie als „Backbonetherapie“ zur HBV-Unterdrückung bei CHD-Patienten eingesetzt. Im Gegensatz dazu hat IFNα eine direkte antivirale Wirkung auf HDV, wie in Fallstudien und mehreren klinischen Studien gezeigt wurde. Es wird bisher als Off-Label-Therapie bei geeigneten Patienten eingesetzt. IFNα hat jedoch nur eine begrenzte Wirksamkeit hinsichtlich der HDV-Suppression und nach Therapieende kommt es häufig zu einem Wiederanstieg der Viruslast, in vielen Fällen sogar nach erfolgter Serum-HDV-RNA-Negativierung [REF]. Daher werden dringend neue Therapien benötigt, um HBV/HDV-koinfizierte Patienten angemessen zu behandeln.Das klinisch am weitesten entwickelte Medikament zur Behandlung von HDV/HBV-Koinfektionen ist BLV, früher als Myrcludex B (MyrB) bezeichnet. Daneben gibt es weitere Ansätze in der klinischen Entwicklung: Lonafarnib, ein Farnesyltransferaseinhibitor; Interferon-Lambda (IFNλ), ein Typ-III-IFN mit höherem Lebertropismus im Vergleich zu IFNα, und Rep2139, ein Oligonukleotid mit komplexen Wirkmechanismen. Ein Überblick über diese Moleküle findet sich in [REF].
Das Fünfte Buch Sozialgesetzbuch (SGB V) verpflichtet nach § 135a alle Leistungserbringer „zur Sicherung und Weiterentwicklung der Qualität der von ihnen erbrachten Leistungen“. Darunter fällt für Krankenhäuser die Verpflichtung, einrichtungsintern ein Qualitätsmanagement einzuführen und weiterzuentwickeln, ein patientenorientiertes Beschwerdemanagement einzuführen und sich an einrichtungsübergreifenden Maßnahmen der Qualitätssicherung (QS) zu beteiligen. Diese allgemeinen Forderungen werden vom Gemeinsamen Bundesausschuss (G-BA) in Form von Richtlinien konkretisiert. So beschließt der G‑BA nach § 136b Richtlinien zur QS im Krankenhaus: (1) zu Nachweisen über die Erfüllung der Fortbildungspflichten der Fachärzte und der Psychotherapeuten, (2) zu Mindestmengen für planbare Leistungen, (3) zu Inhalt, Umfang und Datenformat eines jährlich zu veröffentlichenden strukturierten Qualitätsberichts und (4) zu Leistungen oder Leistungsbereichen, zu denen Qualitätsverträge erprobt werden sollen. Bis Juni 2021 galt seit der Änderung des § 136b im Jahr 2016, dass der G‑BA auch (5) Leistungen oder Leistungsbereiche definieren sollte, die sich für eine qualitätsabhängige Vergütung mit Zu- und Abschlägen eignen. Dieser Passus wurde mit dem Gesundheitsversorgungsweiterentwicklungsgesetz (GVWG) jedoch gestrichen.Unter den genannten gesetzlichen QS-Maßnahmen der akutstationären Versorgung stechen 2 Maßnahmen heraus, die für alle Krankenhäuser mit einem hohen Aufwand einhergehen. Diese Maßnahmen sollen im vorliegenden Beitrag fokussiert werden. Hierbei handelt es sich zum einen um die Qualitätsberichte (QB), die erstmals für das Jahr 2004 und anschließend zunächst zweijährlich anzufertigen waren. Seit 2014 müssen sie jährlich erstellt werden. Zum anderen geht es um die einrichtungsübergreifende, sogenannte externe Qualitätssicherung (eQS). Letztere geht auf die freiwilligen Einrichtungsvergleiche in der Chirurgie und Geburtshilfe zurück, wie sie von Ärzten bzw. Fachgesellschaften seit Anfang der 1970er-Jahre betrieben wurden [REF]. Hierbei dokumentieren die Leistungserbringer Daten zu Versorgungsprozessen und -ergebnissen, die zentral zusammengeführt, analysiert und in Form von Leistungserbringervergleichen zurückgemeldet werden. Die Vergleiche können anschließend im Rahmen des internen Qualitätsmanagements genutzt werden, um Verbesserungsmaßnahmen auszulösen.Seit 1989 verlangte das SGB V eine verpflichtende Teilnahme an der in einzelnen Bundesländern für verschiedene Leistungsbereiche eingeführten eQS. Mit der Einführung der Fallpauschalen und Sonderentgelte für einzelne Prozeduren wurde die eQS für diese Prozeduren ab 1996 bundesweit verpflichtend. Die gesetzlichen Krankenversicherungen befürchteten damals, dass die Einführung dieses Vergütungssystems einen Anreiz für Krankenhäuser darstellt, Gewinne dadurch zu erzielen, dass die Leistungen mit reduzierter Qualität erbracht würden [REF]. Ab 2001 wurden viele der zum Teil freiwilligen, auch in nicht von Fallpauschalen erfassten Leistungsbereichen etablierten eQS-Maßnahmen für alle Krankenhäuser verpflichtend, sodass in insgesamt 31 Leistungsbereichen eQS-Daten zu dokumentieren waren. Die administrative Abwicklung der eQS verantworten in den Bundesländern die (Landes‑)Projektgeschäftsstellen bzw. heute Landesarbeitsgemeinschaften und auf der Bundesebene zunächst die Servicestelle Qualitätssicherung (SQS), dann die Bundesgeschäftsstelle Qualitätssicherung (BQS), das Institut für angewandte Qualitätsförderung und Forschung im Gesundheitswesen (AQUA) und seit 2015 das Institut für Qualitätssicherung und Transparenz im Gesundheitswesen (IQTIG).Im Juni 2021 wurde die Richtlinie über Maßnahmen der QS in Krankenhäusern (QSKH-RL) durch die Richtlinie zur datengestützten einrichtungsübergreifenden QS (DeQS-RL) abgelöst. Diese beschreibt insgesamt 15 bundesweit verpflichtende Verfahren der eQS, die nun zum Teil nicht nur den stationären, sondern auch den ambulanten Sektor betreffen. Die Verfahren reichen von der Dekubitusprophylaxe, die alle Patienten ab 20 Jahren erfasst, über perkutane Koronarinterventionen mit sektorgleicher Gültigkeit bis hin zur Transplantationsmedizin, die nur wenige Krankenhäuser durchführen (Infobox 1). Beginnend mit dem Jahr 2001 waren eQS-Dokumentationen zu zunächst 10 %, ab 2002 zu 20 % der damals rund 16 Mio. Krankenhausfälle anzufertigen. Im Jahr 2019 waren im akutstationären Sektor rund 3 Mio. Fälle dokumentationspflichtig, also rund 16 % aller Krankenhausfälle [REF], wobei allein die Dokumentation bei den Leistungserbringern im Durchschnitt 15 min Arbeitszeit pro Fall benötigt [REF].Theoretisch fußen sowohl die eQS als auch die Qualitätsberichte implizit auf der sozialkognitiven Lerntheorie [REF], also dem Lernen am Modell: Die Rückmeldung der eigenen Leistung im Vergleich zur Leistung anderer bzw. das Bewusstsein über die Transparenz der eigenen Leistungen und die damit verbundenen Vergleichsmöglichkeiten für Patienten, ein- und überweisende Ärzte sowie Kostenträger soll die Leistungserbringer dazu motivieren, am Modell zu lernen, sich also an den jeweils besseren Leistungserbringern zu orientieren. Davon abgeleitet postulierten Berwick et al. [REF] 2 Wirkmechanismen von Vergleichsinformationen: Adressaten wählen auf der Basis der Vergleichsinformationen die jeweils besten Leistungserbringer aus („selection pathway“) oder die Leistungserbringer ändern die Versorgung, da sie zu Veränderungen bzw. zur Qualitätsverbesserung motiviert werden, wenn sie sich selber als schlechter im Vergleich zu anderen erkennen („change pathway“). Im Unterschied zur international üblichen Rückmeldung von Leistungsergebnissen auf der Basis unabhängig erhobener Daten („audit and feedback“) beruht die Rückmeldung in Deutschland jedoch auf selbstberichteten Daten, deren Validität im Rahmen der eQS nur zum Teil überprüft wird.Dennoch geht der Gesetzgeber laut Begründung zum GVWG davon aus, dass die Transparenz auf Grundlage der eQS und Qualitätsberichte zu Auswahl und Veränderung führe und zu erwarten sei, „… dass sich die Struktur‑, Prozess- und Ergebnisqualität bei den Leistungserbringenden mit der Neuregelung weiter stetig verbessern werden“ [REF].Ob sich diese hohen Erwartungen der Bundesregierung an die für alle Leistungserbringer aufwendigen Maßnahmen in der Versorgungsrealität erfüllen werden, ist fraglich. Um sich einer Beantwortung der Frage anzunähern, werden im Folgenden die bislang vorliegenden Erkenntnisse zu den Effekten dieser beiden gesetzlichen Qualitätssicherungsmaßnahmen in der akutstationären Versorgung auf der Basis einer selektiven Literaturanalyse und aktueller Forschungen der Autoren zusammengestellt.
Qualität hat im deutschen Gesundheitswesen einen hohen Stellenwert. So nennt das Fünfte Buch Sozialgesetzbuch (SGB V) Begriffe wie „Qualität“, „Qualitätssicherung“, „Qualitätsmanagement“ fast 400-mal in verschiedenen Kontexten: 1. Leistungen der gesetzlichen Krankenversicherung (GKV) haben in ihrer Qualität dem allgemein anerkannten Stand der medizinischen Erkenntnisse zu entsprechen (§ 2 Absatz 1 Satz 3 SGB V). 2. Wettbewerb zwischen Krankenkassen soll der Verbesserung der Qualität von Leistungen dienen (§ 4a Absatz 1 Satz 1 SGB V). 3. die Kassenärztliche Bundesvereinigung und der Spitzenverband Bund der Krankenkassen können im Bundesmantelvertrag Qualitätszuschläge vereinbaren (§ 87 Absatz 2b Satz 4). 4. der Gemeinsame Bundesausschuss (G-BA) beschließt verbindliche Richtlinien zur Qualitätssicherung (§ 92 Absatz 1 Satz 2 Nummer 13), inklusive Richtlinien zur Qualitätsbeurteilung in der vertragsärztlichen Versorgung (§ 135b Absatz 2 Satz 2 SGB V), zu Qualitätsberichten im Krankenhaus, Mindestmengen und anderen Qualitätssicherungsmechanismen im Krankenhaus (§ 136b SGB V).        Mit dem Gesetz wurde außerdem die Gründung des Instituts für Qualität und Wirtschaftlichkeit im Gesundheitswesen (IQWiG, nach § 139a SGB V) und des Instituts für Qualitätssicherung und Transparenz im Gesundheitswesen (IQTIG, nach § 137a SGB V) vorgeschrieben. Diese Institute nehmen inzwischen verschiedene Aufgaben zur Messung, Darstellung und Bewertung von qualitätsrelevanten Parametern wahr – mit zum Teil erheblichen Implikationen für die jeweils adressierten Marktteilnehmer. Ganz grundsätzlich sind überdies die Leistungserbringer, also Vertragsärzte, Krankenhäuser, medizinische Versorgungszentren etc. verpflichtet, ein internes Qualitätsmanagement (QM) einzuführen und sich an einrichtungsübergreifenden Maßnahmen der Qualitätssicherung (QS) zu beteiligen, mit dem Ziel der Verbesserung der Ergebnisqualität (§ 135a Absatz 2 SGB V).Abseits rechtlicher Verpflichtungen kann auch ein intrinsisches Interesse vieler Institutionen im Gesundheitswesen bestehen, Qualitätsmanagement zu betreiben. Letztlich sind Qualitätssicherung und -management Methoden, die in anderen Wirtschaftszweigen zur Steigerung der Effizienz entwickelt und eingesetzt werden, also die Wettbewerbsfähigkeit steigern [REF]. Darüber hinaus kann dadurch das Risiko möglicher zivilrechtlicher Schadensersatzansprüche (von Patienten) verringert werden. Insofern verwundert es nicht, dass sich neben gesetzlich vorgesehenen Qualitätssicherungsmechanismen auch private Initiativen diesem Ziel verschrieben haben (wie etwa die Initiative Qualitätsmedizin).Eine qualitativ hochwertige Versorgung der Versicherten kann mithin als eine der grundsätzlichen Zielgrößen des Gesundheitswesens angesehen werden. In Anbetracht dessen und vor dem Hintergrund der jüngsten gesetzgeberischen Maßnahmen zur Beförderung der Digitalisierung des Gesundheitssystems (z. B. dem Terminservice- und Versorgungsgesetz – TSVG, dem Digitale-Versorgung-Gesetz – DVG, dem Krankenhauszukunftsgesetz – KHZG, dem Patientendaten-Schutz-Gesetz – PDSG und dem Digitale-Versorgung-und-Pflege-Modernisierungs-Gesetz – DVPMG) stellt sich die Frage, ob die Digitalisierung im Gesundheitswesen auch der Steigerung der Qualität der Gesundheitsversorgung dient. So zumindest wird es vielfach postuliert [REF], zum Teil aber auch in Abrede gestellt [REF]. In der öffentlichen Diskussion wird dabei der Begriff der Qualitätssteigerung bisweilen sehr allgemein gefasst, ohne jedoch konkrete Konzepte des Qualitätsbegriffs im Gesundheitswesen zugrunde zu legen [REF].Der Aufsatz stellt Konzepte der Qualitätssicherung im Gesundheitswesen dar und zeigt anhand von Beispielen, wie der Einsatz digitaler Technologien in den Kontext dieser Konzepte gestellt werden kann.1 Ausgehend von diesen Beispielen soll die Frage beantwortet werden, ob die Annahme der positiven Effekte von digitalen Lösungen auf die Qualität grundsätzlich haltbar ist. Schließlich soll induktiv diskutiert werden, welche Eigenart digitaler Technologien diese für Qualitätssicherung besonders geeignet erscheinen lässt.
Versicherte der gesetzlichen Krankenversicherung (GKV) haben seit dem GKV-Versorgungsstärkungsgesetz 2015 in Deutschland einen Anspruch auf ärztliche Zweitmeinung vor bestimmten planbaren Operationen (§ 27b SGB V). Eingriffe an der Wirbelsäule wurden allerdings erst mit Inkrafttreten zum 19.11.2021 in die Liste der anspruchsberechtigenden Operationen aufgenommen [REF]. Seit 2015 nutzen daher Krankenkassen wie die AOK Nordost Verträge zur besonderen Versorgung (§§ 140 SGB V), um Ihren Versicherten selbst konzipierte Zweitmeinungsverfahren bei Rückenoperationen (ROP) anzubieten und diese auch zu evaluieren.In 2018 gab es in Deutschland 815.295 ROP-Krankenhausfälle mit Operationen- und Prozedurenschlüssel (OPS) Kapitel 5-83 [REF]. In 2018 kostete ein solcher ROP-Krankenhausfall bei der AOK Nordost durchschnittlich 8960 €. Es gibt Hinweise, dass ROP teils keinen Nutzen generieren und teils aufgrund wirtschaftlicher Anreize des Krankenhaus-Vergütungssystems durchgeführt werden [REF].Für ROP fehlen bislang kontrollierte Studien zu Zweitmeinungsverfahren, die in einer Nachbeobachtung die tatsächlich erfolgten Rückenoperationen (ROP) sowie die rückenschmerzbezogenen Kosten (RBK) mit einer Vergleichsgruppe analysieren [REF]. Die Information zu einer ROP-Erstempfehlung liegt in der Regel nicht systematisch in den Abrechnungsdaten der GKV vor. In dieser Studie gelang es, ROP-Erstempfehlungen systematisch über ärztliche Krankenhauseinweisungsscheine zu identifizieren, die der AOK Nordost vorgelegt wurden. Diese Informationsbasis ermöglichte einen Matched-pairs-Vergleich von Versicherten mit einer ROP-Erstempfehlung „mit“ versus „ohne“ besondere Versorgung mit einem Zweitmeinungsverfahren.Die AOK Nordost bietet ihren Versicherten seit dem 15.04.2015 das Versorgungsprogramm RückenSPEZIAL als Vertrag über eine besondere Versorgung nach §§ 140a ff. SGB V an. Versicherte mit Krankenhauseinweisungsschein für eine ROP können teilnehmen und erhalten in 5 ambulanten Rückenzentren in Berlin und Brandenburg eine Vorbefundprüfung, ein interdisziplinäres Zweitmeinungsverfahren (IZMV) sowie auf Empfehlung optional eine interdisziplinär-multimodale Schmerztherapie (IMST). In der Vorbefundprüfung durch den Arzt werden fakultative Ausschlusskriterien zur Orientierung geprüft (siehe Tab. 2). Das IZMV ist zentrale Vertragsleistung. Es enthält ein ca. dreistündiges Assessment mit ärztlicher Diagnostik, schmerzpsychologischem Screening und physiotherapeutischer Befunderhebung, eine interdisziplinäre Fallkonferenz mit Einbindung/Kontaktierung des Erstmeiners, ein Versichertengespräch mit individueller Therapieempfehlung sowie die Arztbrieferstellung. Auf Empfehlung des IZMV hin ergeben sich drei Subgruppen der Weiterbehandlung nach dem IZMV (Abb. 1). Im Rahmen der IMST [REF] erbringen in einem ambulanten Versorgungssetting Fachärzte für Orthopädie, physikalische und Rehabilitationsmedizin, Anästhesiologie, Ärzte mit Zusatzqualifikation Schmerztherapeut, Psychotherapeuten, Sozialtherapeuten, Physiotherapeuten, Sporttherapeuten, Ergotherapeuten je nach Versichertensituation drei mögliche Leistungspakete: (i) Vollzeitteilnahme für 100 h in 20 Tagen, (ii) Vollzeitteilnahme für 60 h in ≤ 4 Wochen, (iii) berufsbegleitende Teilnahme für 40–50 h in ≤ 3 Monaten. Die IMST umfasst als ambulante modulare Gruppentherapie die Komponenten Schmerztherapie, Edukation, Psycho-, medizinische Trainings- und Ergotherapie sowie regelmäßige interdisziplinäre Fallkonferenzen.Ziel dieser Studie ist es, die Wirksamkeit des Vertrags RückenSPEZIAL hinsichtlich einer Reduktion der ROP und RBK im Folgejahr zu analysieren. Verglichen wird mit Patienten, die gleichermaßen eine ärztliche Erstmeinung für eine ROP, aber keine nachfolgende Zweitmeinung und ggf. IMST erhalten haben.
Die COVID-19-Pandemie (englisches Akronym für: Coronavirus Disease 2019) hat die wissenschaftsbasierte Risikokommunikation vor neue Herausforderungen gestellt. Dem Wissenschaftsbarometer 2020 [REF] zufolge ist das Vertrauen der deutschen Bevölkerung in Wissenschaft und Forschung zwar höher als in den Jahren zuvor, wohl aber ist der Anteil der Befragten, die angaben, Wissenschaft und Forschung eher oder voll und ganz zu vertrauen, von anfänglich 73 % im April 2020 auf zwischenzeitlich 60 % im Dezember 2020 gefallen. Ähnliches zeigt sich in vielen anderen Ländern [REF]. Somit wird deutlich, wie volatil der Vertrauensvorsprung, welcher wissenschaftsbasierter Risikokommunikation zugeschrieben wird, sein kann. Diverse Herausforderungen stellen sich hier besonders für Organisationen wie das Bundesinstitut für Risikobewertung (BfR), dessen Aufgabe es ist, auf Basis wissenschaftlicher Stellungnahmen – zum Beispiel im Zusammenhang mit Viruskontaminationen auf Lebensmitteln und Oberflächen – politische Entscheidungsträger zu beraten und die Öffentlichkeit zu informieren.So werden einerseits Risikozusammenhänge zum Beispiel aufgrund globaler Waren- und Produktionsketten, aber auch aufgrund veränderter Konsumtionsmuster immer komplexer, weshalb auch die Kommunikation solcher Risiken diffiziler wird. Zum anderen werden die Informationsdichte und -heterogenität zu einer Herausforderung für die Risikokommunikation [REF]. Wissenschaftliche Studien, Gutachten oder Stellungnahmen werden in ihren Schlussfolgerungen auch dadurch rechtfertigungsbedürftig. Dies gilt ganz besonders für Organisationen wie das BfR, die sich in kontroversen Feldern wie dem der Ernährung bewegen [REF]. Ernährung bedeutete im Lauf der Menschheitsgeschichte nie nur die reine Aufnahme von Nahrungsmitteln, sondern ist vielmehr zu einem Gegenstand des Genusses und Erlebens, des Wissens und der Kompetenzen, der Performanz und des sozialen Prestiges, aber auch der Sorge avanciert [REF]. Die sich daraus ergebende Konsumentennähe zu Lebensmittelprodukten wird konterkariert von einer zunehmenden Globalisierung und Komplexität in der Produktion von Lebensmitteln [REF]. So wurde die globale Lebensmittelproduktion das Subjekt zahlreicher Krisen, die nicht nur punktuell Produkte, sondern gesamte Vertrauensbildungsprozesse infrage stellten.In der Tat kann bereits die Gründung des BfR als eine Herausforderung bezeichnet werden. Diese fand im Rahmen einer fundamentalen europaweiten Neuordnung des Lebensmittelsicherheitssystems statt, die das im Kontext der BSE-Krise1 geschwächte öffentliche Vertrauen durch eine klarere Trennung von Politik und Wissenschaft sowie eine ausdrücklichere Fokussierung auf Verbraucherinteressen stärken sollte [REF]. Gegründet als unabhängige regulierungswissenschaftliche Organisation2 im Geschäftsbereich des Bundesministeriums für Ernährung und Landwirtschaft (BMEL) sind die Hauptaufgaben des BfR die Bewertung bestehender und das Erkennen neuer gesundheitlicher Risiken in den Bereichen Lebensmittel‑, Produkt- und Chemikaliensicherheit, die Erarbeitung von Empfehlungen zur Risikobegrenzung sowie die transparente Kommunikation dieses Prozesses. Um diese Tätigkeiten zu unterstützen, betreibt das BfR auch eigene sozialwissenschaftliche Forschung auf den Gebieten der Risikokommunikation und der Risikowahrnehmung.Die Einrichtung regulierungswissenschaftlicher Organisationen steht im engen Zusammenhang mit einer Neuordnung entlang des sogenannten Red-Book-Modells des United States National Research Council (NRC). In der vorherrschenden Auslegung dieses Modells sollen politische Entscheidungen, zum Beispiel im Bereich der Lebensmittelsicherheit, durch wissenschaftliche Bewertungen informiert werden, die wiederum frei von politischen Einflüssen verfasst wurden [REF]. So soll das BfR „durch eine klare organisatorische Trennung von den politisch geprägten Strukturen des Risikomanagements … frei von äußerer Einflussnahme und unabhängig sein“ [REF]. In der Praxis sieht sich regulatorisches Wissen [REF] jedoch mit einer Vielzahl an Erwartungen und Zieldefinitionen konfrontiert. Für die Risikokommunikation ergeben sich daraus verschiedene Herausforderungen – nicht nur, weil die Risikowahrnehmung von Bürgerinnen und Bürgern durch psychologische, soziale und kulturelle Faktoren [REF] sowie politische Präferenzen [REF] konturiert wird und Einstellungen gegenüber bestimmten Organisationen die Wahrnehmung spezifischer Risiken beeinflussen können [REF]. Im öffentlichen Diskurs artikulieren sich zudem immer wieder politische Ansprüche, die auch unmittelbar an Organisationen wie das BfR adressiert werden – trotz des institutionellen Rahmens, welcher das BfR von politischen Ansprüchen immunisieren soll [REF]. Vor diesem Hintergrund skizziert dieser Beitrag anhand des Beispiels des BfR praktische Herausforderungen für die wissenschaftsbasierte Risikokommunikation und diskutiert mögliche Lösungsansätze sowie Forschungsbedarfe.
In Zeiten von Bedrohungen der kollektiven Sicherheit oder Krisen erwarten die Menschen von Politik und gesellschaftlichen Entscheidungsgremien wirksame Maßnahmen zur Eindämmung der Risiken sowie eine vertrauenswürdige Kommunikation über die Hintergründe und Beweggründe für diese Maßnahmen [REF]. Wie sehr diese Erwartung erfüllt bzw. enttäuscht werden kann, hat die Coronakrise allen deutlich vor Augen geführt [REF]. Konnte man in der ersten Welle im Jahr 2020 noch von einem Schulterschluss zwischen Regierung, Wirtschaft, Zivilgesellschaft und allgemeiner Öffentlichkeit ausgehen, zerbrach diese Übereinstimmung im Verlauf der zweiten und dritten Welle; zunehmend breitete sich Misstrauen zwischen den verschiedenen gesellschaftlichen Kräften aus.Zentral für dieses wachsende Misstrauen im Verlauf der Coronakrise ist die Art und Weise, wie über die Risiken der Pandemie und die notwendigen Maßnahmen kommuniziert wurde. Auftrag öffentlicher Institutionen zur Gesundheitsfürsorge und zur Gesundheitserhaltung ist es, Entscheidungen zu treffen und Maßnahmen zu erlassen, die das Leben, die Gesundheit und das Wohlergehen der Bürgerinnen und Bürger schützen und die Umwelt erhalten sollen. Die Entscheidungen müssen zum einen dem Kriterium der Effektivität im Sinne der nachweislichen Wirksamkeit entsprechen, zum anderen aber die Regeln und Kriterien der demokratischen Entscheidungsfindung im föderalen System der Bundesrepublik Deutschland einhalten [REF]. Der Schutzauftrag muss so weit erfüllt werden, dass die zentralen gesellschaftlichen Werte, vor allem die, die im Grundgesetz festgelegt sind, nicht zur Disposition stehen [REF]. Beispiele dafür sind die Abwehr von Gesundheitsgefahren, die Einhaltung von Grenzwerten, die Vorsorge gegen katastrophale Ereignisse, aber auch nachvollziehbare Regeln für Abwägung zwischen gesundheitlichen, wirtschaftlichen und sozialen Zielsetzungen.Gesellschaftliche Legitimität und Akzeptanz entstehen nicht durch einseitige Information oder durch öffentliche Belehrung. Legitimität und Akzeptanz entstehen durch verständigungsorientierte Dialoge, bei denen alle beteiligten Akteure gemeinsam lernen und nach tragbaren Handlungsoptionen suchen [REF].So viel zur Theorie. In der Praxis ist verständigungsorientierte Krisen- und Risikokommunikation keine leichte Aufgabe. In gesellschaftlichen Debatten über Risiken wie Infektionsschutz, Klimaschutz, Vorsorge gegen Unfälle oder Naturgefahren treffen die unterschiedlichsten Interessen und Anliegen zusammen. Oft erscheint es kaum möglich, einen gemeinsamen Nenner zu finden und sich über die Situation selbst und über mögliche Lösungswege zu verständigen. Meistens verschärfen sich die Konflikte im Verlauf der Debatte, man wirft sich gegenseitiges Unverständnis vor [REF]. Viele öffentliche Institutionen geraten dabei zunehmend unter Druck.Wie kommt man in dieser zugespitzten Situation zu einer vertrauensvollen Kooperation? Zentral für die Verständigung zwischen Institutionen der Risikovorsorge und des Risikomanagements und der Bevölkerung ist zum einen eine professionell und effektiv gestaltete Risikokommunikation, aber zum anderen die Schaffung und der Erhalt eines gegenseitigen Vertrauensverhältnisses zwischen den Akteuren und mit der Bevölkerung [REF]. Im Folgenden werde ich mich vor allem auf die Schaffung und Wahrung einer Vertrauensbasis zwischen Institutionen des Risikomanagements und den von Risiken betroffenen Menschen und Gruppen konzentrieren. Die beiden wesentlichen Fragen, die ich im Sinne eines narrativen Reviews aus Literatur und eigener Kommunikationspraxis beantworten möchte, lauten:Welche Faktoren sind maßgeblich dafür verantwortlich, ob und in welchem Ausmaß eine Vertrauensbasis und Glaubwürdigkeit geschaffen werden kann? Sowie: Wie lässt sich Vertrauen in Krisenzeiten schaffen bzw. erhalten?
Die Website gesundheitsinformation.de ist ein Angebot des Instituts für Qualität und Wirtschaftlichkeit im Gesundheitswesen (IQWiG). Das IQWiG hat den gesetzlichen Auftrag, für erkrankte sowie für gesunde Bürgerinnen und Bürger kostenlos unabhängige und wissenschaftlich geprüfte Informationen zur Verfügung zu stellen. Dieser Auftrag wird vor allem über 2 Wege umgesetzt. Zum einen erarbeitet und aktualisiert das IQWiG Informationen für einen Katalog von Themen, der insbesondere häufige Krankheiten, Diagnosen und Gesundheitsfragen umfasst. Dieser wird den Bürgerinnen und Bürgern auf der Website gesundheitsinformation.de zur Verfügung gestellt. Die Website umfasst derzeit Informationspakete zu gut 200 Erkrankungen und Behandlungsmöglichkeiten. Das IQWiG erhält zum Zweiten auch direkt vom Gemeinsamen Bundesausschuss (G-BA) oder vom Bundesministerium für Gesundheit (BMG) Aufträge, Patienteninformationen zu bestimmten Themen zu erstellen.Methoden und Prozesse der Erstellung leiten sich generell aus der Verpflichtung des IQWiG gegenüber der evidenzbasierten Medizin ab. Für Gesundheitsinformationen bedeutet das, dass zum einen die Inhalte der Informationen dem aktuellen Stand des Wissens entsprechen müssen. Zum anderen folgen die Gestaltung und Kommunikation konsentierten Qualitätsanforderungen. Ziele der Gesundheitsinformationen des IQWiG sind u. a., das Verständnis von Vor- und Nachteilen wichtiger Behandlungsmöglichkeiten zu erhöhen und die Gesundheitskompetenz zu stärken.Das IQWiG wurde im Jahr 2014 vom G‑BA mit der Erstellung eines Einladungsschreibens und einer Entscheidungshilfe zum Mammografiescreening beauftragt (Projektnummer P14-03; [REF]). Ein Jahr später folgten Aufträge zum Darmkrebsscreening (Projektnummer P15-01; [REF]) und zum organisierten Zervixkarzinomscreening (Projektnummer P15-02; [REF]). Diese Früherkennungsprogramme sind im § 25 des Fünften Sozialgesetzbuches (SGB V) verankert. Dieser Paragraf wurde 2013 mit dem „Krebsfrüherkennungs- und -registergesetz – KFRG“ reformiert [REF] und dabei die gesetzliche Anforderung verankert, dass eine „umfassende und verständliche Information der Versicherten über Nutzen und Risiken der jeweiligen Untersuchung“ erfolgen muss [REF]. In der Gesetzesbegründung dieser Änderung formulierte der Gesetzgeber klare Ansprüche an die Inhalte der Information. Denn auch bevölkerungsmedizinisch sinnvolle und empfehlenswerte Krebsfrüherkennungsmaßnahmen beinhalten für die gesunde bzw. beschwerdefreie Person ein Risiko. Hierzu gehören – neben den Risiken der Untersuchung selbst – die Konsequenzen falsch-negativer oder falsch-positiver Testbefunde, invasive Abklärungsuntersuchungen (z. B. die Entnahme von Gewebeproben) sowie die mögliche Diagnose und Behandlung von Krebserkrankungen, von denen die Person ohne die Früherkennung in ihrem Leben nie etwas gemerkt hätte. Solche Risiken lassen sich auch durch die bestmögliche Qualitätssicherung nicht in allen Fällen vermeiden, sondern allenfalls minimieren. Daher sollte das Inanspruchnahmeverhalten der einzelnen Person allein durch eine ausreichende, neutrale und verständliche Information und Beratung sowie deren individuelle Werte und Präferenzen bestimmt sein und nicht durch Anreizsysteme beeinflusst werden. Dies entspricht auch den Empfehlungen des Rates der Europäischen Union, der Europäischen Leitlinien und der Expertinnen und Experten des Nationalen Krebsplans, welche die eigenständige, freiwillige und informierte Entscheidung über die Teilnahme in den Vordergrund stellen. Hierbei ist das Ziel einer informierten individuellen Entscheidung dem Ziel einer möglichst hohen Teilnahmerate übergeordnet [REF].Tatsächlich entspricht diese Neutralität auch dem Kernverständnis evidenzbasierter Gesundheitsinformationen, die Patientinnen und Patienten unterstützen wollen, informierte gesundheitliche Entscheidungen zu treffen [7, S. 135 ff.]. Die Mehrzahl der Patientinnen und Patienten bevorzugt es, solche Entscheidungen gemeinsam mit Ärztinnen, Ärzten und anderen Gesundheitsfachkräften vorzubereiten. Diese sogenannte gemeinsame Entscheidungsfindung kann durch Entscheidungshilfen (Decision Aids) unterstützt werden. Dabei handelt es sich um spezielle Informationsformate, wie z. B. Broschüren oder Fragebögen, die Nutzerinnen und Nutzer befähigen sollen, gemeinsam mit Ärztinnen und Ärzten oder Angehörigen anderer medizinischer Berufsgruppen informierte, den persönlichen Präferenzen entsprechende medizinische Entscheidungen zu treffen [REF]. Studien zeigen, dass Entscheidungshilfen Wissen vermehren und die Risikoeinschätzungen verbessern, Entscheidungskonflikte mindern und die Einbindung im Sinne einer gemeinsamen Entscheidungsfindung (Shared Decision Making – SDM) fördern können [REF]. Auch Entscheidungshilfen zur Krebsfrüherkennung können sich positiv auf die Entscheidungsfindung auswirken [REF]. Menschen, die Entscheidungshilfen nutzen, sind zufriedener mit den erlebten Entscheidungsprozessen.Die vorliegende Arbeit beschreibt den Entwicklungsprozess der 3 Entscheidungshilfen zu den Krebsfrüherkennungsprogrammen. Dabei werden ausgewählte Ergebnisse der qualitativen und quantitativen Nutzertestungen präsentiert und diskutiert.
Im Jahre 2018 wurde der Nobelpreis für Physiologie und Medizin dem US-Amerikaner James P. Allison und dem Japaner Tasuku Honjo für die Entdeckung der Krebstherapie durch Hemmung der Immunregulation verliehen. Sie hatten die Bedeutung der CTLA4(„cytotoxic T‑lymphocyte antigen 4“)- und PD1(„programmed death receptor 1“)-Signalwege, über welche die Regulation der Immunabwehr gesteuert wird, untersucht und als erste gezeigt, dass durch Medikamente diese physiologisch sinnvolle Blockade des Immunsystems aufgehoben werden kann, um Tumoren in Remission zu bringen. Dabei wird mit Antikörpern die Interaktion der genannten Rezeptoren mit ihren Liganden effektiv unterdrückt und so der Immunzelle freie Bahn zur Abwehr einer Tumorzelle gegeben. Beim metastasierten malignen Melanom ist der Erfolg dieses völlig neuen Therapieprinzips erstmals beschrieben worden, die Prognose konnte eindrucksvoll verbessert werden.Die physiologische Hemmung des Immunsystems ist ein sinnvoller Prozess, da er eine unkontrollierte Immunreaktion gegen körpereigenes Gewebe unterdrückt. Dies erklärt somit auch das völlig neue Spektrum an Nebenwirkungen beim Einsatz dieser Medikamente. Beschrieben werden dementsprechend Autoimmunreaktionen an fast allen Körpersystemen: Thyreoiditis, Adrenalitis, Hypophysitis, Hepatitis, Kolitis, Dermatitis, Nephritis, Karditis, Neuritis etc. Diese Reaktionen ähneln den Symptomen rheumatischer Erkrankungen; dementsprechend wird spekuliert, dass Defekte der Interaktion zwischen PD1 und PD-L1 („programmed cell death 1 ligand 1“) mit Funktionsstörungen der regulierenden T‑Zellen an der Entwicklung von Autoimmunerkrankungen beteiligt sein könnten [REF].Grundsätzlich kommen 3 Therapieprinzipien in Frage, die sich gegenseitig ergänzen können, aber ein etwas unterschiedliches Nebenwirkungsspektrum haben: 1. PD‑1 wird vor allem auf B- und T‑Lymphozyten exprimiert und gilt als Inhibitor der Immunabwehr [REF]. In der gynäkologischen Onkologie werden als PD-1-Antikörper vor allem Pembrolizumab (zugelassen bei Mammakarzinom, Zervixkarzinom und Endometriumkarzinom), Dostarlimab (zugelassen beim Endometriumkarzinom) und experimentell Nivolumab (Mammakarzinom, Zervixkarzinom) eingesetzt. Als immunologische Nebenwirkungen (irAE) werden Fatigue (ca. 30 %), Hautausschläge und Pruritus (bis zu 30 %), Diarrhöen/Kolitis (20 %), Funktionsstörungen der Schilddrüse (15 %), Arthralgien (> 10 %), Anämie (3 %), Pneumonitis (3 %), Hepatitis (3–6 %), Fieber (ca. 7 %), Nebenniereninsuffizienz (ca. 2,5 %) und Hypophysitis (1–2 %) beschrieben; auch höhergradige Toxizitäten (Grad 3/4) werden beobachtet (insgesamt 6–11 %; [REF]). 2. PD-L1 wird sowohl auf Immunzellen (T-, B‑Zellen, Monozyten, APC [„antigen presenting cells“]) als auch auf epithelialen Zellen exprimiert und ist einer der beiden spezifischen Liganden von PD‑1 [REF]. In der gynäkologischen Onkologie eingesetzte, gegen PD-L1 gerichtete Antikörper sind Atezolizumab (zugelassen beim Mammakarzinom), sowie experimentell Durvalumab (Mammakarzinom, Endometrium, Zervixkarzinom), Avelumab (Ovarialkarzinom) und andere. Als Nebenwirkungen werden beschrieben: Hautreaktionen (Rash; 12–34 %), Funktionsstörungen der Schilddrüse (9,5–23 %), Pneumonitis (4 %), Hepatitis (2–6 %), Kolitis/Diarrhoe (1–8 %), Nebenniereninsuffizienz (< 1 %); höhergradige Toxizitäten (Grad 3/4) sind seltener als bei anti-PD1-Therapie (insgesamt 3,5 %) [REF]. 3. CTLA4 wird vor allem auf T‑Zellen exprimiert und steuert z. B. die Interaktion der T‑Zellen mit den Antigen-präsentierenden dendritischen Zellen [REF]. In der gynäkologischen Onkologie wurde bisher nur im experimentellen Ansatz Ipilimumab in Kombination mit Nivolumab beim metastasierten Mammakarzinom geprüft. Als Hauptnebenwirkungen werden Leberwertveränderungen (35 %), Nebenniereninsuffizienz 24 %, Durchfall (24 %), außerdem makulopapulöse Exantheme (18 %), Hypothyreose/Hyperthyreose (18 %), Kolitis (12 %), Bilirubinanstieg (12 %), Lipaseanstieg (12 %) und Pruritus (12 %) beschrieben [REF]. Insgesamt scheinen Anti-PD-L1-Antikörper im Vergleich zu Anti-PD1-Antikörper etwas besser verträglich zu sein, so führen sie wohl seltener zum Therapieabbruch (Hazard Ratio [HR] 0,46; 95 %-Konfidenzintervall [KI] 0,19–0,95; [REF]; Tab. 1).
Die Prävalenz des Alkoholkonsums und insbesondere von riskantem Konsum, wie z. B. Rauschtrinken, ist weiterhin hoch, auch wenn laut aktuellen Daten der Alkoholkonsum unter Jugendlichen in Europa in den letzten Jahren leicht, aber stetig zurückgegangen ist [REF]. So gaben im European School Survey Project for Alcohol and other Drugs von 2019 (ESPAD) 90 % der 15- und 16-jährigen Schüler*innen in Deutschland an, mindestens einmal in ihrem Leben Alkohol konsumiert zu haben [REF]. 65 % berichteten, in den letzten 30 Tagen Alkohol konsumiert zu haben, und 20 % hatten in den letzten 30 Tagen mindestens einmal 5 oder mehr alkoholische Getränke bei einem Anlass konsumiert (Rauschtrinken; [REF]). Die Drogenaffinitätsstudie der Bundeszentrale für gesundheitliche Aufklärung (BZgA) 2019 weist für 12- bis 17-Jährige in Deutschland eine Alkoholkonsum-Lebenszeitprävalenz von 63 % aus, in Bezug auf Rauschtrinken liegt diese bei 15 % [REF].Im Hinblick auf Geschlechtsunterschiede zeigen die ESPAD-Daten für Deutschland eine höhere 30-Tages-Prävalenz unter Mädchen gegenüber Jungen (68 % vs. 63 %), jedoch geben Jungen geringfügig häufiger als Mädchen an, betrunken gewesen zu sein (21 % vs. 19 %). Dies deckt sich mit den Daten der Drogenaffinitätsstudie, welche unter Jungen eine weitere Verbreitung von Rauschtrinken aufweist als unter Mädchen [REF]. Zudem gibt es Unterschiede, je nachdem welcher Schultyp besucht wird: So zeigte sich bei Jugendlichen an Gymnasien eine signifikant niedrigere Prävalenz des regelmäßigen und des riskanten Alkoholkonsums als bei Jugendlichen, die andere Schultypen besuchen [REF].Insgesamt weisen die Daten zum Alkoholkonsum unter Schüler*innen in Deutschland einen Handlungsbedarf auf, effektive präventive Ansätze in diesem Bereich zu stärken und weiterzuentwickeln sowie diese an spezifische Bedürfnisse der Adressat*innengruppe anzupassen. Solch eine Primärprävention bei Jugendlichen ist entscheidend, da Alkoholkonsum ein wesentlicher Risikofaktor für eine Reihe von gesundheitlichen, sozialen und möglichen rechtlichen Konsequenzen ist [REF]. So ist ein früher Alkoholkonsumbeginn assoziiert mit einer höheren Wahrscheinlichkeit für Alkoholmissbrauch im späteren Leben sowie der Entwicklung von Alkoholabhängigkeit [REF]. Des Weiteren ist Alkoholkonsum in der Jugend mit unerwünschten Schwangerschaften, Kriminalität und Schulversagen verbunden [REF].Präventionsmaßnahmen kommen bei Jungen und Mädchen unterschiedlich an und variieren in ihrer Wirkung [REF]. Wenn Jungen erreicht werden, zeigten sich bei Programmen zunächst teilweise stärkere, langfristig aber wieder nachlassende Effekte in der Alkoholreduktion als bei Mädchen [REF]. Für Jungen sind interaktive Komponenten bei Präventionsangeboten besonders wichtig, während für Mädchen die Vermittlung von verhaltensbezogenen Lebenskompetenzen bedeutender sind [REF]. Insgesamt sind Ansprache, Einbindung und Anpassung von Präventionsprogrammen an die Bedürfnisse von Jugendlichen zentrale Aspekte für wirksame Präventionsprogramme [REF].Ein besseres Verständnis dessen, was Jugendliche dazu veranlasst, Alkohol zu konsumieren, hat zu optimierten Präventionsprogrammen geführt. Als einer der bedeutendsten Gründe für den Beginn und die Beibehaltung des Alkoholkonsums gilt dabei Gruppendruck unter Jugendlichen [REF]. Obwohl diverse, an sozialen Einflüssen einsetzende Präventionsprogramme in ihrer Wirkung keine einheitlichen Ergebnisse zeigen, werden solche Ansätze in Reviews hingegen insgesamt positiv bewertet [REF]. Die Mehrzahl der als effektiv bewerteten Präventionsprogramme, die im deutschen Schulsetting durchgeführt und evaluiert wurden, umfassen Aspekte eines Kompetenztrainings und adressieren u. a. die Fertigkeiten, Gruppenzwang zu widerstehen (z. B. Aktion Glasklar, ALF, IPSY, Klasse 2000, Unplugged (s. Kurzbeschreibungen auf grüne-liste-praevention.de)).Dieser Artikel stellt dar, inwieweit schulische Alkoholprävention Virtual Reality (VR) nutzen kann, um die Kompetenzen von Schüler*innen im Umgang mit Alkohol zu stärken. Dabei werden ein Überblick über die bisherigen Forschungs- und Entwicklungsprojekte und Evaluationsergebnisse in diesem Bereich gegeben, auf Möglichkeiten der zugeschnittenen Konzipierung von Interventionen nach Geschlecht eingegangen und die Chancen und Grenzen dieser Ansätze für die schulische Alkoholprävention herausgearbeitet.
Unter dem Begriff Aphasie versteht man eine erworbene Sprachstörung infolge einer Hirnverletzung, die meist im Bereich der linken Hemisphäre des Gehirns verortet ist [REF]. Merkmale einer Aphasie sind Störungen der verbalen und schriftsprachbasierten Kommunikation, wodurch die Modalitäten Lesen, Schreiben, Benennen und Sprachverstehen leicht bis schwer beeinträchtigt sein können [REF]. Ursachen für eine Aphasie können Schlaganfälle, Hirntumore, Schädel-Hirn-Traumata und entzündliche Krankheiten sein. In Zeiten der demografischen Alterung kann mit einem weiteren Anstieg von Aphasien infolge eines Schlaganfalles gerechnet werden [REF]. Untersuchungen zufolge liegt die Inzidenzrate für eine Aphasie infolge eines erstmaligen Schlaganfalles bei 43/100.000 Einwohner*innen [REF]. Ausgehend von einer deutschen Gesamtbevölkerung von 83,1 Millionen, lag die Inzidenzrate 2019 demzufolge bei etwa 35.000 Einwohner*innen [REF]. Abhängig von dem Ort und der Schwere der Läsion, kann eine Aphasie verschiedene Formen der sprachlichen Beeinträchtigung annehmen und unterschiedlich klassifiziert werden [REF]. Eine der häufigsten Manifestationen von Aphasien sind Wortfindungsstörungen, bei denen das Schreiben und Benennen von Wörtern sowie das Zuordnen der Wortbedeutung gestört sein können [REF]. Ursachen für Wortfindungsstörungen können Defizite des semantischen Systems, des phonologischen Lexikons oder beider Komponenten sein [REF]. In der Sprachtherapie wird die Schwere von Wortfindungsstörungen unter anderem anhand der Benennleistung (z.B. Anzahl der sprachlich richtig benannten Items) mit standardisierten Testverfahren diagnostiziert [REF]. Aus Beobachtungsstudien ist bekannt, dass ein partieller bis vollständiger Erholungsprozess nach dem Erwerb einer Aphasie stattfifnden kann, der als Netzwerkreorganisation bezeichnet wird [REF]. Die größten Verbesserungen der Sprachfähigkeit können während der ersten Tage und Wochen nach Ereignis der Aphasie beobachtet werden. Nach etwa sechs Monaten erreicht der Erholungsprozess ein Plateau [REF]. Ab diesem Zeitpunkt dauert es länger, merkbare Fortschritte der Sprachfähigkeit bei einer Aphasie zu erzielen [REF]. Es wurde in verschiedenen Studien belegt, dass eine Sprachtherapie die Prozesse der Netzwerkreorganisation verstärken kann [REF]. Des Weiteren liegen Evidenzen vor, dass der Erfolg einer Sprachtherapie von der Höhe der wöchentlichen Therapiefrequenz abhängt [REF]. Demzufolge bewirkt eine Sprachtherapie an mindestens fünf Tagen in der Woche signifikant bessere Ergebnisse als eine niedrigere Therapiefrequenz [REF]. In einer prospektiven randomisierten Aphasieversorgungsstudie von Breitenstein et al. [REF], welche 156 Studienteilnehmer*innen mit chronischer Aphasie untersuchte, konnten signifikante Verbesserungen der verbalen Kommunikationsleistung nach einer mindestens dreiwöchigen Intensivsprachtherapie (mindestens zehn Stunden Sprachtherapie und fünf Stunden Eigenübungen pro Woche) gemessen werden, die auch sechs Monate nach der Intervention fortbestanden. Dieser Effekt konnte lediglich für den Zeitraum der Intensivsprachtherapie, nicht aber für den vorangegangenen dreiwöchigen Wartezeitraum der Kontrollgruppe gemessen werden, welcher mit eineinhalb Stunden Sprachtherapie pro Woche begleitet wurde.Eine Aphasie geht häufig mit körperlichen Einschränkungen einher, wodurch es für betroffene Menschen nicht oder nur im begrenzten Maße möglich ist, den Weg zu einer sprachtherapeutischen Praxis auf sich zu nehmen [REF]. Darüber hinaus zählt ein Großteil der von einer Aphasie betroffenen Menschen in Zeiten der COVID-19-Pandemie zur Risikogruppe und sollte daher weitgehend direkte Kontakte vermeiden [REF]. Um auch diesen Patientengruppen eine Therapie zu ermöglichen, können telemedizinische Modelle herangezogen werden. Aufgrund der Kontaktbeschränkungen im Rahmen der COVID-19-Pandemie war die Screen-to-Screen Therapie (STST) für die Heilmittelberufe in Deutschland von Mitte März bis zum 30. Juni 2020 zugelassen [REF]. Bei einer STST findet die Kommunikation zwischen Patient*in und Therapeut*in über Videotelefonie ortsunabhängig und in Echtzeit statt. Voraussetzungen sind eine funktionierende Internetverbindung, ein Computer oder Tablet mit Mikrofon und Kamera sowie eine Software zur Videotelefonie. Die Therapieinhalte werden entweder virtuell über eine App, bzw. ein Computerprogramm dargestellt oder gezeigt (z.B. in Form von Bildkarten). Eine weitere Möglichkeit ist, dass die Patient*innen die Therapiematerialien in physischer Form, wie z.B. ein Übungsbuch, vorliegen haben [REF]. Diese STST könnte für Menschen mit Aphasie eine interessante Möglichkeit darstellen, um einen ortsunabhängigen Zugang zum maximalen Potenzial einer Rehabilitation sowie fachlicher Expertise, zu erhalten [REF]. Ferner stellt die Teletherapie während der Pandemie des COVID-19 die derzeit einzige Möglichkeit für Sprachtherapeut*innen dar, die sprachtherapeutische Behandlung mit Risikopatient*innen aufrecht zu erhalten. Die dauerhafte Implementierung der Teletherapie in den Heilmittelkatalog wurde durch den gemeinsamen Bundesausschuss noch nicht umgesetzt [REF]. Bevor ein telemedizinisches Versorgungsmodell mittels STST in Deutschland dauerhaft implementiert werden kann, muss untersucht werden, welche Wirksamkeit diese Methode gegenüber der bisher üblichen Face-to-Face Therapie (FTFT), bei der Patient*in und Therapeut*in in einem Raum sitzen und die Therapieinhalte im direkten Kontakt vermittelt werden, hat. Ein systematischer Review von Laver et al. [REF] kann keine eindeutigen Schlussfolgerungen bezüglich der Wirksamkeit einer telemedizinischen Behandlung nach Schlaganfall ziehen, da viele Studien ein hohes Risiko für Bias aufweisen. Daher liegt nur ein niedriges bis moderates Level an Evidenz vor, dass eine Telerehabilitation wirksamer oder genauso wirksam ist, wie eine FTFT [REF]. Ein weiterer systematischer Review von Lavoie et al. [REF] kam zu dem Ergebnis, dass der Einsatz von Technologien, wie z.B. therapeutische Applikationen (Apps), bei Wortfindungsstörungen wirksam sein kann. Jedoch untersuchten die meisten eingeschlossenen Studien die Technologien nicht gezielt in Verbindung mit einer Sprachtherapie [REF]. Aufgrund der geringen Evidenzlage soll in diesem Scoping Review daher untersucht werden, welche Wirksamkeit eine STST im Vergleich zur FTFT auf die Benennleistungen von Menschen mit Aphasie hat.
Die Geschichte der öffentlichen Gesundheit und staatlicher Strategien im Umgang mit Epidemien hat seit Beginn der SARS-CoV-2-Pandemie viel öffentliches Interesse auf sich gezogen. Unter Historiker_innen besteht Uneinigkeit darüber, ob uns Erkenntnisse aus dieser historischen Forschung in der derzeitigen Pandemie weiterhelfen können. Malte Thießen hat jüngst davon abgeraten und empfohlen, die Pandemie eher als Brennglas zu verstehen, durch welches die Besonderheiten unserer heutigen Gesellschaft sichtbar werden. Diese Forschungsperspektive ist wiederum keineswegs neu, sondern seit dem späten 20. Jahrhundert immer wieder in der Kultur- und Medizingeschichte wie auch der Medizinsoziologie eingenommen worden. Peter Baldwin etwa sieht seine historische Untersuchung von Seuchenbekämpfungsstrategien im Europa des 19. und 20. Jahrhundert als Ansatz, allgemeinere Strukturen, Staatlichkeit und politische Kultur in verschiedenen Ländern zu analysieren. Auch er interessierte sich dafür, ob die Bedrohungen und Krisen, die mit Epidemien einhergingen, dauerhaft den Stil der gesetzlichen Intervention und Regierungsweisen eines Landes prägten: „What are the sources of the political traditions that are so often themselves invoked as final historical causes of variation between nations?“2 Besonders seit den 1990er Jahren haben verschiedene Historiker_innen mit ähnlichen Forschungsfragen die Entstehung öffentlicher Gesundheitsdienste und Präventionsstrategien vor allem mit Fokus auf Europa, Nordamerika (Peter Baldwin, Charles Rosenberg, Dorothy Parker) und in transnationaler Perspektive (Mark Harrison, Alison Bashford) in den Blick genommen. In diese Forschungstradition lässt sich auch Charles Allan McCoys vorliegende Studie einordnen.Der Professor für Soziologie an der State University of New York at Plattsburgh kritisierte 2013 und 2014 die Reaktionen der USA auf die Ebolaepidemie in Westafrika und verwies schon damals in Presseartikeln auf die historischen Kontinuitäten der US-amerikanischen Seuchenpolitik. Seit dieser Zeit liegen McCoys Forschungsschwerpunkte im Bereich Medizin sowie Gesundheits- und vergleichenden historischen Soziologie. In seinem Buch „Diseased States“ geht er der Frage nach, warum „Industrienationen“ wie das Vereinigte Königreich und die Vereinigten Staaten von Amerika so unterschiedliche Stile und bevorzugte Techniken im Umgang mit Epidemien aufweisen. Für beide Staaten untersucht er die Entstehungsbedingungen und die -geschichte der Strukturen zur Kontrolle von Seuchen. Auch wenn McCoy die politischen Reaktionen auf das SARS-CoV-2-Virus nicht berücksichtigen konnte – sein Manuskript stammt noch aus der Zeit vor der Corona-Pandemie – hätte der Autor kaum einen besseren Zeitpunkt für seine Fragestellung wählen können. Seine Studie regt zum Nachdenken darüber an, wie die Erfahrungen mit der Pandemie und mit den divergierenden Strategien und Techniken zu ihrer Bewältigung den zukünftigen Umgang mit Infektionskrankheiten dauerhaft prägen werden.In vier Kapiteln und weniger als 200 Buchseiten arbeitet sich McCoy entlang verschiedener Epidemien, die die britischen Inseln und die Vereinigten Staaten zwischen 1793 und 2015 bedrohten, durch die Geschichten der Seuchenbekämpfung der beiden Staaten und achtet dabei vor allem auf die großen Unterschiede. Im Zentrum steht die Frage, unter welchen Umständen Erfahrungen mit Seuchen nicht nur zu kurzfristigen Reaktionen, sondern zu der dauerhaften Einrichtung staatlicher Strukturen führten. War etwa die Einrichtung von Kommissionen und die Errichtung spezieller Krankenhäuser zunächst nur Reaktion auf einen Seuchenausbruch oder eine Epidemie, so konnte diese jedoch eine sich selbst verstärkende Rückkopplungsschleife in Gang bringen, die, wie McCoy argumentiert, eine Stabilisierung und Verstetigung der zu Beginn etablierten Strukturen und Ansätze der Seuchenbekämpfung zur Folge hatte. Das britische Modell bildete sich, wie der Autor veranschaulicht, entlang der Erfahrungen mit den ersten Choleraepidemien sowie Pocken- und Typhusausbrüchen in den 1840er bis 1860er Jahren heraus. Die Ausbrüche fielen in eine Zeit beschleunigter Urbanisierung, Industrialisierung und gravierender sozialer Probleme. Der Einfluss der Theorie einer miasmatischen Ätiologie von Krankheiten führte laut McCoy dazu, dass die Regierung in Großbritannien sich bei der Eindämmung und Prävention von Epidemien auf die Umwelt konzentrierte. Schließlich gelang es durch eine breite Definition der als allgemeine Quellen der Krankheitsentstehung ausgemachten „nuisances“ (S. 86), das Problem der individuellen Freiheitsrechte, die das Regierungshandeln einschränkten, zu umgehen. Die Beseitigung der „nuisances“, selbst in den sonst durch dieses Recht vor staatlichen Eingriffen geschützten Bereichen wie Wohnhäusern, wurde als Maßnahme zur Ermöglichung von Freiheit definiert, nicht als Verletzung dieser (ebd.).Anders in den USA: Obwohl auch hier das 19. Jahrhundert von den Erfahrungen verheerender Gelbfieber- und Choleraepidemien geprägt war, verliefen einzelne Bemühungen, um dauerhafte Strukturen zur Seuchenbekämpfung etwa in Philadelphia, New York oder New Orleans zu errichten, zunächst im Sande. Erst gegen Ende des 19. Jahrhunderts verstetigten sich in den USA Strukturen und Techniken zur Infektionsbekämpfung. In den 1880er und 1890er Jahren geschah dies, wie McCoy zeigt, unter dem Einfluss der nun an Zustimmung gewonnenen Keim-Theorie (germ theory). Die grundlegende Idee, dass Infektionskrankheiten über Bakterien oder andere Mikroorganismen übertragen wurden, stützte Techniken der Kontrolle von Menschen, die als ‚Keimträger_innen‘ und ‚Überträger_innen‘ von Krankheiten identifiziert wurden. Besonders die Immigration wurde in den USA für die Quelle der Einschleppung von Infektionserregern gehalten. Immer wieder sieht McCoy seine These dieser deutlichen Differenz zwischen den beiden untersuchten Ländern bestätigt: Während Maßnahmen in Großbritannien auf die Umwelt,weniger auf die Einzelnen zielten und auch weniger auf Zwang basierten, standen in den USA Maßnahmen der Überwachung von Menschen und Techniken der Isolation wie etwa Quarantäne im Zentrum der Strategien gegen Seuchen (S. 70, S. 110 ff., S. 145 f.).Warum Großbritannien angesichts neuer Erkenntnisse über die Rolle von Mikroorganismen bei der Entstehung und Verbreitung von Infektionskrankheiten auch am Ende des 19. Jahrhunderts seinem tradierten Muster der Seuchenbekämpfung treu blieb, begründet McCoy mit der Theorie der Pfadabhängigkeit. Das aus den Wirtschaftswissenschaften stammende, seit Beginn des neuen Jahrtausends in der Politikwissenschaft aufgegriffene und für vergleichende und zeitgeschichtliche Analysen fruchtbar weiterentwickelte Konzept dient dem Autor zur Erklärung für die unterschiedlichen Wege, die Großbritannien und die USA in ihrem Umgang mit Seuchen beschritten. Im Anschluss an Paul Piersons Theorie der Pfadabhängigkeit führt McCoy die Stabilität politischer und sozialer Techniken und Strukturen überzeugend auf Feedbackschleifen und sich selbst verstärkende Rückkopplungseffekte zurück. So beschreibt er, wie Großbritannien, nachdem es bereits enorm in die sanitäre Infrastruktur investiert hatte, sich angesichts der Choleraepidemie 1866 nicht mehr in der Lage sah, ein kurz erwogenes umfassendes System der Quarantäne an den Häfen zu installieren. Die Regierung reagierte daraufhin erneut mit einem Gesetz, das auf die Umsetzung bestimmter sanitärer Maßnahmen, wie etwa die Desinfektion von Schiffen, zielte, dem Sanitary Act (S. 78 f.). Nachdem Strukturen etabliert und in soziale Prozesse eingebunden waren, war eine spätere Umgestaltung der Seuchenbekämpfung kaum noch möglich. Zwar kam es im 20. Jahrhundert, wie McCoy zugibt, zu kleineren Anpassungen durch die Integration neuer Wissensbestände, doch blieben grundlegende Denk- und Handlungsmuster bis zum heutigen Tag wirkmächtig (S. 19–25). Unter den vielen historischen Umständen, die die Entstehung einer modernen Seuchenbekämpfung beeinflusst haben mögen, hält McCoy in seiner Betrachtung vier für zentral: erstens die Zentralisierung des Staates, zweitens die Verfügbarkeit einer kohärenten Theorie der Krankheiten, drittens der Druck von zivilgesellschaftlichen und Nichtregierungsorganisationen und schließlich viertens das Vorhandensein einer Vorstellung über die Bevölkerung als sozialer Körper (S. 20, 35). Der Nachweis dieser Faktoren anhand des Vergleichs der beiden Fallbeispiele bringt jedoch einige Probleme mit sich.Betrachtet man die gravierenden geografischen, historischen und sozialen Unterschiede der beiden Staaten im untersuchten Zeitraum, sind die offensichtlichen Unterschiede auf den ersten Blick wenig überraschend. Auch wenn McCoy einige Differenzen – etwa die unterschiedliche Ausprägung der Staatlichkeit, ganz zu schweigen von den geografischen Gegensätzen – herabzusetzten versucht, bleibt er die Antwort auf die Frage schuldig, warum er gerade diese beiden Staaten vergleicht, die doch in so vieler Hinsicht und so offensichtlich sehr unterschiedliche Bedingungen für die Herausbildung einer Seuchenpolitik aufwiesen. Großbritannien war, nicht nur was seine Seuchenbekämpfung im 19. Jahrhundert betraf, aufgrund seiner ökonomischen Vorrangstellung selbst mit Blick auf Europa ein Sonderfall. Möglicherweise hätte McCoy seine Thesen zur Pfadabhängigkeit stichhaltiger und eindrücklicher belegen können, hätte er zwei Staaten mit ähnlicheren Voraussetzungen untersucht. Schließlich konnte es mit dem Fokus auf die USA und Großbritannien auch nicht darum gehen, eine klaffende Forschungslücke zu schließen. Wie der Autor selbst feststellt, ist die Seuchengeschichte sowohl der USA als auch Großbritanniens wie die Westeuropas ein eher umfangreich bearbeitetes Forschungsfeld. Zwar haben sozialgeschichtliche, wissenssoziologische und kulturwissenschaftliche Fragestellungen, wie sie auch McCoy in seiner Studie aufgreift, zu einer Öffnung der Forschung beigetragen [REF]. Dennoch blieben selbst Ansätze einer Globalisierung der zuvor auf westliche Nationalstaaten fixierten Seuchengeschichte, wie sie unter anderen Mark Harrison 2015 entwarf [REF], bisher hinter der Forderung einer stärkeren Berücksichtigung nicht-westlicher Perspektiven und zuvor meist nur als Peripherie betrachteter Weltregionen zurück [REF].
Bürgerkriege, Naturkatastrophen und Armut führten in den Jahren 2015 und 2016 mehr als eine Million Menschen als Flüchtlinge nach Deutschland. Dadurch ergaben sich erhebliche gesellschaftliche Herausforderungen, die weitreichende und bis heute anhaltende Veränderungen nicht nur in Deutschland, sondern in ganz Europa auslösten.Auch in den letzten Jahren bleibt die Zahl flüchtender Menschen hoch. Weltweit sind nach Angaben des UNHCR im Jahr 2022 über 80 Mio. Menschen auf der Flucht. Mit zuletzt gut 190.000 neuen Asylanträgen im Jahr 2021, davon etwa 73.000 (entsprechend 39 %) von Minderjährigen, bleibt Deutschland eines der Hauptaufnahmeländer von Asylsuchenden in Europa [REF]. Angesichts des Krieges in der Ukraine, anhaltender weltweiter Konflikte und den immer bedrohlicher werdenden Auswirkungen des weltweiten Klimawandels ist davon auszugehen, dass auch in den kommenden Jahren sehr viele Menschen weltweit Schutz in Ländern außerhalb ihrer Heimat suchen werden.Flüchtlinge und Asylsuchende sind eine besonders vulnerable Gruppe in unserer Gesellschaft. Aus medizinischer Sicht besteht eine Gefährdung durch die möglicherweise mangelhafte medizinische Versorgung im Herkunftsland und während der Flucht, bei gleichzeitig besonderer gesundheitlicher Gefährdung. Nach der Ankunft in Deutschland erschweren sprachliche, soziale und teilweise auch ökonomische Barrieren die gesundheitliche Versorgung. Flüchtlinge benötigen daher angemessene und niederschwellige medizinische Angebote, die an die individuelle Situation angepasst sein müssen. Es ist eine professionelle, soziale und ethische Herausforderung, die medizinische Versorgung von Flüchtlingen adäquat zu organisieren und durchzuführen. Dies gilt insbesondere für Kinder und Jugendliche, die teilweise als unbegleitete minderjährige Flüchtlinge (UMF) nach Deutschland kommen.Wesentliche Voraussetzungen für eine tragfähige medizinische Versorgung von Flüchtlingen im Kindes- und Jugendalter sind im Bremer Modell dargestellt [REF]. Hierin wird das Ziel formuliert, ein sinnvolles medizinisches Angebot für Flüchtlinge bereitzustellen. Eine von den Autoren ausdrücklich befürwortete Grundhaltung zur ärztlichen Versorgung von minderjährigen Flüchtlingen leitet sich unabhängig von ihrem Rechtsstatus aus dem Artikel 24 [Gesundheitsvorsorge] der UN-Kinderrechtskonvention aus dem Jahr 1989 her. Die Vertragsstaaten erkennen hierin das Recht des Kindes auf das erreichbare Höchstmaß an Gesundheit an. Minderjährige Flüchtlinge sind auf dem gleichen medizinischen Niveau zu versorgen wie die einheimische Bevölkerung.Der deutsche wie auch der europäische Gesundheitssektor benötigen hierfür geschultes medizinisches Personal, welches in der Lage ist, die Gesundheitsrisiken und -bedürfnisse dieser Kinder und Jugendlichen zu erkennen und eine kompetente Betreuung zu gewährleisten.Um das Ziel eines umfassenden medizinischen Angebotes für Flüchtlinge zu erreichen, ist es notwendig, den medizinischen Bedarf von minderjährigen Flüchtlingen und ihrer Familien zu kennen. In einem Interview mit Flüchtlingen in Schweden zeigte sich, dass ein routinemäßig durchgeführtes infektiologisches Screening überwiegend als unerfreuliche Pflichtuntersuchung angesehen wurde. Bemerkenswert ist hierbei vor allem, dass die Mehrheit der Flüchtlinge angab, nicht über Art und Umfang der durchgeführten Screeninguntersuchungen informiert worden zu sein [REF].Wichtige infektiologische Fragestellungen bei der Betreuung von minderjährigen Flüchtlingen sind insbesondere die Sicherstellung eines vollständigen Impfschutzes, aber auch die Diagnostik und Therapie von teils importierten und seltenen Infektionskrankheiten trotz Sprachbarrieren, Sammelunterkünften und unterschiedlichem kulturellen Hintergrund.In einer gemeinsamen Stellungnahme hatten die Deutsche Gesellschaft für Pädiatrische Infektiologie (DGPI), die Gesellschaft für Tropenpädiatrie und Internationale Kindergesundheit (GTP) und der Berufsverband der Kinder- und Jugendärzte (BVKJ) erstmals im Jahr 2015 Empfehlungen für die infektiologische Versorgung von Flüchtlingen im Kindes- und Jugendalter veröffentlicht [REF]. Seither konnten die Mitarbeiter im Gesundheitssystem in vielen Bereichen neue Erfahrungen in der medizinischen Versorgung von Flüchtlingen gewinnen, und zahlreiche Untersuchungen ergaben neue Erkenntnisse bezüglich des medizinischen Bedarfs sowie der Prävalenz von spezifischen Infektionserkrankungen bei Flüchtlingen unterschiedlicher Herkunft. Ebenso wurden seit dem Jahr 2015 mehrere nationale Empfehlungen zur infektiologischen Untersuchung von Flüchtlingen im Kindes- und Jugendalter veröffentlicht [REF].Die vorliegende Aktualisierung der Empfehlungen zur infektiologischen Versorgung von Flüchtlingen im Kindes- und Jugendalter in Deutschland berücksichtigt diese neuen Erkenntnisse, die nach Sichtung der Literatur zusammengefasst wurden.Die vorliegenden Empfehlungen sollen Ärzte1/medizinisches Personal in der medizinischen Versorgung von Flüchtlingen im Kindes- und Jugendalter unterstützen, mit dem Ziel, einen unvollständigen Impfschutz frühzeitig zu erkennen und rasch zu vervollständigen – zum individuellen Schutz und um Ausbreitungen von Infektionskrankheiten zu verhindern; übliche Infektionskrankheiten im Kindes- und Jugendalter, auch vor dem Hintergrund von Sammelunterkünften, Sprachbarrieren und unterschiedlichen kulturellen Auffassungen, zu diagnostizieren und zu behandeln; in Deutschland seltene Infektionskrankheiten (z. B. Tuberkulose, Malaria, Schistosomiasis, kutane Leishmaniose) frühzeitig zu erkennen und zu therapieren. Um die Stellungnahme für den Alltag anwendbar zu machen, wurden in der vorliegenden Aktualisierung die konkreten Handlungsempfehlungen möglichst knapp zusammengefasst. Eine ausführliche Begründung für die Handlungsempfehlungen mit der zugrunde liegenden Literatur findet sich im Abschn. „Begründung der vorgeschlagenen Screeninguntersuchungen“. Die vorliegende Empfehlung konzentriert sich bewusst auf die Infektionsdiagnostik und Infektionsprävention. Dabei ist es selbstverständlich, dass Kindergesundheit weit über diese infektiologischen Gesichtspunkte hinaus reicht und insbesondere auch psychische Gesundheit, Integration und Bildungschancen wesentliche Aspekte für ein gesundes Aufwachsen von Flüchtlingen im Kindes- und Jugendalter darstellen.
Die Infektion mit dem Severe-Acute-Respiratory-Snydrome-Coronavirus, Typ II (SARS-CoV-2), ist in hohem Ausmaß, mittlerweile bekannt, verantwortlich für verschiedene chronische Krankheitszustände [REF]. Eine Long-COVID-Erkrankung ist eine länger währende Krankheit im Anschluss an eine akute COVID-19-Erkrankungsphase, die sich durch das syndromale Zusammenkommen der Symptome insomnische Beschwerden, chronische Erschöpfung, verminderte physische Leistungskraft, kognitive Funktionsstörungen, Kopfschmerzen und Atemnot auszeichnet [REF]. Zu den Symptomen der Long-COVID-Erkrankung zählt auch das bereits aus dem Krankheitsbild des Chronic-Fatigue-Syndroms oder der myalgischen Enzephalopathie bekannte Phänomen der postexertionellen Malaise (PM) [REF]. Die PM ist ein Symptom, welches den Patienten ein ausgeprägtes Unvermögen und schnelle Erschöpfbarkeit für geistige und körperliche Belastungen bereitet. Die durch geistige, physische Belastung notwendige Erholung und relevante Reduktion der dadurch verursachten Erschöpfung, typischerweise länger als 40 h, kann über viele Tage andauern [REF]. Die Aspekte der myalgischen Enzephalopathie sind auch bereits für andere virale Infektionserkrankungen bekannt. Hierzu zählen z. B. SARS-CoV‑1 und das Epstein-Barr-Virus sowie Erkrankungen, die durch das Dengue-Virus hervorgerufen werden. Es ist bislang völlig unklar, in welchem Maße diese verschiedenen Viruserkrankungen auch mit einer spezifischen Form eines Chronic-Fatigue-Syndroms oder ME einhergehen [REF].Definitionsgemäß wird für Patienten und Patientinnen mit einer akuten COVID-19-Erkrankung von einer Latenz von maximal vier Wochen als Long-COVID gesprochen. Wenn die Persistenz mehr als zwölf Wochen besteht, wird in der Abfolge von einem Post-COVID-Syndrom gesprochen [REF]. Nach der derzeitigen Literatur muss davon ausgegangen werden, dass um die 15 % der Menschen mit einer COVID-19-Erkrankung (Infektionen durch die sog. Omikron-Variante sind hier derzeitig noch ausgenommen) von einer verlängerten Erkrankung im Sinne eines Long-COVID-Syndroms betroffen sind [REF]. Dabei scheint interessanterweise die Häufigkeit von vorbestehenden Komorbiditäten nicht einflussnehmend auf die Prävalenz der Post-COVID-Erkrankung. In welchem Ausmaß psychosoziale Belastungen, psychosomatische Beschwerden in der Anamnese ein Post-COVID-Syndrom begünstigen können, ist bislang unbekannt [REF].Vor dem Hintergrund der bestehenden und sich häufenden Beschreibung von Patienten und Patientinnen mit Long-COVID/Post-COVID ist mittlerweile oft beschrieben, welche möglichen phänotypischen Auswirkungen diese chronische Form der SARS-CoV-2-Infektionen haben kann. In der Literatur werden unterschiedliche Aspekte oft voneinander getrennt beschrieben. Neben den kognitiven Störungen und den sog. „brain fog“ assoziierten Symptomen [REF] kommen auch die bekanntermaßen ausgeprägten physischen Erschöpfungssymptome und vorzeitige Erschöpfbarkeit in der Literatur zur Beschreibung [REF]. Andere Aspekte wie eine erhöhte Sensitivität für akustische Reize oder Depressivität [REF] sind in diesem Kontext nicht phänotypisch unterschiedlichen Formen zugeordnet [REF]. Schlafstörungen in Form von Schlafunterbrechungen, längerer Einschlaflatenz, gehäuftes Erwachen und dauerhafter Tinnitus nach dem Einschlafen sind nicht für spezifische unterschiedliche Phänotypen eines Long-COVID-Syndroms beschrieben. Die Gesamtheit insomnischer Beschwerden werden in den ersten Arbeiten zu Long-COVID mit einer Prävalenz von 26 % [REF] bis zu 88 % [REF] angegeben. Überlappend mit dieser Gruppe von insomnischen Beschwerden bestehen jedoch auch depressive Symptome, die mit vergleichbarer Häufigkeit beschrieben werden [REF]. In welchem Ausmaß eine direkte, gegenseitig beeinflussende Wirkung der Depressivität auf den Schlaf und vice versa bestehen kann, ist in keiner der derzeitigen Long-COVID-Studien ausreichend untersucht worden.Die Leitlinie „Post-COVID/Long-COVID“ als interdisziplinäre Leitlinie verschiedenster Fachgesellschaften gibt einen ersten Rahmen für die möglichen Behandlungsoptionen der Erkrankung [REF]. Die medikamentösen Optionen sind momentan einerseits durch die Erfahrung aus der Behandlung von Patienten mit Chronic-Fatigue-Syndrom hergeleitet [REF]. Eine Evidenz für die Wirksamkeit ist bislang nicht durch wissenschaftliche Studien belegt. Des Weiteren werden in den letzten Monaten verschiedene experimentelle Verfahren beschrieben [REF]. Bezüglich des rehabilitativen Ansatzes ist jedoch, insbesondere aus der Erfahrung der Behandlung von Patienten mit Chronic-Fatigue-Syndrom/myalgischer Emzephalpoathie die entscheidende Größe für die Patienten, die als Hauptsymptom die postexertionelle Malaise beklagen, das eigene Pacing (Taktung der Leistungsanforderungen und der Intensität), um die Patienten wieder langsam an eine zunehmende körperliche und geistige Leistungsfähigkeit heranzuführen [REF].Vor dem Hintergrund der unterschiedlichen Phänotypen und möglichen therapeutischen Optionen werden im Folgenden vier Fälle beschrieben, die einen Einblick geben sollen, in welchem Ausmaß die Long-COVID-Syndrome für Patienten relevant sind und unter welchen therapeutischen Aspekten in der neurologischen Rehabilitation diese Erkrankungen mit besonderem Augenmerk auf die insomnischen Beschwerden und die PM therapiert werden können.
Im Bereich der Demenzdiagnostik und insbesondere bei der ätiologischen Zuordnung demenzieller Syndrome kommen der Liquordiagnostik zwei wesentliche Funktionen zu. Unter negativdiagnostischem Aspekt dient sie vorrangig dem Ausschluss entzündlicher Ursachen (z. B. Virusenzephalitiden, Lues, Morbus Whipple, Neuroborreliose, Neurosarkoidose, Vaskulitiden, Paraneoplasien) und, im Falle rasch progredienter demenzieller Syndrome, dem Ausschluss von Prionenerkrankungen, während sie im Sinne einer Positivdiagnostik die Diagnosestellung einer (frühen) Alzheimer-Erkrankung und anderer neurodegenerativer Demenzätiologien unterstützen kann [REF]. Ferner wird die Liquordiagnostik in der klinischen Praxis auch genutzt, um eine neurodegenerative Genese neurokognitiver Defizite mit demenzieller Beeinträchtigungsschwere auszuschließen [REF]. Vaskuläre Läsionen, alkoholassoziierte kognitive Störungen, der idiopathische Normaldruckhydrozephalus, chronische Subduralhämatome und auch funktionelle kognitive Störungen stellen beispielsweise mögliche nicht neurodegenerative Demenzursachen dar.Gemäß konsentierten Leitlinienempfehlungen sollten Zellzahl, Gesamtprotein, Laktat und Glukose, der Albuminquotient sowie die intrathekale IgG-Produktion inkl. oligoklonaler Banden bestimmt werden [REF]. Je nach klinischer Indikation kann unter Umständen auch die Bestimmung der intrathekalen IgA- und IgM-Produktion sinnvoll sein.Die demenzspezifische neurochemische Liquordiagnostik hat einen unbestrittenen Wert in der Erstdiagnostik klinisch unklarer Fälle und insbesondere in der Differenzierung neurodegenerativer und nicht neurodegenerativer Demenzursachen [REF]. Dabei wird stets die kombinierte Bestimmung der Parameter beta-Amyloid-1-42 (Aβ42), beta-Amyloid-1-40 (Aβ40), Gesamt-Tau und Phospho-Tau (pTau) empfohlen, da diese der Bestimmung einzelner Parameter überlegen ist. Metaanalytische Daten zeigen beispielsweise, dass die isolierte Aβ42-Bestimmung unzureichend in der Differenzierung verschiedener Demenzentitäten ist und das Risiko falsch-positiver Alzheimer-Demenz-Diagnosen erhöht [REF]. Darüber hinaus weisen weder die isolierten Gesamt-Tau- noch die isolierten pTau-Werte ausreichende Sensitivitäten und Spezifitäten auf, um beim Patienten mit leichter kognitiver Störung („mild cognitive impairment“ [MCI]) eine Konversion zur Alzheimer-Demenz zu prognostizieren [REF]. Die Daten zeigen, dass ein negativer Liquorbefund zwar mit großer diagnostischer Sicherheit eine Alzheimer-Erkrankung ausschließt, ein positiver Liquorbefund das Vorliegen einer Alzheimer-Demenz jedoch nicht zwingend bestätigt. Zurückzuführen ist dies auf die altersabhängig hohe Prävalenz einer Amyloidpathologie, sodass eine neurodegenerative Komorbidität (z. B. in Form einer Lewy-Körperchen-Demenz) auch bei Alzheimer-typischer Liquorkonstellation möglich ist [REF]. Vor diesem Hintergrund erfolgt die ätiologische Zuordnung des Demenzsyndroms in erster Linie nach klinischen Kriterien und kann liquordiagnostisch gestützt werden. Im Prodromalstadium einer Alzheimer-Demenz hingegen ist die biomarkerbasierte ätiologische Zuordnung kognitiver Defizite aufgrund der pathophysiologischen Kaskade (Amyloid- und Tau-Pathologie geht der substanziellen Neurodegeneration und alltagsrelevanten kognitiven Beeinträchtigung um Jahre voraus) den klinischen und bildgebenden Kriterien überlegen.In der Differenzierung verschiedener neurodegenerativer Demenzursachen ist der diagnostische Wert dieser Parameter jedoch sehr begrenzt und gemäß der aktuellen S3-Leitlinie beispielsweise auch nicht ausreichend, um eine vaskuläre Demenz von neurodegenerativen Demenzursachen abzugrenzen [REF].Der Fokus dieser Literaturübersicht ist die Darstellung der aktuellen Evidenz zum diagnostischen Wert etablierter und experimenteller Neurodegenerations- und Demenzparameter im Liquor bei verschiedenen neurodegenerativen Demenzätiologien.
Telefon- oder Videosprechstunden gewinnen in der Therapie von muskuloskeletalem Schmerz über verschiedene Versorgungsbereiche hinweg an Bedeutung. Die verfügbaren technischen Optionen sind für die BehandlerInnen jedoch in ihrer Fülle schwer zu überblicken, und die Anpassung der klinischen Abläufe ist mit vielfältigen Herausforderungen verbunden. Darüber hinaus sind einige spezifische Aspekte hinsichtlich der regulatorischen Rahmenbedingungen sowie der PatientInnenversorgung zu berücksichtigen. Problematisch ist der Mangel an hochwertigen Peer-Review-Publikationen als Orientierungshilfe für die Planung und praktische Implementierung von Fernsprechstunden.Video- und telefongestützte Sprechstunden eröffnen eine Reihe von Chancen in Therapie und Management von muskuloskeletalem Schmerz. Muskuloskeletaler Schmerz umfasst jede akute oder chronische schmerzhafte Erfahrung, assoziiert mit oder wahrgenommen im Zusammenhang mit Störungen oder Verletzungen des Bewegungsapparats. Ausgenommen sind Schmerzen systemischen, neurologischen oder schwerwiegenden lokalen Ursprungs [REF]. PatientInnen mit derartigem Schmerz werden von verschiedenen Berufsgruppen des Gesundheitssystems versorgt, unter anderem von Allgemein- und FachärztInnen, Manual- oder PhysiotherapeutInnen sowie PsychologInnen [REF]. Unter den BehandlerInnen bestehen allerdings Bedenken in Bezug auf Fernsprechstunden, was sich als Hindernis bei der breiten Implementierung erweisen kann. Bedenken bestehen beispielsweise in Bezug auf Therapiesicherheit und -qualität, rechtliche und regulatorische Einschränkungen, technische Machbarkeit und hinsichtlich der Frage, wie die Routinebehandlungen anzupassen sind [REF]. Erfreulicherweise lassen sich viele Herausforderungen und Lösungen in der klinischen Praxis therapieformübergreifend verallgemeinern. Der vorliegende Beitrag richtet sich daher an alle BehandlerInnen, die an Therapie und Management von PatientInnen mit muskuloskeletalem Schmerz beteiligt sind. Er soll ihnen bei ihren Überlegungen bzw. bei der Implementierung von Fernsprechstunden als Informationsquelle dienen.Schmerz und Beeinträchtigung, die mit Erkrankungen des Bewegungsapparats einhergehen, können eine starke Belastung darstellen. Auch wenn die meisten dieser Erkrankungen selbstlimitierend sind, profitieren die PatientInnen von einer konservativen Behandlung [REF]. Anfang 2020 sorgte jedoch die COVID-19-Pandemie für ein jähes Ende der meisten persönlichen Konsultationen, was das Interesse an alternativen Möglichkeiten zur Aufrechterhaltung der PatientInnenversorgung neu geweckt hat. Eccleston et al. [REF] argumentieren stichhaltig, dass das Management von Menschen mit chronischen Schmerzen in einer globalen Pandemie nicht unterbrochen werden sollte, und empfehlen telemedizinische Lösungen. Praktische Orientierungshilfen für den Übergang zu solchen Leistungen und für deren Implementierung sind allerdings rar, und es besteht der zusätzliche Bedarf an Versorgungsoptionen, die über die Behandlung von PatientInnen mit persistierenden Schmerzstörungen hinausgehen, beispielsweise für die Behandlung akuter oder verletzungsbedingter Schmerzen.Der vorliegende Beitrag umreißt die Evidenzbasis zu interpersonellen Video- und Telefonsprechstunden bei muskuloskeletalem Schmerz. Des Weiteren wird ein verallgemeinerter Praxisleitfaden für die Implementierung und Durchführung von Fernsprechstunden geboten, der auf ein Spektrum wissenschaftlicher Bereiche zurückgreift, um Sicherheit und Qualität solcher Sprechstunden zu steigern. Es werden Problemfelder beleuchtet, die im Vergleich zur persönlichen Versorgung besondere Aufmerksamkeit erfordern, wie Beziehungsaufbau, technische Aspekte und klinische Entscheidungsfindung. Auch wenn der Beitrag sich auf die nichtmedikamentösen Behandlungsansätze konzentriert, lassen sich die praktischen Überlegungen zu technischem Aufbau, Beurteilung der PatientIn, Kommunikation und Selbstmanagement auf die allgemeinmedizinische Versorgung von SchmerzpatientInnen übertragen. Speziell auf AllgemeinmedizinerInnen bezogene Orientierungshilfen und Überlegungen zur Integration von Fernsprechstunden in Gesundheitssysteme finden sich andernorts [REF].
Vor gut 10 Jahren starteten Wissenschaftler*innen aus Australien und dem Vereinigten Königreich eine Initiative, um Sprachentwicklung und ihre Störungen zu einem Thema von Public Health zu machen [REF]. Auch in Deutschland sind Gesundheit und Entwicklung von Kindern und Jugendlichen seit vielen Jahren ein wichtiges Thema. Dennoch werden die Sprachentwicklung und ihre Störungen weiterhin fast ausschließlich als ein individuelles Problem betrachtet, das entsprechend auch individuell zu behandeln ist. Das medizinische Fachgebiet zur Diagnostik, Behandlung, Vorbeugung und Rehabilitation von Störungen der menschlichen Kommunikation, die Phoniatrie und Pädaudiologie, sind wie die Logopädie/Sprachtherapie als therapeutische Dienstleister auf den Einzelfall („One-to-one-Service“) ausgerichtet. Auf gesellschaftlicher Ebene erweist es sich aber als zunehmend wichtig, Änderungen auf breiter Ebene anzustoßen, das meint, etwaige Probleme in der Sprachentwicklung bereits im Vorfeld zu erkennen.Im Zuge einer Neuausrichtung von Public Health haben sich – in Abgrenzung zur kurativen Medizin – Krankheitsprävention und Gesundheitsförderung zu Säulen gesundheitspolitischer Maßnahmen entwickelt. Das rechtzeitige Erkennen von Sprachentwicklungsstörungen (SES) und das Einleiten geeigneter Interventionsmaßnahmen im Zuge einer umfassenden Betrachtung der Gesundheit betroffener Kinder entspricht diesem Verständnis. Die Genese von SES, ihre gelungene oder misslungene Bewältigung, strukturelle Merkmale therapeutischer Hilfsangebote und nicht zuletzt die Aufmerksamkeit der familiären und sozialen Umgebung prädestinieren Sprachentwicklung zu einem Public-Health-Thema.Im vorliegenden Beitrag wird auf „primäre SES“ fokussiert, also auf solche Sprachentwicklungsstörungen, die nicht durch andere Entwicklungsstörungen bzw. Erkrankungen (mit-)verursacht sind (im Gegensatz zu „sekundären SES“). Primäre SES sind durch anhaltende Defizite beim Erwerb, Verstehen, Produzieren bzw. Gebrauch von Sprache gekennzeichnet, die Sprachleistung eines Kindes liegt dabei deutlich unter dem, was angesichts dessen Lebensalters zu erwarten wäre. Primäre SES sind genetisch bedingt bei individuell unauffälligen somatischen und Umweltgegebenheiten. Dabei ist die Sprachentwicklung eines Kindes von Geburt an beeinträchtigt, also schon in der präverbalen Entwicklungsphase. Liegt eine primäre SES bei mehrsprachig aufwachsenden Kindern vor, so betrifft sie jede Sprache, mit der ein Kind aufwächst. Primäre SES treten mit einer Prävalenz von etwa 7 % [REF] relativ häufig auf. Jungen sind vermehrt betroffen [REF]. Prävalenzschätzungen in populationsbasierten Studien fallen geringfügig höher aus, z. B. 7,58 % [REF].Über die präzise Definition und die Terminologie von primären SES ist in den letzten Jahren ein wissenschaftlicher und klinischer Disput entstanden. Dabei geht es um Merkmale und Kriterien der Definition, aber ebenso um diagnostische und therapeutische Konsequenzen. Der vorliegende Beitrag möchte darauf aufmerksam machen, dass die Diagnose einer primären SES aufgrund einer geplanten definitorischen Erweiterung zukünftig häufiger gestellt werden könnte, wodurch auch die Public-Health-Relevanz der Erkrankung weiter ansteigt.In diesem Diskussionsbeitrag werden zunächst die gültigen Definitionen von primären SES gemäß der Internationalen Statistischen Klassifikation der Krankheiten und verwandter Gesundheitsprobleme (ICD-10-GM-22 [REF], ICD-11 [REF]) und gemäß englischsprachigem CATALISE-Consortium (Criteria And Terminology Applied to Language Impairments Synthesizing the Evidence) vorgestellt. Darauffolgend werden die derzeitigen Möglichkeiten der Früherkennung einer primären SES in der Praxis sowie ihre Public-Health-Relevanz aufgezeigt. Schließlich wird unter Verweis auf neuroanatomische Forschungsergebnisse die Notwendigkeit innovativer Forschung zur Früherkennung herausgestellt.Eine primäre SES wird in der aktuell verwendeten deutschen Modifikation der ICD-10 als „Umschriebene Entwicklungsstörung der Sprache“ (USES; ICD-10-GM-22: Code F80) bezeichnet (F80.1: expressiv; F80.2: rezeptiv). Im englischsprachigen Raum wird die USES seit den 1980ern auch „Specific Language Impairment“ (SLI) genannt, eine Bezeichnung, die mit dem Namen „Spezifische Sprachentwicklungsstörung“ teilweise in den deutschsprachigen Raum übernommen wurde.Eine USES zeigt sich in der frühen Entwicklung durch wesentliche zeitliche und inhaltliche Abweichungen vom normalen Muster des Sprach- und Sprecherwerbs – bei altersentsprechender Allgemeinentwicklung. Das bedeutet unter anderem, dass neurologische Erkrankungen, Hörstörungen und eine Intelligenzminderung mit einem Intelligenzquotienten (IQ) < 85 als Ausschlusskriterien gelten. Das Risiko für eine Beeinträchtigung der psychischen Gesundheit, für Verhaltensprobleme sowie für die Ausbildung umschriebener Entwicklungsstörungen schulischer Fertigkeiten ist hoch, weswegen zwingend eine frühe Diagnostik indiziert ist, basierend auf Ausschluss‑, Entwicklungs- und Diskrepanzkriterien. Das bedeutet: (1) Erkrankungen, Entwicklungsstörungen, Umgebungsbedingungen für die Sprachprobleme auszuschließen; (2) eine individuelle Sprachleistung von mindestens 1,5 Standardabweichungen unter der Altersnorm in altersadäquaten standardisierten und normierten Sprachtests nachzuweisen; (3) eine bedeutende Differenz (von mindestens 1 Standardabweichung) zwischen den derart erhobenen Sprachleistungen und der nonverbalen Intelligenzhöhe als Indikator für das allgemeine Intelligenzniveau zu belegen. Durch den letztgenannten Nachweis soll sichergestellt sein, dass der Sprachentwicklungsstand eines Kindes bedeutsam unterhalb seines allgemeinen Intelligenzniveaus liegt.Viele Kinder mit USES haben Probleme, Sprache in sozialen Situationen richtig einzusetzen. Es fällt ihnen möglicherweise schwer, z. B. mit Klassenkamerad*innen, Erzieher*innen, Lehrer*innen, in Kontakt zu treten, über ihre Gefühle zu sprechen [REF], sich in einem Gespräch miteinander abzuwechseln, beim Thema zu bleiben oder lange Sätze zu verstehen. Auch kann es ihnen schwerfallen, Informationen verbal weiterzugeben und Geschichten zu erzählen [REF]. Die Schwierigkeit, anderen ein Problem verbal verständlich machen zu müssen, führt mitunter dazu, dass sich die Kinder frustriert oder wütend fühlen und auf eine unangemessene Art und Weise handeln. Die Störung ist therapiebedürftig [REF]. Der Verlauf einer USES ist stetig, ohne Spontanremission. Trotz Therapie können Sprachdefizite bis ins Erwachsenenalter persistieren.Die ICD-11 [REF], die bereits am 01.01.2022 in Kraft getreten ist, jedoch noch einige Jahre bis zu ihrer Etablierung benötigen wird, bis der kontinuierliche Qualitätssicherungsprozess abgeschlossen ist, enthält nun die Kategorie „Developmental Speech or Language Disorder“ (DLD; Oberkategorie 6A01). In diese fallen Sprach- und Kommunikationsprobleme in der frühen Kindheit, die über die Grenzen der normalen Abweichung im Vergleich zu Alter und Intelligenzniveau eines Kindes hinausgehen; sie sind ursächlich nicht mit sozialen oder kulturellen Faktoren (z. B. regionaler Dialekt) assoziiert und nicht durch eine Sinnesbehinderung, neurologische Entwicklungsstörung, Auswirkungen einer Hirnverletzung oder zerebralen Infektion zu erklären. Unterschieden werden: DLD mit Beeinträchtigung von rezeptiver und expressiver Sprache (6A01.20); DLD mit Beeinträchtigung von hauptsächlich expressiver Sprache (6A01.21); DLD mit Beeinträchtigung von vorwiegend pragmatischer Sprache (Kommunikation; 6A01.22) und DLD mit einer anderen spezifischen Sprachentwicklungsbeeinträchtigung (6A01.23).Eine Gruppe von deutschsprachigen Sprachtherapeut*innen (z. B. Logopäd*innen; akademische Sprachtherapeut*innen; klinische Linguist*innen; Sprachheilpädagog*innen) möchte nun, dass der Terminus „USES“ durch den Terminus „DLD“ ersetzt wird [REF]. Dabei sollen Diagnosekriterien verwendet werden, die vom CATALISE-Consortium 2016 und 2017 vorgeschlagen wurden [REF]. Bei dieser Gruppe handelt es sich wiederum um einen Zusammenschluss von 59 internationalen englischsprachigen Expert*innen aus Wissenschaft, mehrheitlich sprachtherapeutischer, aber auch ärztlicher, psychologischer und pädagogischer Praxis aus dem Vereinigten Königreich, Hongkong, USA, Kanada, Australien, Neuseeland, Irland. Ihr DLD-Konzeption schließt auch Kinder mit ein, die neben Sprachproblemen diskrete neuronale Irregularitäten ohne klare biomedizinische Ätiologie haben, etwa eine Hörminderung durch wiederkehrende Otitiden und die somit eine Minderung ihrer Jahreshörbilanz aufweisen oder koexistente neurologische Funktionsschwächen wie eine auffällige Lateralisierung, Auffälligkeiten in den Exekutivfunktionen, im phonologischen Arbeitsgedächtnis, in der Wahrnehmung oder Motorik.Allein die Tatsache, dass solche Auffälligkeiten häufig bei Kindern mit primären SES zu beobachten sind, deutet nach Meinung von CATALISE darauf hin, dass für diese Klientel ein breiterer Phänotyp zutreffender wie auch das enge Verständnis einer USES (bzw. SLI) artifiziell ist. So könnten Kinder mit einer unterdurchschnittlichen nonverbalen Intelligenz durchaus miteingeschlossen werden. Insbesondere soll nicht am Exklusionskriterium IQ < 85 festgehalten werden, sondern es sind nur Kinder auszuschließen, die das Kriterium einer geistigen Behinderung erfüllen (IQ < 70). Gemäß CATALISE wäre dies eine „differenzierende Bedingung“ („differentiating condition“). Für den Fall, dass die Sprachentwicklungsauffälligkeiten Bestandteil einer komplexen Störung sind, würde die Diagnose demnach erweitert werden zu: „SES, assoziiert mit [verursachender Faktor]“, also z. B. „SES, assoziiert mit Down-Syndrom (Trisomie 21)“.Zudem sollen „Begleiterscheinungen“ („co-occuring conditions“) ohne kausalen Bezug (z. B. Aufmerksamkeitsdefizit-Hyperaktivitätsstörung (ADHS)) und Risikofaktoren („risk factors“) unterschieden werden. Letztere sind solche, für die eine positive statistische Korrelation mit Sprachstörungen (ebenfalls ohne Kausalität) belegt ist. Das Diskrepanzkriterium zum nonverbalen IQ gemäß ICD-10 wird abgelehnt. Obwohl die CATALISE-Festlegungen die Verwendung von standardisierten Tests zur Identifizierung von DLD nicht infrage stellen, legen sie mehr Wert auf die Auswirkungen der Störung hinsichtlich sozialer und Schulleistungen als auf niedrige Sprachgrenzwerte. Statt Orientierung an statistisch definierten Grenzwerten in standardisierten und deutschsprachig normierten Tests sei vor allem der Schweregrad funktioneller Auswirkungen hervorzuheben.An der Terminologie nach der CATALISE-Konzeption und den hiermit verbundenen Implikationen haben psychologische und medizinische Berufsgruppen, die in ihrem Arbeitsalltag mit sprachentwicklungsgestörten Kindern zu tun haben, deutliche Kritik formuliert [REF]. Der Austausch des Ansatzes „Komorbidität“ durch „verursachende Faktoren“, „Risikofaktoren“ und „Begleiterscheinungen“ könnte eine Unterlassung von Differenzialdiagnostik bedeuten. Unter anderem bestünde die Gefahr, ätiologisch bedeutsame Klassifikationen und differenzialdiagnostische Grenzen zu verwischen und auf ärztliches und psychologisches Fachwissen in Diagnostik und Therapie von SES im Kindesalter zu verzichten [REF].Die geplante Neuausrichtung von Terminologie und Definition löste eine interdisziplinäre Online-Delphi-Befragung1 von Expert*innen zu „Definition und Nomenklatur von Störungen der Sprache im Kindesalter im typischen Zeitraum der Sprachentwicklung“ aus, organisiert und moderiert von der Gesellschaft für Interdisziplinäre Spracherwerbsforschung und Kindliche Sprachstörungen im Deutschsprachigen Raum (GISKID). Die erste Expert*innenbefragung unter registrierten Teilnehmer*innen aus verschiedenen sachrelevanten Berufsgruppen im deutschsprachigen Raum ist abgeschlossen; an ihr nahmen 212 Sprachtherapeut*innen/Logopäd*innen, 142 Ärzt*innen, 40 (Sonder‑)Pädagog*innen, 31 Linguist*innen/Sprechwissenschaftler*innen, 21 Psycholog*innen und 3 Neurowissenschaftler*innen aus deutschsprachigen Ländern teil. Es bestand Einigkeit darüber, dass eine einheitliche Terminologie in allen deutschsprachigen Ländern und Professionen wünschenswert ist. Auf Basis der Ergebnisse der ersten sowie einer weiteren standardisierten Onlinebefragung (an der weniger Expert*innen teilnahmen) fand eine dritte elektronische Befragung statt. In den nächsten Monaten sollen alle Delphi-Runden ausgewertet, evaluiert und publiziert werden. Die Teilnahmerate nahm über die 3 Delphi-Runden ab (Drop-outs), war aber jedes Mal quantitativ von Sprachtherapeut*innen/Logopäd*innen dominiert, obgleich auch unter ihnen Drop-outs waren. Trotz einer elektronischen und damit eher einfach zu handhabenden ökonomischen Befragung scheint die Bereitschaft zur Teilnahme, zumindest für niedergelassene ärztliche und psychologische Expert*innen, tendenziell niedriger gewesen zu sein; die Befragten waren vor allen Sprachtherapeut*innen/Logopäd*innen, was möglicherweise bedeuten kann, dass sich deren Einschätzungen letztendlich leichter durchsetzen.Da internationale Wissenschaftler*innen in Publikationen den
Durch große Fortschritte in der perinatalen Versorgung überleben mehr Kinder, insbesondere extrem Frühgeborene (FG) an der Grenze der Lebensfähigkeit [REF]. Trotz enormer Anstrengungen, das unreife Gehirn zu schützen, persistieren neurologische Einschränkungen wie z. B. Zerebralparese (CP), Epilepsien, sensorische Störungen, kognitive Einschränkungen und Verhaltensauffälligkeiten [REF]. Darüber hinaus gibt es weitere perinatale Hirnschädigungen, deren Ursachen oft unklar bleiben. Dazu zählen die Asphyxie, schwere Infektionen und Inflammation, arteriell-ischämische Schlaganfälle, Sinusvenenthrombosen, periventrikuläre venöse Infarkte, Hirnschädigungen durch Hypoglykämie und intrazerebrale Blutungen mit Komplikationen wie posthämorrhagische Ventrikelerweiterung oder Shunt-pflichtiger Hydrozephalus [REF]. Für die Mehrzahl dieser Erkrankungen gibt es bisher keine kausale Therapie, daher stehen nur präventive und supportive Maßnahmen zur Verfügung. Bei FG sind dies z. B. die antenatale maternale Steroidgabe, verzögertes Abnabeln, Vermeidung von Blutdruckschwankungen und „minimal handling“, bei Reifgeborenen nach Asphyxie oder Schlaganfall z. B. die gezielte Behandlung (subklinischer) zerebraler Anfälle.Die therapeutische Hypothermie (TH) ist die einzige Maßnahme, die als Standardtherapie der moderaten und schweren hypoxisch-ischämischen Enzephalopathie (HIE) eingesetzt wird. Trotz TH leiden Patienten mit moderater und schwerer HIE an neurologischen Langzeitfolgen (16–30 %), was die Suche nach additiven Therapiestrategien notwendig macht. Auch die Frage nach TH für leichtere Verläufe (HIE I) bleibt zu klären, da auch diese Kinder an Spätfolgen leiden [REF].Multiple Mechanismen (Hypoxie, Ischämie, Inflammation, Infektion, Hyperoxie, Exzitotoxizität, oxidativer Stress) sind in die Pathogenese der Hirnschädigung involviert [REF]. Antenatale Faktoren (z. B. maternale Inflammation, intrauterine Wachstumsretardierung) beeinflussen die Vulnerabilität des sich entwickelnden Gehirns. Die Ausprägung des Schadens hängt maßgeblich vom Grad der Hirnreife, der Schadensursache und dem genetischen Hintergrund ab [REF]. Einige Arbeiten konnten zeigen, dass die individuelle Immunantwort und das Geschlecht wesentliche Einflussfaktoren sind [REF]. Auch Reifungsprozesse der weißen Substanz unterscheiden sich signifikant zwischen den Geschlechtern [REF]. Unterschiedliche molekulare Mechanismen, die die Langzeitentwicklung maßgeblich beeinflussen, charakterisieren jede Schadensphase bis hin zur Regeneration [REF]. Vor dem Hintergrund der hohen Plastizität und Fähigkeit zur Regeneration des sich entwickelnden Gehirns ist eine Identifikation der Risikokinder notwendig. Nur so können gezielt frühe Interventionen zur Förderung der Entwicklung eingeleitet werden [REF]. Auch zur Planung von Studien bezüglich innovativer neuroprotektiver Strategien ist die genaue Charakterisierung des Schadens notwendig [REF].Um die betroffenen Patienten korrekt zu identifizieren und unnötige elterliche Ängste zu vermeiden, ist die Frage nach einer frühen Vorhersage der neurologischen Langzeitentwicklung von größter Bedeutung, bleibt aber eine Herausforderung. Präzise Informationen zu klinischem Hintergrund, Komplikationen, Schwere der Symptomatik sowie Typ und Lokalisation der Hirnschädigung müssen zusammengetragen werden. Darüber hinaus kommt der Erfassung von psychosozialen Faktoren (sozioökonomischer Status der Eltern, Bildungsniveau, elterliche Belastung, Erziehungsverhalten, Sprachbarrieren) eine große Bedeutung zu [REF].Eine Schlüsselfunktion hat die Identifikation von Hirnläsionen und funktionellen Störungen. Zur klinischen Diagnostik in der Neonatal- und frühen Säuglingsperiode stehen im Wesentlichen bildgebende Verfahren, insbesondere Sonographie, Magnetresonanztomographie (MRT) am errechneten Termin (ET), Neuromonitoring, wie z. B. das amplitudenintegrierte (a)EEG und/oder das klassische EEG, Nah-Infrarot-Spektroskopie (NIRS), General Movements Assessment (GMA) sowie die frühe klinische Untersuchung mithilfe der Hammersmith Neonatal/Infant Neurological Examination (HNNE/HINE) zur Verfügung. Allen genannten Verfahren gemeinsam ist der hohe personelle und technische Aufwand. Daher sind diese Voraussetzungen nur in wenigen Zentren mit spezifischer Neuromonitoring-Expertise gegeben [REF]. Gegenstand wissenschaftlicher Untersuchungen ist die Identifikation innovativer Biomarkermuster (Omics) und (epi)genetischer Prädispositionen.Diese Übersichtsarbeit fokussiert die Aussagekraft der genannten Verfahren und deren Kombinationsmöglichkeiten. Zudem wird ein Ausblick gegeben, wie innovative Verfahren die Prädiktion der Entwicklungsprognose dieser Kinder vereinfachen können.
Bedingt durch die COVID-19-Pandemie und das damit einhergehende Post-COVID-Syndrom, unter dem bis zu 15 % der an dem Coronavirus Erkrankten [REF] nach Abklingen der üblichen Beschwerden leiden, hat das Symptom „Fatigue“ deutlich an Bedeutung gewonnen und erlangt nun zunehmende wissenschaftliche und therapeutische Aufmerksamkeit, wie z. B. zur Entwicklung der S1-Leitlinie Post-COVID/Long-COVID [REF]. Auch deshalb, weil die Post-COVID-Fatigue von betroffenen Patient*innen als das häufigste Symptom angegeben wird (vgl. [REF]).So leidet noch Wochen oder Monate nach der Infektion ein nicht unerheblicher Prozentsatz der Betroffenen an Fatigue [REF]. Trotz vollständiger Erholung von der Coronainfektion klagen noch nach zwei bis drei Monaten 71 % bis ca. 87 % der Betroffenen über eine Fatigue [REF]. Ebenso berichtete mehr als die Hälfte aller Patient*innen noch zehn Wochen nach den Infektionssymptomen über eine weiterhin bestehende Fatigue. In einem Review zeigte sich zudem, dass mit 44 % die Fatigue das meist berichtete Symptom bei Post-COVID-Erkrankten ist [REF].Dies führt nun dazu, dass eine adäquate Definition des Begriffs Fatigue notwendig ist, um daraus ableitbare Erklärungen zur Genese sowie für Diagnose- und Therapieempfehlungen gewinnen zu können. Es wird aber im Rahmen der Forschung und der pandemiebedingten Häufung von Patient*innen mit Fatigue deutlich, dass es keine einheitliche Definition für den Begriff Fatigue gibt.So kann Fatigue als allgemeines Gefühl der Kraftlosigkeit oder aber als subjektives unangenehmes Gefühl von Energiemangel bis zum Burnoutgefühl beschrieben werden [REF]. Aber sowohl Definition wie auch Ursachen der Fatigue differieren in Abhängigkeit des jeweils betrachteten Krankheitsbildes, was dazu führt, dass es unterschiedliche krankheitsspezifische Fatiguedefinitionen gibt [REF]. Zudem verwenden Patient*innen, die ihre Symptomatik im alltäglichen Klinikalltag beschreiben, scheinbar nahezu durchgehend die Begriffe Müdigkeit, Fatigue und Erschöpfung synonym. Gleiches gilt aber auch für die wissenschaftlich und therapeutisch Arbeitenden.Es scheint jedoch relevante Unterschiede der Begrifflichkeiten vor allem in Bezug auf die Therapie der jeweiligen Beschwerden zu geben. Vor diesem Hintergrund ist es deshalb aus unserer Sicht zwingend erforderlich, eine Differenzierung zwischen Müdigkeit, Fatigue, und Erschöpfung zu diskutieren.In ähnlicher Weise hat Olson bereits 2007 eine Neuinterpretation der Begriffe Müdigkeit, Fatigue, und Erschöpfung vorgenommen [REF]. Sie beschreibt diese als distinkte Zustände, die aber in einem Kontinuum in Relation zueinander gesetzt werden können. Bei der Zuordnung dieser drei Zustände in einem Kontinuum orientiert sie sich an dem biologischen Modell des Adaptionssyndroms nach Selye [REF], welches drei wesentliche Phasen der Adaption eines Organismus auf externe Reize beschreibt: eine Alarmreaktion, eine Resistenzphase und eine Erschöpfungsphase. Des Weiteren beschreibt sie Unterschiede der drei Begriffe in sechs Kategorien. Konkret waren dies: Schlafqualität, Kognition, Durchhaltevermögen, emotionale Reaktion, Kontrolle über körperliche Begriffe und soziale Interaktion.Olson legte 2007 mit der Unterscheidung der drei Begriffe Müdigkeit, Fatigue und Erschöpfung und der Verortung auf einem Kontinuum einen Grundstein. Interessant ist für uns daher, ob seit diesem Artikel weitere Forschung zur Verifikation, Weiterführung oder Widerlegung stattgefunden hat. Allerdings wird bei einer umfassenden Literaturrecherche deutlich, dass die Begrifflichkeiten entweder weiterhin einheitlich verwendet werden, nicht einheitlich unterschieden werden oder sich ausschließlich auf Olsons damalige Studie beziehen. Demnach ist es auffällig, dass die dynamische Weiterentwicklung in dieser Forschungsfrage ausgeblieben zu sein scheint. Ziel dieses Artikels ist es daher, einen Überblick über die Definitionen der Begriffe Müdigkeit, Fatigue und Erschöpfung und ihrer Differenzierung voneinander zu geben, aber auch mögliche Ideen zur Diagnostik und Interventionen für die Therapie im klinischen Alltag herzuleiten.
Die Versorgung von Patient:innen mit Herz-Kreislauf-Erkrankungen stellt eine große Herausforderung für die deutschen Krankenhäuser dar. Laut Statistischem Bundesamt sind im Jahr 2019 rund 330.000 Menschen an einer Herz-Kreislauf-Erkrankung, 44.000 davon an einem Myokardinfarkt, verstorben. Damit bilden diese Erkrankungen die häufigste Todesursache in Deutschland [REF]. Der (infarktbedingte) kardiogene Schock (IKS) ist ein häufiges Krankheitsbild auf deutschen Intensivstationen und geht mit einer hohen Letalität einher. Die intensivpflegerische und intensivmedizinische Versorgung erfordert ein interprofessionelles und interdisziplinäres Team mit standardisierten Abläufen, um u. a. eine frühzeitige koronare Rekanalisation zu erreichen, mit dem Ziel, die Mortalität möglichst zu senken.Die klinische Versorgung von Patient:innen mit (infarktbedingtem) kardiogenen Schock ist komplex und beginnt bereits mit der Aufnahme in der Zentralen Notaufnahme (ZNA). Der Versorgungsprozess erstreckt sich über das Herzkatheterlabor bis hin zur Intensivstation oder Chest Pain Unit. Bei der Versorgung von Patient:innen mit (infarktbedingtem) kardiogenen Schock kann es jederzeit zu akuten Komplikationen kommen. Daher ist die Einleitung der Sofortmaßnahmen ebenso wichtig wie die Kenntnis der Pflegediagnostik, zutreffender Pflegediagnosen sowie das Vorgehen bei der Nachsorge und Rehabilitation.Diese S1-Leitlinie soll die Komplexität der pflegerischen Versorgung des IKS, ergänzend zur S3-Leitlinie „Infarktbedingter kardiogener Schock – Diagnose, Monitoring und Therapie“ [REF], abbilden und als eine Empfehlung zur Qualitätssicherung und Entscheidungshilfe bei der Versorgung dieser Patient:innengruppe dienen. Neben den bereichsübergreifenden Tätigkeiten benötigen die professionell Pflegenden in den einzelnen Abteilungen zusätzlich spezifische Kompetenzen zur Versorgung der Patient:innen vor Ort. In der vorliegenden Leitlinie sollen sowohl die bereichsübergreifenden Aufgaben als auch die unterschiedlichen Qualifizierungen berücksichtigt werden.Die Qualifizierungsmöglichkeiten für professionell Pflegende in Deutschland sind derzeit vielfältig, und die Implementierung der einzelnen Berufsabschlüsse in der Praxis ist sehr heterogen. Die Steuerungsgruppe dieser Leitlinie hat sich auf folgende Unterscheidungen in der Qualifizierung der Pflegenden geeinigt: 1. Pflegefachpersonen bzw. Pflegefachkraft bezeichnet Pflegende mit einer 3‑jährigen Ausbildung oder primär qualifizierendem Studium mit Erlaubnis zur Führung der Berufsbezeichnung gemäß § 1 nach Pflegeberufegesetz (PflBG). 2. Weitergebildete Pflegefachpersonen bzw. Pflegefachkraft bezeichnet Pflegende, welche nach der 3‑jährigen Ausbildung zusätzlich eine staatlich anerkannte bzw. nach DKG-Empfehlungen durchgeführte 2‑jährige berufsbegleitende Weiterbildung zur Intensivpflege und Anästhesie oder Notfallpflege oder ein gleichwertiges Studium absolviert haben. 3. Spezialisierte Pflegefachpersonen bzw. Pflegefachkraft bezeichnet Pflegende, welche nach der 3‑jährigen Ausbildung eine Zusatzqualifikation in Form einer zertifizierten, fachspezifischen Fortbildung, z. B. Pflegeexpert:in (Chest Pain Unit) absolviert haben. Die Leitliniengruppe stimmt darin überein, dass eine akademische Qualifikation im Sinne einer Advanced Nursing Practice (ANP) auf Masterniveau von o. g. Pflegefachpersonen eine sinnvolle Ergänzung sein kann [REF]. ANPs sollen entsprechend der internationalen Definition des ICN qualifiziert sein und über eine fachpraktische Spezialisierung im jeweiligen Fachbereich, in dem sie eingesetzt werden, verfügen.
Die Prävalenzraten für Alkoholgebrauchsstörungen liegen in Deutschland bei ca. 6 %, alkoholabhängig sind ca. 3 %. Nur ca. 10 % der Patienten befinden sich in suchtmedizinischer Therapie. In der Früherkennung und -intervention besteht zudem eine deutliche Unterversorgung. Eine bislang nicht evaluierte Ergänzung zu bestehenden Interventionsangeboten ist der außerhalb der professionellen Suchthilfe von einer ehemaligen Betroffenen entwickelte Internetauftritt „Ohne Alkohol mit Nathalie“ (OAmN). Die vorliegende Pilotstudie hatte zum Ziel, herauszufinden, ob die Nutzer der OAmN-Angebote zu jenen zählen, die bisher vom Suchthilfesystem nicht erreicht werden konnten.In Deutschland liegen die Prävalenzraten für Alkoholgebrauchsstörungen bei ca. 6 %; als alkoholabhängig gelten 3,1 % der 18- bis 64-Jährigen; weitere 2,8 % betreiben Alkoholmissbrauch [REF]. Neben volkswirtschaftlichen Schäden führt dieses weit verbreitete Konsummuster auf individueller Ebene zu psychischen und physischen Krankheiten, die durch Abstinenz verhindert werden könnten [REF].Ohne Therapie ist die Prognose von Alkoholgebrauchsstörungen meist ungünstig [REF]. In den USA sterben beispielsweise jährlich annähernd 88.000 Menschen auf Grund ihres Alkoholkonsums [REF], in Deutschland wird die Zahl der alkoholassoziierten Todesfälle mit 74.000 angegeben [REF]. Dennoch erreichen die aktuellen Therapieangebote hierzulande nur 10 % der Betroffenen [REF]. Die meisten kommen erst nach vielen Jahren der Erkrankung in suchtmedizinische Behandlung [REF]. Noch schlechter sieht es bei jenen aus, die eine Alkoholabhängigkeit entwickeln: „Insbesondere in den Bereichen der Früherkennung und Frühintervention besteht für Menschen mit alkoholbezogenen Störungen bislang eine deutliche Unterversorgung“ [REF]. Die aktuelle S3-Leitlinie besagt: „Die genannte Unterversorgung geht allerdings nur zum Teil auf die oben schon angesprochenen Defizite auf der Angebotsseite zurück. Viele Betroffene sind unsicher und schrecken gerade zu Beginn einer Abhängigkeit vor dem Aufsuchen einer Beratung und Behandlung zurück“ [REF]. Eine signifikante Anzahl von Patienten kommt somit nicht ins professionelle Suchthilfesystem.Es bedarf daher niederschwelligerer und alternativer Interventionsangebote, die Betroffene in ihrem Alltag erreichen und unterstützen, weil es „[b]edeutsam ist, dass Menschen mit alkoholbezogenen Störungen möglichst frühzeitig und nahtlos zielgerichtete Hilfe erhalten“ [REF]. Hier können webbasierte „e“-Interventionen eine entscheidende Rolle spielen, wozu eine sehr heterogene Gruppe von Maßnahmen zählt [REF], darunter Aufklärung und Information, strukturierte Therapieangebote sowie – im weitesten Sinne – Interventionen von Selbsthilfegruppen. Eine vor Kurzem veröffentlichte skandinavische Studie zeigte, dass Onlineinterventionen gerade aufgrund ihrer Anonymität und Verfügbarkeit gewählt wurden [REF]. Dabei wurde der Aspekt der persönlichen Identifizierung mit den Inhalten als besonders wichtig angegeben, um die Teilnehmer zur Reflektion anzuregen und Abstinenz zu erreichen [REF]. Eine aktuelle systematische Literaturübersicht, die den PRISMA-Guidelines entsprach, zu verschiedenen Suchterkrankungen (Alkoholmissbrauch, Glücksspiel, „binge eating disorder“) zeigte, dass sich von 32 Studien zu Alkoholmissbrauch zwei Drittel als effektiv erwiesen, wobei wiederum zwei Drittel der erfolgreichen Studien einen hohen Qualitätsstandard durch vorherige Entwicklung eines „quality assessment tool“ („selection bias of patients“, „study design“, „blinding“, „drop out rate“ etc.) aufwiesen [REF].Eine stark genutzte deutschsprachige Maßnahme ist das webbasierte Angebot „Ohne Alkohol mit Nathalie“ (OAmN) von Nathalie Stüben, die selbst betroffen war und ihre Alkoholabhängigkeit eigenständig überwunden hat. Ihr Angebot existiert seit Oktober 2019 und gliedert sich in zwei Bereiche: Öffentlichkeitsarbeit und Onlineprogramme. Ersterer umfasst Aufklärungsarbeit (u. a. via Newsletter, Instagram, Podcast, YouTube und klassischer Pressearbeit). Mit den Onlineprogrammen Die ersten 30 Tage ohne Alkohol und Abstinenz stabilisieren werden Betroffene im „Graubereich“ zwischen „Genusstrinken“ und (körperlicher) Abhängigkeit dabei unterstützt, ein Leben ohne Alkohol zu führen – und das als Gewinn zu betrachten (im Sinne einer zufriedenen Abstinenz). Die Programme kombinieren Wissensvermittlung und Motivation durch persönliche Videoansprache, Begleitmaterialien und eine moderierte Onlinegruppe, in der die Betroffenen sich austauschen können. Die vorliegende Pilotstudie hatte zum Ziel, herauszufinden, ob die Nutzer der OAmN-Angebote zu jenen zählen, die bisher vom Suchthilfesystem nicht erreicht werden konnten.
Das adaptive Immunsystem hat die Aufgabe, eindringende Krankheitserreger zu erkennen und zu zerstören. In Verbindung mit dem angeborenen Immunsystem vermittelt es auch die allogene Transplantatabstoßung über zelluläre und humorale Mechanismen. Eigengewebe wird durch das adaptive Immunsystem aufgrund existierender Immuntoleranzmechanismen nicht angegriffen.Die Immunantwort ist im Regelfall durch ein homöostatisches Gleichgewicht zwischen dem regulatorischen Suppressor- und dem Effektorarm gekennzeichnet. Versagt die Suppression zur Steuerung des Effektorarms, gerät das Immunsystem aus dem Gleichgewicht und es entstehen Autoimmun- und Entzündungskrankheiten [REF]. Die derzeitig verfügbaren therapeutischen Konzepte zur Bekämpfung dieser Krankheiten beschränken sich auf den Einsatz unspezifischer Immunsuppressiva [REF]. Der zunehmende Erkenntnisgewinn zur Funktionsweise dieses Gleichgewichts zwischen Regulator- und Effektorarm erlaubt die Entwicklung neuer biologischer Arzneimittel zur Bekämpfung dieser Krankheiten.Die Immuntoleranz wird durch komplementäre rezessive und dominante Mechanismen aufrechterhalten. Rezessive Immuntoleranzmechanismen laufen innerhalb einer Zelle ab. Dazu gehört die Zerstörung selbstreaktiver Immunzellen sowie eine Erhöhung der Anzahl inhibitorischer Rezeptoren auf diesen Zellen, um die Schwelle für ihre Aktivierung zu heben. Im Gegensatz dazu sind dominante Immuntoleranzmechanismen zellextrinsisch und werden von Untergruppen spezialisierter Immunzellen, wie den regulatorischen T‑Zellen (Tregs) ausgeführt. Tregs sind in der Lage, die Aktivierung, Expansion und Funktion anderer Immunzellen aktiv zu unterdrücken, und sind die einzigen Zellen, die ausschließlich der Induktion und Aufrechterhaltung der Immuntoleranz dienen [REF]. Etwa 5–7 % aller CD4+-T-Zellen im menschlichen Körper sind regulatorische T‑Zellen. Die Identität und Funktion von Tregs werden durch die Expression verschiedener Proteine in diesen Zellen charakterisiert. Dazu gehören, neben dem zytotoxischen T‑Lymphozyten-Protein 4 (CTL4-A), der Interleukin(IL)-2-Rezeptor-Untereinheit‑α (CD25) und dem die Transkription aufrechterhaltenden Faktor STAT5 [REF], insbesondere das Forkhead-Box-Protein P3 (FOXP3). Die Schlüsselfunktion von FOXP3 für die Aufrechterhaltung der Immuntoleranz durch die regulatorischen T‑Zellen wird durch FOXP3-Genmutationen deutlich. FOXP3-defiziente Mäuse entwickeln ein autoinflammatorisches Syndrom in mehreren Organen [REF], beim Menschen sind FOXP3-Mutationen ursächlich für das immun-dysregulations-polyendokrinopathie-enteropathie-X-chromosomale (IPEX-)Syndrom, das in den ersten Lebensjahren bei Knaben auftritt und eine Knochenmarktransplantation erfordert [REF]. Man unterscheidet zwei Arten von Treg-Zellen, t‑Tregs und p‑Tregs, basierend auf ihrem Ursprung im Thymus oder in der Peripherie. Das T‑Zellrezeptorrepertoire der beiden Zellpopulationen ist unterschiedlich [REF]. Die t‑Treg-Zellen bilden eine stabile Population von Suppressorzellen mit einer Häufung von T‑Zellantigenrezeptoren (TCRs), welche Selbstantigene erkennen. P‑Tregs werden in der Peripherie aus herkömmlichen CD4+-T-Zellen, die mit Antigenen, insbesondere im Darm, in Kontakt kamen und durch hohe Mengen an transformierendem Wachstumsfaktor‑β (TGFβ) und Retinsäure stimuliert wurden, erzeugt [REF]. Das TCR-Repertoire für p‑Treg-Zellen umfasst auch TCRs, die nicht selbst Antigene aus Viren oder kommensalen Mikroorganismen, die für die Aufrechterhaltung der Schleimhauttoleranz wichtig sind, erkennen [REF]. Derzeit ist kein Marker bekannt, mit dem sich beim Menschen t‑Treg-Zellen von p‑Treg-Zellen unterscheiden lassen. Daher sind Treg-Zellen, die zu therapeutischen Zwecken aus peripherem Blut isoliert wurden, wahrscheinlich eine Kombination von t‑Treg-Zellen und p‑Treg-Zellen.Treg-Zellen vermitteln die Immuntoleranz über verschiedene Mechanismen [REF]. Sie exprimieren entzündungshemmende Mediatoren wie IL-10, TGFβ und IL-35 und reduzieren den IL-2-Spiegel. Darüber hinaus exprimieren sie negativ regulatorische Zelloberflächenrezeptoren wie CTLA‑4, CD39 und CD73, wodurch sie T‑Zellen direkt oder indirekt durch Modulation von antigenpräsentierenden Zellen ansprechen. Zusätzlich führt die Bindung von Tregs an antigenpräsentierende Zellen zur Trogozytose, wodurch die Antigenpräsentation verringert wird. Treg-Zellen können durch die immunsuppressiven Proteine auch das Mikromilieu im Gewebe verändern. Dadurch proliferieren weitere immunsuppressive Zellpopulationen wie Treg-Zellen mit unterschiedlichen Spezifitäten und T‑regulatorische Tr1-Zellen [REF]. Zahlreiche Autoimmunkrankheiten haben ihre Ursachen in einem Mangel an funktionellen Treg-Zellen oder in einer Mutation in den für die Treg-Funktion essenziellen Proteinen. Der Mangel an STAT5b resultiert in einem Immundefekt [REF], Mutationen im CTLA4-Protein in der Entwicklung eines autosomal dominanten Immundysregulationssyndroms [REF]. Durch eine verminderte Expression von FOXP3 in Treg-Zellen von Patienten mit Myasthenia gravis werden autoreaktive T‑Zellen weniger gehemmt, die charakteristischen Symptome dieser Muskelschwächekrankheit verschlimmern sich [REF]. Bei Patienten mit systemischem Lupus erythematodes ist der Anteil aktivierter Treg-Zellen in den frühen Krankheitsphasen verringert [REF] und die Funktion der vorhandenen Treg-Zellen durch eine geringe Menge an phosphorylierten STAT5 beeinträchtigt [REF]. Patienten mit Lupusnephritis und antineutrophiler zytoplasmatischer antikörperassoziierter Vaskulitis haben ebenfalls geringere Level an Treg-Zellen in den Nieren [REF]. Ebenso ist in den frühen Phasen des Typ-1-Diabetes die Anzahl der Treg-Zellen signifikant verringert [REF]. In Psoriasispatienten ist die Zahl an CCR5 exprimierenden Treg-Zellen reduziert, ebenso sind die Funktion und die chemotaktischen Eigenschaften der Zellen eingeschränkt [REF]. Eine beeinträchtigte regulatorische Funktion der Tregs wurde auch bei Patienten mit multipler Sklerose beobachtet, was letztendlich zum Verlust der immunologischen Selbsttoleranz führt [REF]. Die Rolle von Treg-Zellen ist bei Organtransplantationen und bei Tumoren weitaus komplexer einzuschätzen. Normalerweise proliferieren Tregs während der aktiven Immunreaktion auf das Transplantat und infiltrieren das transplantierte Organ allmählich. In den frühen Phasen nach Transplantation können die Treg-Zellen die alloimmune Entzündungsreaktion jedoch noch nicht unterdrücken [REF]. Bei Tumorpatienten ist die Zahl der Treg-Zellen im Blut und auch im Tumorgewebe erhöht. Die Rekrutierung von Tregs in den Tumor wird durch Chemokine vermittelt, die im Tumormikromilieu produziert werden. Tregs unterdrücken dort die Antitumorimmunität, indem sie ein Eliminieren der Tumorzellen durch antigenspezifische CD8+-T-Zellen hemmen [REF]. Auch scheinen die Tumorzellen die intratumorale Retention und das Überleben der Treg-Zellen zu begünstigen [REF]. Daher wird die intratumorale Akkumulation von Treg-Zellen als Marker für einen metastasierten Tumor im fortgeschrittenen Stadium und für eine verringerte Überlebensrate bei Krebspatienten diskutiert [REF].Mit diesem Wissen wurden in den letzten Jahren Strategien entwickelt, um Treg-Zellen therapeutisch zu nutzen. Allerdings gibt es noch keine spezifischen Arzneimittel, die die Treg-Aktivität positiv oder negativ beeinflussen können. IL‑2 kann zwar die Treg-Zellfunktion verbessern, es fördert aber allerdings auch die Funktion der Effektor-T-Zellen (Teffs). Da Treg-Zellen eine höhere Affinität zu IL‑2 und eine stärkere IL-2-rezeptorvermittelte Signalübertragung aufweisen als Teff-Zellen, wurde Patienten in klinischen Studien nur niedrig dosiertes IL‑2 verabreicht. Eine Verbesserung der klinischen Symptomatik wurde bei Patienten mit Hepatitis-C-Virus(HCV)-induzierter Vaskulitis [REF] und steroidresistenter Graft-versus-Host Disease (GvHD) beobachtet [REF]. Die Identifikation eines sicheren und spezifischen therapeutischen Dosisfensters gestaltet sich allerdings schwierig, da dieses stark von patienten- und indikationsspezifischen Parametern abhängt und eine suboptimale Dosis zur Stimulation von Effektor-T-Zellen führen würde. Ein alternativer Ansatz zur Treg-Beeinflussung mittels IL‑2 ist die Gabe autologer ex vivo expandierter Treg-Zellen. Mit diesem zelltherapeutischen Ansatz wurden bereits vielversprechende Ergebnisse bei Patienten mit chronischer GvHD [REF] und Typ-1-Diabetes [REF] erzielt. Die ex vivo expandierten polyklonalen Zellen zeigten jedoch eine limitierte Lebensdauer und ihre Funktion war durch den Mangel an Spezifität eingeschränkt. Zielführender wäre hier sicherlich die Verwendung antigenspezifischer Treg-Zellen [REF]. Bei Krebspatienten wurde im Gegensatz dazu die Depletion von Treg-Zellen versucht. Hierbei besteht jedoch die Gefahr von schweren Nebenwirkungen durch Autoimmuneffekte [REF]. Mit NRP1, einem in intratumoralen Tregs selektiv und stark exprimierten Rezeptor für den Liganden SEMA4A, wurde ein potenzielles Ziel identifiziert, die immunsupprimierende Funktion intratumoraler Tregs aufzuheben. Damit könnte die Wirksamkeit von Immuncheckpointinhibitoren verbessert werden, während, anders als bei der Treg-Depletion, die periphere Immuntoleranz unverändert ist [REF]. Die jüngsten Entwicklungen erlauben die Expression von klonalen, spezifischen rekombinanten TCRs oder chimären Antigenrezeptoren (CARs) in Treg-Zellen nach Transfer von Nukleinsäuren, die für diese Proteine kodieren, um die Spezifizität der Tregs gegen die entsprechenden Antigene im betroffenen Gewebe umzuleiten. Durch CRISPR-Cas9-mediiertes Genom-Editing ließen sich gleichzeitig mehrere Gene in den Tregs spezifisch editieren oder HLA-defiziente Tregs designen, um allogene Treg-Zellen „off-the-shelf“ für verschiedene klinische Anwendungen zur Verfügung zu stellen. Mit derartigen Strategien ließen sich Treg-Zellen mit verbesserter Funktionalität und schnellerer Verfügbarkeit für therapeutische Ansätze bei Organtransplantationen und zur Behandlung von Autoimmunerkrankungen oder Krebs generieren. Im Folgenden werden die regulatorisch-wissenschaftlichen Anforderungen und Herausforderungen hinsichtlich Herstellung und Qualitätskontrolle sowie nichtklinischer und klinischer Testung genetisch modifizierter regulatorischer T‑Zellen als Arzneimittel für neuartige Therapien diskutiert.
In der Europäischen Union werden Krankheiten als „Seltene Erkrankungen“ (SE) bezeichnet, wenn nicht mehr als 5 von 10.000 Menschen betroffen sind. Aufgrund der Seltenheit sind klinische Expertise und qualitätsgesicherte Versorgungsstrukturen rar, die Forschung ist hier im Vergleich zu anderen Krankheiten erschwert. Nationale und länderübergreifende Netzwerke im Bereich der SE sind geeignet diese grundlegenden Probleme zu überwinden. In diesem Artikel wird diese Hypothese anhand der Darstellung und Erläuterung ausgewählter datenbasierter Leistungen der Europäischen Referenznetzwerke (ERN) für Seltene und Komplexe Erkrankungen plausibilisiert. „Seltenheit“ ist dabei konzeptionell und realiter mehrdimensional und bezieht sich auf: die Anzahl der Patient*innen einer SE, die Seltenheit von klinischen Expert*innen für bestimmte SE, die Seltenheit von Wissen und Evidenz über individuelle Erkrankungen und die Seltenheit von qualitätsgesicherten Versorgungsstrukturen.Netzwerke im Gesundheitswesen stellen eine grundlegende Neuerung für die medizinische Versorgung dar. Für die Leistungserbringung in Versorgernetzwerken ist die Kooperation der Netzwerkteilnehmenden essentiell. Netzwerke entstehen und verstetigen sich nicht von selbst. Als Schlüsselfaktoren für die Entwicklung und den Erfolg von Kooperationsnetzen im Gesundheitswesen wurden die gemeinsame Nutzung von Wissen, ein positives soziales Klima und enge Beziehungen zwischen den Netzwerkpartnern ermittelt. Vor allem die gemeinsame Nutzung von Wissen ist wichtig [REF]. Zu den Strategien für die Nutzung von Wissen gehören die Verteilung von Wissen, die Vermittlung von Wissen zur Förderung und Unterstützung der Teilnahme und die Einbeziehung von Interessengruppen in das Netz sowie die Steuerung von Wissen zur Unterstützung der Einrichtung formeller Partnerschaften und Strategien [REF]. Die Einbindung der Betroffenen und ihrer Angehörigen in den Wissenstransfer hat sich im Bereich der SE als essentiell erwiesen [REF]. ERN für Seltene und Komplexe Erkrankungen adressieren das grundlegende Problem der SE durch grenzübergreifende Zusammenarbeit der wichtigsten Interessensgruppen in diesem Bereich, der Patient*innen und der klinischen Expertisezentren, im europäischen Kontext. Die ERN folgen dabei der Einsicht, dass kein europäischer Staat allein die aus der Seltenheit der SE resultierenden Probleme lösen kann. In Hinsicht auf die Anzahl der Patient*innen und der Expert*innen ist der Mehrwert der Netzwerke offensichtlich. Für die Überwindung der dritten und vierten Dimension der Seltenheit, der Seltenheit von Wissen und Evidenz und der Seltenheit von qualitätsgesicherten Versorgungsstrukturen, ist die Situierung der Netzwerke im digitalen Gesundheitsdatenraum essentiell.Wichtige Aspekte der Leistungserbringung der ERN basieren auf Datensammlung, Datenaustausch sowie Datenanalyse und Datennutzung. Datenstandards sind dabei ein essentielles Werkzeug zur Ermöglichung dieser Datenaktivitäten (siehe Artikel von Robinson und Graessner in diesem Themenheft [REF]). Die diesbezügliche Leistung der ERN umfasst die Ermöglichung und Umsetzung der diagnostischen Kodierung der SE in der Europäischen Union (EU) – um die SE überhaupt in den Gesundheitssystemen sichtbar zu machen –, die Durchführung von virtuellen, grenzübergreifenden Fallkonferenzen und die Etablierung von europäischen Registern, die Versorgungsdaten sammeln und in Hinsicht auf Versorgungsqualität analysieren können.Allerdings ist für die digitale Leistungserbringung der ERN und die Nutzung der Potenziale des Gesundheitsdatenraumes die Zweitnutzung von Daten wie Laboranalysen, Bildern, klinischen Befunden oder patientengenerierten Befunden notwendig. Im Datenraum der ERN verbinden sich insofern die lokale Datenerzeugung und Dokumentation in den Expertisezentren auf der einen Seite und die ERN-Dateninfrastrukturen auf der anderen Seite. Da die Gesundheitsversorgung in der EU in der Verantwortung der Mitgliedsstaaten liegt, kommt den nationalen Gesundheitssystemen eine integrative Funktion zu. Allerdings wird diese – insbesondere datenintegrative – Funktion gegenwärtig noch nicht bzw. auf sehr heterogene Art und Weise wahrgenommen. In Deutschland werden die Erschließung und Bereitstellung von im klinischen Kontext erfassten Daten für Forschungs- und Versorgungsfragen insbesondere durch die vom Bundesministerium für Bildung und Forschung geförderte Medizininformatik-Initiative (MI-I) adressiert. Im Rahmen dieser entstehen an allen deutschen Universitätskliniken spezialisierte Datenintegrationszentren.Eine funktionierende Integration der ERN und der nationalen Gesundheitssysteme ist eine essentielle Bedingung für die nachhaltige Realisierung der Potenziale der ERN. Der Aspekt der Integration wird im letzten Abschnitt dieses Artikels im Hinblick auf den Mehrwert datenbasierter Leistungen in und für europäische und deutsche Netzwerke aufgegriffen. Datenbasierte Leistungen sind an dieser Stelle Leistungen, die auf der Basis von standardisiert gesammelten und in digitaler Form vorliegenden Daten erbracht werden. Im nächsten Abschnitt wird zunächst der klinische Mehrwert der genannten 3 datenbasierten Leistungserbringungen, d. h. diagnostische Kodierung, grenzüberschreitende Fallkonferenzen und ERN-Register für die Versorgung der SE-Patient*innen analysiert. Aspekte des Datenschutzes und der Daten-Governance stehen hingegen nicht im Fokus.
Die durch die „coronavirus disease 2019“ (COVID-19) ausgelöste Pandemie hat sich in den letzten Jahren zu einem besonderen Thema der gesundheitsbezogenen Sorgen im Alter („health anxiety“) entwickelt. Unter gesundheitsbezogenen Sorgen werden gemeinhin Befürchtungen über eine Verschlechterung der Gesundheit und das potenzielle Vorhandensein von Krankheiten bezeichnet. Diese allgemeinen Sorgen können Ausprägungen von schwachen gesundheitsbezogenen Bedenken bis hin zu hypochondrischen Krankheitsüberzeugungen aufweisen [REF]. Während sich Erstere durchaus auch positiv auf das Gesundheitsverhalten auswirken können, stellen Letztere eine psychische Erkrankung dar [REF]. Zu Beginn der Pandemie wurden ältere Menschen und v. a. Pflegeheimbewohner*innen als Risikogruppe für einen schweren Verlauf der Erkrankung adressiert [REF], was mit strengen Einschränkungen der sozialen Kontakte einherging sowie mit erhöhten psychischen Belastungen und Ängsten in Verbindung gebracht wurde [REF].Im Rahmen bisheriger Forschungsprojekte wurden die Bewohner*innen von stationären Langezeitpflegeeinrichtungen vergleichsweise selten direkt zu ihren Corona-Sorgen befragt. Die vorhandenen Studien weisen v. a. ein qualitatives Design auf, und ihre Ergebnisse zeigen, dass die pandemiebedingten Einschränkungen bei Pflegeheimbewohner*innen zur Verringerung der Lebensqualität sowie zur Verstärkung von Depression und Angst geführt haben [REF]. Bewohner*innen berichten von Angst und Einsamkeit, die v. a. durch fehlende soziale Kontakte zu nahestehenden Personen und fehlende soziale Aktivitäten verstärkt wird [REF]. In der Publikation einer weiteren Studie wird beschrieben, dass sich die sozialen Beziehungen durch die Einschränkungen deutlich gewandelt haben sowie neue Unsicherheiten und Ängste erlebt wurden [REF].Ergebnisse von Studien mit der älteren Bevölkerung in Privathaushalten unterstreichen diese Befunde und zeigen, dass ältere Menschen, die Einsamkeit erlebt hatten, signifikant höhere Corona-Sorgen aufweisen als jene, die keine Einsamkeit angaben [REF]. Ebenfalls konnte in einer Langzeitstudie von 2015 bis 2020 festgestellt werden, dass die Pandemie v. a. bei einsamen älteren Menschen zu höheren allgemeinen Ängsten geführt hat [REF]. Resultate aus Studien mit älteren Menschen in Privathaushalten unterstreichen die Bedeutung von sozialen Beziehungen als protektiven Faktor gegenüber Sorgen, die mit der COVID-19-Pandemie zusammenhängen.Erklärt werden können solche Befunde mit Referenz auf die sozioemotionale Selektivitätstheorie [REF], die davon ausgeht, dass soziale Beziehungen eine protektive Wirkung haben, da sie zur Regulation von negativen Emotionen beitragen. Die Theorie geht davon aus, dass gerade bei Menschen im höheren Alter enger Kontakt zu nahestehenden Menschen wichtig für das Abfangen von negativen Emotionen, Ängsten und Sorgen ist. Zentrale Annahme der Theorie ist, dass die Motivation für Handlungen entweder auf die Aneignung von Wissen oder die Regulation von Emotionen ausgerichtet ist. Ältere Personen nehmen die verbliebene Lebenszeit als begrenzt wahr und fokussieren emotionale Ziele. Carstensen et al. konnten [REF] in mehreren Studien deutlich machen, dass enge, jahrelange Beziehungen bei Menschen im höheren Alter von großer Bedeutung für das Erleben von sozialer Einbindung und positiven Emotionen sowie den Umgang mit negativen Emotionen sind.Gerade bei Bewohner*innen von Pflegeheimen unterlagen die sozialen Kontakte jedoch strengen Einschränkungen. Wie die Pflegeheimbewohner*innen den ersten Lockdown und Sommer 2020 erlebten, wie sehr sie durch Sorgen und Ängste belastet waren, und welche Rolle soziale Kontakte beim Umgang mit diesen Sorgen spielten, ist bisher kaum quantitativ erforscht worden. Hier setzt der vorliegende Beitrag an und präsentiert die Ergebnisse einer repräsentativen Befragung von 259 Pflegeheimbewohner*innen in Österreich, um die folgende Forschungsfrage zu beantworten: Wie beeinflussten die sozialen Beziehungen von Pflegeheimbewohner*innen ihre Corona-Sorgen während des ersten Lockdowns und des Sommers 2020? Dabei wird folgende Hypothese überprüft: Pflegeheimbewohner*innen, die einen weniger engen Kontakt zu anderen Personen (Angehörigen, Pflegekräften, andere Bewohner*innen) und eine stärkere Einsamkeit während der Pandemie angeben, weisen eher Corona-Sorgen auf.
Bürgerkriege, Naturkatastrophen und Armut führten in den Jahren 2015 und 2016 mehr als eine Million Menschen als Flüchtlinge nach Deutschland. Dadurch ergaben sich erhebliche gesellschaftliche Herausforderungen, die weitreichende und bis heute anhaltende Veränderungen nicht nur in Deutschland, sondern in ganz Europa auslösten.Auch in den letzten Jahren bleibt die Zahl flüchtender Menschen hoch. Weltweit sind nach Angaben des United Nations High Commissioner for Refugees (UNHCR) im Jahr 2022 über 80 Mio. Menschen auf der Flucht. Mit zuletzt gut 190.000 neuen Asylanträgen im Jahr 2021, davon etwa 73.000 (entsprechend 39 %) von Minderjährigen, bleibt Deutschland eines der Hauptaufnahmeländer von Asylsuchenden in Europa [REF]. Angesichts des Krieges in der Ukraine, anhaltender weltweiter Konflikte und den immer bedrohlicher werdenden Auswirkungen des weltweiten Klimawandels ist davon auszugehen, dass auch in den kommenden Jahren sehr viele Menschen weltweit Schutz in Ländern außerhalb ihrer Heimat suchen werden. Bis Anfang Juni 2022 sind 310.000 Kinder und Jugendliche unter 18 Jahre aus der Ukraine im Ausländerzentralregister erfasst [REF]. Minderjährige Flüchtlinge haben wie alle Kinder das Recht auf das erreichbare Höchstmaß an GesundheitFlüchtlinge und Asylsuchende sind eine besonders vulnerable Gruppe in unserer Gesellschaft. Aus medizinischer Sicht besteht eine Gefährdung durch die möglicherweise mangelhafte medizinische Versorgung im Herkunftsland und während der Flucht, bei gleichzeitig besonderer gesundheitlicher Gefährdung. Nach der Ankunft in Deutschland erschweren sprachliche, soziale und teilweise auch ökonomische Barrieren die gesundheitliche Versorgung. Flüchtlinge benötigen daher angemessene und niederschwellige medizinische Angebote, die an die individuelle Situation angepasst sein müssen. Es ist eine professionelle, soziale und ethische Herausforderung, die medizinische Versorgung von Flüchtlingen adäquat zu organisieren und durchzuführen.Eine von den Autoren ausdrücklich befürwortete Grundhaltung zur ärztlichen Versorgung von minderjährigen Flüchtlingen leitet sich unabhängig von ihrem Rechtsstatus aus dem Artikel 24 (Gesundheitsvorsorge) der UN-Kinderrechtskonvention aus dem Jahr 1989 her. Die Vertragsstaaten erkennen hierin das Recht des Kindes auf das erreichbare Höchstmaß an Gesundheit an. Minderjährige Flüchtlinge sind auf dem gleichen medizinischen Niveau zu versorgen wie die einheimische Bevölkerung.Um das Ziel eines umfassenden medizinischen Angebotes für Flüchtlinge zu erreichen, ist es notwendig, den medizinischen Bedarf von minderjährigen Flüchtlingen und ihren Familien zu kennen. Wichtige infektiologische Fragestellungen bei der Betreuung von minderjährigen Flüchtlingen sind insbesondere die Sicherstellung eines vollständigen Impfschutzes, aber auch die Diagnostik und Therapie von teils importierten und seltenen Infektionskrankheiten trotz Sprachbarrieren, Sammelunterkünften und unterschiedlichem kulturellen Hintergrund.In einer gemeinsamen Stellungnahme hatten die Deutsche Gesellschaft für Pädiatrische Infektiologie (DGPI), die Gesellschaft für Tropenpädiatrie und Internationale Kindergesundheit (GTP) und der Berufsverband der Kinder- und Jugendärzte (BVKJ) erstmals im Jahr 2015 Empfehlungen für die infektiologische Versorgung von Flüchtlingen im Kindes- und Jugendalter veröffentlicht [REF]. Diese sind im März 2022 aktualisiert worden [REF].Die aktualisierten Empfehlungen sollen Ärzte1/medizinisches Personal in der medizinischen Versorgung von Flüchtlingen im Kindes- und Jugendalter nicht nur aus der Ukraine unterstützen, mit dem Ziel 1. einen unvollständigen Impfschutz frühzeitig zu erkennen und rasch zu vervollständigen – zum individuellen Schutz und um Ausbreitungen von Infektionskrankheiten zu verhindern; 2. übliche Infektionskrankheiten im Kindes- und Jugendalter, auch vor dem Hintergrund von Sammelunterkünften, Sprachbarrieren und unterschiedlichen kulturellen Auffassungen, zu diagnostizieren und zu behandeln; 3. in Deutschland seltene Infektionskrankheiten (z. B. Tuberkulose, kutane Leishmaniose) frühzeitig zu erkennen und zu therapieren. Die vorliegenden Empfehlungen konzentrieren sich bewusst auf die Infektionsdiagnostik und Infektionsprävention. Selbstverständlich ist, dass Kindergesundheit weit über diese infektiologischen Gesichtspunkte hinausreicht und insbesondere psychische Gesundheit, Integration und Bildungschancen wesentliche Aspekte für ein gesundes Aufwachsen von Flüchtlingen im Kindes- und Jugendalter darstellen.Ein 4 Jahre altes Mädchen aus der Ostukraine lebt mit ihrer Mutter und dem 8‑jährigen Bruder bei einer deutsch-russischen Familie in einer 2‑Zimmer-Anliegerwohnung und wird dem Kinder- und Jugendarzt der Familie wegen Fieber und Husten vorgestellt. Den Fluchtweg hat die Familie mit Zug und Mitfahrgelegenheiten hinter sich gebracht. Auf der Flucht in der Ukraine hat die Familie mehrere Wochen in einer U‑Bahnstation Schutz gesucht. Grunderkrankungen bei der Patientin und auch chronische Erkrankungen in der Familie sind nicht bekannt. Einen Impfpass hat die Familie nicht mitgebracht, das Kind sei aber bis zum Ausbruch des Krieges nach ukrainischem Impfplan geimpft worden.Welche Untersuchungen und Präventionsmaßnahmen soll der Hausarzt einleiten?
Feminismus wird im gesellschaftlichen Diskurs oft als eine einheitliche Bewegung zur „Emanzipation der Frau“ verstanden. Dieses Verständnis greift allerdings zu kurz, da es bereits früh verschiedene Ausdifferenzierungen gab. In wissenschaftlichen Publikationen lassen sich beispielsweise Differenzfeminismus, dekonstruktivistischer Feminismus, Queer-Feminismus, Ökofeminismus, materialistischer Feminismus, intersektionaler Feminismus, Schwarzer Feminismus und liberaler Feminismus differenzieren. Angesichts dieser multiplen feministischen Strömungen, die eine Bezeichnung als „Feminismen“ angemessener erscheinen lässt [REF], ist die Angabe einer übergeordneten Definition ein anspruchsvolles Unterfangen. In einer ersten Annäherung verstehen wir Feminismus als eine Bewegung, die die Überwindung ungleicher Verhältnisse anstrebt, die aufgrund von vergeschlechtlichenden patriarchalen, cis-heteronormativen und sexistischen Strukturen entstehen [REF]. Hierbei berücksichtigen wir, dass diese Strukturen ihre Wirkmacht nicht isoliert entfalten, sondern konstitutiv mit weiteren Unterdrückungsstrukturen, wie rassistischen oder klassistischen Strukturen, verwoben sind [REF].Als Querschnittsforschung diverser geistes- und sozialwissenschaftlicher Disziplinen hat feministische Forschung gezeigt, dass vergeschlechtlichte soziale Normen Menschen benachteiligen. Darüber hinaus hat feministische Forschung entscheidend dazu beigetragen, die Rechtfertigungssysteme ungerechter gesellschaftlicher Praktiken grundlegend zu hinterfragen, indem sie Hintergrundannahmen dekonstruiert hat. So zeigte post-strukturalistisch orientierte und queer-feministische Forschung, dass viele der gegenderten sozialen Normen auf einem wissenschaftlich nicht haltbaren Geschlechter-Binarismus sowie einer homogenisierten Vorstellung der Kategorie „Frau“ beruhen, die Menschen marginalisieren, die sich nicht innerhalb der binären Kategorien „Mann“ und „Frau“ definieren [REF]. Intersektionale Forschung zeigte, dass soziale Gendernormen zudem mit anderen sozialen Kategorien, wie Race oder Klasse überschnitten (Intersektionen) sind, die ebenfalls mit Marginalisierungen einhergehen können [REF]. Feministische Forschung bietet somit Analyse-Instrumente, die einer Medizinethik, die soziale Normen und Praktiken kritisch hinterfragt, dienen können.Die Medizinethik bezieht ihre Reflexionen vornehmlich auf die Medizin und Gesundheitssysteme, also auf Disziplinen bzw. Praxen, die immer noch stark von patriarchalen und cis-heteronormativen Strukturen geprägt sind. Frauen*1 sowie non-binäre, trans* und inter* Personen erfahren in der Medizin und in Gesundheitssystemen Benachteiligungen. Ein Beispiel ist das Yentl-Syndrom, das Phänomen, dass Frauen* Fehldiagnosen und -behandlungen erhalten, wenn ihre Symptome oder Erkrankungen nicht denen der Männer entsprechen [REF]. Ein weiteres Beispiel ist die generell schlechtere gesundheitliche Situation und Versorgung von Schwarzen2 Frauen*, etwa mit Blick auf Herzerkrankungen [REF], oder von non-binären, trans* und inter* Personen im Rahmen der Covid-19-Pandemie [REF]. Eine explizit feministische Perspektive scheint umso relevanter, da aktuelle technologische und digitale Entwicklungen in der Medizin, etwa auf selbstlernende Algorithmen gestützte Systeme, oft auf genderbasierten Datenlücken aufbauen. Diese bringen das Risiko mit sich, diese Lücken und somit Diskriminierungen zu perpetuieren [REF]. Es bleibt also weiterhin wichtig, auf diese ungerechten Strukturen aus der Perspektive von Frauen* hinzuweisen. Aus diesem Grund halten wir eine explizit feministische Perspektive auf medizinethische Themen für notwendig und angemessen.Wie andere wissenschaftliche Disziplinen verfügt die Medizinethik über einen differenzierten Diskurs, der neben Fachtagungen und Konferenzen vorrangig in deutsch- und englischsprachigen Fachzeitschriften, wie Ethik in der Medizin oder Bioethics, geführt wird. Dieser Diskurs ist zentral für die Weiterentwicklung medizinethischen Wissens und medizinethischer Theorien, da hier wichtige innerdisziplinäre Fachdebatten geführt werden. Dabei geht es sowohl um aktuelle Fragestellungen rund um technologische oder politische Entwicklungen wie auch um theoretisch-begriffliche Ausdifferenzierungen und Klärungen, die für die Qualität medizinethischer Theoriebildung zentral sind. Welche Positionen innerhalb dieses medizinethischen Diskurses vertreten sind, hat damit einen großen Einfluss auf die inhaltliche, theoretische und methodologische Weiterentwicklung des Faches.Aufgrund der disziplinären Sichtbarkeit von Positionen ist relevant, welche Debatten in welchen Kontexten geführt werden. Im deutschsprachigen Raum werden unterschiedliche feministische Debatten geführt, die auch für die Medizinethik von Relevanz sein können, z. B. im Bereich des Post- und Transhumanismus [REF], der feministischen Wissenschafts- und Technikforschung [REF] oder der feministischen Pflegeethik [REF]. Dabei ist auffallend, dass diese explizit feministischen Debatten vorranging in Fachzeitschriften für feministische Philosophie oder in Gender-wissenschaftlichen Zeitschriften geführt werden und in den medizinethischen Fachzeitschriften weniger präsent sind. Die Gründe für das scheinbare Ausweichen feministisch-medizinethischer Forschung auf Publikationsmedien außerhalb der Medizinethik mögen vielfältig sein und könnten sowohl in den Entscheidungen der Autor*innen als auch in denen der Editor*innen liegen. Gleichwohl scheint uns, dass durch eine mögliche Marginalisierung des feministischen Diskurses innerhalb der Medizinethik eine kritische Auseinandersetzung mit feministischen Perspektiven auf medizinethische Fragestellungen erschwert sein könnte. Daraus ergibt sich die Forschungsfrage, welche feministischen Perspektiven auf medizinethische Themen innerhalb des deutschsprachigen medizinethischen Diskurses vertreten sind. Diese diskutieren wir im ersten Teil des Artikels durch die Vorstellung einer systematisierten Literaturrecherche, in der wir uns explizit auf medizinethische Fachpublikationen beschränken, um eine mögliche Marginalisierung feministischer Diskurse in diesen Medien zu untersuchen. Im zweiten Teil des Artikels lösen wir uns von der empirischen Analyse des aktuellen Forschungsstandes und wenden uns Perspektiven zu, mit denen eine feministische Medizinethik weiterentwickelt werden kann. Basierend auf eigenen Vorarbeiten, der Literaturrecherche sowie der Arbeit innerhalb der Arbeitsgruppe in der Akademie für Ethik in der Medizin (AEM) „Feministische Perspektiven in der Bio- und Medizinethik“ (FME) stellen wir drei Thesen vor, die einer Weiterentwicklung des medizinethischen Diskurses im deutschsprachigen Raum dienen können. Dabei erörtern wir die Relevanz der feministischen Themen der 1) epistemischen Gerechtigkeit, 2) Kontext-Sensitivität und 3) Intersektionalität für den medizinethischen Diskurs. Wir schließen mit Implikationen für den allgemeinen medizinethischen Diskurs sowie einem Ausblick auf die Arbeit der AG FME.
Der Begriff „Blickdiagnose“ besagt, dass durch alleinige Inspektion des Erscheinungsbildes (Phänotyp) auf eine zugrunde liegende Erkrankung geschlossen werden kann. Bei Syndromen, also einer bestimmten Konstellation von Anomalien, reicht oftmals schon eine Abweichung in der Struktur und Form (Dysmorphologie) des Gesichts für eine Blickdiagnose. Aufgrund seines hohen Informationsgehalts wird das Gesicht (die Fazies) manchmal auch als für eine bestimmte Erkrankung charakteristische „faziale Gestalt“ bezeichnet – ein Fachbegriff, der auch in der englischsprachigen Literatur verwendet wird („facial gestalt“). Interessanterweise wird dieser Befund jedoch manchmal gar nicht näher beschrieben. Die Fachliteratur der letzten Jahre, in der über viele Gen-Phänotyp-Assoziationen berichtet wurde, ist inzwischen aber voll von „novel syndromes with intellectual disability with a characteristic facial gestalt“. Eine Suche mit dem Terminus „abnormal facial shape“ in der Human Phenotype Ontology (HPO), die dieses Wissen strukturiert, liefert bereits Verknüpfungen mit über tausend Genen1 [REF]. Ein Beispiel für eine Seltene Erkrankung mit markanten Gesichtsmerkmalen ist das Kabuki-Syndrom, benannt nach einer traditionellen japanischen Theaterform mit spezieller Schminktechnik. Bei Abweichungen von der Norm sollte aber immer an eine Glockenkurve gedacht werden. Vieles kann abweichend sein, aber nur manches ist so klar, dass es namensgebend ist.Hieraus kann man zweierlei ableiten: 1) Es ist möglich, präzise mit phänotypischer Evidenz zu diagnostizieren, aber 2) die sprachliche Begründung einer Entscheidung ist schwer. Der Hinweis auf die informative Region im Gesicht kann zwar richtig, aber für eine dysmorphologisch unkundige Person dennoch nicht informativ sein. In diesem Beitrag soll u. a. gezeigt werden, dass dies ebenso für computergestützte Verfahren in der Dysmorphologie gilt. Ein Teilbereich der Informatik, der sich mit der Automatisierung intelligenten Verhaltens bzw. komplexer Entscheidungsstrukturen beschäftigt, versucht den Prozess der Blickdiagnose abzubilden. In Anlehnung an den positiv besetzten Begriff des Next-Generation Sequencing (NGS), der in den letzten Jahren genomweite Analysen ermöglichte, wird der Einsatz von künstlicher Intelligenz (KI) bei der Phänotyp-Analyse in der englischsprachigen Literatur häufig auch mit Next-Generation Phenotyping (NGP) bezeichnet. Wie die Erfolge der letzten Jahre zeigen, lässt sich durch den Einsatz von NGP ebenfalls die Effizienz in der Diagnostik steigern. Dies ist insbesondere für den Bereich der Seltenen Erkrankungen von Bedeutung, da hier viele Ärzt:innen Diagnosen nur einmalig in ihrem Berufsleben stellen können, obwohl das „Muster“ erkennbar ist.In diesem Beitrag werden Methoden und Ergebnisse medizinischer KI erläutert. Es wird zunächst die Funktionsweise künstlicher neuronaler Netzwerke beschrieben, die sich am biologischen Vorbild orientiert. Um die Leistungsfähigkeit von KI einordnen zu können, wird erklärt, wie man diese bestimmt. Danach wird auf die Herausforderung geeigneter Datensammlungen eingegangen. Zuletzt wird ein Ausblick auf mögliche Entwicklungen in der Zukunft gegeben.
In Deutschland leben Schätzungen zufolge derzeit ca. 4 Mio. Patienten mit einer Seltenen Erkrankung (SE), definiert durch eine Prävalenz kleiner als 1:2000 [REF]. Der Bekanntheitsgrad der einzelnen SE ist unter Ärzten gering, zumal eine Diagnosefindung häufig durch ein breites phänotypisches Spektrum mit Beeinträchtigung mehrerer Organsysteme erschwert wird. Viele Patienten mit einer SE durchlaufen daher jahrelange Odysseen von Arzt zu Arzt, bis die korrekte Diagnose gestellt wird. So geht oft wertvolle Zeit für eine wirkungsvolle Therapie verloren [REF].Die Versorgung von Menschen mit SE erfordert aufgrund der Seltenheit der Erkrankungen, ihrer individuellen Besonderheiten und des Multiorganbefalls eine gemeinsame Fallbeurteilung durch ein erfahrenes, interdisziplinäres und multiprofessionelles Team. Das Nationale Aktionsbündnis für Menschen mit SE (NAMSE) hat 2013 einen Aktionsplan veröffentlicht und damit entsprechend ausgestattete, sowohl ambulant als auch stationär arbeitende Fachzentren (Typ-B-Zentren) bzw. rein ambulant arbeitende Kooperationszentren (Typ-C-Zentren) als erforderlich für eine bestmögliche Versorgung von Betroffenen definiert [REF]. Die Typ‑A Zentren, sogenannte Referenzzentren für SE, verstehen sich als Anlaufstelle für ärztliche Kollegen und Patienten. Sie sind behilflich bei der Suche nach geeigneten Ansprechpartnern innerhalb des Zentrums/Klinikums oder auch bei der Vermittlung von anderen Kompetenzzentren und Selbsthilfegruppen. Der Bedarf an diesen Angeboten ist in den vergangenen Jahren gerade bei SE erheblich gewachsen und wird gedeckt durch eine selbst für Betroffene und Fachleute überraschend positive, dynamische Entwicklung und Ausweitung der Diagnostik, Versorgung und spezifischen Behandlungen.Die Abklärung einer unklaren Symptomatik und des Verdachts auf eine SE erfolgt durch die krankheitsübergreifend arbeitenden Referenzzentren bzw. Typ-A-Zentren. Sie ziehen je nach Bedarf Unterstützung durch lokale Experten im Rahmen von Konsilen oder Fallkonferenzen hinzu. Derzeit existieren in Deutschland an Universitätskliniken ca. 30 Zentren dieser Art. Aufgrund ihrer geringen Anzahl können die Wege zwischen den einzelnen Zentren weit sein. Betreuende Ärzte vor Ort und regionale Kliniken übernehmen daher einen (großen) Teil der Versorgung und können sich mit den Zentren in Verbindung setzen.Hierfür ist ein enger Austausch, ggf. auf der Basis gemeinsamer Fallbesprechungen, erforderlich. Der Gemeinsame Bundesausschuss (G-BA) hat Regelungen zu den besonderen Aufgaben von Zentren für SE veröffentlicht, zu denen u. a. interdisziplinäre Fallkonferenzen und die Prüfung von Patientenakten für andere Einrichtungen gehören.Für eine überregionale Zusammenarbeit bei SE wurden in den vergangenen Jahren mehrere IT-Systeme entwickelt und erprobt, die ein organisationsübergreifendes Patientenmanagement, Konsile und auch Fallkonferenzen erlauben. Basis dieser IT-Systeme stellt eine Elektronische Patientenakte (EPA) dar, in der alle Daten eines Patienten von den Beteiligten erfasst und den Behandlern über ein an die Anforderungen angepasstes Rechte- und Freigabemanagement bereitgestellt werden.In diesem Beitrag möchten wir die Möglichkeiten und Grenzen von EPA am Beispiel der beiden Pilotprojekte „BASE-Netz“ und „TRANSLATE-NAMSE“ vorstellen, die sich mit der Implementierung von EPA für Menschen mit Seltenen Erkrankungen befasst haben. Die Erfahrungen bzgl. Machbarkeit und Akzeptanz aus dem Praxiseinsatz werden zusammengefasst und Schlussfolgerungen für die Entwicklung der EPA in Deutschland gezogen.
Das Sonderfach Kinder und Jugendpsychiatrie (KJP) besteht nun bereits seit 2007 und wurde seitens der Bundesregierung 2015 [REF] neugeregelt und in der Ärzteausbildungsordnung als Fachärzt*in für Kinder- und Jugendpsychiatrie, Psychosomatik und psychotherapeutische Medizin definitorisch festgehalten. Eine detaillierte Beschreibung der Entwicklung der KJP in Österreich findet sich im Sonderheft „10 Jahre Fach Kinder- und Jugendpsychiatrie“ [REF].Obwohl bereits seit 1997 [REF] als Kinder- + Jugendneuropsychiatrie in die Leistungsorientierte Krankenhausfinanzierung (LKF) aufgenommen, zog erst die Schaffung des eigenen Sonderfaches 2007 einen eigenen für dieses Fach geltenden, allgemeinen Versorgungsauftrag im Österreichischen Strukturplan Gesundheit (ÖSG) nach sich. Der ÖSG gilt als „das zentrale Planungsinstrument auf Bundesebene für die integrative Versorgungsplanung in Österreich und“ ist „seit 2013 integraler Bestandteil der Zielsteuerung-Gesundheit. Er enthält als Rahmenplan verbindliche Vorgaben für die Planung bestimmter Bereiche des Gesundheitsversorgungsystems sowie Kriterien für die Gewährleistung der bundesweit einheitlichen Versorgungsqualität. Mit dem ÖSG wird sichergestellt, dass Gesundheitsversorgung in Österreich ausgewogen verteilt und gut erreichbar ist und in vergleichbarer Qualität auf hohem Niveau angeboten wird“ [REF]. Der ÖSG wird von der Bundeszielsteuerungskommission herausgegeben.Der ÖSG ist also das Instrument der Versorgungsplanung für das österreichische Gesundheitssystem, also auch für das Sonderfach KJP und „durch die Vereinbarung österreichweiter Versorgungsstandards sollen die in einzelnen Versorgungsbereichen bestehende Über‑, Unter- oder Fehlversorgung der Bevölkerung hintangehalten und eine entsprechende Qualität der Versorgung sichergestellt werden“ [REF]. In diesem Sonderheft werden die Vorgaben des ÖSG in verschiedenen Bereichen der Kinder- und Jugendpsychiatrie als Grundlage der Diskussion herangezogen.Versorgungsforschung ist ein wichtiger Teil der Gesundheitssystemforschung und beschäftigt sich in erster Linie mit der Organisation, der Steuerung und Finanzierung des Kranken- und Gesundheitswesens. Versorgung wird in diesem Zusammenhang als die Bedienung relevanter Bedürfnisse oder Defizite von Lebewesen bezeichnet, die Bedienung mit medizinischen Leistungen als Gesundheitsversorgung. Ganz zentral wird bei Erbringung von Gesundheitsleistungen der Ort der Versorgung bzw. der Erbringung der Leistung berücksichtigt. Das Österreichische Bundesinstitut für Gesundheitswesen hat 2004 [REF] und die Österreichische Gesellschaft für Kinder- und Jugendpsychiatrie 2006 [REF] haben im Rahmen der Vorbereitung des neuen Sonderfaches die Gesundheitsversorgung durch die Fachärzt*innen für KJPP beschrieben und insbesondere die verschiedenen Versorgungsebenen (siehe Tab. 1) definiert. Zuletzt wurden diese Arbeiten durch Fliedl et al. [REF] aktualisiert. In Abb. 1 ist das vom ÖSG definierte Versorgungsmodell dargestellt. Der ÖSG 2021 [REF] definiert die Anforderungen an eine qualitätvolle Versorgungsplanung wie folgt: es geht um die „Sicherstellung des offenen Patientenzugangs zum evidenzgesicherten medizinischen Fortschritt. Die Sicherstellung von fachlicher Expertise in den Behandlungsteams (ÄrztInnen und Angehörige pflegerischer und therapeutischer Gesundheitsberufe) durch Qualifikation (Aus‑, Fort- und Weiterbildung) und Routine“ weiters um die „Einhaltung der im ÖSG enthaltenen Qualitätskriterien und Einhaltung ökonomischer Grundprinzipien im Hinblick auf ausreichende Leistungsmengen, Fixkostendegression und Nutzungsgrad von eingesetzten Ressourcen und Kapazitäten ohne Qualitätseinbußen“ (Hervorhebungen lt. Originaltext). Die Fragen, die sich in diesem Zusammenhang stellen, sind: wer definiert in unserem Fach was „medizinischer Fortschritt“ ist und welche Qualitätskriterien werden zur Beurteilung und Planung herangezogen und wie wirken sich ökonomische Grundprinzipien auf diese Planungen aus? Weiters wie fließen diese Bewertungen in die Versorgungsaufträge?Wie sieht nun die – zumindest theoretische Umsetzung der Versorgung (Abb. 2) für unser Fach aus? Der ÖGD (Öffentlicher Gesundheitsdienst) ist in unserem Fachbereich im Wesentlichen für die Begutachtung nach dem Unterbringungsgesetz im ambulanten Bereich zuständig, dies in enger Kooperation mit Rettung und Polizei (Präklinische Notfallversorgung). Die ambulante Versorgung umfasst nach dem ÖSG die Bereiche Primärversorgung, in dem bisher die Beteiligung der Kinder- und Jugendpsychiatrie seitens der Gesundheitsplanung nicht vorgesehen ist, obwohl es dafür gute Sach- und Fachbezogene Argumente gibt [REF]. In die sogenannte Ambulante Fachversorgung fallen Einzel- und Gruppenpraxen sowie die selbstständigen Ambulatorien, Miniambulatorien (siehe auch Stellungnahme des ÖGKJP-Vorstandes [REF]) und die, an klinische Abteilungen angeschlossenen Fachambulanzen. Laut ÖSG sollen neben den Allgemeinen Aufgaben für alle Fachbereiche eine auf das Fach hin abgestimmte Leistungsmatrix, Aufgabenprofile und Ausstattungskriterien definiert werden. Diese Definition ist bisher nicht erfolgt und wird vor allem aufgrund des Föderalismus in den einzelnen Bundesländern höchst unterschiedlich wahrgenommen. Der ÖSG hat auch Richtwerte zur ambulanten Versorgung angeführt, es sollten demnach 0,6–1,2 Ärztlich Ambulante KJP-Versorgungseinheiten (ÄAVE; entspricht ärztlichen Vollzeitäquivalenten [REF]) /100.000 EW die Bevölkerung versorgen.Die nächste Ebene der Versorgung sind Tagesklinische Einrichtungen, die im ÖSG 2017 noch zu den stationären Einrichtungen gezählt werden, nach LKF aber bereits seit einigen Jahren als ambulante Einrichtungen geführt werden.Im stationären Bereich wird unterschieden je nach Status (Standard, Schwerpunkt, Zentralkrankenanstalt) der Krankenanstalt, an der die jeweilige Abteilung für KJP angesiedelt ist. Insgesamt existieren in Österreich zurzeit 13 Abteilungen für KJP, davon sind mit Stand 30.4. insgesamt fünf Abteilungen für KJP an Universitätskliniken integriert und somit automatisch Teil einer Zentralkrankenanstalt. In der Klassifikation der Krankenanstalten in Österreich [REF] werden psychiatrische Abteilungen oder Kinder- und Jugendabteilungen als „Spezialversorgung“ gesehen und sind somit per se Schwerpunktkrankenanstalten. Referenzzentren (RFZ) oder hochspezifische Spezialzentren im engeren Sinne (lt.ÖSG) gibt es in Österreich im Fach KJP (noch) nicht. Vorgaben bezüglich des Umfangs der Versorgung sind definiert (z. B.: Bettenmessziffer BMZ zwischen 0,6–1,1 Betten auf 100.000 EW oder auch bestimmte Personalstrukturkriterien). Auch für die stationären Einrichtungen gibt es keine definierte Kooperationsformen – zum Beispiel ist der gesamte KJP-Konsiliar- und Liäsondienst (v. a. in die Pädiatrie, Psychiatrie und Kinder- und Jugendhilfe) nicht definiert, einige Zentralspitäler schaffen die Möglichkeiten von sogenannten Interdisziplinären Schwerpunkten, die aber aufgrund der fehlenden Abbildung in ÖSG und LKF nicht leicht umzusetzen sind.In den letzten Jahren gab es einige Aktivitäten seitens der Politik (Kinder- und Jugendgesundheitsstrategie, Nationaler Aktionsplan etc.), die sich aber mit der Erstellung von Berichten und Feststellen der Defizits begnügt haben. Die wesentlichste Veränderung war die Erweiterung der Mangelfachregelung auf einen neuen Ausbildungsschlüssel von 1 Facharzt:in auf 2 Ausbildungsärzt:innen.Das ist die politische, gesetzgeberische und auch finanzpolitische Sicht auf die KJP-Versorgung, doch könnte und sollte Versorgung auch von Seiten der Inanspruchnahme-Population aus betrachtet werden. Zumeist wird dabei von epidemiologischen Daten ausgegangen bzw. auch von einzelnen Erkrankungen und ihrer Versorgungsnotwendigkeit. Epidemiologische Daten aus Österreich (MHAT-Studie [REF]) weisen auf eine hohe Punktprävalenz psychischer Auffälligkeiten (23,9 %) hin, die durch die SARS-Cov-2-Pandemie noch deutlich angehoben wurde (40 % Steigerung der Inanspruchnahme [REF], Zunahme psychopathologischer Auffälligkeiten Depression, Angst, Essstörungssymptomatik [REF]). Die Inanspruchnahme war allerdings schon vor der Pandemie nicht ausreichend, lediglich 48 % gaben an sich Hilfe geholt zu haben, weitere 25 % wünschen sich Hilfe [REF]. Eine Folge dieser mangelnden Inanspruchnahme ist das Ausweichen auf nicht-medizinische Angebote, das in erster Linie von wohlhabenderen Familien und von Familien mit Kindern, die an externalisierenden Störungen leiden, wahrgenommen wird. Versorgungsmängel führen somit zu zusätzlicher Benachteiligung ärmerer Betroffener [REF].Kinder- und Jugendlichen sind in allen Sektoren der Gesellschaft zu finden: Gesundheit, Schule, Soziales, Justiz, Wirtschaft und Familie. Es ist daher in all diesen Bereichen auf die Situation der psychisch wie körperlich kranken Kinder und Jugendlichen Rücksicht zu nehmen und planerisch wie präventiv tätig zu werden. In allen genannten Bereichen ist dies im Moment nur ansatzweise vorhanden und eine Kooperation dieser Bereiche miteinander ist kaum zu finden. Aus diesem Grunde ist, um dieser Materie auch nur annähernd gerecht zu werden, die Forderung nach einem eigenen Mental Health Ministerium dringend zu unterstützen, wie dies in Australien, Kanada, Großbritannien und Irland bereits umgesetzt ist.Weitere Aspekte der Versorgung sind krankheitsspezifische Aspekte. Welche Erkrankung muss von wem, auf welcher Ebene der Versorgung, wie versorgt werden? Beispiel Anorexia nervosa: in der S3-Leitlinie der AWMF [REF] ist festgehalten, dass als Mindeststandard zu gelten habe: Versorgung durch eine/n Kinder- und Jugendpsychiater:in gemeinsam mit Diätolog:in und Psychotherapeut:in, alle müssen Knowhow in der Behandlung von Anorexia haben. Diese Kriterien erfüllen in Österreich nur wenige der ambulanten Einrichtungen, im stationären Bereich ist die Situation noch dünner. Und dennoch wurde die medizinische Versorgung von PatientInnen mit Anorexie seitens des Gesundheitsministeriums in einer Beantwortung einer parlamentarischen Anfrage als ausgezeichnet beschrieben [REF]. Diese unterschiedliche Wahrnehmung seitens der offiziell Verantwortlichen und der Betroffenen zieht sich eigentlich durch die gesamte Materie der Kinder- und Jugendpsychiatrie, Bespiele gäbe es genug: um nur einige zu nennen: Autismus-Versorgung, die Versorgung von straffällig gewordenen Jugendlichen, die Transition, minderjährige Flüchtlinge, Mobbing etc.Neben der Krankheitsspezifischen Versorgung könnte man auch eine Altersspezifische Versorgung überlegen, da die psychischen Erkrankungen sich je nach Alter unterschiedlich manifestieren, zu unterschiedlichen Altern auftreten und entsprechend unterschiedliche Herangehensweisen in Diagnostik und Therapie benötigen. KJP-Versorgung sollte daher dort sein, wo die Kinder und Jugendlichen sind, an Kindergärten und Schulen, an allen Krankenanstalten, die Kinder und Jugendliche betreuen, in den Primärversorgungszentren und nahe bei den Kinder- und Jugendärztinnen – liegt doch das Risiko einer psychiatrischen Störung bei körperlich erkrankten oder behinderten Kindern und Jugendlichen nochmals einiges höher als bei den gesunden Kindern und Jugendlichen [REF]. Schlussendlich sollten die Fachärzt:innen für KJP in allen Einrichtungen, die Kinder und Jugendliche betreuen (Kinder- und Jugendhilfe, AMS, Justiz etc.) in enger Kooperation mit den Trägerorganisationen arbeiten können. Diese Versorgungsstrukturen müssen sich neben dem Alter an der Entwicklung orientieren und entsprechend personell und fachlich adäquat aufgestellt sein.Kinder- und Jugendpsychiatrische Versorgung weist aber noch weitere Besonderheiten auf. Es ist neben der Psychiatrie das einzige Fach, das einen impliziten, nämlich in der Facharztdefinition integrierten, psychotherapeutischen Versorgungsauftrag beinhaltet, dem natürlich in Ausbildung, Lehre aber auch im Rahmen der Formulierung der Versorgungsaufträge, der Bereitstellung von Ressourcen und Finanzierung strukturell Rechnung getragen werden muss. Dies ist bisher nur ansatzweise erfolgt. Das Fach KJP ist DAS Fach, in dem Interdisziplinarität, Multimodalität und Kooperation zentral im Selbstverständnis des Faches enthalten sind [REF]. Diese Themen werden im stationären Rahmen am ehesten umgesetzt, für den niedergelassenen Arzt ist es bisher
Damit die Digitalisierung auch für die Menschen mit Seltenen Erkrankungen (SE)1 von Nutzen ist, bedarf es verschiedener Maßnahmen, bei denen die Gegebenheiten des deutschen Gesundheitssystems, aber auch die Besonderheiten der SE berücksichtigt werden müssen. Das deutsche Gesundheitssystem weist sehr ausgeprägt gegliederte Versorgungsstrukturen und entsprechend vielseitige Sammlungen von Daten, sogenannte Datensilos, auf. Dies hat bisher die vollständige Zusammenstellung von Gesundheits‑, Erkrankungs- und genetischen Daten einzelner Personen erschwert und merklich die konsistente Bildung von effizient auswertbaren Datenbeständen für Gruppen von Betroffenen behindert. Gerade für SE, bei denen die Daten meist in verschiedenen medizinischen Einrichtungen erfasst werden, sind die Zusammenführung von Daten und deren aussagekräftige Auswertung für Diagnostik- und Therapieerfolge unerlässlich.Denn mit der Seltenheit der SE, dem fehlenden Überblick über die Verbreitung der einzelnen Erkrankungen sowie der Schwierigkeit, diagnostische und therapeutische Studiengruppen zusammenzustellen, sind große Herausforderungen in diesem Bereich markiert. Um die Situation grundsätzlich zu verbessern, ist im Jahr 2009 von den Bundesministerien für Bildung und Forschung sowie Gesundheit (BMBF und BMG) und von der Patientendachorganisation Allianz Chronischer Seltener Erkrankungen (ACHSE e. V.) das Nationale Aktionsbündnis für Menschen mit Seltenen Erkrankungen (NAMSE; [REF]) gegründet worden. Dort sind alle relevanten Stakeholder des Gesundheitssystems (28 Bündnispartner) vertreten.Infolge der deutschen NAMSE-Aktivitäten, die die Voraussetzung für die deutsche Beteiligung an den europäischen Anstrengungen sind, entstanden im Laufe der 2010er-Jahre in fast allen deutschen Universitätskrankenhäusern zentrale Anlaufstellen für die Versorgung und Erforschung der SE: die Zentren für Seltene Erkrankungen (ZSE). Die ZSE haben verschiedene Erkrankungsexpertisen und sind in der Arbeitsgemeinschaft der ZSE organisatorisch miteinander vernetzt. Vernetzung und Abstimmung sind unter anderem in den Innovationsfondsprojekten TRANSLATE-NAMSE2 und ZSE DUO3 vorangebracht worden. Es wurde deutlich gezeigt, was im Bereich der SE erreicht werden kann, wenn Diagnose und Versorgungswege interdisziplinär konzipiert und strukturiert angelegt werden. Es ist zugleich nochmals deutlich geworden, dass für weitere Verbesserungen eine gemeinsame digitale Infrastruktur und präzise, abgestimmte Dokumentationen in den ZSE benötigt werden.Angeregt durch das NAMSE wurde in der Medizininformatik-Initiative (MII) des Bundesministeriums für Forschung und Bildung (BMBF) das Thema der SE aufgegriffen [REF]. In der MII wird aktuell ausgehend von den Universitätskliniken eine digitale Infrastruktur für die datenschutzkonforme Mehrfachnutzung von standardisierten Versorgungs- und Forschungsdaten aufgebaut. Teil der Initiative ist seit dem Jahr 2020 das Projekt CORD-MI (Collaboration on Rare Diseases; [REF]), in dem sich Universitätskliniken und weitere Partner deutschlandweit zusammengeschlossen haben, um die Patientenversorgung und die Forschung im Bereich der SE zu verbessern.Die „länderübergreifende Vernetzung“ [REF] ist im Bereich der SE, insbesondere der ultraseltenen Erkrankungen, eine notwendige Ergänzung der nationalen Zusammenarbeit. Daher erfolgt im Vorhaben CORD-MI insbesondere die Abstimmung mit den Europäischen Referenz-Netzwerken (ERN), mit der Europäische Plattform für die Registrierung Seltener Krankheiten (EU RD Platform) sowie mit dem Netzwerk Orphanet, das ein Portal für seltene Krankheiten und deren Therapeutika, die „Orphan Drugs“, betreibt. Darüber hinaus soll mit der „Global Alliance for Genomics and Health“ (GA4GH) der Erfahrungsaustausch über die Verwendung von „Phenopackets“4 [REF] bei der standardisierten Phänotypisierung fortgesetzt werden.Ziel des vorliegenden Beitrags ist es, die deutsche Medizininformatik-Initiative vorzustellen und speziell die Integration des Bereichs der SE zu beschreiben. Der Schwerpunkt der Betrachtung liegt auf den Anstrengungen für die standardisierte Verfügbarkeit von aufbereiteten Versorgungs- und Forschungsdaten. Die Chancen, welche durch die „neuen Routinedaten“ für SE entstehen, werden aufgezeigt.
Im Jahr 2011 erfolgte ein erster österreichweiter Consensus zum Umgang mit latenter Tuberkulose vor Therapie mit biologischen „disease modifying antirheumatic drugs“ (bDMARDs) [REF]. Im Laufe der letzten 10 Jahre konnten nicht nur wesentliche neue Erkenntnisse hinsichtlich der Medikamentensicherheit gewonnen werden, sondern es wurde auch eine Vielzahl weiterer Präparate dieser Gruppe zugelassen. Aus diesen Gründen wurden in einer ExpertInnengruppe der ÖGR (Österreichische Gesellschaft für Rheumatologie und Rehabilitation), ÖGP (Österreichische Gesellschaft für Pneumologie), ÖGGH (Österreichische Gesellschaft für Gastroenterologie und Hepatologie), ÖGDV (Österreichische Gesellschaft für Dermatologie und Venerologie) und ÖGIT (Österreichische Gesellschaft für Infektionskrankheiten und Tropenmedizin) die Empfehlungen für die Diagnose und Therapie einer latenten Tuberkulose vor Beginn einer b/tsDMARD-Therapie neu erarbeitet und die Ergebnisse im Folgenden zusammengefasst.Trotz des weltweiten Rückgangs der Zahlen stellt die Tuberkulose auch im Jahr 2021 ein immenses globales Gesundheitsproblem dar. Allein im Jahr 2019 erkrankten laut WHO-Bericht 10 Mio. Menschen an dieser Infektionserkrankung, und 1,4 Mio. Menschen sind an den Folgen der Tuberkulose verstorben [REF]. Das Auftreten der Erkrankung ist jedoch in den unterschiedlichen Regionen der Erde sehr ungleich verteilt, und somit sind die Inzidenzen von Land zu Land äußerst variabel. Fast die Hälfte der Erkrankten befindet sich in nur 30 Ländern, denen allesamt eine schlechte sozioökonomische Versorgung ihrer Einwohner gemein ist.Im Vergleich dazu besteht in Österreich – wie in den meisten nord-, zentral- und westeuropäischen Ländern – eine sehr niedrige Tuberkuloseinzidenz mit 4,4 Fällen pro 100.000 Einwohnern bzw. 388 gemeldeten Tuberkulosefällen gesamt-österreichweit im Jahr 2020. Diese Zahl ist auch im Vergleich zu 2011 weiter gesunken [REF] (ESM Abb. 1).Im Gegensatz zur Verbreitung der manifesten Tuberkulose ist die Prävalenz der latenten Tuberkulose (LTBI) unklar. Als latente Tuberkulose bezeichnet man die asymptomatische Persistenz vitaler tuberkulöser Mykobakterien im Organismus nach einer Infektion. Die infizierte Person ist klinisch gesund und nicht ansteckend für ihre Umgebung. Als Zeichen einer stattgehabten Immunantwort auf den Erreger ist der Interferon-Gamma-Release-Test (IGRA) oder der Tuberkulinhauttest (TST) positiv, eine aktive Tuberkuloseerkrankung ist jedoch ausgeschlossen (unauffällige klinische Untersuchung und unauffälliges Thoraxröntgen) [REF]. Verschiebt sich jedoch das Gleichgewicht zwischen immunologischer Kontrolle und bakterieller Aktivität zuungunsten der Immunität (wie z. B. unter immunsupprimierender Therapie), kann sich aus der LTBI eine aktive Tuberkulose entwickeln (Reaktivierung). Dabei kommt es zum Auftreten von klinischen Symptomen einer Tuberkuloseinfektion wie Fieber, Husten, Nachtschweiß oder Gewichtsverlust; auch fulminante Verläufe sind möglich.Seit 1998 wird dieser gegen B‑Zellen gerichtete Antikörper in der Therapie von Lymphomen eingesetzt. Im Jahr 2006 erfolgte die Zulassung zur Behandlung der rheumatoiden Arthritis (RA). Des Weiteren wird Rituximab auch zur Behandlung ANCA-assoziierter Vaskulitis (Granulomatose mit Polyangiitis, mikroskopische Polyangiitis) sowie von Pemphigus vulgaris eingesetzt. In einer Vielzahl randomisierter kontrollierter Studien wie auch in Beobachtungsstudien und Registerdaten hat sich keinerlei Hinweis auf ein vermehrtes Auftreten von Tuberkulose gezeigt [REF]. Auch in Tuberkulosehochinzidenzländern fand sich diesbezüglich kein Sicherheitsrisiko [REF]. Die European Society of Clinical Microbiology and Infectious Diseases (ESCMID) verfasste 2018 hierzu eine klare Stellungnahme, die die Unbedenklichkeit hinsichtlich Tuberkulose unterstrichen hat [REF]. Auch in der Fachinformation des Medikaments gibt es zu diesem Thema keine Warnung.Mit Beginn des Einsatzes des ersten TNF-Blockers (Infliximab) um die Jahrtausendwende fiel unter der laufenden Therapie bald ein gehäuftes Auftreten von Tuberkulosereaktivierungen auf, was in der Folge die Einführung eines routinemäßigen Screenings auf latente Tuberkulose vor Beginn der Therapie zur Folge hatte [REF]. Durch Auswertung verschiedener Registerdaten zeigte sich, dass das Risiko, an einer aktiven Tuberkulose zu erkranken, unter Therapie in etwa 4fach erhöht ist [REF]. Es fanden sich jedoch gewisse Unterschiede hinsichtlich der Tuberkuloseinzidenz zwischen den einzelnen TNF-Blockern. Das Fusionsprotein Etanercept zeigte wiederholt weniger Zahlen von Tuberkulosefällen und dürfte somit ein geringeres Risiko bergen als die anderen [REF]. Aufgrund der Lehre, die man aus der Markteinführungsphase der TNF-Blocker gezogen hatte, wurden die klinischen Studien der danach folgenden Biologikatherapien meist nur nach ausgeschlossener oder behandelter latenter Tuberkulose durchgeführt, was die Beurteilung des realen Risikos somit erschwert. Auch wurde in den Fachinformationen bei Wirkstoffen, die aus pathophysiologischer Sicht keinen wesentlichen Einfluss auf die Mykobakterienimmunantwort haben können und auch in Studien keinen Hinweis auf erhöhtes Tuberkuloserisiko zeigten, die Testung und Behandlung einer latenten Tuberkulose meist empfohlen.Der erste Antikörper gegen IL‑1, einem zentralen Element der angeborenen Immunabwehr, kam 2002 auf den Markt. Die Bedeutung dieses Zytokins bei der Abwehr gegen Mykobakterien ist nicht ganz geklärt [REF]. Im Rahmen der Zulassungsstudien wie auch in der weiteren Beobachtung zeigte sich jedoch nie ein Signal für ein vermehrtes Auftreten von Tuberkulose [REF]. Zwar wurden die meisten Studien zu rheumatoider Arthritis hauptsächlich in Tuberkuloseniedriginzidenzländern durchgeführt, aber auch in Studien zur Behandlung des Morbus Behçet traten keine Tuberkulosefälle auf, obwohl hier eine erhöhte Inzidenz vorlag [REF].Nichtsdestotrotz findet sich in den Fachinformationen von Anakinra und Canakinumab die Empfehlung zu einem Screening auf LTBI.Nach den Tuberkulosefällen rund um die Markteinführung der TNF-Blocker erfolgte in den Zulassungs- und Dosisfindungsstudien rund um den T‑Zell-Co-Stimulationshemmer Abatacept meist ein Screening nach LTBI. In den über 15 Jahren seit Markteinführung 2005 in den USA bzw. 2007 in Europa fand sich jedoch seither keinerlei Hinweis auf ein vermehrtes Auftreten von Tuberkulose. Zahlreiche Registerdaten und Beobachtungsstudien (teilweise aus Tuberkulosehochinzidenzländern) erbrachten keine oder ganz vereinzelte Fälle von Tuberkulose unter Therapie mit Abatacept [REF]. Eine Analyse im Jahr 2018 von Daten verschiedener großer Abatacept-Studien mit insgesamt 21.335 Patientenjahren ergab 17 Fälle von Tuberkulose – allesamt in Hochrisikoländern und somit mit einem niedrigen Risiko zu bewerten [REF]. Diese Daten sprechen somit für ein äußerst geringes Risiko einer Tuberkuloseerkrankung unter Abatacept. Dennoch ist in der Fachinformation weiterhin das Screening auf LTBI empfohlen.Bei der Therapie mit Tocilizumab (IL-6-Rezeptor-Blockade) ging man nach den Ergebnissen bei TNF-Blockern ebenfalls äußerst vorsichtig vor. Die Medikamentengabe in den Zulassungsstudien wurde insgesamt nur nach Ausschluss einer latenten Tuberkulose durchgeführt, sodass das tatsächliche Tuberkuloserisiko unter dieser Therapie unerforscht blieb. Die Registerdaten, Beobachtungsstudien sowie auch einzelne Berichte von unbehandelter LTBI unter Tocilizumab ergaben zwar kein Signal hinsichtlich Tuberkulosegefahr dieser Medikamentenklasse, dennoch wurde angesichts des routinemäßigen Screenings vor Therapie die Gesamteinschätzung meist zugunsten einer präventiven Tuberkulosetherapie gemacht [REF]. Auch die Fachinformationen der Medikamente empfehlen das Screening und die Therapie einer LTBI.Die Blockade von IL-12 und IL-23 führt in der Theorie zu einer Beeinträchtigung der Immunantwort gegen Mykobakterien [REF]. In Beobachtungen seit Markteinführung im Jahr 2009 zeigte sich bisher kein vermehrtes Auftreten von Tuberkulose unter Therapie mit Ustekinumab [REF]. Es wurde jedoch ebenso wie bei den zuletzt genannten Therapien in Studien und auch im breiten Einsatz vor Beginn der Therapie auf LTBI gescreent, was die endgültige Beurteilung auch hier erschwert. Unterschiedliche Reviews beurteilen das Risiko für das Auftreten einer Tuberkulose als sehr niedrig. In der Fachinformation von Ustekinumab werden das Screening und die Therapie der LTBI empfohlen.Bei dieser gegen den B‑Lymphozyten stimulierenden Faktor (BLyS) gerichteten Antikörpertherapie kommt es zu einer Reduktion der Lebensdauer und Aktivität von B‑Lymphozyten. Die Therapie ist seit 2011 für die Behandlung von systemischem Lupus erythematodes (SLE) zugelassen. Weder besteht hier ein suspiziertes Tuberkuloserisiko, noch zeigte sich in Studien das Auftreten von Tuberkulosefällen [REF]. In den meisten Studien wird das Wort Mykobakterien oder Tuberkulose gar nicht erwähnt. Auch laut Fachinformation von Belimumab ist ein Risiko bei latenter oder aktiver Tuberkulose unbekannt.Der Hemmer der Phosphodiesterase 4 war das erste Präparat, das der Gruppe der tsDMARDs („targeted synthetic Disease Modifying Anti Rheumatic Drug“) zugerechnet wird, die ihre Wirksamkeit durch Einfluss auf Signalwege innerhalb der Zellen entfalten. Apremilast, das 2015 auf den Markt kam, führt durch Hemmung von PDE4 zum Anstieg von cAMP (zyklisches Adenosinmonophosphat) und damit zu verminderter Bildung und Ausschüttung von Entzündungsmediatoren. Interessanterweise findet faktisch keine klinisch relevante Beeinträchtigung der Immunantwort hinsichtlich Infektionen bei Einsatz des Medikaments statt, weswegen es als sicher bei LTBI gesehen wird [REF]. In der Fachinformation steht keinerlei Hinweis zu Bedenken bezüglich Tuberkulose.Im selben Jahr kam auch der erste Vertreter dieser Medikamentenklasse auf den Markt. Das von T‑Helfer-Zellen gebildete IL-17 hat verschiedene Funktionen, insbesondere die Steigerung einer proinflammatorischen Immunantwort. Seine alleinige Blockade führt jedoch zu keiner klinisch relevanten Beeinflussung der allgemeinen Infektabwehr, lediglich zu einem etwas vermehrten Auftreten von Candida-Infektionen. Hinsichtlich einer verminderten Mykobakterienimmunantwort finden sich keinerlei Anzeichen [REF].Auch gibt es in Untersuchungen aus Registerdaten und Beobachtungsstudien keine Hinweise für ein erhöhtes Risiko von Tuberkulose unter Anti-IL-17 [REF]. Es existieren auch Fallberichte und Fallserien, wo Patienten mit LTBI ohne präventive Therapie eine Anti-IL-17-Therapie erhielten und kein einziger Fall einer Tuberkulosereaktivierung auftrat [REF].In den Zulassungsstudien der IL-17-Blocker wurde jedoch immer auf LTBI gescreent und diese ggf. behandelt, sodass keine evidenzbasierte Aussage zum definitiven Tuberkuloserisiko gemacht werden kann. Aufgrund der zahlreichen indirekten Hinweise der Unbedenklichkeit bezüglich Tuberkuloserisiko wird in den Fachinformationen der verschiedenen IL-17-Blocker lediglich formuliert, dass ein Screening in Betracht oder Erwägung gezogen werden kann.Eine Therapie gegen Anti-IL-23 ist seit 2017 zugelassen. Die Blockade dieses Zytokins führt zur Beeinflussung der Aktivität verschiedener Zellen des angeborenen und adaptiven Immunsystems, insbesondere T‑Zellen, Makrophagen und dendritische Zellen und hat somit theoretisch auch Einfluss auf die Mykobakterienimmunantwort [REF]. In den Zulassungsstudien zu den Anti-IL-23-Therapien wurde immer auf LTBI gescreent und bei Vorliegen zumeist gegen LTBI behandelt. Bis dato kam es in klinischen und Real-World-Studien zu keiner Reaktivierung einer Tuberkulose [REF].In den Fachinformationen der Anti-IL-23-Therapien steht, dass auf LTBI untersucht und eine Therapie in Erwägung gezogen werden soll.Der erste Vertreter der JAK-Inhibitoren Tofacitinib ist seit 2017 zugelassen und erweiterte die Gruppe der tsDMARDs. Durch die Blockade der Januskinasen, die für die Signalwirkung verschiedener Zytokine von der Zelloberfläche in den
Im Rahmen der Arbeitsgemeinschaft Versorgung der Österreichischen Gesellschaft für Kinder- und Jugendpsychiatrie, Psychosomatik und Psychotherapie (ÖGKJP) soll die kinder- und jugendpsychiatrische Versorgungslandschaft in Österreich auf mehreren Ebenen untersucht, ausgewertet und analysiert werden. Dies ist umso bedeutender, da uns die Auswirkungen der Pandemie auf die psychische Gesundheit vor eine große Herausforderung stellen.Schon in Vergangenheit hat die ÖGKJP Berichte zur Versorgungslage verfasst. Die Autor:innen Fliedl, Ecker und Karwautz [REF] untersuchten die fachärztliche Situation und kamen bereits vor der Pandemie zum Schluss, dass zum einen eine große Heterogenität der Versorgungslage in den Bundesländern besteht und zum anderen die Mangelfachregelung nicht ausreichend ist, um die Ausbildung zu konsolidieren und eine Vollversorgung zu erreichen. So sei die vor der Pandemie vereinbarte stationäre Bettenmessziffer (BMZ) von 0,11 in Vorarlberg und Salzburg nahezu erreicht, in der Steiermark und Wien aber mit 0,04 bzw. 0,05 unter der Hälfte des zu erreichenden Wertes. Die Autor:innen gaben an, dass insgesamt österreichweit bei einer BMZ von 0,11 890 voll- bzw. tagesstationäre Plätze zur Verfügung stehen müssten, wovon real im Jahr 2019 lediglich 520 Behandlungsplätze umgesetzt waren.Inzwischen konnte durch die ÖGKJP erreicht werden, dass mit 07.02.2022 der Ausbildungsschlüssel im Rahmen der Mangelfachverordnung befristet bis 31.05.2027 auf 1:2 angehoben wurde [REF].Das Fach Kinder- und Jugendpsychiatrie und -psychotherapeutische Medizin ist im Österreichischen Strukturplan Gesundheit (ÖSG) [REF] verankert mit dem Ziel eines flächendeckenden Aufbaus von öffentlichen Versorgungsstrukturen für psychisch kranke Kinder und Jugendliche.Die Grundlage der integrativen Versorgungsplanung ist die zwischen dem Bund und allen Bundesländern abgeschlossene Vereinbarung gemäß Artikel 15a B‑VG über die Organisation und Finanzierung des Gesundheitswesens sowie das Bundesgesetz zur partnerschaftlichen Zielsteuerung-Gesundheit [REF] mit der Festlegung des ÖSG sowie der Regionalen Strukturpläne Gesundheit (RSG) als zentrale Planungsinstrumente.Der ÖSG [REF] ist der österreichweit verbindliche Rahmenplan für die in den RSG vorzunehmende konkrete Gesundheitsstrukturplanung und Leistungsangebotsplanung. Er bindet diesbezüglich Bund, Länder und Sozialversicherungsträger (siehe Abb. 1). Damit legt der ÖSG [REF] einen Richtwert für die Ärzt:innendichte im Fachgebiet für jede der 32 Versorgungsregionen in Österreich fest. Allerdings ist die Treffgenauigkeit dieser Richtwerte äußerst gering, da sie zum einen regional nicht ausreichend differenzieren, eine große Bandbreite innerhalb der Versorgungsregionen aufweisen, nicht auf definierten Versorgungszielen basieren und natürlich nicht die aktuellen Veränderungen durch die Pandemie berücksichtigen konnten.Zur Unterstützung der spezifisch notwendigen regionalen Ausgestaltung von Versorgungsangeboten bzw. zur Berücksichtigung regionaler Besonderheiten werden Bandbreiten (Ober- und Untergrenzen) für Planungsrichtwerte angegeben. Ziel ist es, durch das Festlegen von Obergrenzen negative Auswirkungen von allfälliger Angebotsinduktion zu vermeiden sowie durch Untergrenzen Minimalanforderungen an das Leistungsspektrum bzw. an den Versorgungsauftrag in Art und Umfang festzulegen. Die Einhaltung dieser Grenzen ist in der Leistungssteuerung und/oder im Rahmen von regionalen Detailplanungen (RSG) zu beachten. Generell beruhen die Planungsrichtwerte für die Angebotsplanung auf folgenden Annahmen in Bezug auf komplementäre Versorgungsbereiche: 1. Sicherstellung einer präklinischen Notfallversorgung durch ein verlässliches Notfallversorgungssystem unter zentraler Einbindung des Notarzt‑/Rettungswesens. 2. Sicherstellung einer bedarfsgerechten Verfügbarkeit von Betreuungsangeboten (Rehabilitation, Sozial- und Pflegebereich, Hauskrankenpflege, therapeutisches Angebot) und Unterstützen von Laienhilfe, Selbsthilfegruppen, etc.              Die Grundsätze und Ziele für unser Fach sind der Auf- und Ausbau von stationär und ambulant verschränkten, vorrangig multiprofessionellen Angeboten in der Kinder- und Jugendpsychiatrie (KJP) und Psychosomatik für Kinder und Jugendliche und deren Vernetzung insbesondere mit Angeboten im Sozial- und Bildungsbereich [REF]. Von der Gesundheit Österreich GmbH (GÖG) wurden die vorhandenen Bettenzahlen im Fach Kinder- und Jugendpsychiatrie in den Jahren 2010, 2013 und 2018 erhoben (siehe Abb. 2). In diesem Zeitraum kamen demnach rund 100 Betten in ganz Österreich hinzu. Dennoch ist in Österreich keine hinreichende Bettendichte für Kinder und Jugendliche mit psychischen Erkrankungen entsprechend den – vor der Pandemie – definierten Mindestrichtwerten in den einzelnen Bundesländern, mit der Ausnahme von Vorarlberg, gegeben. So hat z. B. das Burgenland keine eigenen stationären KJP-Kapazitäten, stattdessen werden die Kinder und Jugendlichen aus dem Burgenland in Niederösterreich und in der Steiermark mitversorgt, was für die jeweiligen Patient:innen und deren Familien lange Anfahrtswege bedeutet. In den Bundesländern Vorarlberg, Salzburg und Niederösterreich ist die Versorgungsdichte am höchsten [REF]. In den ÖSG-Verhandlungen hat man sich auf die Zielgröße Bettenmessziffer (BMZ, Formel: BMZ = tatsächliche Betten/Gesamtbevölkerung × 1000) geeinigt. Die neun Landesfonds melden jährlich ihre Bettenzahl inklusive ihrer tagesklinischen Behandlungsplätze an die GÖG. Die BMZ 2018 bezieht sich auf alle für die stationäre Versorgung im Jahr 2018 systemisierten Betten (inklusive Tagklinikplätze). Das Intervall der Bettenmessziffer vollstationär (BMZvs) ist Ausdruck der zum Planungshorizont erforderlichen Kapazitätsdichte für die vollstationäre Versorgung der Wohnbevölkerung pro Bundesland. Das Intervall ermöglicht, epidemiologische und intersektorale Versorgungs-Spezifika einer Region zu berücksichtigen und trägt dem heterogenen Ausmaß erzielter Tagklinikanteile bzw. der Verlagerung von Teilen der stationären Versorgung in den spitalambulanten Bereich Rechnung. Die Zurechnung von ausländischen Gastpatient:innen erfolgt zielbezogen (zum Leistungsstandort) gemäß Inanspruchnahme 2018.Im Rahmen der regionsspezifischen Detailplanung sind Patient:innenströme zu berücksichtigen. Planungsrichtwerte zur Kapazitätsdichte für bettenführende, tagklinische und tagesambulante Strukturen berücksichtigen die regionale Bevölkerungsstruktur und Besiedelungsdichte, die Erreichbarkeitsverhältnisse im Straßen-Individualverkehr (ohne Berücksichtigung wetter-/verkehrsbedingter Verzögerungen), die beobachtete Auslastung bereits bestehender stationärer Einheiten sowie die expertengestützt erwartbaren Tendenzen in der medizinischen Entwicklung in den einzelnen Fach- bzw. Versorgungsbereichen (inklusive der mit dem medizinischen Fortschritt sich ergebenen Möglichkeiten für eine verstärkte Verlagerung in die tagklinische bzw. ambulante Leistungserbringung).
Seit 1991 sind die Behandlungsfälle in deutschen Kliniken um 25 % gestiegen, während gleichzeitig die Anzahl der Krankenhäuser von 2400 auf 1942 gesunken ist [REF]. Hieraus lässt sich eine deutliche Arbeitsverdichtung ableiten, die durch den demografischen Wandel verstärkt wird. In den nächsten 20 Jahren werden in Deutschland ein weiterer Rückgang der Bevölkerung im Erwerbsalter und ein Anstieg der Seniorenzahl zu verzeichnen sein [REF]. Gesundheitsorganisationen müssen ökonomisch agieren und stehen unter einem zunehmenden Druck, ihre Kosten zu senken [REF]. Auf der Ausgabenseite sind die Personalkosten der größte Kostenfaktor im Krankenhaus. Im Gegensatz zu den ärztlichen Leistungen werden die pflegerischen Leistungen im deutschen DRG-System nur unzureichend auf der Erlösseite abgebildet [REF]. Hieraus begründet wurden vor allem im pflegerischen Bereich in den letzten Jahren massive Personaleinsparungen getätigt. Eine Folge sind ein zunehmender Personalmangel auf der Intensivstation (ITS) und Intermediate-Care-Stationen (IMC; [REF]). Obwohl es in Deutschland, verglichen mit allen anderen europäischen Ländern, überdurchschnittlich viele Intensiv- und Intermediate-Care-Betten bezogen auf die Einwohnerzahl gibt (etwa 30 Betten pro 100.000 Einwohner; [REF]), kommt es paradoxerweise in diesem Bereich immer wieder zu Engpässen in der Patientenversorgung. Diese Engpässe sind maßgeblich darauf zurückzuführen, dass sowohl die Anzahl der Intensivbetten als auch die Behandlungsfälle in Deutschland kontinuierlich gestiegen sind, während im selben Zeitraum die Anzahl an Pflegekräften und Fachpflegekräften abgenommen hat [REF]. Eine geringe Personalstärke und ein geringer Qualifikationsmix erhöhen die Wahrscheinlichkeit, dass Patienten im Krankenhaus sterben [REF]. Mittlerweile ist die Anzahl der Pflegekräfte auf deutschen Intensivstationen nicht mehr ausreichend, um eine kontinuierliche und adäquate Patientenversorgung sicherzustellen. In der Folge müssen Betten und Behandlungsplätze in der Regel- und Notfallversorgung gesperrt werden [REF]. Mit verschiedenen Mitteln wird versucht, den akuten Personalmangel auszugleichen [REF]. Krankenhausmanager haben seit mehr als 15 Jahren zunehmend Schwierigkeiten, Pflegepersonal zu rekrutieren und/oder zu halten. Dabei wird auf entsprechende Vermittlerfirmen zurückgegriffen, um freie Stellen mit Leiharbeitern zu besetzen und eine adäquate Patientenversorgung aufrechtzuerhalten [REF]. Eine Umfrage im Namen der Deutschen Gesellschaft für Internistische Intensiv- und Notfallmedizin (DGIIN) zeigte, dass trotz aller dieser Bemühungen der letzten Jahre 37 % der befragten Intensivpflegenden planen, ihren Beruf in den nächsten fünf Jahren zu verlassen. Zudem wollen insgesamt 34 % ihre Arbeitszeit in den nächsten zwei Jahren reduzieren. Nicht wenige Pflegekräfte scheinen dabei momentan einen Wechsel in die Leih- und Zeitarbeit zu bevorzugen [REF].Die Begriffe Leih- und Zeitarbeiter werden meist synonym verwendet und bezeichnen Mitarbeiter, die zeitlich begrenzt durch einen entsprechenden Personaldienstleister (Arbeitnehmerüberlassung) vermittelt werden. In der internationalen Literatur werden ebenfalls verschiedene Begriffe zur Leih- und Zeitarbeit synonym verwendet. Die Vermittlung durch eine Agentur ist auch in anderen Ländern üblich. Eine reine freiberufliche und/oder selbstständige Tätigkeit als Pflegekraft ist in deutschen Kliniken durch das aktuelle Urteil des Bundessozialgerichts (Az. BSG B 12 R 11/18 R) nicht mehr möglich. Für diesen Text wird im Folgenden der Begriff Leiharbeiter für alle Formen der Leih- und Zeitarbeit und Arbeitnehmerüberlassung verwendet.Für die Intensivstation ist beschrieben, dass der Betreuungsschlüssel die sogenannten pflegesensitiven Outcomeparameter beeinflusst [REF]. Für die Normalstationen gibt es Hinweise, dass eine Versorgung mit mehr als 1,5 h pro Patient und Tag durch Zeitarbeiter mit einem erhöhten Sterberisiko verbunden ist [REF]. Eine Ursache könnte fehlende Vertrautheit mit den Stationsabläufen und den internen Kommunikationsabläufen sein [REF]. Ob dies auch auf Intensivstationen zutrifft, ist unklar.
Das Hochrisikoprostatakarzinom (PCA) zeichnet sich durch das Vorliegen einer oder mehrerer der prätherapeutischen Tumorcharakteristika aus: prostataspezifisches Antigen (PSA) > 20 ng/ml, Gleason-Score ≥ 8/ISUP (International Society of Urological Pathology) 4–5 oder cT-Kategorie ≥ 2c [REF]. Eine besondere Untergruppe des Hochrisiko-PCA stellt dabei das lokal fortgeschrittene PCA dar, welches über das klinische Stadium eines extraprostatischen Wachstums (cT3–4) bzw. einen lokalen Lymphknotenbefall (cN1) definiert wird.Aktuelle Studien, welche die Stadienmigration des PCA untersuchen, beschreiben zuletzt keine signifikante Zunahme der Patienten mit einem Hochrisiko-PCA in den letzten Jahren [REF]. Der Anteil der Patienten mit einer Hochrisiko Konstellation machte an größeren Zentren ungefähr 20–40 % der operierten Patienten aus [REF]. Es ist jedoch anzunehmen, dass es mit einer gewissen Latenz aufgrund der COVID-19-Pandemie und dem natürlichen Verlauf des PCA in den nächsten Jahren zu einem weiteren Anstieg an Patienten mit aggressiven und lokal fortgeschrittenen Tumoren kommen wird.Patienten haben in den Pandemiewellen ihre Vorsorgetermine nicht oder nur verzögert wahrgenommenViele Patienten haben v. a. in den ersten Pandemiewellen ihre Vorsorgetermine nicht oder nur verzögert wahrgenommen. Erste Studien zur Stadienmigration nach den ersten Pandemiewellen zeigen bereits eine deutliche Zunahme der Patienten mit einem extraprostatischen Wachstum und Lymphknotenbefall die sich einer radikalen Prostatektomie unterzogen [REF]. Zudem wurde auch eine Zunahme der Wartezeit bis zur Operation festgestellt, was u. a. auf den großen Mangel an Pflegepersonal zurückzuführen ist, wodurch elektive Operationen wie radikale Prostatektomien verschoben werden müssen oder Krankenhausbetten nicht betrieben werden können.Es ist somit zu erwarten, dass es in den nächsten Jahren zu einer weiteren Zunahme der Patienten mit einem Hochrisiko- oder lokal fortgeschrittenem PCA kommen wird und somit das Wissen um eine optimale Versorgung unserer Patienten aktuell von großem Interesse ist.Entsprechend der deutschen S3-Leitlinie zum PCA sollten alle Patienten mit einem PSA > 10 ng/ml, Gleason-Score ≥ 8, cT3/4-Stadium oder Knochenschmerzen ein Knochenszintigramm als Staging erhalten. Bei einem Gleason-Score ≥ 8 oder cT3/4-Stadium sollte zusätzlich eine Schnittbildgebung mit Computertomographie (CT) oder Magnetresonanztomographie (MRT) des Beckens erfolgen [REF]. Demgegenüber geben die europäischen Leitlinien eine starke Empfehlung zur Durchführung einer Schnittbildgebung (CT oder MRT des Becken/Abdomen) in Kombination mit einem Knochenszintigramm für alle Hochrisikopatienten als initiales Staging aus [REF].Zur Beurteilung des lokalen Tumorstadiums kommt der DRU ein großer Stellenwert zuZur Beurteilung des lokalen Tumorstadiums kommt im klinischen Alltag dem idealerweise vom jeweiligen Operateur durchgeführten transrektalen Tastbefund ein großer Stellenwert zu. Die S3-Leitlinie empfiehlt zur Bestimmung des lokalen Tumorstadiums bzw. der Resektabilität die digital-rektale Untersuchung (DRU). In Bezug auf eine Samenblaseninfiltration oder einen Kapseldurchbruch zeigte sich die DRU gleichwertig mit dem transrektalen Ultraschall (TRUS). Die Kombination aus TRUS und DRU zeigte hingegen keinen zusätzlichen Vorteil [REF]. Zusätzlich sollte, wenn bereits von der Fusionsbiopsie vorliegend, die MRT zur Therapieplanung bzw. Beurteilung der lokalen Tumorausbreitung verwendet werden (Abb. 1; z. B. Infiltration der Blase, Rektum). Als Alternative zur konventionellen Bildgebung kann die Durchführung einer prostataspezifischen Membranantigen-Positronenemissionstomographie/CT (PSMA-PET/CT) bei Hochrisikopatienten als Ausbreitungsdiagnostik eingesetzt werden [REF]. Die PSMA-PET/CT hat erwiesenermaßen eine höhere Genauigkeit und Sensitivität, was die Detektion von Metastasen angeht, sodass diese früher detektiert werden als in der konventionellen Bildgebung [REF]. Bislang existiert jedoch noch keine Evidenz, die ein besseres Outcome durch eine Veränderung in der Behandlung durch den früheren Nachweis von Metastasen belegen kann. Es ist jedoch anzunehmen, dass die PSMA-PET/CT mittel- bis langfristig die konventionelle Bildgebung als Ausbreitungsdiagnostik ablösen wird, sobald Daten vorliegen, die ihren Nutzen zeigen.
Die Ständige Impfkommission (STIKO) entwickelt für die Bevölkerung Deutschlands Impfempfehlungen, die sich auf in der Europäischen Union (EU) zugelassene Impfstoffe beziehen. Die STIKO erarbeitet ihre Empfehlung mit Methoden der evidenzbasierten Medizin (EBM). Sie prüft dabei nicht nur den Nutzen der Impfung für das geimpfte Individuum, sondern berücksichtigt auch die potenziellen Effekte der Impfung auf die gesamte Bevölkerung und bezieht weitere Aspekte, wie die Umsetzbarkeit der Impfempfehlung und die Akzeptanz der Impfung in der Bevölkerung, in ihre Bewertung mit ein. Neben Empfehlungen zu Standard- und Indikationsimpfungen gibt die STIKO Empfehlungen für Impfungen im Rahmen von Ausbruchsgeschehen bzw. Pandemien. Hier kommt dem Impfen eine besondere Bedeutung für den Schutz der bisher nicht infizierten Bevölkerung vor Erkrankung und für die Eindämmung der Weiterverbreitung des Erregers zu.Die Impfkampagne im Rahmen der Coronavirus-Disease-2019(COVID-19)-Pandemie ist im Vergleich zu in der Vergangenheit aufgebauten Impfprogrammen in vieler Hinsicht einzigartig: Noch nie wurden Impfstoffe gegen einen neuen Erreger und mit einer neuen Impfstofftechnologie so schnell – im Zeitraum von weniger als einem Jahr – entwickelt, in klinischen Studien geprüft und für den Einsatz in der Bevölkerung zugelassen. Zu Beginn der Pandemie wurde an mehr als 170 Impfstoffkandidaten geforscht und weitere 15 Impfstoffe befanden sich in der klinischen Erprobung. Noch nie wurde in Deutschland in nur wenigen Monaten ein so großer Anteil der Bevölkerung geimpft. Noch nie war daher die STIKO gefordert, Impfempfehlungen innerhalb kürzester Zeit zu erstellen und über Monate hinweg fortlaufend zu aktualisieren und an die sich verändernde Epidemiologie und neue Erkenntnisse anzupassen.Die Ständige Impfkommission konstituierte sich 1972 und besteht im Jahr 2022 seit 50 Jahren. Die STIKO ist beim Robert Koch-Institut (RKI) angesiedelt, wo sich auch ihre Geschäftsstelle befindet [REF]. Die Aufgaben der Kommission sind im Infektionsschutzgesetz (IfSG) seit 2001 gesetzlich festgelegt. Nach § 20 Abs. 2 gibt die Kommission Empfehlungen zur Durchführung von Schutzimpfungen und anderer Maßnahmen der spezifischen Prophylaxe, auf deren Grundlage die obersten Landesgesundheitsbehörden der Bundesländer ihre Empfehlungen aussprechen [REF]. Der Gemeinsame Bundesausschuss (G-BA) entscheidet seit 2007 auf Basis der STIKO-Empfehlungen, ob eine Impfung in die Schutzimpfungsrichtlinie aufgenommen und zur Pflichtleistung der Gesetzlichen Krankenversicherung wird.Die STIKO hat bis zu 18 ehrenamtlich tätige Mitglieder. Die ExpertInnen kommen aus verschiedenen Fachdisziplinen (z. B. Pädiatrie, Virologie, Allgemeinmedizin, Immunologie, Epidemiologie, Arbeitsmedizin, Öffentlicher Gesundheitsdienst) und werden alle 3 Jahre vom Bundesministerium für Gesundheit (BMG) neu berufen. Um die Transparenz der Kommissionsarbeit zu stärken, werden die Mitglieder seit mehr als 10 Jahren aufgefordert, vor ihrer Berufung und vor jeder regulären STIKO-Sitzung mögliche Interessenkonflikte in einem standardisierten Fragebogen offenzulegen. Mitglieder, bei denen zu einzelnen Beratungsgegenständen ein Anschein von Befangenheit besteht, z. B. weil sie an einer Impfstoffstudie zu der betreffenden Erkrankung aktiv oder beratend beteiligt sind, dürfen bei der Beratung und Beschlussfassung nicht mitwirken.Die STIKO trifft sich regulär 3‑mal im Jahr in Präsenz, um über neue oder angepasste Impfempfehlungen zu beraten. Seit 2011 werden die Impfempfehlungen nach der Standardvorgehensweise (SOP) für die Entwicklung evidenzbasierter Impfempfehlungen erarbeitet [REF]. Diese beinhaltet die systematische Aufarbeitung aller relevanten Inhalte anhand eines vorgegebenen Fragenkatalogs durch die wissenschaftlichen MitarbeiterInnen der Geschäftsstelle der STIKO. Die WissenschaftlerInnen arbeiten dabei eng mit den STIKO-Mitgliedern in Arbeitsgruppen zu den einzelnen Impfungen bzw. zu den impfpräventablen Erkrankungen zusammen. Mittels mathematischer Modellierungen, die durch Mitarbeitende des Fachgebietes Impfprävention des RKI oder externe Partner durchgeführt werden, werden direkte und indirekte Effekte einer neuen oder geänderten Impfempfehlung anhand von unterschiedlichen Szenarien abgeschätzt. Die Ergebnisse dieser Berechnungen sind ein wichtiger Baustein im Entscheidungsprozess und in der abschließenden Nutzen-Risiko-Abwägung der STIKO.Bevor eine Impfempfehlung verabschiedet wird, geht der Entwurf zusammen mit der wissenschaftlichen Begründung üblicherweise in ein 6‑wöchiges Stellungnahmeverfahren. Eine neue Empfehlung wird durch die Veröffentlichung im Epidemiologischen Bulletin gültig und erscheint zusammen mit der wissenschaftlichen Begründung. Für die Begleitkommunikation (Pressemitteilungen, Faktenblätter, Impfnews in der STIKO-App, „Antworten auf häufig gestellte Fragen“ (FAQs)) wird die STIKO von einem Kommunikationsteam des Fachgebiets Impfprävention am RKI unterstützt. Da die Ressourcen in der Geschäftsstelle sowie in der STIKO begrenzt sind, werden zu Beginn der Berufungsperiode die Themen des Arbeitsplans durch die Kommission nach ihrer Wichtigkeit und Dringlichkeit priorisiert und das zeitliche Vorgehen abgestimmt.Die beschriebene Vorgehensweise hat auch während einer Pandemie unabhängig vom auslösenden Erreger Bestand. Auch im Rahmen der COVID-19-Pandemie hat die STIKO die gesetzliche Aufgabe, auf Basis der verfügbaren Daten zur Epidemiologie der Erkrankung und zu den verfügbaren Impfstoffen evidenzbasierte Impfempfehlungen für die deutsche Bevölkerung zu entwickeln und diese immer wieder dem aktuellen Wissensstand anzupassen. Bei limitierten Impfstoffressourcen bilden Risikogruppen-spezifische Empfehlungen der STIKO die Grundlage für die Impfstoffverteilung.
Die Transparenz von klinischen Studien, deren Design und deren Ergebnissen wird von allen AkteurInnen der klinischen Forschung als entscheidend angesehen, um den Wert dieser Forschung für die Gesellschaft sicherzustellen und kontinuierlich zu verbessern. Transparenz reduziert Ineffizienzen z. B. durch die Vermeidung von Forschung, die nicht oder anders durchgeführt worden wäre, hätte man zeitnah Informationen über die Existenz von ähnlichen Studien gehabt. Transparenz ist zudem von zentraler Relevanz, um vollständige und damit unverzerrte Informationen über die erwünschten und unerwünschten Effekte medizinsicher Maßnahmen zu erhalten. Unzureichende Transparenz der Ergebnisse klinischer Forschung kann bei Behandelnden und ihren PatientInnen zu Fehlentscheidungen im Zusammenhang mit der Anwendung medizinischer Maßnahmen führen, wodurch wiederum der PatientInnenschutz gefährdet wird. Für weitere Ziele und Mechanismen von Transparenz zu klinischen Studien siehe Tab. 1. Trotz der Einigkeit über den Bedarf an Transparenz in der klinischen Forschung gibt es anhaltende Kontroversen über deren angemessene Spezifizierung. Wie zeitnah, wie zugänglich (Open Access), in welchem Format (Fachartikel, Summary Results) und in welcher Verbindung zur vorherigen Registrierung der Studie sollten die Ergebnisse veröffentlicht werden, damit sie nicht nur für regulatorische Prozesse, sondern insbesondere auch für die Wissenschaft und die Gesellschaft nützlich sind? Empirische Evaluationen zur Umsetzung „nützlicher“ Transparenz zeigen verschiedene Optimierungspotenziale auf. Das Ziel dieses Beitrages ist es, diese Kontroversen und Umsetzungsfragen für klinische Studien darzustellen, unabhängig davon, ob sie durch die EU-Verordnung 536/2014 (Clinical Trials Regulation – CTR) adressiert werden oder nicht. Zugleich wird die besondere Bedeutung der CTR herausgearbeitet, da sie als gesetzliche Vorgabe zur Transparenz von interventionellen Arzneimittelstudien direkt und indirekt Auswirkungen auf das Verständnis und die Umsetzung von Transparenz bei allen klinischen Studien hat.Der Fokus dieses Beitrages liegt auf interventionellen klinischen Studien, welche grundsätzlich zur Evaluation aller medizinischen Maßnahmen verwendet werden können. Dazu gehören neben Arzneimitteln auch Medizinprodukte, psychotherapeutische oder chirurgische Verfahren.1 Die Transparenz nichtinterventioneller Studien ist aus den oben genannten Gründen genauso wichtig wie die Transparenz zu interventionellen Studien. Es würde allerdings einen eigenen Beitrag benötigen, um die zum Teil sehr grundlegenden Kontroversen zur Binnendifferenzierung von nichtinterventionellen Studien darzustellen (Stichwort: Anwendungsbeobachtungen [REF]). Des Weiteren existieren bislang nur sehr wenige Studien, die die Umsetzung von Transparenz bei nichtinterventionellen Studien systematisch untersucht haben [REF].Nach einer Einführung in 4 komplementäre Transparenzbereiche bei klinischen Studien, werden einige empirische Untersuchungen zu deren konkreter Umsetzung präsentiert. Vor diesem Hintergrund wird in 2 folgenden Kapiteln dargestellt, wie sich über die letzten Jahre eine „2-Klassen-Transparenz“ zu entwickeln scheint. Diese ist dadurch gekennzeichnet, dass nur solche Studien transparenter werden, die von der CTR adressiert werden, und dies auch nur in bestimmten Transparenzbereichen, die in der CTR festgelegt werden. Abschließend wird das Potenzial vorgestellt, welches in der Transparenz von studienbezogenen Dokumenten liegt, und es werden Einschränkungen von Transparenz aufgezeigt, die durch unabhängige Forschergruppen evaluiert werden sollten.
Die COVID-19-Pandemie stellte die Bedeutung des Öffentlichen Gesundheitsdienstes (ÖGD) in den Fokus [REF]. Zahlreiche Berichte über Personalmangel [REF] und eine nicht ausreichende digitale Infrastruktur [REF] wurden von der Presse veröffentlicht. Zum Beispiel stellte die Kontaktnachverfolgung als wesentlicher Bestandteil der Eindämmung von COVID-19-Ausbrüchen [REF] angesichts der begrenzten personellen und IT-technischen Ressourcen eine Herausforderung dar. Während dieser Krise nutzten einige Gesundheitsämter selbsterstellte Excel-Tabellen zur Datenspeicherung und Faxgeräte zum Empfang von Laborberichten. Diese zeit- und personalintensiven Maßnahmen führten insbesondere in Zeiten der COVID-19-Pandemie zu Engpässen, so dass Mitarbeitende außergewöhnlicher Arbeitsbelastung ausgesetzt waren [REF]. Die Digitalisierung von Gesundheitsämtern gilt deshalb spätestens seit der COVID-19-Pandemie als Voraussetzung, um Prozesse effizienter zu gestalten und Mitarbeitende zu entlasten. Im Rahmen des „Paktes für den Öffentlichen Gesundheitsdienst“ (ÖGD-Pakt) werden 4 Mrd. € für die personelle Aufstockung, Modernisierung und Vernetzung der deutschen Gesundheitsämter bereitgestellt, wobei der digitale Ausbau des ÖGD mit 800 Mio. € gefördert wird [REF].Der ÖGD spielt aber nicht nur eine zentrale Rolle bei der Pandemiebekämpfung, sondern auch bei Prävention sowie Gesundheitsschutz und -förderung der Bevölkerung. In Deutschland setzt sich der ÖGD aus den Einrichtungen der Gesundheitsverwaltung auf Bundesebene (z. B. Robert Koch-Institut – RKI), Länderebene (z. B. Ländergesundheitsministerien, Landesämter bzw. Landesinstitute für Gesundheit) und kommunaler Ebene (z. B. Gesundheitsämter) zusammen [REF]. Auf kommunaler Ebene ist der ÖGD mit ca. 375 Gesundheitsämtern vertreten. Neben den akut anfallenden Aufgaben in der Bekämpfung der COVID-19-Pandemie, wie der Meldung von Infektionsfällen oder der Kontaktnachverfolgung, umfasst das Aufgabenspektrum der Gesundheitsämter dauerhaft relevante Aufgabenfelder, wie Beratungs- und Unterstützungsangebote für Individuen, Familien und Einrichtungen, den Kinder- und Jugendgesundheitsschutz, das amtsärztliche Bescheinigungswesen sowie Kontroll- und Überwachungsfunktionen, bspw. in der Krankenhaushygiene.Der mit dem ÖGD-Pakt angestrebte digitale Ausbau der Gesundheitsämter unterliegt jedoch einigen strukturellen und technischen Rahmenbedingungen. So sind die Gesundheitsämter Teil der Landkreise und der kreisfreien Städte und damit in deren Organisation der IT-Infrastruktur und der digitalen Ausstattung eingebunden. Der überwiegende Anteil der Landkreise und kreisfreien Städte verfügt über eine IT-Abteilung, die die digitale Anbindung und Ausstattung der Gesundheitsämter verantwortet [REF]. Für den Bereich der Infektionsprävention und -bekämpfung lässt sich hinsichtlich der Nutzung von Fachanwendungen ein heterogenes Bild zeichnen. Das RKI stellt den Gesundheitsämtern mit SurvNet@RKI das am häufigsten genutzte Tool zur Verfügung. Darüber hinaus werden zahlreiche andere Fachanwendungen wie OctoWare, ISGA, Äskullab 21 oder Mikropro genutzt [REF]. Viele dieser Fachanwendungen decken ein Leistungsspektrum über den Infektionsschutz hinaus ab und können so vielseitig in den Gesundheitsämtern eingesetzt werden. Jedoch fehlen in kommerziellen Fachanwendungen häufig passende Schnittstellen, die z. B. notwendig sind, um eine nahtlose Kommunikation zu SORMAS zu ermöglichen. SORMAS ist eine Software, die deutschlandweit häufig zum Pandemiemanagement eingesetzt wird. Durch diese fehlende Interoperabilität im Bereich des Infektionsschutzes kommt es häufig noch zu manuellem Mehraufwand. Diese Heterogenität der technischen Landschaft und die unzureichende Interoperabilität erschweren zentral gesteuerte Digitalisierungsvorhaben. Weiterhin gibt es eine anhaltende Diskussion, dass die Gesundheitsämter in Deutschland auch aufgrund struktureller Gegebenheiten Schwierigkeiten bei der Digitalisierung haben [REF]. Die Digitalisierung findet in Deutschland aufgrund föderaler Strukturen oft in bundeslandspezifischen „Silos“ statt, was zu unterschiedlichen Fähigkeiten in den einzelnen Bundesländern führt [REF]. Aufgrund dieser technischen und strukturellen Hemmnisse braucht es Unterstützung, um die in der Krise arbeitenden Gesundheitsämter schrittweise anzuleiten und mit einer einheitlichen Zielvorstellung zu digitalisieren. Für diese schrittweisen Vorhaben in Richtung eines gemeinsamen Ziels haben sich in der Vergangenheit Reifegradmodelle (RGM) bewährt. Diese Modelle beschreiben entlang definierter Reifegradstufen konkrete Entwicklungspfade, die insbesondere in komplexen technischen Projekten helfen, konkrete Maßnahmen abzuleiten, um die Digitalisierung sukzessive zu verbessern [REF].Bestehende Reifegradmodelle zeigen bislang Evidenzlücken sowohl in der Evaluation genutzter RGM als auch in der spezifischen Entwicklung eines RGM für den ÖGD. Um diese Evidenzlücken zu füllen, wurde das Projekt „Reifegradmodelle für die Unterstützung des Pakts für den öffentlichen Gesundheitsdienst“ (ReDiGe) ins Leben gerufen. Ziel dieses Projektes ist es, ein empiriebasiertes RGM für den ÖGD zu entwickeln und dieses unter Einbezug der Nutzenden zu evaluieren und zu optimieren. In diesem Beitrag wird das RGM als Messmodell für den digitalen Status quo deutscher Gesundheitsämter vorgestellt.
Das Netzwerk Universitätsmedizin (NUM) wurde im April 2020 im Rahmen einer Projektförderung des Bundesministeriums für Bildung und Forschung (BMBF) als Reaktion auf die erste COVID-19-Infektionswelle gegründet. Aufgabe ist die Koordination der COVID-19-Forschung der Universitätsmedizin mit dem Ziel, Patient*innenversorgung und Pandemiemanagement möglichst unmittelbar mit Evidenz zu unterstützen und dabei gut abgestimmt zu agieren. Das NUM setzt somit am Ende der Translationskette an. Auf Bundesebene ist das NUM die erste Initiative zur interdisziplinären Vernetzung, Zusammenarbeit und Koordination im Bereich der patientenorientierten Forschung, an der alle deutschen Universitätskliniken (UK) beteiligt sind. Das NUM setzt auf einen partnerschaftlich-kooperativen, nichtkompetitiven Ansatz. Ziel ist u. a. die Stärkung der „pandemic preparedness“ des Versorgungs- und Forschungssystems. Dazu gehört insbesondere der Aufbau geeigneter Forschungsinfrastrukturen (FIS).1 Dafür benötigt das NUM drei wesentliche Grundlagen: arbeitsfähige standortübergreifende interdisziplinäre Expert*innennetzwerke2, die die relevanten Teilaspekte des Pandemiegeschehens organisieren, standortübergreifende abgestimmte Sammlung von Forschungsdaten zu COVID-19 und deren Bereitstellung für die wissenschaftliche Gemeinschaft, übergreifende, alle UK umfassende Organisationsstrukturen. Das NUM hat in der 1. Förderperiode (01.04.2020–31.12.2021) in wenigen Wochen 13 große, jeweils als standortübergreifende Verbünde konzipierte Projekte zu unterschiedlichen Aspekten des Pandemiegeschehens auf den Weg gebracht. Diese Projekte wurden jeweils von unterschiedlichen Expert*innengruppen durchgeführt. Es wurde dabei schnell deutlich, dass in Deutschland zu Beginn der Pandemie die notwendigen Plattformen fehlten, um über die 36 UK hinweg gemeinsam Forschungsdaten strukturiert zu erheben, zu dokumentieren und zu teilen. Deshalb haben einige Projekte FIS entweder neu aufgebaut oder bereits vorhandene FIS gemäß den jeweiligen Anforderungen und Bedürfnissen ausgebaut.Fünf dieser entstandenen NUM-FIS, die unterschiedliche Arten von medizinischen Forschungsdaten adressieren, werden in der seit 01.01.2022 angelaufenen 2. Förderperiode in der neben den Forschungsprojekten neu etablierten „Infrastrukturlinie“ fortgeführt: 1. NUM Klinische Epidemiologie- und Studienplattform (NUKLEUS): Diese Plattform unterstützt die standardisierte Erhebung und Bereitstellung von prospektiv im Rahmen klinischer (Beobachtungs‑)Studien gewonnenen Daten, Bildern, Bioproben und daraus gewonnener Informationen. Sie setzt technisch und konzeptionell auf der Studienplattform des Deutschen Zentrums für Herz-Kreislauf-Forschung e. V. (DZHK) auf, welches in der 1. Förderphase eine schnelle Anpassung dieser Plattform an die Bedürfnisse des NUM unterstützt hat. 2. NUM-Routinedatenplattform (NUM-RDP): Sie zielt auf die (retrospektive) Gewinnung von Behandlungsdaten zu COVID-19 aus den klinischen Primärsystemen ab. Eine Ergänzung um Bildgebungs- und Bioprobendaten ist vorgesehen. Das Projekt wird in enger Kooperation mit der Medizininformatik-Initiative (MII) durchgeführt und setzt auf deren Vorarbeiten auf, indem es u. a. die Datenintegrationszentren (DIZ) durch eine zentrale Datenhaltungskomponente ergänzt und verbindet. Die DIZ dienen zudem als Datenquelle für das NUM-Dashboard, welches das Pandemiemanagement mit echtzeitnahen Daten aus der Routineversorgung unterstützt. 3. Radiological Cooperative Network (RACOON): Auf dieser Plattform haben sich alle universitären Radiologien Deutschlands zusammengeschlossen, um radiologische Bild- und Befunddaten zu COVID-19 standortübergreifend strukturiert zu erfassen und große Datensätze für die gemeinsame Forschung und das Trainieren von Algorithmen verfügbar zu machen. 4. AKTIN-Notaufnahmeregister (AKTIN@NUM): Das bestehende Register des „Aktionsbündnisses zur Verbesserung der Kommunikations- und Informationstechnologie in der Intensiv- und Notfallmedizin“ (AKTIN) wurde durch die NUM-Förderung weiter ausgebaut. Es hat eine dezentrale standardisierte und strukturierte Dokumentation in Notaufnahmen geschaffen und stellt Behandlungsdaten aus diesem spezifischen Setting für Forschungsvorhaben bereit. 5. NUM Genomic Pathogen Surveillance and Translational Research (GenSurv): Auf dieser Plattform werden Sequenzierungs- und Metadaten von SARS-CoV-2-Varianten gesammelt, um z. B. die Surveillance bzgl. neu auftretender Virusvarianten mit Hilfe von Verbreitungsanalysen zu unterstützen. GenSurv aggregiert komplexe Omics-Datensätze in geotemporaler Auflösung und interagiert eng mit den Datenbanken des Robert Koch-Instituts (RKI). Sofern aus den weiteren NUM-Forschungsprojekten zusätzliche (Daten‑)FIS entstehen, besteht die Möglichkeit, die genannten fünf NUM-FIS um weitere Komponenten zu erweitern.Konzeption und Betrieb von FIS im NUM sind an zwei wesentliche Bedingungen geknüpft: Einhaltung der international etablierten „FAIR Data Principles“4, sofern anwendbar, Unterstützung des im NUM einheitlich definierten COVID-19-Kerndatensatzes GECCO5, sofern sinnvoll. Entsprechend diesen Vorgaben stehen die im Rahmen von NUM-Projekten erhobenen Daten und Bioproben der Wissenschaftsgemeinschaft zur Beantwortung von Forschungsfragen über die jeweils im Projekt etablierten Use-and-Access-Verfahren zur Verfügung. Die FIS können grundsätzlich – unter Einhaltung der Nutzungsbedingungen – von allen interessierten Wissenschaftler*innen genutzt werden.
Die Infektiologie hat sich in Deutschland als klinische Fachdisziplin in den letzten Jahren stetig weiterentwickelt. Seit 2016 gab es seitens der Deutschen Gesellschaft für Infektiologie (DGI), der Deutschen Gesellschaft für Innere Medizin (DGIM) und dem Bundesverband Deutscher Internisten (BDI) intensivierte Bestrebungen, die Position der Infektiolog*innen im deutschen Gesundheitssystem zu stärken und deren Ausbildung zu verbessern [REF]. Mit der im Mai 2021 auf dem Deutschen Ärztetag beschlossenen Einführung eines Facharztes „Innere Medizin und Infektiologie“ soll perspektivisch sichergestellt werden, dass die Ausbildung der Komplexität des Faches gerecht und damit die Versorgungssituation für Patient*innen mit einem infektiologischen Krankheitsbild langfristig verbessert wird [REF]. Es gilt nun, die rasche Umsetzung des Facharztes und darüber hinaus die parallele Etablierung infektiologischer Fachabteilungen zu ermöglichen. Dies ist insbesondere vor dem Hintergrund, dass klare wissenschaftliche Evidenz für eine Verbesserung der Behandlungsergebnisse z.B. bei Patient*innen mit schweren systemischen Infektionen durch die spezifische Behandlung durch Infektiolog*innen besteht, relevant [REF]. Auch weitere positive Auswirkungen infektiologischer Fachexpertise auf den rationalen Einsatz von Antibiotika (sog. Antibiotic Stewardship, kurz: ABS) und die Verbesserung der Behandlungsqualität sind belegt und in der deutsch-österreichischen S3-Leitlinie „Strategien zur Sicherung rationaler Antibiotika-Anwendung im Krankenhaus“ berücksichtigt [REF].Die aktuelle Pandemie durch das Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) hat die Bedeutung von Infektionskrankheiten stärker in das öffentliche Bewusstsein gerückt und gleichzeitig den Mangel an klinischen Infektiolog*innen in Deutschland verdeutlicht [REF]. Ein Blick über die Grenzen, zum Beispiel in die Schweiz, nach Schweden oder in die USA zeigt, dass die Verfügbarkeit eines Facharztes für Infektiologie einerseits Voraussetzung für ABS-Programme (USA) und eigenständige Fachabteilungen (Schweden, USA) ist, andererseits diese Fachärzte zusätzlich für eine patientennahe, klinische Krankenhaushygiene verantwortlich sind (Schweiz) [REF]. Neben einem Programm zur vermehrten Weiterbildung von Infektiolog*innen ist es in diesem Zusammenhang wichtig, klinisch-infektiologische Versorgungsstrukturen in Deutschland zu etablieren und bestehende auszubauen, um auch zukünftigen Herausforderungen in der Infektionsmedizin adäquat zu begegnen. Im Zusammenhang mit den Auswirkungen des Klimawandels ist zu vermuten, dass es zu einer Zunahme vektorübertragener Infektionskrankheiten kommt oder sich neuartige etablieren [REF].Für die Verbesserung infektiologischer Versorgungsstrukturen ist es essenziell, die Ausgangssituation zu kennen. Um die Differenz zwischen vorhandener und notwendiger Versorgungsqualität abzubilden, sollten geeignete Qualitätsindikatoren zur Erhebung verwendet werden. Qualitätsindikatoren gelten international als Goldstandard in der Qualitätssicherung und werden nach Donabedian in Struktur-, Prozess- und Ergebnisindikatoren eingeteilt [REF]. Die strukturierte Bewertung von Qualitätsindikatoren erfolgt nach Wissenschaftlichkeit, Relevanz und Praktikabilität. Das Institut für Qualitätssicherung und Transparenz im Gesundheitswesen (IQTIG) diskutiert für unterschiedliche Qualitätsmerkmale erstmals die unmittelbare und mittelbare Patient*innenrelevanz und trennt bei der Beurteilung zwischen Patient*innenrelevanz und Verbesserungspotenzial. Das IQTIG begründet dies damit, dass für die Feststellung eines Verbesserungspotenzials nicht die gesamte betrachtete Population gleichmäßig betroffen sein muss, sondern dass ein Verbesserungspotenzial auch dann vorliegen kann, wenn einzelne Subgruppen besonders betroffen sind [REF]. Aus diesem Grund haben wir die Relevanz klinisch-infektiologischer Expertise für die Versorgung von Patient*innen, die an der Coronavirus-Krankheit-2019 (COVID-19) erkrankt sind, als eine Subgruppe infektiologisch erkrankter Patient*innen in dieser Arbeit gesondert untersucht.
Baumaßnahmen im Betrieb eines Krankenhauses haben meist einen indirekten oder direkten Einfluss auf die Krankenhausfunktionalität, ggf. seine Kapazität und auch auf medizinische Behandlungsprozesse [REF]. Nicht nur die Implementierung neuester Technik, sondern auch die Instandhaltung und Erweiterung der Betriebstechnik sind wichtige Aspekte in der Fortentwicklung eines Krankenhauses [REF].Im Rahmen von Erweiterungsbaumaßnahmen am Uniklinikum Dresden war eine vorübergehende Überbrückung der medizinischen Gasversorgung in Teilen des chirurgischen Zentrums notwendig. Daten zur Bemessung der Vorhaltungen für einen derartigen Eingriff sind bislang nicht publiziert. Die Umbaumaßnahme erfolgte im Gebäudeteil 4 (Abb. 1, Kreis). Grundsätzlich wird das Gebäude an 2 Stellen über eine Ringleitung redundant eingespeist (Abb. 1, Ring EG). Die direkte Verbindung der Ringleitungseinspeisungen konnte nur zwischen den Gebäudeteilen 1 und 2 getrennt werden (Abb. 1, Markierung A0). Die Unterbrechung und Einspeisung ist aber auf der Stationsebene bzw. für den OP-Bereich selektiv möglich (Abb. 1, Markierungen A1–A8). Das abtrennbare Gebäudeteil 1 konnte während der Maßnahme weiter aus der Ringleitung gespeist werden. Für die Gebäudeteile 2–4, in denen 3 intensivmedizinische Teilstationen und 6 Normalstationen mit je 28 Betten untergebracht waren, bestand der Bedarf einer interimistischen dezentralen Gasversorgung. Aufgrund der im Wochenendbetrieb verfügbaren OP-Kapazitäten wurde der ebenfalls betroffene OP-Trakt für die Dauer der Umschlussmaßnahme außer Betrieb genommen. Selbst unter der Voraussetzung, dass die Patientensicherheit zu 100 % gewährlistet werden sein muss, ist eine mit 4 h veranschlagte Baumaßnahme, auch in einem medizinischen Hochrisikobereich, keine Rechtfertigung für eine komplette Unterbrechung des chirurgischen Krankenhausbetriebs; dies gilt insbesondere in der komplexen Versorgung an einem Universitätsklinikum. Ein komplettes Freiziehen aller betroffenen Bettplätze und die sukzessive Wiederbelegung nach der Maßnahme hätten eine überregional relevante Einbuße von spezialisierter chirurgischer Behandlungskapazität über mehrere Tage bedeutet [REF]. Daneben müssen Nutzen und Risiken einer kompletten Patientenverlagerung, insbesondere von High-Care-Patienten in andere Klinikteile oder gar nach extern, genau erwogen werden [REF]. Im Sinne des konsequenzbasierten Modells der Krankenhausalarm- und Krankenhauseinsatzplanung nach Wurmb [REF] handelte es sich im vorliegenden Szenario um einen Eingriff in die Krankenhausbetriebstechnik mit potenziellen Folgen für die Funktionalität der Klinik und in der Konsequenz ihrer Kapazität. Eine ausreichende Vorbereitungszeit ohne Bestehen einer Eigengefährdung war gegeben.Kommunikationsmängel sind die Hauptursache für Fehler in medizinischen Hochrisikobereichen [REF]. Entsprechend wurde zur Risikoabschätzung und Projektabstimmung der Beauftragte für das Notfallmanagement und den Katastrophenschutz des Uniklinikums involviert [REF]; dieser stellte in seinen weiteren Rollen als Sprecher der OP-Steuergruppe und leitender Oberarzt in der Anästhesie eine interprofessionelle Projektgruppe aus Technikern, Klinikern, Pflegeleitungen sowie dem medizinischen Qualitäts- und Risikomanagement zusammen.Ziel war, eine Umsetzungsmöglichkeit mit Nullfehlertoleranz zu schaffen [REF]. Dazu gehört sowohl eine Bedarfsabschätzung als auch eine Fehlermöglichkeitsanalyse [REF], um mögliche Gefahrenquellen im Vorfeld erkennen sowie den Prozess für alle Beteiligten verständlich und sicher planen zu können. Ein wichtiger Erfolgsfaktor ist die direkte Einbindung aller am Prozess beteiligten Fachdisziplinen und Berufsgruppen [REF]. Höchste Priorität in der Vorbereitung der Maßnahme hatte deshalb die enge Abstimmung zwischen den medizinischen Abläufen sowie der technischen und personellen Absicherung in der Durchführung. Mithilfe der Error-Risk-Analyse sollten mögliche Fehlerquellen im Vorfeld aufgedeckt werden [REF].
In den vergangenen Jahren hat das noch relativ junge Fach Public Mental Health in der internationalen Forschung zunehmend an Bedeutung gewonnen. Derzeitige Konzeptualisierungen der Public Mental Health im internationalen Raum fokussieren auf Indikatoren, Determinanten, Förderung bzw. Schutz der psychischen Gesundheit, Verbesserung der Gesundheitskompetenz, die Reduktion von Stigmata sowie auf Prävention von psychischen Störungen und deren Versorgung [REF]. Häufig standen dabei bislang entweder die Anwendung von vorwiegend an somatischen Erkrankungen entwickelten Ansätzen der Public Health auf den Bereich der psychischen Gesundheit und Erkrankung im Vordergrund [REF] oder deren populationsbasierte, epidemiologische oder systemische Betrachtung aus psychiatrischer Perspektive [REF]. Ein Grund dafür, dass die Public Mental Health lange Zeit nicht per se im Mittelpunkt der Betrachtung stand, dürfte darin liegen, dass im Fach Public Health über viele Jahre somatische Erkrankungen im Fokus der Aufmerksamkeit standen [REF], während sich die Psychiatrie – wie Geoffrey Rose bereits in den 1990er-Jahren feststellte – nicht der Existenz und Wichtigkeit des Gesundheitszustandes von ganzen Populationen bewusst zu sein schien, sondern sich vielmehr, ihrer primären Aufgabe folgend, vorwiegend mit der Subpopulation von Menschen mit diagnostizierter psychischer Erkrankung befasste [REF].Dabei wird schnell deutlich, dass es eine besondere Herausforderung der Public Mental Health ist, sie in ihrer Interdisziplinarität von Public Health, psychiatrischer Epidemiologie, psychischer Gesundheitsförderung, Prävention und Versorgungsforschung über das gesamte Spektrum von psychischer Gesundheit und Erkrankung hinweg abzubilden, um ihr innovatives Potenzial vollständig auszuschöpfen [REF]. Diese Interdisziplinarität ist der Public Mental Health insofern inhärent, als dass sie sich mit dem gesamten – kontinuierlich verteilten – Spektrum von psychischer Gesundheit und Erkrankung befasst. Sie muss ein zentraler Gegenstand von Wissenschaft und Praxis der Public Mental Health sein, um eine entscheidende Rolle für die wissenschaftliche Untersuchung und evidenzbasierte Verbesserung der psychischen Gesundheit für die gesamte Bevölkerung spielen zu können [REF]. Obwohl auch im internationalen Raum Psychiatrie und Public Health zuweilen noch in ihren disziplinären Perspektiven verharren (wie bspw. im Vereinigten Königreich in der Debatte um den Jahresbericht des Chief Medical Officer 2013 „Public Mental Health Priorities: Investing in the Evidence“ deutlich wurde [REF]) und die Beantwortung fundamentaler konzeptioneller und methodischer Probleme noch aussteht [REF], so hat die Public Mental Health in den letzten Jahren dennoch an Momentum gewonnen. Die Gründe dafür sind vielfältig: Gesellschaftlich hat das Thema der psychischen Gesundheit – zuletzt katalysiert durch die psychosozialen Folgen der COVID-19-Pandemie [REF] – mehr Aufmerksamkeit erfahren [REF], auf politischer Ebene haben Weltgesundheitsorganisation (WHO) und Regierungen [REF] erkannt, dass Forschung und Praxis der Public Mental Health gestärkt werden müssen, und in der internationalen Wissenschaftsgemeinde ist Inter- und Transdisziplinarität nicht mehr die Ausnahme, sondern die Regel.Der vorliegende Beitrag gibt einen Überblick über derzeitige Konzeptualisierungen von Indikatoren und Determinanten der Public Mental Health sowie von populationsbasierten Interventionsstrategien aus internationaler Perspektive. Aktuelle konzeptionelle und methodische Herausforderungen von Hochrisikostrategien, Populationsstrategien und dem vulnerablen Populationsansatz werden dargestellt und kritisch beleuchtet.
Das SARS-Coronavirus Typ 2 („severe acute respiratory syndrome coronavirus type 2“ [SARS-CoV-2]) ist für die Pandemie mit der Coronaviruskrankheit 2019 (COVID-19) verantwortlich, die zu globalen Krisen mit hohem Ressourcenverbrauch in Gesundheitssystemen geführt hat. COVID-19 (ICD U07.1) ist heute als Multiorgankrankheit mit einem breiten Spektrum von Manifestationen anerkannt. Ähnlich wie bei anderen Infektionskrankheiten [REF] gibt es nach einer akuten COVID-Infektion Patient*innen mit anhaltenden Beschwerden, die ab einer Zeitspanne von vier Wochen nach Infektion als Long-COVID oder postakute Folgen von COVID („post-acute sequelae of COVID“) und bei Persistenz von mehr als zwölf Wochen als Post-COVID-Syndrom bezeichnet werden [REF]. Die Weltgesundheitsorganisation WHO definierte im Oktober 2021 das Post-COVID-Syndrom als Symptomkomplex, der drei Monate nach einer SARS-CoV-2-Infektion auftritt und mindestens zwei Monate anhält. Die bestehenden Symptome können nicht durch alternative Diagnosen oder Vorerkrankungen erklärt werden. Zu den typischen Symptomen werden Fatigue, Dyspnoe und kognitive Dysfunktionen sowie weitere Einschränkungen im Bereich der alltäglichen Funktionen gerechnet. Die Symptome können über die Zeit in ihrer Stärke konfluieren, spontan remittieren oder auch neu hinzukommen [REF].Die aktualisierte S1-Leitlinie „Long/Post-COVID“ (Stand 22.08.2022) ist ein klinisch-praktischer Leitfaden, der bei Long/Post-COVID-begründenden Symptomen eine diagnostisch-therapeutische Orientierung auf dem Boden einer noch begrenzten Datenlage liefern soll [REF]. In dieser Leitlinie wird insbesondere dem klinischen Versorgungsweg Rechnung getragen. Zeitnahe Aktualisierungen sollen bei Zunahme der Evidenz durchgeführt werden. Die Verantwortlichkeiten für die einzelnen fachspezifischen Abschnitte sowie deren Aktualisierungen lagen jeweils bei der entsprechenden wissenschaftlichen Fachgesellschaft und deren Vertretern. Die Deutsche Schmerzgesellschaft war für das Kapitel „Schmerzen“ zuständig.In dieser Zeitschrift wurden und werden Leitlinien der Arbeitsgemeinschaft der Wissenschaftlich Medizinischen Fachgesellschaften dargestellt, an denen die Deutsche Schmerzgesellschaft beteiligt war. Federführend koordiniert durch die Deutsche Schmerzgesellschaft wurden die S3-Leitlinien Fibromyalgiesyndrom [REF] und Langzeitanwendung von Opioiden bei chronischen nicht-tumorbedingten Schmerzen (LONTS) [REF] und im Volltext publiziert. Leitlinien, an denen die Deutsche Schmerzgesellschaft mitgearbeitet hat, wurden in Auszügen bezüglich ihrer Relevanz für die Schmerzmedizin dargestellt, z. B. die S2-Leitlinie zur Endometriose [REF]. Im Folgenden werden für die Schmerzmedizin relevante Aspekte der aktualisierten S1-Leitlinie Long/Post-COVID [REF] dargestellt und diskutiert.Die Leitlinie wurde von der Deutschen Gesellschaft für Pneumologie angemeldet und koordiniert. 21 Fachgesellschaften, zwei Berufsverbände, zwei Zentren, ein Institut und eine gesetzliche Unfallversicherung nahmen an der Erarbeitung der Kapitel und dem mehrstufigen Konsensusprozess (drei Videokonferenzen; Onlinedokument, das von jedem Teilnehmer der Leitliniengruppe kommentiert werden konnte) teil (Tab. 1).Im Rahmen der AWMF-Task-Force-COVID-Leitlinien erfolgte die methodische Unterstützung durch M. Nothacker, AWMF. Der fertiggestellte Leitlinienentwurf wurde von der AWMF Task Force kommentiert und im Anschluss final von allen beteiligten Fachgesellschaften und Organisationen verabschiedet [REF].Im folgenden Abschnitt wird das von der Deutschen Schmerzgesellschaft erstellte Kapitel nach redaktionellen Kürzungen dargestellt. Der Leitlinientext ist fett, die redaktionell gestrichenen Passagen in Normalschrift dargestellt [REF].
Geht es um die Potenziale von digitaler Unterstützung im Kontext der Pflege, so liegt der Fokus entweder auf Aspekten der betrieblichen Verwaltung, Dokumentation und Organisation von Arbeitsprozessen (sowohl als Praktiken zwischen Menschen als auch technische Unterstützung nicht-menschenbezogener Unterstützungshandlungen) in Pflegeeinrichtungen [REF] oder aber auf dem direkten Pflegegeschehen (z. B. Telepflege oder Pflegerobotik) und damit auf der Interaktion von Pflegefachkräften bzw. pflegenden Angehörigen einerseits und den zu Pflegenden andererseits. Die treibende Annahme ist dabei, dass Pflegende – seien es professionelle Pflegefachkräfte oder pflegende Angehörige – mittels Technologie in ihren Pflegehandlungen unterstützt, entlastet, angeleitet oder gemonitort werden, so dass die pflegerische Versorgung sichergestellt ist. Trotz der zahlreichen Innovationen und sowohl politischen wie auch organisationalen Bemühungen, die Nutzung digitaler Technologien in der Pflege voranzubringen, ist dieser Bereich weiterhin ausbaufähig [REF]. In diesem Zusammenhang wurde bereits vielfach die sozialethische Frage diskutiert, inwieweit Institutionen der Altenhilfe und -pflege im Spannungsfeld zwischen betrieblicher Organisation und sozialem Auftrag Digitalisierung nutzen dürfen, können oder gar sollen, um Klient:innen in ihrer Lebenswelt zu unterstützen [REF]. Auch Fragen der Technikakzeptanz und -kompetenz sowohl aufseiten der Pflegenden [REF] als auch älterer Menschen bzw. der zu Pflegenden [REF] standen im Blickpunkt der Betrachtung.Im Rahmen dieses Diskussionspapiers wollen wir nun einen Perspektivwechsel wagen, welcher zumindest schon teilweise Anklang in der aktuellen Diskussion in Forschung und Praxis gefunden hat: Dabei geht es um die Frage, inwieweit digitale Angebote zur Prävention und Gesundheitsförderung für professionelle Pflegefachkräfte in stationären Settings ein Mittel sein können, um die Akzeptanz sowie die positive Assoziation mit Technik und Digitalität zu fördern, indem es gerade nicht um dyadische Pflegesituationen, sondern um die eigene Gesundheit geht. Dahinter steht die Annahme, dass Digitalisierung mehr als eine Intervention auf Verhaltensebene ist (Bsp.: App zur Bewegungsförderung), sondern es gezielt einer Verhältnisorientierung (sprich einer Veränderung der den Menschen umgebenden Faktoren) bedarf. In einem lebenswelt- bzw. settingorientierten Ansatz werden Maßnahmen der Prävention und Gesundheitsförderung sowohl auf das Verhalten als auch die Verhältnisse ausgerichtet [REF]. In diesem Zusammenhang können digitale Applikationen zur Entwicklung gesundheitsförderlicher Lebens- und Arbeitsbedingungen beitragen [REF] und darin – quasi sekundär – auch die Akzeptanz gegenüber anderen digitalen Angeboten und Anwendungen verbessern und die Aneignung digitaler Kompetenzen bzw. die Motivation zum Kompetenzaufbau befördern [REF]. Zudem können Pflegefachkräfte in ihrer Rolle als Multiplikator:innen einen starken Einfluss auf die kompetente Nutzung digitaler Technologien bei Pflegebedürftigen haben. Zentral hierfür sind eine positive Einstellung gegenüber digitalen Technologien und deren aktive Nutzung. Entsprechend gilt es zu untersuchen, inwieweit Gesundheitsförderung hier nicht nur ein Auftrag mit Blick auf die zu Pflegenden, sondern auch auf das Personal selbst ist.Hierzu bedarf es einer systemisch-organisationalen Perspektive, in welcher die Expertise aus verschiedenen Disziplinen – insbesondere Public Health und Pflegewissenschaft – zusammengebracht wird. Im Folgenden wird dies am Beispiel der Digitalisierung und Gesundheitsförderung und Prävention in der stationären Langzeitversorgung aufgezeigt. Dabei wird die Notwendigkeit zur Etablierung gesundheitsfördernder Einrichtungen in der Langzeitpflege hervorgehoben, in welchen Gesundheitsförderung durch Digitalisierung als organisationale Aufgabe mit einer bedarfssensiblen und diversitätsorientierten Ausrichtung verstanden wird.
Allergische Erkrankungen sind Überempfindlichkeitsreaktionen des Immunsystems und betreffen bis zu 30 % der Bevölkerung industrialisierter Länder. Sie stellen neben dem individuellen Leiden und Verlust an Lebensqualität eine hohe sozioökonomische Belastung dar [REF]. Häufig werden Allergien durch Immunglobulin E (IgE) vermittelt [REF]. Sie weisen unterschiedliche klinische Manifestationen auf, wie beispielsweise allergische Rhinokonjunktivitis, atopische Dermatitis, Urtikaria, Angioödem, allergisches Asthma und systemische allergische Reaktionen bis hin zum anaphylaktischen Schock [REF]. Weitverbreitete Auslöser dieser Allergien vom IgE-vermittelten Soforttyp („Typ I“) sind Baum‑, Gräser- und Kräuterpollen, Tierepithelien, Insektengifte, Hausstaub- und Vorratsmilben, Schimmelpilze, Nahrungsmittel und Arzneimittel. Darüber hinaus sind zahlreiche Typ-I-Allergene von beruflicher Bedeutung (z. B. Mehlstäube, Holzstäube, Enzyme, Naturgummilatex; [REF]). Die molekularen Auslöser sind überwiegend Proteine (Allergene), die nur bei bereits sensibilisierten Personen nach Haut- und Schleimhautkontakt, Inhalation, Injektion oder Ingestion allergische Reaktionen auslösen können. Auftretende Symptome können medikamentös supprimiert werden; diese symptomatischen Therapieansätze modulieren die Grundkrankheit nicht. Dagegen bewirkt die allergenspezifische Immuntherapie (AIT), als einzige kausale Therapieform, durch Modulation des Immunsystems eine allergenspezifische Toleranzentwicklung und infolgedessen Linderung der Symptome und damit einhergehend Reduktion der Medikation. Langfristig kann auch eine Krankheitsprogression, z. B. der sogenannte Etagenwechsel von der allergischen Rhinokonjunktivitis hin zum allergischen Asthma, verhindert werden [REF]. Verschiedene komplexe Mechanismen der AIT werden je nach Therapieansatz beschrieben (s. unten; [REF]).Die derzeit marktfähigen, in der klinischen Versorgung routinemäßig eingesetzten AIT-Präparate werden weltweit ausschließlich aus natürlichen Allergenquellen, im Wesentlichen durch Extraktion des Gesamtproteinanteils einschließlich der therapiewirksamen allergenen Proteine gewonnen. Aufgrund des natürlichen Ausgangsmaterials unterliegen diese AIT-Präparationen Schwankungen in ihrer Komposition (s. unten). Hohe Ansprüche an die Standardisierung sind daher notwendig, um eine angemessene und konsistente Chargenqualität der Allergenprodukte zu gewährleisten [REF].Therapierelevante Allergene können auch biotechnologisch oder chemisch-synthetisch in hoher definierter Qualität hergestellt werden. Dies wurde durch die Identifizierung der einzelnen Allergenkomponenten und die Aufklärung ihrer codierenden Desoxyribonukleinsäuresequenzen (komplementäre DNA, cDNA) möglich. Seit der biotechnologischen Herstellung der ersten rekombinanten Allergene Ende der 1980er-Jahre wurden bislang etwa 1000 Allergenkomponenten entschlüsselt und mit einheitlicher Nomenklatur registriert [REF]. Die grundlegende Kenntnis dieser allergenen Strukturen und die Verfügbarkeit von rekombinanten Allergenen mittels Gentechnologie ermöglichte Verbesserungen der Standardisierung von traditionellen Allergenextrakten [REF], Verbesserungen der In-vitro-Diagnostik im Sinne einer Component Resolved Diagnosis (bezogen auf klinische Situation und Risiko schwerer Reaktionen), welche die extraktbasierte Diagnostik ergänzt [REF], die Identifizierung therapierelevanter Allergene (sowohl für molekulare Therapieansätze als auch für extraktbasierte Produkte), auch im Sinne einer personalisierten Medizin, sowie die gentechnische Modifikation der Allergene (hypoallergene Strukturen, Hybridstrukturen, Design von Carrier-Systemen) mit optimierten allergenen und immunogenen Eigenschaften. Trotz einer Vielzahl klinischer Studien zum Konzeptnachweis stehen rekombinante oder chemisch-synthetische Allergenpräparationen bisher nicht als zugelassene Allergentherapeutika zur Verfügung (s. Beitrag von Mahler et al. in diesem Themenheft). Im Folgenden werden diese Ansätze näher erläutert und der potenzielle Mehrwert sowie die Herausforderungen bei diesen innovativen Allergietherapeutika im Vergleich zu den marktüblichen extraktbasierten AIT-Präparationen diskutiert.
Migräne ist eine primäre Kopfschmerzerkrankung von hoher Prävalenz. Weltweit sind 13,8 % der Frauen und 6,9 % der Männer betroffen [REF]. Die Erkrankung kann zu hohen Einschränkungen der Lebensqualität führen und ist unter den Top Ten der Ursachen für Beeinträchtigung [REF]. Kennzeichen der Migräne sind wiederkehrende Kopfschmerzattacken von mittlerer bis hoher Intensität mit einer Dauer von 4 bis 72 h [REF]. Begleitsymptome der Attacke sind Übelkeit sowie Licht- und Geräuschempfindlichkeit. Bei etwa 30 % der Migränebetroffenen treten vor der Kopfschmerzphase Aurasymptome (neurologische Reiz- oder Ausfallerscheinungen) auf [REF].Die Entstehung der Krankheit ist multifaktoriell, wie bei anderen Schmerzerkrankungen auch kann ein biopsychosoziales Störungsmodell zugrunde gelegt werden [REF]. Bei den psychischen Faktoren, die den Krankheitsverlauf beeinflussen, spielt neben Stress die Emotion Angst eine wichtige Rolle. Angst ist in vielerlei Hinsicht mit Migräne assoziiert. So konnte gezeigt werden, dass Migränebetroffene im Vergleich zu Personen ohne Migräne mehr Ängste berichten [REF] und häufiger an Angststörungen leiden [REF]. Eine höhere Kopfschmerzaktivität ist mit mehr Angstsymptomen assoziiert [REF]. Die Emotion Angst wiederum kann direkt zur Auslösung von Migräneattacken beitragen [REF]. Außerdem kann Angst dazu führen, dass durchgeführte Behandlungen weniger wirksam sind [REF]. Angst kann sich also ungünstig auf den Krankheits- und Behandlungsverlauf auswirken und somit zur Chronifizierung der Migräneerkrankung beitragen. Eine theoretische Grundlage hierfür bildet das Fear-avoidance-Modell (z. B. [REF]), welches für primäre Kopfschmerzerkrankungen adaptiert wurde („trigger avoidance model of headaches“ [TAMH]; [REF]; Abb. 1). Eine Erweiterung des Fear-avoidance-Modells stellt das Avoidance-endurance-Modell dar, welches zusätzlich zu angstassoziiertem Vermeidungsverhalten auch dysfunktionales Durchhalteverhalten integriert und für das Krankheitsbild Migräne anwendbar ist [REF]. Neben einer erhöhten Komorbidität von klar definierten Angststörungen (z. B. Panikstörung [REF]) und dem verstärkten Auftreten von allgemeinen Angstsymptomen [REF] gibt es eine Reihe von sehr migränespezifischen Ängsten. Eine häufige Befürchtung bei Migränebetroffenen ist die antizipatorische Angst vor dem Auftreten einer Kopfschmerzattacke (sogenannte „Attackenangst“). In einer internationalen Studie mit Migränebetroffenen gaben 55 % der Befragten an, Angst vor einer auftretenden Kopfschmerzattacke zu haben [REF]. Weitere spezifische Ängste im Kontext einer Migräneerkrankung können die Angst vor der Aura oder dem Erbrechen (Emetophobie) sein. Die Angst vor Kopfschmerzattacken kann bei Personen mit Migräne dazu führen, dass bereits im Vorfeld von möglichen Attacken („präventiv“) Schmerzmittel (Triptane, Analgetika) eingenommen werden. Durch eine zu häufige Präventiveinnahme kann sich ein langfristig übermäßiger Schmerzmittelkonsum etablieren, der zur Entstehung eines Kopfschmerzes durch Medikamentenübergebrauch führen und dadurch auch indirekt zur Erhöhung der Kopfschmerzaktivität beitragen kann [REF]. Kopfschmerzaktivität und (Attacken‑)Angst können sich bei Migräne also wechselseitig „hochschaukeln“ (Abb. 2). Während der COVID-19-Pandemie kam es tendenziell zu einer Zunahme von Ängsten bei Migränebetroffenen (z. B. [REF]), wenngleich sich durchaus auch positive Effekte im Sinne einer reduzierten Kopfschmerzaktivität (z. B. durch geringere Stressbelastung, Abnahme von Reizdichte) beobachten ließen [REF]. Angesichts der insgesamt hohen Prävalenz von Ängsten und deren ungünstigen Einflusses auf den Krankheitsverlauf ist es sinnvoll, Angstsymptome möglichst früh gezielt zu erfassen und zu behandeln.
Die weltweite COVID-19-Pandemie, ausgelöst durch das Severe Acute Respiratory Syndrome Coronavirus Type 2 (SARS-CoV-2), verursacht seit dem Frühjahr 2020 erhebliche gesundheitliche, gesundheitspolitische und volkswirtschaftliche Belastungen. Es wurde schnell erkannt, dass es sich nicht nur um eine schwere Atemwegsinfektion handelt. Vielmehr führt das Virus mit seinen multiplen Varianten zu teils ausgeprägten Multiorganerkrankungen mit sehr facettenreichen klinischen Manifestationen, Symptomen und Verläufen ([REF]; Abb. 1). Hinsichtlich der postviralen Folgen gibt es auch andere Viruserkrankungen, die mit zeitlicher Verzögerung Einschränkungen wie beispielsweise Fatigue, wie beim Epstein-Barr- oder Ebola-Virus bekannt, verursachen. Diese gehen mit protrahierten, lang dauernden klinischen Symptomen und subjektiven Beschwerden nach der akuten Infektionsphase bzw. auch erst Wochen bis Monate nach der primären Erkrankung einher. Infolge von COVID-19 weisen ca. 10–15 % der Erkrankten eine schwere Krankheitssymptomatik auf; von diesen entwickelt wiederum etwa ein Drittel lebensbedrohliche Komplikationen.Obwohl die meisten der Betroffenen rasch vollständig genesen, verbleibt bei einem prozentual ähnlich großen Kollektiv eine anhaltende und umfangreiche Beschwerdesymptomatik, die unter dem Begriff des „Long-COVID“- bzw. „Post-COVID“-Syndroms subsumiert wird. Die genauen Ursachen für Long/Post-COVID sind bislang nicht bekannt und Gegenstand weltweiter Forschung [REF].Die Experten der AWMF bezeichnen ab einem Zeitraum von 4 Wochen nach einer Infektion in der „S1-Leitlinie Long/Post-COVID“ (Stand 17.08.2022) Long-COVID (LCS) oder postakute Folgen von COVID-19 („post-acute sequelae of COVID-19“), und mit einer Persistenz über 12 Wochen hinaus als PCS. Dies wird in Abb. 2 dargestellt. Zwischen einem Long-COVID- und einem Post-COVID-Syndrom besteht in der Begrifflichkeit ein fließender Übergang, d. h., dass Post-COVID als über 3 Monate bestehende Problematik unter dem Dachbegriff Long COVID zu verstehen ist. Häufig findet sich eine synonyme Begriffsverwendung, besonders umgangssprachlich.Als über 3 Monate bestehende Problematik ist Post-COVID unter den Dachbegriff Long COVID einzuordnenSomatische oder psychosomatische Vorerkrankungen in der Anamnese, aber auch hohe psychosoziale Belastungen begünstigen die Manifestation eines PCS. Die Betroffenen lassen sich grob in 4 Kategorien unterteilen, in diejenigen, die nach einer notwendigen intensivmedizinischen Behandlung an einem „post-intensive care syndrome“ (PICS) erkranken [REF]; erst mit zeitlicher Latenz an teils auch kombinierten Symptomen wie z. B. kardiovaskulären/-pulmonalen Beschwerden, neurokognitiven Beeinträchtigungen bzw. einer posttraumatischen Belastungsstörung leiden, wegen einer fortdauernden Erschöpfungssymptomatik und Belastungsminderung mit oder ohne Dyspnoe in ihrer Teilhabe am Privat- und am Berufsleben nachhaltig beeinträchtigt sind, von residualen Beschwerden berichten, die sie in der Lebensführung nicht wesentlich beeinträchtigen. In der Wertung der Erkrankung bestehen die Herausforderung und Schwierigkeit zugleich, zwischen SARS-CoV-2-bedingten, unmittelbar auftretenden somatischen und psychischen Störungen gegenüber Verstärkungen von Vorerkrankungen sowie pandemiebedingten psychosozialen Belastungsfolgen zu differenzieren. Die zunehmende Erfahrung der Behandler und die Erkenntnisse der weltweit umfangreichen Forschung zu Post-COVID bieten die einzigartige Möglichkeit, sowohl exemplarisch die Langzeitfolgen biopsychosozial umfangreich zu erfassen als auch Behandlungsstrategien zu entwickeln und Begutachtungskriterien festzulegen [REF].
In Zeiten großer Erschöpfung durch eine Pandemie, eines Krieges in Europa und zunehmender Folgen des Klimawandels ist die Bedeutung einer Ethik der Pflegeberufe relevanter denn je. Während der COVID-19-Pandemie sind zahlreiche vernachlässigte Themen der Sozial- und Gesundheitspolitik der letzten 20 Jahre zutage getreten, so auch die Situation der Pflege in Deutschland. Die Bedeutung einer Ethik der Fürsorglichkeit und Menschenwürde in der stationären Altenpflege wurde eindrücklich vor Augen geführt [REF]. Pflege als ein Bereich des breiten Ensembles der Gesundheitswissenschaften legt das Interesse auf individuelle Gesundheits- und Krankheitsverläufe sowie die Ableitung von setting- und akteursspezifischen Versorgungsstrukturen, die den persönlichen Bedürfnissen und aktuellen Bedarfen Rechnung tragen. Die jeweils individuellen Pflegemaßnahmen werden hierbei mit dem pflegebedürftigen Menschen situations- und lebensweltbezogen abgewogen, an seinen Werten ausgerichtet und in einem respektvollen Aushandlungsprozess festgelegt. In diesem Zusammenhang umfasst die professionelle Pflege stets ein Beziehungs- und Interaktionsgeschehen als ein grundlegendes Merkmal professioneller Pflege [REF]. Es geht folglich im Kern um Pflege- und Sorgebeziehungen [REF], aber auch um Verantwortungsbeziehungen, die seitens der Pflegefachpersonen in den Mittelpunkt des Entscheidens und Handelns rücken.Care-Ethik ist ein Ansatz der Pflegeethik, der Sorge als Anteilnahme stärkt. Darüber hinaus geht es um ein Dem-anderen-zugewandt-Sein, aber auch um Mitgefühl sowie um professionelle Verantwortung in der Versorgung von Menschen.„Care“ enthält in diesem Ansatz auf der Seite der Pflegenden auch das Element der Selbstsorge, die essentiell ist, wenn sich die Bedingungen des Sorgens verbessern und die Strukturen, die die Sorgetätigkeiten prägen, positiv verändern sollen. Care beinhaltet diesem Ansatz entsprechend auch eine achtsame Reflexion potenzieller Machtstrukturen in der Beziehung zwischen Sorgenden und Sorgeempfangenden [REF]. Care als gelebte ethische Praxis fordert demgemäß die Ausgewogenheit fürsorglicher und achtsamer Tätigkeiten im Rahmen der Pflegebeziehung, die Anerkennung von Abhängigkeiten und ein hohes Maß an Verantwortlichkeit [REF]. Erst diese Kombination ermöglicht eine professionelle Pflege und zugewandte Sorgebeziehung, die sich am Gegenüber, das heißt an den persönlichen Werten und höchst individuellen Bedürfnisse wie auch situativen Bedarfe des meist vulnerablen Gegenübers, ausrichtet. Grundlegend für das ethische Handeln der Pflegenden ist der Ethikkodex für Pflegefachpersonen des International Council of Nurses (ICN; [REF]). Dieser Kodex ist Ausdruck von Wertsetzungen einer Profession und stellt nachfolgend einen wichtigen Bezugspunkt dar. Der professionelle Anspruch, diesen umfassenden Anforderungen an professionelles und ethisch begründetes Handeln gerecht zu werden, führt nicht nur in Zeiten der Ungewissheit und Krisen zu Unsicherheit und Kontroversen [REF]. Aufgrund dessen bildet der Ethikkodex in den nachfolgenden Ausführungen einen zentralen Bezugspunkt.In diesem Diskussionsartikel wird zunächst die Bedeutsamkeit einer professionellen Pflegeethik dargelegt. In einem zweiten Schritt werden Rahmenbedingungen und aktuelle Probleme aufgezeigt, die einer umfassenden Umsetzung zentraler Werte in der Altenpflege entgegenstehen. Dabei wird der Schwerpunkt auf die Auswirkungen der prekären Personalsituation gelegt. Hierbei steht die Frage im Mittelpunkt, ob der Anspruch an eine ethisch reflektierte Pflegepraxis unter den aktuellen Arbeitsbedingungen in der stationären Langzeitpflege noch umsetzbar bzw. lebbar ist. Der Beitrag soll angesichts der aktuellen Gegebenheiten zur Diskussion, aber auch zum Nach‑, Neu- und Umdenken bezüglich der Fürsorge sowohl für die pflegebedürftigen Menschen als auch für die Pflegenden in den Einrichtungen der Altenpflege anregen.
Bakterielle Zoonosen sind wechselseitig von Tier zu Mensch und von Mensch zu Tier übertragbare Infektionskrankheiten. Sie können als „klassische Zoonose“ mit obligater Tier-Mensch-Übertragung auftreten oder als sogenannte Spillover-Zoonose, bei der im Anschluss an die Tier-zu-Mensch-Übertragung eine Mensch-zu-Mensch-Übertragung erfolgt. Bei klassischen Zoonosen ist die kontinuierliche Kontrolle im Tierreservoir essenziell für den öffentlichen Gesundheitsschutz. Spillover-Zoonosen können gravierende Auswirkungen auf die öffentliche Gesundheit haben, wenn sie sich leicht von Mensch zu Mensch ausbreiten.Vom Tier ausgehende bakterielle Erreger können beim Menschen schwere, manchmal sogar tödliche Erkrankungen verursachen. In einigen Fällen sind umfangreiche und langwierige Behandlungen notwendig, z. B. beim hämolytisch-urämischen-Syndrom (HUS), das durch enterohämorrhagische Escherichia coli (EHEC) ausgelöst wird. In vielen Fällen wird eine Behandlung durch die vermehrte Bildung von Antibiotikaresistenzen erschwert.Bakterielle Erreger verursachen – als Auslöser von Tierseuchen – in der Landwirtschaft erhebliche ökonomische Schäden und Beeinträchtigungen des Tierwohls. Sie können zu einer erhöhten Sterblichkeit (z. B. Salmonellose, Tularämie), Fehlgeburten (z. B. Q‑Fieber) und Sterilität (z. B. Brucellose) in den Tierbeständen führen. Erhebliche wirtschaftliche Verluste für die landwirtschaftlichen Unternehmen entstehen auch durch Keulung der Bestände oder Handelsbeschränkungen (z. B. bovine Tuberkulose). Die Gesundheit von Gesellschaftstieren (Tiere, von Menschen aus Vergnügen gehalten) wird ebenfalls durch Infektionskrankheiten (z. B. Ornithose bei Vögeln, Leptospirose bei Menschen und Hunden) bedroht. Darüber hinaus können einige bakterielle zoonotische Erreger (z. B. Bacillus anthracis, Francisella tularensis) aufgrund ihrer biologischen Eigenschaften für eine böswillige Ausbringung geeignet sein und zu einer Gefahr werden, wenn sie im Rahmen von Bioterrorismus freigesetzt würden.Zur Sicherstellung der Handlungsfähigkeit der Behörden des Gesundheitsschutzes ist der Labornachweis der meisten zoonotischen Erreger beim Menschen gemäß § 7 Abs. 1 des Infektionsschutzgesetzes (IfSG) dem Gesundheitsamt namentlich meldepflichtig, sofern der Hinweis auf eine akute Infektion besteht. Das Robert Koch-Institut (RKI) erstellt Falldefinitionen für meldepflichtige Infektionskrankheiten zum Zwecke der Infektions-Surveillance [REF]. Darauf basierend erfolgt eine Übermittlung der Informationen über die Landesbehörden an das RKI.Einige bakterielle zoonotische Krankheiten kommen endemisch vor (z. B. Lyme-Borreliose) oder treten sporadisch auf (z. B. Tularämie). Andere Krankheiten können zu Ausbruchsszenarien bei Menschen und Tieren (z. B. Salmonellose, EHEC, Q‑Fieber) führen und stellen eine besondere Herausforderung bei der Ursachenfindung und der Verhinderung von weiteren Erkrankungen dar. Eine der größten durch eine bakterielle Infektionskrankheit ausgelösten Krisen der letzten Jahre war ein durch den Verzehr von rohen Sprossen ausgelöster bakterieller EHEC/HUS-Ausbruch im Jahr 2011 [REF].Dieser Artikel stellt verschiedene bakterielle zoonotische Infektionskrankheiten dar, die exemplarisch das Spektrum der unterschiedlichen Übertragungswege (Lebensmittel, Zeckenstich), Reservoirtiere (landwirtschaftliche Nutztiere, Nagetiere, Gesellschaftstiere) und Problemfelder wie Antibiotikaresistenz abbilden. Ziel ist es, ein besseres Verständnis für die wichtige Arbeit in den öffentlichen Gesundheitsdiensten, den Tiergesundheitsdiensten und der Lebensmittelkontrolle zu vermitteln.
Mit dem Inkrafttreten von § 5c des Infektionsschutzgesetzes (IfSG), des sog. Triagegesetzes, am 14.12.2022 ist eine langwierige, kontroverse Diskussion zum vorläufigen Abschluss gekommen. Mit dem Ergebnis sind Ärzt:innen und Sozialverbände, aber auch Jurist:innen und Ethiker:innen gleichermaßen unzufrieden [REF]. Die Strafrechtslehrerin Hörnle nennt es einen „Schlag ins Gesicht der Behandelnden“ [REF]. Ausgangspunkt war eine Verfassungsklage von Menschen mit Einschränkungen bzw. Behinderungen. Das Bundesverfassungsgericht hatte Ende 2021 vor dem Hintergrund der COVID-19-Pandemie entschieden, dass sich aus Artikel 3 Absatz 3 Satz 2 des Grundgesetzes für den Staat ein Auftrag ergibt, Menschen mit einer Behinderung wirksam vor einer Benachteiligung zu schützen (s. hierzu auch Walther et al. in dieser Ausgabe).Die getroffene Regelung im § 5c IfSG gilt ausschließlich für eine pandemiebedingte Knappheit intensivmedizinischer Behandlungskapazitäten, nicht jedoch für andere Szenarien mit entsprechendem Ressourcenmangel, z. B. Großschadenslagen der Versorgungstufen III und IV [REF], Bündnis- oder Zivilschutzfall. Eine Zuteilungsentscheidung darf nach § 5c IfSG nur unter Berücksichtigung der aktuellen und kurzfristigen Überlebenswahrscheinlichkeit erfolgen. Komorbiditäten dürfen nur dann berücksichtigt werden, wenn sie die aktuelle und kurzfristige Überlebenswahrscheinlichkeit erheblich verringern.Durch den ausdrücklichen Ausschluss des Abbruchs einer bereits begonnenen Behandlung zugunsten neuer Patient:innen mit besseren Erfolgsaussichten (sog. Ex-post-Triage) werden Zuteilungsentscheidungen mit dem Ziel, „möglichst vielen Patienten eine nutzbringende Teilhabe an der medizinischen Versorgung unter Krisenbedingungen zu ermöglichen“ [REF] verhindert. Während die Ex-post-Triage zunächst noch Teil der ersten „Formulierungshilfe“ des Bundesgesundheitsministeriums war, wurde sie bis zur Abstimmung im Bundestag gestrichen [REF]. Sozialverbände hatten vehement gefordert, die Ex-post-Triage aus dem Gesetz zu entfernen, da ihrer Meinung nach Menschen mit Behinderung möglicherweise benachteiligt würden. Es sind 40 Stellungnahmen zum Gesetz unterschiedlichster Interessengruppen öffentlich zugänglich [REF], darunter die des Berufsverbandes Deutscher Anästhesisten und der Deutschen Gesellschaft für Anästhesiologie und Intensivmedizin [REF], der Bundesärztekammer [REF] und der Arbeitsgemeinschaft der Wissenschaftlichen Medizinischen Fachgesellschaften (AWMF, [REF].) Hauptkritikpunkte aus medizinischer Sicht sind das Übergehen wissenschaftlicher Erkenntnisse, mit der Folge einer passiven und damit selbstselektierenden und Outcome-verschlechternden Ressourcenverteilung nach dem „First-come-first-served“-Prinzip [REF]; letztlich auch die fehlende Rechtssicherheit, ja sogar die Kriminalisierung von Ärzten [REF].Weiterhin werden in dem Gesetz ein Mehraugenprinzip, Dokumentationspflichten und Zuständigkeiten sowie Verfahrensabläufe in Krankenhäusern bestimmt, die größtenteils ohnehin in der AWMF-Leitlinie unter Federführung der Deutschen Interdisziplinären Vereinigung Intensivmedizin (DIVI) mit sieben beteiligten Fachgesellschaften enthalten waren [REF]. Diese Leitlinie beschreibt neben der heranzuziehenden Datengrundlage (klinischer Zustand, Patientenwille, Komorbiditäten, Allgemeinzustand, einschl. Gebrechlichkeit, prognostisch relevante Scores) den Ablauf und die Dokumentation von Priorisierungsentscheidungen bei intensivmedizinischer Ressourcenknappheit [REF]. Grundsätzlich werden alle intensivbehandlungsbedürftigen Patient:innen, nicht nur COVID-Patient:innen, gleichermaßen bei der Priorisierung berücksichtigt. Posteriorisierte Patient:innen sind regelmäßig neu zu bewerten, wenn sich ihr Zustand ändert oder sich die Ressourcensituation verbessert [REF].
Die Nutzung aufbereiteten Abwassers zu Bewässerungszwecken im Pflanzenbau ist bereits in vielen Regionen der Welt notwendig. Sie wird im Hinblick auf zunehmende Trockenperioden auch für die Landwirtschaft in Europa diskutiert. Da die Verordnung (EU) 2020/741 des Europäischen Parlaments und des Rates eine solche Bewässerung mit Einschränkungen vorsieht, soll dieser Übersichtsartikel den derzeitigen wissenschaftlichen Kenntnisstand und die sich daraus ergebende Risikobewertung für Deutschland darstellen [REF]. Es wird diskutiert, ob für Rückstände von Antibiotika‑, Metall- und Biozidverbindungen bzw. das Vorhandensein von resistenten Bakterien besondere Vorsichtsmaßnahmen erforderlich sind, die über die im Hinblick auf pathogene Mikroorganismen ohnehin erforderlichen Maßnahmen hinausgehen.Bewässerungswasser gilt als wichtige Quelle für den Eintrag von Antibiotika‑, Metall- und Biozidverunreinigungen sowie von antibiotikaresistenten Bakterien in Nutzpflanzenkulturen [REF]. Es wird ein direkter Zusammenhang zwischen der Wasserqualität und der Kontamination von roh verzehrtem Obst und Gemüse vermutet. Das Vorkommen von Mikroschadstoffen, wie Antibiotikarückständen, und deren Aufnahme in die Pflanze kann sowohl die Zusammensetzung des natürlichen Pflanzenmikrobioms beeinflussen als auch die Etablierung der mit dem aufbereiteten Abwasser eingebrachten Keime. Das Pflanzenmikrobiom wiederum kann einen direkten Einfluss auf das menschliche Darmmikrobiom nehmen [REF].Zum Schutz vor lebensmittelbedingten Erkrankungen durch pathogene Bakterien, Viren und Parasiten durch Rohverzehr von frischem Obst und Gemüse sind entsprechende Anforderungen an die Wasserqualität zu berücksichtigen [REF]. Mit diesem Übersichtsartikel wird gezeigt, dass die Qualität des zur Bewässerung genutzten aufbereiteten Abwassers auch im Hinblick auf die Antibiotikaresistenz-Problematik von großer Bedeutung ist.Einerseits enthält aufbereitetes Abwasser Nährstoffe für das Pflanzenwachstum, andererseits sollten Mikroschadstoffe, die antibiotikaresistente Bakterien selektieren und die Übertragung von Genen zwischen Bakterien (horizontaler Gentransfer) stimulieren, vor der Nutzung von aufbereitetem Abwasser aus dem Bewässerungswasser entfernt werden [REF]. Aufbereitete Abwässer enthalten deutlich geringere Rückstandskonzentrationen wie Antibiotika‑, Metall- und Biozidverbindungen als z. B. Gülle. Allerdings übersteigt bei einer regelmäßigen Bewässerung die Gesamtmenge der aufbereiteten Abwässer die der ausgebrachten Gülle deutlich, so dass es über die Zeit zu einer stärkeren Exposition des Bodens und der Pflanzen gegenüber diesen Substanzen kommen kann. Wesentlich für die Fragestellung dieses Artikels sind einerseits der Gehalt an antimikrobiell wirksamen Substanzen im aufbereiteten Abwasser sowie der Gehalt an resistenten Mikroorganismen und ggf. freier DNA, die Resistenzgene tragen kann. Zweitens ist zu prüfen, in welchem Umfang sowohl die resistenten Mikroorganismen als auch die Rückstände von Arzneimitteln von den Pflanzen aufgenommen werden bzw. auf den Pflanzen verbleiben. Drittens ist zu fragen, in welchem Umfang übliche Maßnahmen, z. B. das Waschen von pflanzlichen Lebensmitteln vor dem Verzehr, zu einem relevanten Rückgang solcher Mikroorganismen und ggf. der Rückstände führen, der über den Effekt der Entfernung von Bodenpartikeln hinausgeht.Dieser Übersichtsartikel strebt keine umfassende Risikobewertung der möglichen Exposition von Verbraucherinnen und Verbrauchern gegenüber resistenten Mikroorganismen, Resistenzgenen und Rückständen von Arzneimitteln durch die Bewässerung von Nutzpflanzen mit aufbereitetem Abwasser an, sondern fokussiert auf mögliche Aspekte, die über die Herausforderungen hinausgehen, die sich im Hinblick auf pathogene Mikroorganismen stellen und an anderer Stelle bewertet worden sind [REF]. Ausgehend von den im geklärten Abwasser vorhandenen Kontaminanten und resistenten Mikroorganismen betrachten wir zunächst deren Effekt auf die bewässerten Pflanzen. Anschließend analysieren wir den Effekt der Verarbeitung der Lebensmittel auf die Konzentrationen von Antibiotikarückständen und Bakterien im verzehrfertigen Lebensmittel.
Um eine sichere und effektive Gesundheitsversorgung zu gewährleisten, sind eine starke ambulante haus- bzw. primärärztliche Grundversorgung der Bevölkerung sowie eine allgemeinmedizinische Forschung in diesem Setting unabdingbar [REF]. Diese Forschung ist dabei kein Selbstzweck, sondern unterstützt die bedarfsgerechte und effektive Versorgung sowie die Patientensicherheit. Klinische Forschung findet bislang meist in (Universitäts‑)Kliniken statt, der überwiegende Teil der Patient*innen wird jedoch ambulant und meist im primärärztlichen Setting behandelt [REF]. Infolgedessen waren und sind in klinischen Studien vorwiegend Betroffene mit einzelnen Krankheitsbildern, kaum Neben- oder Begleitdiagnosen und meist jüngeren Alters (< 65 Jahre) eingeschlossen. Ältere Menschen, Personen mit leichteren oder mit Mehrfacherkrankungen und solche, die sich nicht an die empfohlene Therapie halten, standen bislang selten im Mittelpunkt der Aufmerksamkeit [REF]. Diese Gruppe macht jedoch den Großteil der Patient*innen im Versorgungssystem aus und fast alle gehen regelmäßig bzw. mehrfach im Jahr zu ihrer Hausärztin bzw. ihrem Hausarzt [REF].Ergebnisse aus Forschungsprojekten, die an einer hochselektierten Patientengruppe durchgeführt werden und meist nur eine (schwerwiegende) Erkrankung in den Fokus nehmen, sind nur begrenzt übertragbar auf eine ältere, multimorbide, chronisch erkrankte Population. Da die Allgemeinmedizin die Disziplin ist, in der diese Patientengruppen am häufigsten gesehen werden [REF], kommt ihr eine zentrale Rolle zu. Um den Herausforderungen einer alternden Gesellschaft und dem ambulanten Versorgungsbedarf von zunehmend mehr Patient*innen mit mindestens einer chronischen Erkrankung begegnen zu können, werden wissenschaftlich hochwertige klinische und versorgungsepidemiologische Studien zu allgemeinmedizinischen Fragestellungen benötigt.Im Vergleich zu anderen Ländern fehlen aber in Deutschland regionale und überregionale Studien zu den Themen der Allgemeinmedizin. Zurückzuführen ist dies vor allem darauf, dass die Allgemeinmedizin bei der Durchführung der Studien auf das ambulante Setting angewiesen ist, in dem jedoch die bestehenden Strukturen für die Durchführung klinischer Forschung und Versorgungsforschung noch unzureichend ausgebaut sind. Hier setzt ein allgemeinmedizinisches Forschungspraxennetz (FPN) an, das durch die Etablierung geeigneter Forschungsstrukturen die Realisierung qualitativ hochwertiger Forschungsprojekte ermöglicht.Primary Care Research Networks (PCRN) wurden in den USA [REF] schon vor über 40 Jahren eingerichtet mit dem Ziel, die Rekrutierung und Teilnahme an Primärversorgungsstudien zu erhöhen und die Zusammenarbeit zwischen den ambulanten Versorger*innen und den akademisch Forschenden zu stärken [REF]. Hierzulande wurde die akademische Basis der Allgemeinmedizin in den letzten Jahren durch die Einrichtung von selbstständigen Professuren an vielen medizinführenden Universitäten verbreitert [REF], zunächst um allgemeinmedizinische Lehre an den Fakultäten sicherzustellen. Nun müssen dieser Akademisierung Strukturen folgen, die sowohl die Koordination, Organisation und Durchführung als auch die wissenschaftliche Aufarbeitung der Studien gewährleisten und den Praxen die Möglichkeit geben, versorgungsrelevante Forschungsfragen „aus der Praxis – für die Praxis“ zu generieren.Um eine entsprechende Forschungspraxeninfrastruktur in Deutschland systematisch zu unterstützen, fördert das Bundesministerium für Bildung und Forschung (BMBF) seit 2020 den Aufbau von 6 regionalen bzw. transregionalen allgemeinmedizinischen FPN und einer Koordinierungsstelle in Berlin1. Zusammen bilden sie die Initiative Deutscher Forschungspraxennetze (DESAM-ForNet2).Eines der geförderten FPN ist das „Forschungspraxennetz Allgemeinmedizin Dresden/Frankfurt am Main – SaxoForN“3, welches aus 2 lokalen FPN in Dresden/Sachsen (namens „SaxoN“) und Frankfurt am Main (namens „ForN“) besteht. In der Region Dresden (TU Dresden, Bereich Allgemeinmedizin) wird ein neues FPN aufgebaut (Ziel: 50 Praxen) und in der Region Frankfurt/Rhein-Main-Gebiet/Hessen (Goethe-Universität Frankfurt am Main, Institut für Allgemeinmedizin) ein bereits existierendes FPN ausgebaut (Ziel: 200 Praxen). SaxoForN deckt verschiedene Regionen (eine Metropolregion, städtische, ländliche, unterversorgte Regionen) in Deutschland ab und ermöglicht die Durchführung von klinischen Studien und Studien der Versorgungsforschung, die die besonderen Bedürfnisse von hausärztlichen Teams sowie Patient*innen widerspiegeln.Im vorliegenden Artikel wird das Konzept des allgemeinmedizinischen FPN Dresden/Frankfurt am Main – SaxoForN – vorgestellt. Dessen innovative Funktionsweise wird aufgezeigt und es werden einige zentrale Strukturen und deren Bedeutung erläutert.
Allergische Erkrankungen betreffen in Deutschland ca. 30 Mio. Menschen [REF]. Es ist von über 3 Mio. ärztlich diagnostizierten Asthmaerkrankungen auszugehen, für Heuschnupfen liegt die Zahl der laut Selbsteinschätzung Betroffenen bei über 12 Mio. (>20 % der erwachsenen Bevölkerung) [REF]. Allergische Erkrankungen verlaufen fast immer chronisch oder chronisch rezidivierend und schränken die Lebensqualität sowie die berufliche und schulische Leistungsfähigkeit der Betroffenen erheblich ein [REF]. Aktuelle Zahlen des Robert Koch-Instituts (RKI) und des Statistischen Bundesamtes (DESTATIS) [REF] zur Verbreitung von Allergien bei Erwachsenen, Kindern und Jugendlichen sind in Infobox 1 zusammengefasst.Nach aktuellen Zahlen des Robert Koch-Instituts (RKI) und des Statistischen Bundesamtes (DESTATIS) [REF] wird bei 30,0 % der 18- bis 79-jährigen Bevölkerung (36 % der Frauen und 24 % der Männer) in Deutschland mindestens eine allergische Erkrankung im Laufe des Lebens ärztlich diagnostiziert. Erfragt wurden dabei Asthma bronchiale, Heuschnupfen, Neurodermitis, Urtikaria (Nesselsucht), Kontaktekzeme, Nahrungsmittelallergien und Insektengiftallergien [REF]: Heuschnupfen und Asthma bronchiale waren bei Erwachsenen mit einer Lebenszeitprävalenz von 15,6 % bzw. 8,7 % am häufigsten; die dritthöchste Lebenszeitprävalenz war für das Kontaktekzem zu verzeichnen (8,6 %) [REF]. Weniger prävalent waren jemals gestellte Arztdiagnosen für Nahrungsmittelallergie (5,0 %), Neurodermitis (3,7 %), Urtikaria (3,6 %) und Insektengiftallergie (3,0 %) [REF]. Außer von Neurodermitis waren Frauen häufiger von allergischen Erkrankungen betroffen als Männer; besonders deutlich waren die Prävalenzunterschiede beim Kontaktekzem. Jüngere Erwachsene (bis 49 Jahre) sind häufiger von allergischen Erkrankungen betroffen als ältere [REF]. Die Häufigkeit allergischer Erkrankungen bei Erwachsenen bleibt in den letzten 10 Jahren auf hohem Niveau konstant, nur bei Asthma bronchiale gibt es Anzeichen für eine Zunahme [REF].Die Lebenszeitprävalenz von Kindern und Jugendlichen (im Alter von 0 bis 17 Jahren), an einer der 3 atopischen Erkrankungen (Asthma bronchiale, Heuschnupfen oder Neurodermitis) zu erkranken, lag bei 23 % in der Basiserhebung (2003–2006) der RKI-Studie zur Gesundheit von Kindern und Jugendlichen in Deutschland (KiGGS; [REF]). Die Verbreitung von Heuschnupfen, Asthma und Neurodermitis bei Kindern und Jugendlichen in Deutschland blieb zwischen der KiGGS-Basiserhebung (2003–2006) und KiGGS Welle 2 (2014–2017) weitgehend konstant [REF]. Als Lebenszeitprävalenzen von allergischen Erkrankungen (KiGGS Welle 2) wurden 6 % für Asthma bronchiale, 11 % für Heuschnupfen, 12,8 % für Neurodermitis und 2,8 % für allergisches Kontaktekzem festgestellt [REF]. In der Querschnittsuntersuchung (2014–2017; KiGGS Welle 2) lag der Anteil der Kinder und Jugendlichen in der Altersgruppe der 11- bis 17-Jährigen mit ärztlicher Heuschnupfen- oder Neurodermitisdiagnose, die nach einem positiven Allergietest eine Allergenimmuntherapie erhielten, bei 30,1 % [REF]. Für die allergenspezifische Immuntherapie (AIT) zur Kausalbehandlung von Allergien stehen in Deutschland und den Mitgliedsstaaten der Europäischen Union (EU) zugelassene oder verkehrsfähige Fertigarzneimittel für folgende Diagnosen (allergische Erkrankungen) zur Verfügung: Allergien gegen Inhalationsallergene (Rhinitis allergica, Conjunctivitis allergica, Asthma bronchiale) und Insektengiftallergien (Anaphylaxie) ([REF]; Tab. 1). Einige Produkte verfügen auch über die Indikation „Prävention von allergischem Asthma bronchiale“ oder „Prävention von Neusensibilisierung gegen weitere Allergene“. Verfahren zur EU-weiten Zulassung erster AIT-Produkte zur Behandlung von Nahrungsmittelallergien wurden initiiert und sind derzeit bei den europäischen Zulassungsbehörden in Prüfung. Der vorliegende Beitrag gibt einen aktuellen Überblick zu a) immunologischer Wirkweise, b) Indikationen und Kontraindikationen der AIT, c) regulatorischen Voraussetzungen in Deutschland und der EU sowie d) neuen Wirkstoffen und Adjuvanzien.
