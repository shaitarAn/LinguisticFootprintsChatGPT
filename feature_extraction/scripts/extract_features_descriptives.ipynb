{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import textdescriptives as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<textdescriptives.load_components.TextDescriptives at 0x7fe67ce58f40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe('textdescriptives/all')\n",
    "\n",
    "Dnlp = spacy.load('de_core_news_lg')\n",
    "Dnlp.add_pipe('textdescriptives/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = os.path.expanduser(f'~/switchdrive/IMAGINE_files/chatGPT/project_2/final_files_simple_prompt_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, lang):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        # replace multiple spaces with a single space\n",
    "        content = \" \".join(content.split())\n",
    "        # print(content)\n",
    "        if lang == 'en':\n",
    "            doc = nlp(content)\n",
    "        elif lang == 'de':\n",
    "            doc = Dnlp(content)\n",
    "        \n",
    "    return td.extract_dict(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files_in_directory(directory_path, output_csv_path, lang):\n",
    "    data = []\n",
    "    file_counter = 1\n",
    "    \n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            feature_values_list = process_file(file_path, lang)\n",
    "            \n",
    "            for feature_values in feature_values_list:\n",
    "                feature_values[\"file_id\"] = file_counter\n",
    "                data.append(feature_values)\n",
    "            file_counter += 1\n",
    "    \n",
    "    # Create a pandas DataFrame and write it to a CSV file\n",
    "    df = pd.DataFrame(data)\n",
    "    #  drop columns that have NaN\n",
    "    df.dropna(axis=1, inplace=True)\n",
    "    # drop columns that have only 0\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    \n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "GGPONC machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGPONC human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "zora_en machine\n",
      "zora_en human\n",
      "en\n",
      "e3c machine\n",
      "e3c human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "pubmed_de machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmed_de human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "pubmed_en machine\n",
      "pubmed_en human\n",
      "de\n",
      "zora_de machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zora_de human\n",
      "en\n",
      "cnn machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "20min machine\n",
      "20min human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gentextdetect/lib/python3.8/site-packages/textdescriptives/components/coherence.py:42: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  similarities.append(sent.similarity(sents[i + order]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "# itarate over subdirectories in directory_path\n",
    "for subdirectory in os.listdir(directory_path):\n",
    "    # itarate over files in subdirectory\n",
    "    corpus = subdirectory\n",
    "    if corpus in ['20min', 'GGPONC', \"pubmed_de\", \"zora_de\"]:\n",
    "        lang = 'de'\n",
    "    else:\n",
    "        lang = 'en'\n",
    "    print(lang)\n",
    "    if not os.path.isdir(os.path.join(directory_path, subdirectory)):\n",
    "            continue\n",
    "    for system in os.listdir(os.path.join(directory_path, subdirectory)):\n",
    "        # check if system is a directory\n",
    "        print(corpus, system)\n",
    "        output_csv_path = \"../output/\" + corpus + \"_\" + system + \".csv\"\n",
    "        input_path = os.path.join(directory_path, subdirectory, system)\n",
    "        process_text_files_in_directory(input_path, output_csv_path, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    # Load your labeled dataset containing the text samples and their corresponding classes\n",
    "    data = pd.read_csv(file_path, delimiter='\\t')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_text_with_spacy(text):    \n",
    "\n",
    "    doc = nlp(text)\n",
    "    return td.extract_dict(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_data(f'../data/subtask1/{language}/raw/test.tsv')\n",
    "dataseries = data['text'].apply(describe_text_with_spacy)\n",
    "ld = []\n",
    "for d in dataseries:\n",
    "    ld.append(d[0])\n",
    "df = pd.DataFrame(ld)\n",
    "\n",
    "#  merge df with data on \"text\" column\n",
    "final = pd.merge(data, df, on='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  print size of final\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  drop columns that have NaN\n",
    "final.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are all 0\n",
    "final.loc[:, (final != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final.shape)\n",
    "print(final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(f'../data/subtask1/{language}/output/{language}_test_textdescriptives.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gentextdetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3343d3ed4634bb56dee33b3bc6b87370f4a16b5de58281c959fb47e4e8603ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
